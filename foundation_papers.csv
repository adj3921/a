title,paperId,authors,year,venue,cited_survey_titles,cited_survey_count,abstract,citationCount,Calculation,includeFoundational
Concrete Problems in AI Safety,e86f71ca2948d17b003a5f068db1ecb2b77827f7,"[{'authorId': '2698777', 'name': 'Dario Amodei'}, {'authorId': '37232298', 'name': 'C. Olah'}, {'authorId': '5164568', 'name': 'J. Steinhardt'}, {'authorId': '145791315', 'name': 'P. Christiano'}, {'authorId': '47971768', 'name': 'J. Schulman'}, {'authorId': '30415265', 'name': 'Dandelion Mané'}]",2016,arXiv.org,"['AGI Safety Literature Review', 'Unsolved Problems in ML Safety', 'AI safety: state of the field through quantitative lens', 'AI Research Considerations for Human Existential Safety (ARCHES)']",4,"Rapid progress in machine learning and artificial intelligence (AI) has brought increasing attention to the potential impacts of AI technologies on society. In this paper we discuss one such potential impact: the problem of accidents in machine learning systems, defined as unintended and harmful behavior that may emerge from poor design of real-world AI systems. We present a list of five practical research problems related to accident risk, categorized according to whether the problem originates from having the wrong objective function (""avoiding side effects"" and ""avoiding reward hacking""), an objective function that is too expensive to evaluate frequently (""scalable supervision""), or undesirable behavior during the learning process (""safe exploration"" and ""distributional shift""). We review previous work in these areas as well as suggesting research directions with a focus on relevance to cutting-edge AI systems. Finally, we consider the high-level question of how to think most productively about the safety of forward-looking applications of AI.",1503.0,6012,checked
Research Priorities for Robust and Beneficial Artificial Intelligence,da08d9d5e0c12da55f3f86ef994759d6dd29f639,"[{'authorId': '145107462', 'name': 'Stuart J. Russell'}, {'authorId': '40829712', 'name': 'Dan Dewey'}, {'authorId': '2011933', 'name': 'Max Tegmark'}]",2015,The AI Magazine,"['AGI Safety Literature Review', 'Unsolved Problems in ML Safety', 'AI safety: state of the field through quantitative lens', 'AI Research Considerations for Human Existential Safety (ARCHES)']",4,"Success in the quest for artificial intelligence has the potential to bring unprecedented benefits to humanity, and it is therefore worthwhile to investigate how to maximize these benefits while avoiding potential pitfalls. This article gives numerous examples (which should by no means be construed as an exhaustive list) of such worthwhile research aimed at ensuring that AI remains robust and beneficial.",516.0,2064,checked
Speculations Concerning the First Ultraintelligent Machine,d7d9d643a378b6fd69fff63d113f4eae1983adc8,"[{'authorId': '145179124', 'name': 'I. Good'}]",1965,Advances in Computing,"['AGI Safety Literature Review', 'AI Research Considerations for Human Existential Safety (ARCHES)', 'Responses to catastrophic AGI risk: a survey', 'Superintelligence: Paths, Dangers, Strategies']",4,,470.0,1880,checked
Artificial Intelligence as a Positive and Negative Factor in Global Risk,fcf7368061a544a09d16826eb4c5a8463ee5482e,"[{'authorId': '2542795', 'name': 'Eliezer Yudkowsky'}]",2006,,"['AGI Safety Literature Review', 'AI Research Considerations for Human Existential Safety (ARCHES)', 'Responses to catastrophic AGI risk: a survey', 'Superintelligence: Paths, Dangers, Strategies']",4,"By far the greatest danger of Artificial Intelligence is that people conclude too early that they understand it. Of course this problem is not limited to the field of AI. Jacques Monod wrote: ""A curious aspect of the theory of evolution is that everybody thinks he understands it."" (Monod 1974.) My father, a physicist, complained about people making up their own theories of physics; he wanted to know why people did not make up their own theories of chemistry. (Answer: They do.) Nonetheless the problem seems to be unusually acute in Artificial Intelligence. The field of AI has a reputation for making huge promises and then failing to deliver on them. Most observers conclude that AI is hard; as indeed it is. But the embarrassment does not stem from the difficulty. It is difficult to build a star from hydrogen, but the field of stellar astronomy does not have a terrible reputation for promising to build stars and then failing. The critical inference is not that AI is hard, but that, for some reason, it is very easy for people to think they know far more about Artificial Intelligence than they actually do.",358.0,1432,checked
Alignment for Advanced Machine Learning Systems,7ac7b6dbcf5107c7ad0ce29161f60c2834a06795,"[{'authorId': '144364160', 'name': 'Jessica Taylor'}, {'authorId': '2542795', 'name': 'Eliezer Yudkowsky'}, {'authorId': '2254026', 'name': 'Patrick LaVictoire'}, {'authorId': '2651789', 'name': 'Andrew Critch'}]",2020,Ethics of Artificial Intelligence,"['AGI Safety Literature Review', 'Unsolved Problems in ML Safety', 'AI safety: state of the field through quantitative lens', 'AI Research Considerations for Human Existential Safety (ARCHES)']",4,"This chapter surveys eight research areas organized around one question: As learning systems become increasingly intelligent and autonomous, what design principles can best ensure that their behavior is aligned with the interests of the operators? The chapter focuses on two major technical obstacles to AI alignment: the challenge of specifying the right kind of objective functions and the challenge of designing AI systems that avoid unintended consequences and undesirable behavior even in cases where the objective function does not line up perfectly with the intentions of the designers. The questions surveyed include the following: How can we train reinforcement learners to take actions that are more amenable to meaningful assessment by intelligent overseers? What kinds of objective functions incentivize a system to “not have an overly large impact” or “not have many side effects”? The chapter discusses these questions, related work, and potential directions for future research, with the goal of highlighting relevant research topics in machine learning that appear tractable today.",62.0,248,checked
Why the future doesn’t need us,7189f82fb16b35ac630446179320b12d4a642345,"[{'authorId': '2142826362', 'name': 'Joy Bill'}]",2003,,"['AI Research Considerations for Human Existential Safety (ARCHES)', 'Responses to catastrophic AGI risk: a survey', 'Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",3,"Our most powerful 21st-century technologies robotics, genetic engineering, and nanotech are threatening to make humans an endangered species. From the moment I became involved in the creation of new technologies, their ethical dimensions have concerned me, but it was only in the autumn of 1998 that I became anxiously aware of how great are the dangers facing us in the 21st century. I can date the onset of my unease to the day I met Ray Kurzweil, the deservedly famous inventor of the first reading machine for the blind and many other amazing things.",845.0,2535,checked
"Superintelligence: Paths, Dangers, Strategies",7bba95b3d145564025e26b49ca67f13f884f8560,"[{'authorId': '2193691', 'name': 'N. Bostrom'}]",2014,,"['AGI Safety Literature Review', 'Superintelligence: Paths, Dangers, Strategies', 'Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",3,"The human brain has some capabilities that the brains of other animals lack. It is to these distinctive capabilities that our species owes its dominant position. Other animals have stronger muscles or sharper claws, but we have cleverer brains. If machine brains one day come to surpass human brains in general intelligence, then this new superintelligence could become very powerful. As the fate of the gorillas now depends more on us humans than on the gorillas themselves, so the fate of our species then would come to depend on the actions of the machine superintelligence. But we have one advantage: we get to make the first move. Will it be possible to construct a seed AI or otherwise to engineer initial conditions so as to make an intelligence explosion survivable? How could one achieve a controlled detonation? To get closer to an answer to this question, we must make our way through a fascinating landscape of topics and considerations. Read the book and learn about oracles, genies, singletons; about boxing methods, tripwires, and mind crime; about humanity's cosmic endowment and differential technological development; indirect normativity, instrumental convergence, whole brain emulation and technology couplings; Malthusian economics and dystopian evolution; artificial intelligence, and biological cognitive enhancement, and collective intelligence. This profoundly ambitious and original book picks its way carefully through a vast tract of forbiddingly difficult intellectual terrain. Yet the writing is so lucid that it somehow makes it all seem easy. After an utterly engrossing journey that takes us to the frontiers of thinking about the human condition and the future of intelligent life, we find in Nick Bostrom's work nothing less than a reconceptualization of the essential task of our time.",611.0,1833,checked
"The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation",a4d513cfc9d4902ef1a80198582f29b8ba46ac28,"[{'authorId': '35167962', 'name': 'Miles Brundage'}, {'authorId': '49344883', 'name': 'S. Avin'}, {'authorId': '2115193883', 'name': 'Jack Clark'}, {'authorId': '48625835', 'name': 'H. Toner'}, {'authorId': '2654106', 'name': 'P. Eckersley'}, {'authorId': '39928654', 'name': 'Ben Garfinkel'}, {'authorId': '3198576', 'name': 'A. Dafoe'}, {'authorId': '35681920', 'name': 'P. Scharre'}, {'authorId': '46225273', 'name': 'T. Zeitzoff'}, {'authorId': '7888676', 'name': 'Bobby Filar'}, {'authorId': '2639880', 'name': 'H. Anderson'}, {'authorId': '47075894', 'name': 'H. Roff'}, {'authorId': '2059529715', 'name': 'Gregory C. Allen'}, {'authorId': '5164568', 'name': 'J. Steinhardt'}, {'authorId': '152629250', 'name': 'Carrick Flynn'}, {'authorId': '35793299', 'name': 'Seán Ó hÉigeartaigh'}, {'authorId': '38992229', 'name': 'S. Beard'}, {'authorId': '36729401', 'name': 'Haydn Belfield'}, {'authorId': '33859827', 'name': 'Sebastian Farquhar'}, {'authorId': '39439114', 'name': 'Clare Lyle'}, {'authorId': '35431817', 'name': 'Rebecca Crootof'}, {'authorId': '47107786', 'name': 'Owain Evans'}, {'authorId': '2054564415', 'name': 'Michael Page'}, {'authorId': '2055073817', 'name': 'Joanna J. Bryson'}, {'authorId': '26336155', 'name': 'Roman V. Yampolskiy'}, {'authorId': '2698777', 'name': 'Dario Amodei'}]",2018,arXiv.org,"['AGI Safety Literature Review', 'Unsolved Problems in ML Safety', 'X-Risk Analysis for AI Research']",3,"The following organisations are named on the report: Future of Humanity Institute, University of Oxford, Centre for the Study of Existential Risk, University of Cambridge, Center for a New American Security, Electronic Frontier Foundation, OpenAI. The Future of Life Institute is acknowledged as a funder.",452.0,1356,checked
Cooperative Inverse Reinforcement Learning,1e6abd43fcb157fde4d4ddc3ac8787ae45dbf777,"[{'authorId': '1397904824', 'name': 'Dylan Hadfield-Menell'}, {'authorId': '145107462', 'name': 'Stuart J. Russell'}, {'authorId': '1689992', 'name': 'P. Abbeel'}, {'authorId': '2745001', 'name': 'A. Dragan'}]",2016,NIPS,"['AGI Safety Literature Review', 'Unsolved Problems in ML Safety', 'AI Research Considerations for Human Existential Safety (ARCHES)']",3,"For an autonomous system to be helpful to humans and to pose no unwarranted risks, it needs to align its values with those of the humans in its environment in such a way that its actions contribute to the maximization of value for the humans. We propose a formal definition of the value alignment problem as cooperative inverse reinforcement learning (CIRL). A CIRL problem is a cooperative, partial-information game with two agents, human and robot; both are rewarded according to the human's reward function, but the robot does not initially know what this is. In contrast to classical IRL, where the human is assumed to act optimally in isolation, optimal CIRL solutions produce behaviors such as active teaching, active learning, and communicative actions that are more effective in achieving value alignment. We show that computing optimal joint policies in CIRL games can be reduced to solving a POMDP, prove that optimality in isolation is suboptimal in CIRL, and derive an approximate CIRL algorithm.",449.0,1347,checked
When Will AI Exceed Human Performance? Evidence from AI Experts,6b93cedfe768eb8b5ece92612aac9cc8e986d12a,"[{'authorId': '40582697', 'name': 'K. Grace'}, {'authorId': '3373139', 'name': 'J. Salvatier'}, {'authorId': '3198576', 'name': 'A. Dafoe'}, {'authorId': '1406217399', 'name': 'Baobao Zhang'}, {'authorId': '47107786', 'name': 'Owain Evans'}]",2017,Journal of Artificial Intelligence Research,"['AGI Safety Literature Review', 'AI Research Considerations for Human Existential Safety (ARCHES)', 'X-Risk Analysis for AI Research']",3,"
 
 
 
Advances in artificial intelligence (AI) will transform modern life by reshaping transportation, health, science, finance, and the military. To adapt public policy, we need to better anticipate these advances. Here we report the results from a large survey of machine learning researchers on their beliefs about progress in AI. Researchers predict AI will outperform humans in many activities in the next ten years, such as translating languages (by 2024), writing high-school essays (by 2026), driving a truck (by 2027), working in retail (by 2031), writing a bestselling book (by 2049), and working as a surgeon (by 2053). Researchers believe there is a 50% chance of AI outperforming humans in all tasks in 45 years and of automating all human jobs in 120 years, with Asian respondents expecting these dates much sooner than North Americans. These results will inform discussion amongst researchers and policymakers about anticipating and managing trends in AI. 
 
 
This article is part of the special track on AI and Society. 
 
 
 
 
 
",446.0,1338,checked
The coming technological singularity: How to survive in the post-human era,3ab2d953596fea9131e622410c64b3d114a84f0c,"[{'authorId': '2012264', 'name': 'V. Vinge'}]",1993,,"['AGI Safety Literature Review', 'Responses to catastrophic AGI risk: a survey', 'Superintelligence: Paths, Dangers, Strategies']",3,"The acceleration of technological progress has been the central feature of this century. I argue in this paper that we are on the edge of change comparable to the rise of human life on Earth. The precise cause of this change is the imminent creation by technology of entities with greater than human intelligence. There are several means by which science may achieve this breakthrough (and this is another reason for having confidence that the event will occur): (1) the development of computers that are 'awake' and superhumanly intelligent (to date, most controversy in the area of AI relates to whether we can create human equivalence in a machine. But if the answer is 'yes, we can', then there is little doubt that beings more intelligent can be constructed shortly thereafter); (2) large computer networks (and their associated users) may 'wake up' as a superhumanly intelligent entity; (3) computer/human interfaces may become so intimate that users may reasonably be considered superhumanly intelligent; and (4) biological science may find ways to improve upon the natural human intellect. The first three possibilities depend in large part on improvements in computer hardware. Progress in computer hardware has followed an amazingly steady curve in the last few decades. Based largely on this trend, I believe that the creation of greater than human intelligence will occur during the next thirty years.",425.0,1275,checked
Universal Artificial Intelligence,30a64bdf778b8f561af9ae589e822c2c800920b1,"[{'authorId': '1406347404', 'name': 'Dr. Marcus Hutter'}]",2004,Texts in Theoretical Computer Science. An EATCS Series,"['AGI Safety Literature Review', 'AI Research Considerations for Human Existential Safety (ARCHES)', 'Superintelligence: Paths, Dangers, Strategies']",3,,240.0,720,checked
Some Moral and Technical Consequences of Automation.,8a403ab09db56252fb538dfd95509472661c6eb6,"[{'authorId': '145707626', 'name': 'N. Wiener'}]",1960,Science,"['AI Research Considerations for Human Existential Safety (ARCHES)', 'Responses to catastrophic AGI risk: a survey', 'Superintelligence: Paths, Dangers, Strategies']",3,,170.0,510,checked
AI Safety Gridworlds,d09bec5af4eef5038e48b26b6c14098f95997114,"[{'authorId': '2990741', 'name': 'J. Leike'}, {'authorId': '26890260', 'name': 'Miljan Martic'}, {'authorId': '2578985', 'name': 'Victoria Krakovna'}, {'authorId': '145981974', 'name': 'Pedro A. Ortega'}, {'authorId': '1868196', 'name': 'Tom Everitt'}, {'authorId': '8455031', 'name': 'Andrew Lefrancq'}, {'authorId': '1749270', 'name': 'Laurent Orseau'}, {'authorId': '34313265', 'name': 'S. Legg'}]",2017,arXiv.org,"['AGI Safety Literature Review', 'AI safety: state of the field through quantitative lens', 'AI Research Considerations for Human Existential Safety (ARCHES)']",3,"We present a suite of reinforcement learning environments illustrating various safety properties of intelligent agents. These problems include safe interruptibility, avoiding side effects, absent supervisor, reward gaming, safe exploration, as well as robustness to self-modification, distributional shift, and adversaries. To measure compliance with the intended safe behavior, we equip each environment with a performance function that is hidden from the agent. This allows us to categorize AI safety problems into robustness and specification problems, depending on whether the performance function corresponds to the observed reward function. We evaluate A2C and Rainbow, two recent deep reinforcement learning agents, on our environments and show that they are not able to solve them satisfactorily.",163.0,489,checked
Scalable agent alignment via reward modeling: a research direction,c6f913e4baa7f2c85363c0625c87003ad3b3a14c,"[{'authorId': '2990741', 'name': 'J. Leike'}, {'authorId': '145055042', 'name': 'David Krueger'}, {'authorId': '1868196', 'name': 'Tom Everitt'}, {'authorId': '26890260', 'name': 'Miljan Martic'}, {'authorId': '51965508', 'name': 'Vishal Maini'}, {'authorId': '34313265', 'name': 'S. Legg'}]",2018,arXiv.org,"['Unsolved Problems in ML Safety', 'AI safety: state of the field through quantitative lens', 'AI Research Considerations for Human Existential Safety (ARCHES)']",3,"One obstacle to applying reinforcement learning algorithms to real-world problems is the lack of suitable reward functions. Designing such reward functions is difficult in part because the user only has an implicit understanding of the task objective. This gives rise to the agent alignment problem: how do we create agents that behave in accordance with the user's intentions? We outline a high-level research direction to solve the agent alignment problem centered around reward modeling: learning a reward function from interaction with the user and optimizing the learned reward function with reinforcement learning. We discuss the key challenges we expect to face when scaling reward modeling to complex and general domains, concrete approaches to mitigate these challenges, and ways to establish trust in the resulting agents.",144.0,432,checked
Responses to catastrophic AGI risk: a survey,fb6eefc6a42fbbf286476e435d87eacb99a9a047,"[{'authorId': '2821562', 'name': 'Kaj Sotala'}, {'authorId': '1976753', 'name': 'Roman V Yampolskiy'}]",2014,,"['AGI Safety Literature Review', 'AI safety: state of the field through quantitative lens', 'Responses to catastrophic AGI risk: a survey']",3,"Many researchers have argued that humanity will create artificial general intelligence (AGI) within the next twenty to one hundred years. It has been suggested that AGI may inflict serious damage to human well-being on a global scale (‘catastrophic risk’). After summarizing the arguments for why AGI may pose such a risk, we review the fieldʼs proposed responses to AGI risk. We consider societal proposals, proposals for external constraints on AGI behaviors and proposals for creating AGIs that are safe due to their internal design.",103.0,309,checked
The Off-Switch Game,808dec0828a74fecab07a497c10cd93e3748a5e2,"[{'authorId': '1397904824', 'name': 'Dylan Hadfield-Menell'}, {'authorId': '2745001', 'name': 'A. Dragan'}, {'authorId': '1689992', 'name': 'P. Abbeel'}, {'authorId': '145107462', 'name': 'Stuart J. Russell'}]",2016,International Joint Conference on Artificial Intelligence,"['Unsolved Problems in ML Safety', 'AI Research Considerations for Human Existential Safety (ARCHES)', 'X-Risk Analysis for AI Research']",3,"It is clear that one of the primary tools we can use to mitigate the potential risk from a misbehaving AI system is the ability to turn the system off. As the capabilities of AI systems improve, it is important to ensure that such systems do not adopt subgoals that prevent a human from switching them off. This is a challenge because many formulations of rational agents create strong incentives for self-preservation. This is not caused by a built-in instinct, but because a rational agent will maximize expected utility and cannot achieve whatever objective it has been given if it is dead. Our goal is to study the incentives an agent has to allow itself to be switched off. We analyze a simple game between a human H and a robot R, where H can press R's off switch but R can disable the off switch. A traditional agent takes its reward function for granted: we show that such agents have an incentive to disable the off switch, except in the special case where H is perfectly rational. Our key insight is that for R to want to preserve its off switch, it needs to be uncertain about the utility associated with the outcome, and to treat H's actions as important observations about that utility. (R also has no incentive to switch itself off in this setting.) We conclude that giving machines an appropriate level of uncertainty about their objectives leads to safer designs, and we argue that this setting is a useful generalization of the classical AI paradigm of rational agents.",87.0,261,checked
The Vulnerable World Hypothesis,41918a9ed66135691d741d0285c5ef24ea9597ff,"[{'authorId': '2193691', 'name': 'N. Bostrom'}]",2019,Global Policy,"['Unsolved Problems in ML Safety', 'AI Research Considerations for Human Existential Safety (ARCHES)', 'X-Risk Analysis for AI Research']",3,"Scientific and technological progress might change people’s capabilities or incentives in ways that would destabilize civilization. For example, advances in DIY biohacking tools might make it easy for anybody with basic training in biology to kill millions; novel military technologies could trigger arms races in which whoever strikes first has a decisive advantage; or some economically advantageous process may be invented that produces disastrous negative global externalities that are hard to regulate. This paper introduces the concept of a vulnerable world: roughly, one in which there is some level of technological development at which civilization almost certainly gets devastated by default, i.e. unless it has exited the ‘semi-anarchic default condition’. Several counterfactual historical and speculative future vulnerabilities are analyzed and arranged into a typology. A general ability to stabilize a vulnerable world would require greatly amplified capacities for preventive policing and global governance. The vulnerable world hypothesis thus offers a new perspective from which to evaluate the risk-benefit balance of developments towards ubiquitous surveillance or a unipolar world order. Policy Implications • Technology policy should not unquestioningly assume that all technological progress is beneficial, or that complete scientific openness is always best, or that the world has the capacity to manage any potential downside of a technology after it is invented. • Some areas, such as synthetic biology, could produce a discovery that suddenly democratizes mass destruction, e.g. by empowering individuals to kill hundreds of millions of people using readily available materials. In order for civilization to have a general capacity to deal with “black ball” inventions of this type, it would need a system of ubiquitous real-time worldwide surveillance. In some scenarios, such a system would need to be in place before the technology is invented. • Partial protection against a limited set of possible black balls is obtainable through more targeted interventions. For example, biorisk might be mitigated by means of background checks and monitoring of personnel in some types of biolab, by discouraging DIY biohacking (e.g. through licencing requirements), and by restructuring the biotech sector to limit access to some cutting-edge instrumentation and information. Rather than allow anybody to buy their own DNA synthesis machine, DNA synthesis could be provided as a service by a small number of closely monitored providers. • Another, subtler, type of black ball would be one that strengthens incentives for harmful use—e.g. a military technology that makes wars more destructive while giving a greater advantage to the side that strikes first. Like a squirrel who uses the times of plenty to store up nuts for the winter, we should use times of relative peace to build stronger mechanisms for resolving international disputes. Is there a black ball in the urn of possible inventions? One way of looking at human creativity is as a process of pulling balls out of a giant urn. The balls represent possible ideas, discoveries, technological inventions. Over the course of history, we have extracted a great many balls – mostly white (beneficial) but also various shades of gray (moderately harmful ones and mixed blessings). The cumulative effect on the human condition has so far been overwhelmingly positive, and may be much better still in the future (Bostrom, 2008). The global population has grown about three orders of magnitude over the last ten thousand years, and in the last two centuries per capita income, standards of living, and life expectancy have also risen. What we haven’t extracted, so far, is a black ball: a technology that invariably or by default destroys the civilization that invents it. The reason is not that we have been particularly careful or wise in our technology policy. We have just been lucky. It does not appear that any human civilization has been destroyed – as opposed to transformed – by its own inventions. We do have examples of civilizations being destroyed by inventions made elsewhere. For example, the European inventions that enabled transoceanic travel and force projection could be regarded as a black-ball event for the indigenous populations of the Americas, Australia, Tasmania, and some other places. The extinction of archaic hominid populations, such as the Neanderthals and the Denisovans, was probably facilitated by the technological superiority of Homo sapiens. But thus far, it seems, we have seen no sufficiently auto-destructive invention to count as a black ball for humanity. What if there is a black ball in the urn? If scientific and technological research continues, we will eventually reach it and pull it out. Our civilization has a considerable ability to Global Policy (2019) 10:4 doi: 10.1111/1758-5899.12718 © 2019 The Authors. Global Policy published by Durham University and John Wiley & Sons Ltd. This is an open access article under the terms of the Creative Commons Attribution-NonCommercial License, which permits use, distribution and reproduction in any medium, provided the original work is properly cited and is not used for commercial purposes. Global Policy Volume 10 . Issue 4 . November 2019 455",80.0,240,checked
AI safety via debate,5a5a1d666e4b7b933bc5aafbbadf179bc447ee67,"[{'authorId': '2060655766', 'name': 'G. Irving'}, {'authorId': '145791315', 'name': 'P. Christiano'}, {'authorId': '2698777', 'name': 'Dario Amodei'}]",2018,arXiv.org,"['AGI Safety Literature Review', 'Unsolved Problems in ML Safety', 'AI safety: state of the field through quantitative lens']",3,"To make AI systems broadly useful for challenging real-world tasks, we need them to learn complex human goals and preferences. One approach to specifying complex goals asks humans to judge during training which agent behaviors are safe and useful, but this approach can fail if the task is too complicated for a human to directly judge. To help address this concern, we propose training agents via self play on a zero sum debate game. Given a question or proposed action, two agents take turns making short statements up to a limit, then a human judges which of the agents gave the most true, useful information. In an analogy to complexity theory, debate with optimal play can answer any question in PSPACE given polynomial time judges (direct judging answers only NP questions). In practice, whether debate works involves empirical questions about humans and the tasks we want AIs to perform, plus theoretical questions about the meaning of AI alignment. We report results on an initial MNIST experiment where agents compete to convince a sparse classifier, boosting the classifier's accuracy from 59.4% to 88.9% given 6 pixels and from 48.2% to 85.2% given 4 pixels. Finally, we discuss theoretical and practical aspects of the debate model, focusing on potential weaknesses as the model scales up, and we propose future human and computer experiments to test these properties.",72.0,216,checked
Risks from Learned Optimization in Advanced Machine Learning Systems,7ee12d3bf8e0ce20d281b4550e39a1ee53839452,"[{'authorId': '146614650', 'name': 'Evan Hubinger'}, {'authorId': '146400709', 'name': 'Chris van Merwijk'}, {'authorId': '148305440', 'name': 'Vladimir Mikulik'}, {'authorId': '147156275', 'name': 'Joar Skalse'}, {'authorId': '1740494', 'name': 'Scott Garrabrant'}]",2019,arXiv.org,"['Unsolved Problems in ML Safety', 'AI safety: state of the field through quantitative lens', 'X-Risk Analysis for AI Research']",3,"We analyze the type of learned optimization that occurs when a learned model (such as a neural network) is itself an optimizer - a situation we refer to as mesa-optimization, a neologism we introduce in this paper. We believe that the possibility of mesa-optimization raises two important questions for the safety and transparency of advanced machine learning systems. First, under what circumstances will learned models be optimizers, including when they should not be? Second, when a learned model is an optimizer, what will its objective be - how will it differ from the loss function it was trained under - and how can it be aligned? In this paper, we provide an in-depth analysis of these two primary questions and provide an overview of topics for future research.",48.0,144,checked
Omohundro ’ s “ Basic AI Drives ” and Catastrophic Risks,738db99fd925069c988f9bd1df52446309d5e9c4,"[{'authorId': '3389522', 'name': 'Carl Shulman'}]",2013,,"['AI Research Considerations for Human Existential Safety (ARCHES)', 'Responses to catastrophic AGI risk: a survey', 'Superintelligence: Paths, Dangers, Strategies']",3,"Shulman, Carl. 2010. Omohundro's "" Basic AI Drives "" and Catastrophic Risks.",19.0,57,checked
The Superintelligent Will: Motivation and Instrumental Rationality in Advanced Artificial Agents,6c25aae58187f716d1b6db34200bbf3b63007aeb,"[{'authorId': '2193691', 'name': 'N. Bostrom'}]",2012,Minds and Machines,"['AI Research Considerations for Human Existential Safety (ARCHES)', 'Responses to catastrophic AGI risk: a survey', 'Superintelligence: Paths, Dangers, Strategies']",3,,13.0,39,checked
"Superintelligence: Paths, Dangers, Strategies",e0fe9b2f77288bc5e6f778611a49e62e98231f8c,"[{'authorId': '145155783', 'name': 'C. Robert'}]",2017,,"['AGI Safety Literature Review', 'AI Research Considerations for Human Existential Safety (ARCHES)']",2,,707.0,1414,checked
"Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy",6a132499ca1dc0d23cfe7d5db841b819df63b51b,"[{'authorId': '1724490', 'name': 'Yogesh Kumar Dwivedi'}, {'authorId': '2066599148', 'name': 'Laurie Hughes'}, {'authorId': '3446873', 'name': 'Elvira Ismagilova'}, {'authorId': '91018104', 'name': 'G. Aarts'}, {'authorId': '2235927', 'name': 'C. Coombs'}, {'authorId': '2285182', 'name': 'Tom Crick'}, {'authorId': '144521858', 'name': 'Y. Duan'}, {'authorId': '1410968693', 'name': 'R. Dwivedi'}, {'authorId': '143698898', 'name': 'J. Edwards'}, {'authorId': '121714298', 'name': 'Aled Eirug'}, {'authorId': '26898704', 'name': 'Vassilis Galanos'}, {'authorId': '2937059', 'name': 'P. V. Ilavarasan'}, {'authorId': '50817520', 'name': 'M. Janssen'}, {'authorId': '2168272578', 'name': 'Paul Jones'}, {'authorId': '2733956', 'name': 'A. Kar'}, {'authorId': '41074539', 'name': 'Hatice Kizgin'}, {'authorId': '1418648774', 'name': 'Bianca Kronemann'}, {'authorId': '1696747', 'name': 'Banita Lal'}, {'authorId': '3094343', 'name': 'B. Lucini'}, {'authorId': '2575165', 'name': 'R. Medaglia'}, {'authorId': '1404797274', 'name': 'K. L. Meunier-FitzHugh'}, {'authorId': '1415234363', 'name': 'L. L. Meunier-FitzHugh'}, {'authorId': '47033159', 'name': 'S. Misra'}, {'authorId': '52563419', 'name': 'Emmanuel Mogaji'}, {'authorId': '2038468629', 'name': 'S. Sharma'}, {'authorId': '153278840', 'name': 'Jang B. Singh'}, {'authorId': '35098701', 'name': 'Vishnupriya Raghavan'}, {'authorId': '2775338', 'name': 'R. Raman'}, {'authorId': '1688149', 'name': 'N. Rana'}, {'authorId': '2032434', 'name': 'Spyridon Samothrakis'}, {'authorId': '66781543', 'name': 'Jak Spencer'}, {'authorId': '2191954', 'name': 'K. Tamilmani'}, {'authorId': '102927759', 'name': 'Annie Tubadji'}, {'authorId': '144340154', 'name': 'P. Walton'}, {'authorId': '2116399602', 'name': 'Michael D. Williams'}]",2019,International Journal of Information Management,"['AI safety: state of the field through quantitative lens', 'Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",2,,670.0,1340,checked
Global Catastrophic Risks,7a530509b53bf0374d7d04bb99ba4ea831e33ebc,"[{'authorId': '2193691', 'name': 'N. Bostrom'}, {'authorId': '2547770', 'name': 'M. Ćirković'}]",2008,,"['Responses to catastrophic AGI risk: a survey', 'X-Risk Analysis for AI Research']",2,"Acknowledgements Foreword Introduction I BACKGROUND Long-term astrophysical processes Evolution theory and the future of humanity Millenial tendencies in responses to apocalyptic threats Cognitive biases potentially affecting judgement of global risks Observation selection effects and global catastrophic risks Systems-based risk analysis Catastrophes and insurance Public policy towards catastrophe II RISKS FROM NATURE Super-volcanism and other geophysical processes of catastrophic import Hazards from comets and asteroids Influence of Supernovae, gamma-ray bursts, solar flares, and cosmic rays on the terrestrial environment III RISKS FROM UNINTENDED CONSEQUENCES Climate change and global risk Plagues and pandemics: past, present, and future Artificial Intelligence as a positive and negative factor in global risk Big troubles, imagined and real IV RISKS FROM HOSTILE ACTS Catastrophe, social collapse, and and human extinction The continuing threat of nuclear war Catastrophic nuclear terrorism: a preventable peril Biotechnology and biosecurity Nanotechnology as global catastrophic risk The totalitarian threat Author's biographies Index",346.0,692,checked
Existential Risk Prevention as Global Priority,ced289065723368bca48636edf71eeed50f40a39,"[{'authorId': '2193691', 'name': 'N. Bostrom'}]",2013,,"['AGI Safety Literature Review', 'AI Research Considerations for Human Existential Safety (ARCHES)']",2,"risks are those that threaten the entire future of humanity. Many theories of value imply that even relatively small reductions in net existential risk have enormous expected value. Despite their importance, issues surrounding human-extinction risks and related hazards remain poorly understood. In this article, I clarify the concept of existential risk and develop an improved classification scheme. I discuss the relation between existential risks and basic issues in axiology, and show how existential risk reduction (via the maxipok rule) can serve as a strongly action-guiding principle for utilitarian concerns. I also show how the notion of existential risk suggests a new way of thinking about the ideal of sustainability. Policy Implications • Existential risk is a concept that can focus long-term global efforts and sustainability concerns. • The biggest existential risks are anthropogenic and related to potential future technologies. • A moral case can be made that existential risk reduction is strictly more important than any other global public good. • Sustainability should be reconceptualised in dynamic terms, as aiming for a sustainable trajectory rather than a sus- tainable state. • Some small existential risks can be mitigated today directly (e.g. asteroids) or indirectly (by building resilience and reserves to increase survivability in a range of extreme scenarios) but it is more important to build capacity to improve humanity's ability to deal with the larger existential risks that will arise later in this century. This will require collective wisdom, technology foresight, and the ability when necessary to mobilise a strong global coordi- nated response to anticipated existential risks. • Perhaps the most cost-effective way to reduce existential risks today is to fund analysis of a wide range of existen- tial risks and potential mitigation strategies, with a long-term perspective.",323.0,646,checked
Thinking Inside the Box: Controlling and Using an Oracle AI,6d78d67d4f7f5fe2e66933778ab1faf119d21547,"[{'authorId': '2054678912', 'name': 'S. Armstrong'}, {'authorId': '144816231', 'name': 'A. Sandberg'}, {'authorId': '2193691', 'name': 'N. Bostrom'}]",2012,Minds and Machines,"['AGI Safety Literature Review', 'Responses to catastrophic AGI risk: a survey']",2,,105.0,210,checked
Aligning AI With Shared Human Values,65906e6027246ae9e4ecd18d6e019a24505c842e,"[{'authorId': '3422872', 'name': 'Dan Hendrycks'}, {'authorId': '90909974', 'name': 'Collin Burns'}, {'authorId': '104444594', 'name': 'Steven Basart'}, {'authorId': '2651789', 'name': 'Andrew Critch'}, {'authorId': '46276596', 'name': 'J. Li'}, {'authorId': '143711382', 'name': 'D. Song'}, {'authorId': '5164568', 'name': 'J. Steinhardt'}]",2020,International Conference on Learning Representations,"['Unsolved Problems in ML Safety', 'X-Risk Analysis for AI Research']",2,"We show how to assess a language model's knowledge of basic concepts of morality. We introduce the ETHICS dataset, a new benchmark that spans concepts in justice, well-being, duties, virtues, and commonsense morality. Models predict widespread moral judgments about diverse text scenarios. This requires connecting physical and social world knowledge to value judgements, a capability that may enable us to steer chatbot outputs or eventually regularize open-ended reinforcement learning agents. With the ETHICS dataset, we find that current language models have a promising but incomplete understanding of basic ethical knowledge. Our work shows that progress can be made on machine ethics today, and it provides a steppingstone toward AI that is aligned with human values.",97.0,194,checked
Unsolved Problems in ML Safety,05c2e1ee203be217f100d2da05bdcc52004f00b6,"[{'authorId': '3422872', 'externalIds': {'DBLP': ['Dan Hendrycks']}, 'url': 'https://www.semanticscholar.org/author/3422872', 'name': 'Dan Hendrycks', 'aliases': None, 'affiliations': ['UC Berkeley'], 'homepage': 'danhendrycks.com', 'paperCount': 49, 'citationCount': 11806, 'hIndex': 28}, {'authorId': '2483738', 'externalIds': {'DBLP': ['Nicholas Carlini']}, 'url': 'https://www.semanticscholar.org/author/2483738', 'name': 'Nicholas Carlini', 'aliases': None, 'affiliations': [], 'homepage': None, 'paperCount': 101, 'citationCount': 22448, 'hIndex': 40}, {'authorId': '47971768', 'externalIds': {'DBLP': ['John D. Schulman', 'John Schulman']}, 'url': 'https://www.semanticscholar.org/author/47971768', 'name': 'J. Schulman', 'aliases': ['J Schulman', 'John D. Schulman', 'John Schulman'], 'affiliations': [], 'homepage': None, 'paperCount': 68, 'citationCount': 38493, 'hIndex': 41}, {'authorId': '5164568', 'externalIds': {'DBLP': ['Jacob Steinhardt']}, 'url': 'https://www.semanticscholar.org/author/5164568', 'name': 'J. Steinhardt', 'aliases': ['J. Steinhardt', 'Jacob Steinhardt'], 'affiliations': [], 'homepage': None, 'paperCount': 98, 'citationCount': 7795, 'hIndex': 33}]",2021,arXiv.org,"['Unsolved Problems in ML Safety', 'X-Risk Analysis for AI Research']",2,"Machine learning (ML) systems are rapidly increasing in size, are acquiring new capabilities, and are increasingly deployed in high-stakes settings. As with other powerful technologies, safety for ML should be a leading research priority. In response to emerging safety challenges in ML, such as those introduced by recent large-scale models, we provide a new roadmap for ML Safety and refine the technical problems that the field needs to address. We present four problems ready for research, namely withstanding hazards (""Robustness""), identifying hazards (""Monitoring""), reducing inherent model hazards (""Alignment""), and reducing systemic hazards (""Systemic Safety""). Throughout, we clarify each problem's motivation and provide concrete research directions.",88.0,176,checked
AGI Safety Literature Review,6b0b1ecca32809b1d5b70b978521a8c27bc98e25,"[{'authorId': '1868196', 'externalIds': {'DBLP': ['Tom Everitt'], 'ORCID': '0000-0003-1210-9866'}, 'url': 'https://www.semanticscholar.org/author/1868196', 'name': 'Tom Everitt', 'aliases': None, 'affiliations': ['DeepMind'], 'homepage': 'http://tomeveritt.se', 'paperCount': 40, 'citationCount': 955, 'hIndex': 14}, {'authorId': '1954561', 'externalIds': {'DBLP': ['Gary Lea']}, 'url': 'https://www.semanticscholar.org/author/1954561', 'name': 'G. Lea', 'aliases': ['G. Lea', 'Gary R Lea', 'Gary R. Lea', 'Gary Lea'], 'affiliations': [], 'homepage': None, 'paperCount': 15, 'citationCount': 168, 'hIndex': 4}, {'authorId': '144154444', 'externalIds': {'DBLP': ['Marcus Hutter'], 'ORCID': '0000-0002-3263-4097'}, 'url': 'https://www.semanticscholar.org/author/144154444', 'name': 'Marcus Hutter', 'aliases': ['M. Hutter'], 'affiliations': [], 'homepage': 'http://www.hutter1.net/', 'paperCount': 300, 'citationCount': 7548, 'hIndex': 39}]",2018,International Joint Conference on Artificial Intelligence,"['AGI Safety Literature Review', 'AI safety: state of the field through quantitative lens']",2,"The development of Artificial General Intelligence (AGI) promises to be a major event. Along with its many potential benefits, it also raises serious safety concerns. The intention of this paper is to provide an easily accessible and up-to-date collection of references for the emerging field of AGI safety. A significant number of safety problems for AGI have been identified. We list these, and survey recent research on solving them. We also cover works on how best to think of AGI from the limited knowledge we have today, predictions for when AGI will first be created, and what will happen after its creation. Finally, we review the current public policy on AGI.",79.0,158,checked
Aligning Superintelligence with Human Interests: A Technical Research Agenda,d8033a314493c8df3791912272ac4b58d3a7b8c2,"[{'authorId': '1719968', 'name': 'N. Soares'}, {'authorId': '1825635', 'name': 'Benja Fallenstein'}]",2015,,"['AGI Safety Literature Review', 'AI Research Considerations for Human Existential Safety (ARCHES)']",2,"The property that has given humans a dominant advantage over other species is not strength or speed, but intelligence. If progress in artificial intelligence continues unabated, AI systems will eventually exceed humans in general reasoning ability. A system that is “superintelligent” in the sense of being “smarter than the best human brains in practically every field” could have an enormous impact upon humanity (Bostrom 2014). Just as human intelligence has allowed us to develop tools and strategies for controlling our environment, a superintelligent system would likely be capable of developing its own tools and strategies for exerting control (Muehlhauser and Salamon 2012). In light of this potential, it is essential to use caution when developing AI systems that can exceed human levels of general intelligence, or that can facilitate the creation of such systems.",77.0,154,checked
"Delusion, Survival, and Intelligent Agents",d8a200f7e0ef5659d544b4f5306251deaf781d1b,"[{'authorId': '11023955', 'name': 'Mark B. Ring'}, {'authorId': '1749270', 'name': 'Laurent Orseau'}]",2011,Artificial General Intelligence,"['AGI Safety Literature Review', 'Responses to catastrophic AGI risk: a survey']",2,,76.0,152,checked
Safety Engineering for Artificial General Intelligence,2b1f2b906c23444e38e95d40779377db6ac51bb6,"[{'authorId': '1976753', 'name': 'Roman V Yampolskiy'}, {'authorId': '49824819', 'name': 'Joshua Fox'}]",2012,,"['AI safety: state of the field through quantitative lens', 'Responses to catastrophic AGI risk: a survey']",2,,68.0,136,checked
Intelligence Explosion: Evidence and Import,40e3086dd279fd23042b37962d8e67f5fd9801ae,"[{'authorId': '2138143', 'name': 'Luke Muehlhauser'}, {'authorId': '144958976', 'name': 'A. Salamon'}]",2012,,"['Responses to catastrophic AGI risk: a survey', 'Superintelligence: Paths, Dangers, Strategies']",2,,66.0,132,checked
The Nature of Self-Improving Artificial Intelligence,4618cbdfd7dada7f61b706e4397d4e5952b5c9a0,"[{'authorId': '1808760', 'name': 'S. Omohundro'}]",2008,,"['AGI Safety Literature Review', 'Responses to catastrophic AGI risk: a survey']",2,"An electronic still camera indicates the available capacity of a recording medium and the number of frames remaining that may be photographed, as well as warning by stages in at least two forms, based on the available capacity of the recording medium. The electronic still camera includes a photo lens, mirror, CCD, signal executing circuit, A/D converter, buffer memory, compression circuit, CPU, memory card, and indicator device. A CPU reads in the gross capacity and the written-on portion from a memory card and calculates the available capacity. When the available capacity becomes small, the indicator device that indicates the available capacity or the number of frames remaining that can be photographed begins to flash the display slowly on and off. When the available capacity grows even smaller and is almost gone, the display flashes rapidly on and off. In this way, the photographer can easily detect how many more frames he or she may photograph on the memory card.",59.0,118,checked
How long until human-level AI? Results from an expert assessment,17863bc49f8337b1646ad203ab12d716b8e90e48,"[{'authorId': '2097800', 'name': 'S. Baum'}, {'authorId': '1738080', 'name': 'B. Goertzel'}, {'authorId': '49961666', 'name': 'T. Goertzel'}]",2011,,"['AGI Safety Literature Review', 'Responses to catastrophic AGI risk: a survey']",2,,54.0,108,checked
The Singularity and Machine Ethics,63cd604886aa453ce626626d97959e378ccdc788,"[{'authorId': '2138143', 'name': 'Luke Muehlhauser'}, {'authorId': '102699296', 'name': 'Louie Helm'}]",2012,,"['Responses to catastrophic AGI risk: a survey', 'Superintelligence: Paths, Dangers, Strategies']",2,,49.0,98,checked
Agent Foundations for Aligning Machine Intelligence with Human Interests: A Technical Research Agenda,88fbf0a6ab0ce2329e3c9aca0d6f80cf405e207a,"[{'authorId': '1719968', 'name': 'N. Soares'}, {'authorId': '3442559', 'name': 'Benya Fallenstein'}]",2017,,"['AGI Safety Literature Review', 'AI safety: state of the field through quantitative lens']",2,,46.0,92,checked
AI Research Considerations for Human Existential Safety (ARCHES),a9c46dfd9a24c754a67386e02424ad68b1f4ab3b,"[{'authorId': '2651789', 'name': 'Andrew Critch'}, {'authorId': '145055042', 'name': 'David Krueger'}]",2020,arXiv.org,"['Unsolved Problems in ML Safety', 'AI Research Considerations for Human Existential Safety (ARCHES)']",2,"Framed in positive terms, this report examines how technical AI research might be steered in a manner that is more attentive to humanity's long-term prospects for survival as a species. In negative terms, we ask what existential risks humanity might face from AI development in the next century, and by what principles contemporary technical research might be directed to address those risks. 
A key property of hypothetical AI technologies is introduced, called \emph{prepotence}, which is useful for delineating a variety of potential existential risks from artificial intelligence, even as AI paradigms might shift. A set of \auxref{dirtot} contemporary research \directions are then examined for their potential benefit to existential safety. Each research direction is explained with a scenario-driven motivation, and examples of existing work from which to build. The research directions present their own risks and benefits to society that could occur at various scales of impact, and in particular are not guaranteed to benefit existential safety if major developments in them are deployed without adequate forethought and oversight. As such, each direction is accompanied by a consideration of potentially negative side effects.",21.0,42,checked
Vingean Reflection : Reliable Reasoning for Self-Improving Agents,77c7bbe446564bd3253bf720e4729a113b01b798,"[{'authorId': '1825635', 'name': 'Benja Fallenstein'}, {'authorId': '1719968', 'name': 'N. Soares'}]",2015,,"['AGI Safety Literature Review', 'AI Research Considerations for Human Existential Safety (ARCHES)']",2,"Today, human-level machine intelligence is in the domain of futurism, but there is every reason to expect that it will be developed eventually. Once artificial agents become able to improve themselves further, they may far surpass human intelligence, making it vitally important to ensure that the result of an “intelligence explosion” is aligned with human interests. In this paper, we discuss one aspect of this challenge: ensuring that the initial agent’s reasoning about its future versions is reliable, even if these future versions are far more intelligent than the current reasoner. We refer to reasoning of this sort as Vingean reflection. A self-improving agent must reason about the behavior of its smarter successors in abstract terms, since if it could predict their actions in detail, it would already be as smart as them. This is called the Vingean principle, and we argue that theoretical work on Vingean reflection should focus on formal models that reflect this principle. However, the framework of expected utility maximization, commonly used to model rational agents, fails to do so. We review a body of work which instead investigates agents that use formal proofs to reason about their successors. While it is unlikely that real-world agents would base their behavior entirely on formal proofs, this appears to be the best currently available formal model of abstract reasoning, and work in this setting may lead to insights applicable to more realistic approaches to Vingean",17.0,34,checked
Human Compatible: Artificial Intelligence and the Problem of Control,6df2126301ab415aed034b0bcd9589b1897fe983,"[{'authorId': '2055581632', 'externalIds': {'DBLP': ['Stuart Russell']}, 'url': 'https://www.semanticscholar.org/author/2055581632', 'name': 'Stuart Russell', 'aliases': None, 'affiliations': [], 'homepage': None, 'paperCount': 7, 'citationCount': 453, 'hIndex': 4}]",2019,,['Human Compatible: Artificial Intelligence and the Problem of Control'],1,"""The most important book I have read in quite some time"" (Daniel Kahneman); ""A must-read"" (Max Tegmark); ""The book we've all been waiting for"" (Sam Harris) LONGLISTED FOR THE 2019 FINANCIAL TIMES AND MCKINSEY BUSINESS BOOK OF THE YEAR; A FINANCIAL TIMES BEST BOOK OF THE YEAR 2019 Humans dream of super-intelligent machines. But what happens if we actually succeed? Creating superior intelligence would be the biggest event in human history. Unfortunately, according to the world's pre-eminent AI expert, it could also be the last. In this groundbreaking book on the biggest question facing humanity, Stuart Russell explains why he has come to consider his own discipline an existential threat to our species, and lays out how we can change course before it's too late. There is no one better placed to assess the promise and perils of the dominant technology of the future than Russell, who has spent decades at the forefront of AI research. Through brilliant analogies and crisp, lucid prose, he explains how AI actually works, how it has an enormous capacity to improve our lives - but why we must ensure that we never lose control of machines more powerful than we are. Here Russell shows how we can avert the worst threats by reshaping the foundations of AI to guarantee that machines pursue our objectives, not theirs. Profound, urgent and visionary, Human Compatible is the one book everyone needs to read to understand a future that is coming sooner than we think.",326.0,326,checked
Life 3.0: Being Human in the Age of Artificial Intelligence,64fda6dc4dfe08f515439c4f255c9766c7487880,"[{'authorId': '2011933', 'externalIds': {'DBLP': ['Max Tegmark']}, 'url': 'https://www.semanticscholar.org/author/2011933', 'name': 'Max Tegmark', 'aliases': ['M Tegmark', 'M. Tegmark'], 'affiliations': [], 'homepage': None, 'paperCount': 274, 'citationCount': 28979, 'hIndex': 79}]",2017,,['Life 3.0: Being Human in the Age of Artificial Intelligence'],1,"New York Times Best Seller How will Artificial Intelligence affect crime, war, justice, jobs, society and our very sense of being human? The rise of AI has the potential to transform our future more than any other technologyand theres nobody better qualified or situated to explore that future than Max Tegmark, an MIT professor whos helped mainstream research on how to keep AI beneficial. How can we grow our prosperity through automation without leaving people lacking income or purpose? What career advice should we give todays kids? How can we make future AI systems more robust, so that they do what we want without crashing, malfunctioning or getting hacked? Should we fear an arms race in lethal autonomous weapons? Will machines eventually outsmart us at all tasks, replacing humans on the job market and perhaps altogether? Will AI help life flourish like never before or give us more power than we can handle? What sort of future do you want? This book empowers you to join what may be the most important conversation of our time. It doesnt shy away from the full range of viewpoints or from the most controversial issuesfrom superintelligence to meaning, consciousness and the ultimate physical limits on life in the cosmos.",173.0,173,checked
X-Risk Analysis for AI Research,3ba793e937cb90ea3e82b4a6903ee4a95f307ddf,"[{'authorId': '3422872', 'externalIds': {'DBLP': ['Dan Hendrycks']}, 'url': 'https://www.semanticscholar.org/author/3422872', 'name': 'Dan Hendrycks', 'aliases': None, 'affiliations': ['UC Berkeley'], 'homepage': 'danhendrycks.com', 'paperCount': 49, 'citationCount': 11806, 'hIndex': 28}, {'authorId': '16787428', 'externalIds': {'DBLP': ['Mantas Mazeika']}, 'url': 'https://www.semanticscholar.org/author/16787428', 'name': 'Mantas Mazeika', 'aliases': ['Mantas Mažeika'], 'affiliations': [], 'homepage': None, 'paperCount': 18, 'citationCount': 2928, 'hIndex': 12}]",2022,arXiv.org,['X-Risk Analysis for AI Research'],1,"Artificial intelligence (AI) has the potential to greatly improve society, but as with any powerful technology, it comes with heightened risks and responsibilities. Current AI research lacks a systematic discussion of how to manage long-tail risks from AI systems, including speculative long-term risks. Keeping in mind the potential benefits of AI, there is some concern that building ever more intelligent and powerful AI systems could eventually result in systems that are more powerful than us; some say this is like playing with fire and speculate that this could create existential risks (x-risks). To add precision and ground these discussions, we provide a guide for how to analyze AI x-risk, which consists of three parts: First, we review how systems can be made safer today, drawing on time-tested concepts from hazard analysis and systems safety that have been designed to steer large processes in safer directions. Next, we discuss strategies for having long-term impacts on the safety of future systems. Finally, we discuss a crucial concept in making AI systems safer by improving the balance between safety and general capabilities. We hope this document and the presented concepts and tools serve as a useful guide for understanding how to analyze AI x-risk.",17.0,17,checked
AI safety: state of the field through quantitative lens,2e5ab5250e524801e2efd24249d75fcbb80f2b99,"[{'authorId': '2066411673', 'externalIds': {}, 'url': 'https://www.semanticscholar.org/author/2066411673', 'name': 'Mislav Juric', 'aliases': None, 'affiliations': [], 'homepage': None, 'paperCount': 2, 'citationCount': 14, 'hIndex': 1}, {'authorId': '2064105646', 'externalIds': {}, 'url': 'https://www.semanticscholar.org/author/2064105646', 'name': 'A. Sandic', 'aliases': ['A. Sandic', 'Agneza Sandic'], 'affiliations': [], 'homepage': None, 'paperCount': 2, 'citationCount': 15, 'hIndex': 1}, {'authorId': '35167635', 'externalIds': {'DBLP': ['Mario Brcic']}, 'url': 'https://www.semanticscholar.org/author/35167635', 'name': 'Mario Brčič', 'aliases': ['Mario Brcic', 'Mario Brčić'], 'affiliations': [], 'homepage': None, 'paperCount': 45, 'citationCount': 667, 'hIndex': 7}]",2020,"International Convention on Information and Communication Technology, Electronics and Microelectronics",['AI safety: state of the field through quantitative lens'],1,"Last decade has seen major improvements in the performance of artificial intelligence which has driven wide-spread applications. Unforeseen effects of such massad-option has put the notion of AI safety into the public eye. AI safety is a relatively new field of research focused on techniques for building AI beneficial for humans. While there exist survey papers for the field of AI safety, there is a lack of a quantitative look at the research being conducted. The quantitative aspect gives a data-driven insight about the emerging trends, knowledge gaps and potential areas for future research. In this paper, bibliometric analysis of the literature finds significant increase in research activity since 2015. Also, the field is so new that most of the technical issues are open, including: explainability and its long-term utility, and value alignment which we have identified as the most important long-term research topic. Equally, there is a severe lack of research into concrete policies regarding AI. As we expect AI to be the one of the main driving forces of changes, AI safety is the field under which we need to decide the direction of humanity’s future.",14.0,14,checked