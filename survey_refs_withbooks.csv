title,paperId,authors,year,venue,cited_survey_titles,cited_survey_count,abstract,citationCount
Alignment for Advanced Machine Learning Systems,7ac7b6dbcf5107c7ad0ce29161f60c2834a06795,"[{'authorId': '144364160', 'name': 'Jessica Taylor'}, {'authorId': '2542795', 'name': 'Eliezer Yudkowsky'}, {'authorId': '2254026', 'name': 'Patrick LaVictoire'}, {'authorId': '2651789', 'name': 'Andrew Critch'}]",2020.0,Ethics of Artificial Intelligence,"['AGI Safety Literature Review', 'Unsolved Problems in ML Safety', 'AI safety: state of the field through quantitative lens', 'AI Research Considerations for Human Existential Safety (ARCHES)']",4,"This chapter surveys eight research areas organized around one question: As learning systems become increasingly intelligent and autonomous, what design principles can best ensure that their behavior is aligned with the interests of the operators? The chapter focuses on two major technical obstacles to AI alignment: the challenge of specifying the right kind of objective functions and the challenge of designing AI systems that avoid unintended consequences and undesirable behavior even in cases where the objective function does not line up perfectly with the intentions of the designers. The questions surveyed include the following: How can we train reinforcement learners to take actions that are more amenable to meaningful assessment by intelligent overseers? What kinds of objective functions incentivize a system to “not have an overly large impact” or “not have many side effects”? The chapter discusses these questions, related work, and potential directions for future research, with the goal of highlighting relevant research topics in machine learning that appear tractable today.",62.0
Incomplete Contracting and AI Alignment,3be6455d00ff4a69e023b7b5c5d4367decb652c0,"[{'authorId': '1397904824', 'name': 'Dylan Hadfield-Menell'}, {'authorId': '40051700', 'name': 'Gillian K. Hadfield'}]",2018.0,"AAAI/ACM Conference on AI, Ethics, and Society",['AGI Safety Literature Review'],1,"We suggest that the analysis of incomplete contracting developed by law and economics researchers can provide a useful framework for understanding the AI alignment problem and help to generate a systematic approach to finding solutions. We first provide an overview of the incomplete contracting literature and explore parallels between this work and the problem of AI alignment. As we emphasize, misalignment between principal and agent is a core focus of economic analysis. We highlight some technical results from the economics literature on incomplete contracts that may provide insights for AI alignment researchers. Our core contribution, however, is to bring to bear an insight that economists have been urged to absorb from legal scholars and other behavioral scientists: the fact that human contracting is supported by substantial amounts of external structure, such as generally available institutions (culture, law) that can supply implied terms to fill the gaps in incomplete contracts. We propose a research agenda for AI alignment work that focuses on the problem of how to build AI that can replicate the human cognitive processes that connect individual incomplete contracts with this supporting external structure.",34.0
The Big Red Button…,4241be6ab01409403af1e1ca0986633481f23dc5,"[{'authorId': '123677008', 'name': 'Colin Tucker'}]",2019.0,How to Drive a Nuclear Reactor,['AGI Safety Literature Review'],1,,4.0
The Alignment Problem for Bayesian History-Based Reinforcement Learners∗,b7da135294eb5ae6828fddea373b1405ffd68968,"[{'authorId': '1868196', 'name': 'Tom Everitt'}, {'authorId': '144154444', 'name': 'Marcus Hutter'}]",2019.0,,['AGI Safety Literature Review'],1,"Value alignment is often considered a critical component of safe artificial intelligence. Meanwhile, reinforcement learning is often criticized as being inherently unsafe and misaligned, for reasons such as wireheading, delusion boxes, misspecified reward functions and distributional shifts. In this report, we categorize sources of misalignment for reinforcement learning agents, illustrating each type with numerous examples. For each type of problem, we also describe ways to remove the source of misalignment. Combined, the suggestions form high-level blueprints for how to design value aligned RL agents.",11.0
Regulating Artificial Intelligence: Proposal for a Global Solution,eea6eb95a16885a3b9c017eaa4f11ea8260bd48a,"[{'authorId': '2397584', 'name': 'O. J. Erdélyi'}, {'authorId': '1715289', 'name': 'J. Goldsmith'}]",2018.0,"AAAI/ACM Conference on AI, Ethics, and Society",['AGI Safety Literature Review'],1,"Given the ubiquity of artificial intelligence (AI) in modern societies, it is clear that individuals, corporations, and countries will be grappling with the legal and ethical issues of its use. As global problems require global solutions, we propose the establishment of an international AI regulatory agency that --- drawing on interdisciplinary expertise --- could create a unified framework for the regulation of AI technologies and inform the development of AI policies around the world. We urge that such an organization be developed with all deliberate haste, as issues such as cryptocurrencies, personalized political ad hacking, autonomous vehicles and autonomous weaponized agents are already a reality, affecting international trade, politics, and war.",80.0
Towards Provably Moral AI Agents in Bottom-up Learning Frameworks,2f9fe68ac92baacffac83178eb73680d7a2b4e3f,"[{'authorId': '38865652', 'name': 'N. Shaw'}, {'authorId': '3949042', 'name': 'Andreas Stöckel'}, {'authorId': '40466713', 'name': 'Ryan W. Orr'}, {'authorId': '145277912', 'name': 'T. Lidbetter'}, {'authorId': '144860558', 'name': 'R. Cohen'}]",2018.0,AAAI Spring Symposia,['AGI Safety Literature Review'],1,"We examine moral machine decision making as inspired by a central question posed by Rossi with respect to moral preferences: can AI systems based on statistical machine learning (which do not provide a natural way to explain or justify their decisions) be used for embedding morality into a machine in a way that allows us to prove that nothing morally wrong will happen? We argue for an evaluation which is held to the same standards as a human agent, removing the demand that ethical behaviour is always achieved. We introduce four key meta-qualities desired for our moral standards, and then proceed to clarify how we can prove that an agent will correctly learn to perform moral actions given a set of samples within certain error bounds. Our group-dynamic approach enables us to demonstrate that the learned models converge to a common function to achieve stability. We further explain a valuable intrinsic consistency check made possible through the derivation of logical statements from the machine learning model. In all, this work proposes an approach for building ethical AI systems, coming from the perspective of artificial intelligence research, and sheds important light on understanding how much learning is required in order for an intelligent agent to behave morally with negligible error.",15.0
"AI: Intelligent machines, smart policies",995423681a407fabc4a7d4652e5ebb1bf8b1b1b6,"[{'authorId': '121352282', 'name': 'Clara Neppel'}]",2018.0,OECD Digital Economy Papers,['AGI Safety Literature Review'],1,,6.0
MDL Intelligence Distillation : Exploring Strategies for Safe Access to Superintelligent Problem-Solving Capabilities,d42ed7c86a7ea86a84eaa739799354b75436ad5f,"[{'authorId': '143977849', 'name': 'K. Drexler'}]",2018.0,,['AGI Safety Literature Review'],1,,5.0
The Singularity Is Near,df811154de02fb77a337e19dc82fa8b38c673eea,"[{'authorId': '2186634', 'name': 'R. Kurzweil'}]",2018.0,The Infinite Desire for Growth,['AGI Safety Literature Review'],1,"In this chapter, Ray Kurzweil presents and defends his view that we will reach a technological singularity in the next few decades, which he defines as a “period during which the pace of technological change will be so rapid, its impact so deep, that human life will be irreversibly transformed.” Kurwzweil argues that the pace of technological change, particularly with respect to information technologies, is exponential, and that we are near the “knee” of the exponential curve (i.e. the point at which the curve changes from largely horizontal to largely vertical). Kurzweil predicts that a core feature of the singularity will be the merging of biological and machine intelligence, such that the majority of “human” intelligence will become non-biological, and the merging of virtual and physical reality. Kurzweil considers this the next step in human-machine co-evolution.",844.0
Classification of global catastrophic risks connected with artificial intelligence,2fdb269f1edbe6a1adaa47b4ed8abd19bc4f2a2d,"[{'authorId': '50639091', 'name': 'Alexey Turchin'}, {'authorId': '3758267', 'name': 'D. Denkenberger'}]",2018.0,Ai & Society,['AGI Safety Literature Review'],1,,48.0
AI safety via debate,5a5a1d666e4b7b933bc5aafbbadf179bc447ee67,"[{'authorId': '2060655766', 'name': 'G. Irving'}, {'authorId': '145791315', 'name': 'P. Christiano'}, {'authorId': '2698777', 'name': 'Dario Amodei'}]",2018.0,arXiv.org,"['AGI Safety Literature Review', 'Unsolved Problems in ML Safety', 'AI safety: state of the field through quantitative lens']",3,"To make AI systems broadly useful for challenging real-world tasks, we need them to learn complex human goals and preferences. One approach to specifying complex goals asks humans to judge during training which agent behaviors are safe and useful, but this approach can fail if the task is too complicated for a human to directly judge. To help address this concern, we propose training agents via self play on a zero sum debate game. Given a question or proposed action, two agents take turns making short statements up to a limit, then a human judges which of the agents gave the most true, useful information. In an analogy to complexity theory, debate with optimal play can answer any question in PSPACE given polynomial time judges (direct judging answers only NP questions). In practice, whether debate works involves empirical questions about humans and the tasks we want AIs to perform, plus theoretical questions about the meaning of AI alignment. We report results on an initial MNIST experiment where agents compete to convince a sparse classifier, boosting the classifier's accuracy from 59.4% to 88.9% given 6 pixels and from 48.2% to 85.2% given 4 pixels. Finally, we discuss theoretical and practical aspects of the debate model, focusing on potential weaknesses as the model scales up, and we propose future human and computer experiments to test these properties.",72.0
Categorizing Variants of Goodhart's Law,5a17fbdc5e61a39ff10984c3e0602dd2b9b54b57,"[{'authorId': '2026420', 'name': 'David Manheim'}, {'authorId': '1740494', 'name': 'Scott Garrabrant'}]",2018.0,arXiv.org,"['AGI Safety Literature Review', 'Unsolved Problems in ML Safety', 'AI Research Considerations for Human Existential Safety (ARCHES)']",3,"There are several distinct failure modes for overoptimization of systems on the basis of metrics. This occurs when a metric which can be used to improve a system is used to an extent that further optimization is ineffective or harmful, and is sometimes termed Goodhart's Law. This class of failure is often poorly understood, partly because terminology for discussing them is ambiguous, and partly because discussion using this ambiguous terminology ignores distinctions between different failure modes of this general type. This paper expands on an earlier discussion by Garrabrant, which notes there are ""(at least) four different mechanisms"" that relate to Goodhart's Law. This paper is intended to explore these mechanisms further, and specify more clearly how they occur. This discussion should be helpful in better understanding these types of failures in economic regulation, in public policy, in machine learning, and in Artificial Intelligence alignment. The importance of Goodhart effects depends on the amount of power directed towards optimizing the proxy, and so the increased optimization power offered by artificial intelligence makes it especially critical for that field.",64.0
The Surprising Creativity of Digital Evolution: A Collection of Anecdotes from the Evolutionary Computation and Artificial Life Research Communities,ba49751f2f119c4f87d52a79d7cd84c441702b51,"[{'authorId': '39799304', 'name': 'J. Lehman'}, {'authorId': '2552141', 'name': 'J. Clune'}, {'authorId': '1707548', 'name': 'D. Misevic'}, {'authorId': '145882965', 'name': 'C. Adami'}, {'authorId': '2597208', 'name': 'L. Altenberg'}, {'authorId': '2086111462', 'name': 'Julie Beaulieu'}, {'authorId': '145074286', 'name': 'P. Bentley'}, {'authorId': '2070814530', 'name': 'Samuel Bernard'}, {'authorId': '1809282', 'name': 'G. Beslon'}, {'authorId': '37251097', 'name': 'David M. Bryson'}, {'authorId': '22259392', 'name': 'P. Chrabaszcz'}, {'authorId': '2800259', 'name': 'Nick Cheney'}, {'authorId': '1934171', 'name': 'Antoine Cully'}, {'authorId': '1765955', 'name': 'S. Doncieux'}, {'authorId': '2621816', 'name': 'F. Dyer'}, {'authorId': '3341968', 'name': 'Kai Olav Ellefsen'}, {'authorId': '145278906', 'name': 'R. Feldt'}, {'authorId': '152511330', 'name': 'Stephan Fischer'}, {'authorId': '30123380', 'name': 'S. Forrest'}, {'authorId': '1697243', 'name': 'Antoine Frénoy'}, {'authorId': '1712120', 'name': 'Christian Gagné'}, {'authorId': '144346948', 'name': 'L. L. Goff'}, {'authorId': '3327342', 'name': 'L. Grabowski'}, {'authorId': '3024670', 'name': 'B. Hodjat'}, {'authorId': '144661829', 'name': 'F. Hutter'}, {'authorId': '143868232', 'name': 'L. Keller'}, {'authorId': '35083108', 'name': 'C. Knibbe'}, {'authorId': '1965168', 'name': 'Peter Krcah'}, {'authorId': '2450431', 'name': 'R. Lenski'}, {'authorId': '51022452', 'name': 'H. Lipson'}, {'authorId': '2192847', 'name': 'R. MacCurdy'}, {'authorId': '144661263', 'name': 'Carlos Maestre'}, {'authorId': '1686788', 'name': 'R. Miikkulainen'}, {'authorId': '49837180', 'name': 'Sara Mitri'}, {'authorId': '1859405', 'name': 'D. E. Moriarty'}, {'authorId': '145312416', 'name': 'Jean-Baptiste Mouret'}, {'authorId': '151414531', 'name': 'Anh M Nguyen'}, {'authorId': '152559260', 'name': 'C. Ofria'}, {'authorId': '1747913', 'name': 'M. Parizeau'}, {'authorId': '144641159', 'name': 'David P. Parsons'}, {'authorId': '1779320', 'name': 'Robert T. Pennock'}, {'authorId': '2344970', 'name': 'W. Punch'}, {'authorId': '1793950', 'name': 'T. Ray'}, {'authorId': '69881991', 'name': 'Marc Schoenauer'}, {'authorId': '2050584511', 'name': 'E. Shulte'}, {'authorId': '1391181439', 'name': 'Karl Sims'}, {'authorId': '1846883', 'name': 'Kenneth O. Stanley'}, {'authorId': '1697477', 'name': 'F. Taddei'}, {'authorId': '144304371', 'name': 'Danesh Tarapore'}, {'authorId': '38433480', 'name': 'S. Thibault'}, {'authorId': '47211092', 'name': 'Westley Weimer'}, {'authorId': '48364115', 'name': 'R. Watson'}, {'authorId': '1406729774', 'name': 'Jason Yosinksi'}]",2018.0,Artificial Life,"['AGI Safety Literature Review', 'Unsolved Problems in ML Safety']",2,"Evolution provides a creative fount of complex and subtle adaptations that often surprise the scientists who discover them. However, the creativity of evolution is not limited to the natural world: Artificial organisms evolving in computational environments have also elicited surprise and wonder from the researchers studying them. The process of evolution is an algorithmic process that transcends the substrate in which it occurs. Indeed, many researchers in the field of digital evolution can provide examples of how their evolving algorithms and organisms have creatively subverted their expectations or intentions, exposed unrecognized bugs in their code, produced unexpectedly adaptations, or engaged in behaviors and outcomes, uncannily convergent with ones found in nature. Such stories routinely reveal surprise and creativity by evolution in these digital worlds, but they rarely fit into the standard scientific narrative. Instead they are often treated as mere obstacles to be overcome, rather than results that warrant study in their own right. Bugs are fixed, experiments are refocused, and one-off surprises are collapsed into a single data point. The stories themselves are traded among researchers through oral tradition, but that mode of information transmission is inefficient and prone to error and outright loss. Moreover, the fact that these stories tend to be shared only among practitioners means that many natural scientists do not realize how interesting and lifelike digital organisms are and how natural their evolution can be. To our knowledge, no collection of such anecdotes has been published before. This article is the crowd-sourced product of researchers in the fields of artificial life and evolutionary computation who have provided first-hand accounts of such cases. It thus serves as a written, fact-checked collection of scientifically important and even entertaining stories. In doing so we also present here substantial evidence that the existence and importance of evolutionary surprises extends beyond the natural world, and may indeed be a universal property of all complex evolving systems.",191.0
The “big red button” is too late: an alternative model for the ethical evaluation of AI systems,57a6679980bd7ca3d64f49fae4e85ad4f9ff2f18,"[{'authorId': '2053868213', 'name': 'Thomas Arnold'}, {'authorId': '1793014', 'name': 'Matthias Scheutz'}]",2018.0,Ethics and Information Technology,['AGI Safety Literature Review'],1,,50.0
"The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation",a4d513cfc9d4902ef1a80198582f29b8ba46ac28,"[{'authorId': '35167962', 'name': 'Miles Brundage'}, {'authorId': '49344883', 'name': 'S. Avin'}, {'authorId': '2115193883', 'name': 'Jack Clark'}, {'authorId': '48625835', 'name': 'H. Toner'}, {'authorId': '2654106', 'name': 'P. Eckersley'}, {'authorId': '39928654', 'name': 'Ben Garfinkel'}, {'authorId': '3198576', 'name': 'A. Dafoe'}, {'authorId': '35681920', 'name': 'P. Scharre'}, {'authorId': '46225273', 'name': 'T. Zeitzoff'}, {'authorId': '7888676', 'name': 'Bobby Filar'}, {'authorId': '2639880', 'name': 'H. Anderson'}, {'authorId': '47075894', 'name': 'H. Roff'}, {'authorId': '2059529715', 'name': 'Gregory C. Allen'}, {'authorId': '5164568', 'name': 'J. Steinhardt'}, {'authorId': '152629250', 'name': 'Carrick Flynn'}, {'authorId': '35793299', 'name': 'Seán Ó hÉigeartaigh'}, {'authorId': '38992229', 'name': 'S. Beard'}, {'authorId': '36729401', 'name': 'Haydn Belfield'}, {'authorId': '33859827', 'name': 'Sebastian Farquhar'}, {'authorId': '39439114', 'name': 'Clare Lyle'}, {'authorId': '35431817', 'name': 'Rebecca Crootof'}, {'authorId': '47107786', 'name': 'Owain Evans'}, {'authorId': '2054564415', 'name': 'Michael Page'}, {'authorId': '2055073817', 'name': 'Joanna J. Bryson'}, {'authorId': '26336155', 'name': 'Roman V. Yampolskiy'}, {'authorId': '2698777', 'name': 'Dario Amodei'}]",2018.0,arXiv.org,"['AGI Safety Literature Review', 'Unsolved Problems in ML Safety', 'X-Risk Analysis for AI Research']",3,"The following organisations are named on the report: Future of Humanity Institute, University of Oxford, Centre for the Study of Existential Risk, University of Cambridge, Center for a New American Security, Electronic Frontier Foundation, OpenAI. The Future of Life Institute is acknowledged as a funder.",452.0
Psychlab: A Psychology Laboratory for Deep Reinforcement Learning Agents,927d904d2aad002eb71e8c6ee45218f31a103100,"[{'authorId': '1700356', 'name': 'Joel Z. Leibo'}, {'authorId': '1413221272', 'name': ""Cyprien de Masson d'Autume""}, {'authorId': '2944502', 'name': 'Daniel Zoran'}, {'authorId': '2064400086', 'name': 'David Amos'}, {'authorId': '50388928', 'name': 'Charlie Beattie'}, {'authorId': '152450665', 'name': 'Keith Anderson'}, {'authorId': '31746264', 'name': 'Antonio García Castañeda'}, {'authorId': '2117081949', 'name': 'Manuel Sanchez'}, {'authorId': '2070088720', 'name': 'Simon Green'}, {'authorId': '2203658', 'name': 'A. Gruslys'}, {'authorId': '34313265', 'name': 'S. Legg'}, {'authorId': '48987704', 'name': 'D. Hassabis'}, {'authorId': '46378362', 'name': 'M. Botvinick'}]",2018.0,arXiv.org,['AGI Safety Literature Review'],1,"Psychlab is a simulated psychology laboratory inside the first-person 3D game world of DeepMind Lab (Beattie et al. 2016). Psychlab enables implementations of classical laboratory psychological experiments so that they work with both human and artificial agents. Psychlab has a simple and flexible API that enables users to easily create their own tasks. As examples, we are releasing Psychlab implementations of several classical experimental paradigms including visual search, change detection, random dot motion discrimination, and multiple object tracking. We also contribute a study of the visual psychophysics of a specific state-of-the-art deep reinforcement learning agent: UNREAL (Jaderberg et al. 2016). This study leads to the surprising conclusion that UNREAL learns more quickly about larger target stimuli than it does about smaller stimuli. In turn, this insight motivates a specific improvement in the form of a simple model of foveal vision that turns out to significantly boost UNREAL's performance, both on Psychlab tasks, and on standard DeepMind Lab tasks. By open-sourcing Psychlab we hope to facilitate a range of future such studies that simultaneously advance deep reinforcement learning and improve its links with cognitive science.",66.0
AI Safety and Reproducibility: Establishing Robust Foundations for the Neuropsychology of Human Values,0ce7c7ae753c46a3618a216f7c78ca80a330437e,"[{'authorId': '3451324', 'name': 'G. Sarma'}, {'authorId': '2934247', 'name': 'Nick J. Hay'}, {'authorId': '5525463', 'name': 'A. Safron'}]",2017.0,SAFECOMP Workshops,['AGI Safety Literature Review'],1,,11.0
An AI Race for Strategic Advantage: Rhetoric and Risks,92f092ef414d24b8facc6e2114c07abe4e6bc16d,"[{'authorId': '51129868', 'name': 'S. Cave'}, {'authorId': '2351755', 'name': 'Seán S. ÓhÉigeartaigh'}]",2017.0,"AAAI/ACM Conference on AI, Ethics, and Society",['AGI Safety Literature Review'],1,"The rhetoric of the race for strategic advantage is increasingly being used with regard to the development of artificial intelligence (AI), sometimes in a military context, but also more broadly. This rhetoric also reflects real shifts in strategy, as industry research groups compete for a limited pool of talented researchers, and nation states such as China announce ambitious goals for global leadership in AI. This paper assesses the potential risks of the AI race narrative and of an actual competitive race to develop AI, such as incentivising corner-cutting on safe-ty and governance, or increasing the risk of conflict. It explores the role of the research community in respond-ing to these risks. And it briefly explores alternative ways in which the rush to develop powerful AI could be framed so as instead to foster collaboration and respon-sible progress.",78.0
Rainbow: Combining Improvements in Deep Reinforcement Learning,0ab3f7ecbdc5a33565a234215604a6ca9d155a33,"[{'authorId': '39357484', 'name': 'Matteo Hessel'}, {'authorId': '3321484', 'name': 'Joseph Modayil'}, {'authorId': '7634925', 'name': 'H. V. Hasselt'}, {'authorId': '1725157', 'name': 'T. Schaul'}, {'authorId': '2273072', 'name': 'Georg Ostrovski'}, {'authorId': '2605877', 'name': 'Will Dabney'}, {'authorId': '48257711', 'name': 'Dan Horgan'}, {'authorId': '1808897', 'name': 'Bilal Piot'}, {'authorId': '37666967', 'name': 'M. G. Azar'}, {'authorId': '145824029', 'name': 'David Silver'}]",2017.0,AAAI Conference on Artificial Intelligence,['AGI Safety Literature Review'],1,"
 
 The deep reinforcement learning community has made several independent improvements to the DQN algorithm. However, it is unclear which of these extensions are complementary and can be fruitfully combined. This paper examines six extensions to the DQN algorithm and empirically studies their combination. Our experiments show that the combination provides state-of-the-art performance on the Atari 2600 benchmark, both in terms of data efficiency and final performance. We also provide results from a detailed ablation study that shows the contribution of each component to overall performance.
 
",1516.0
Incorrigibility in the CIRL Framework,7ba22bc1cc37336a2a4e769c9798c47c8cad0b11,"[{'authorId': '2057014575', 'name': 'Ryan Carey'}]",2017.0,"AAAI/ACM Conference on AI, Ethics, and Society","['AGI Safety Literature Review', 'AI Research Considerations for Human Existential Safety (ARCHES)']",2,"A value learning system has incentives to follow shutdown instructions, assuming the shutdown instruction provides information (in the technical sense) about which actions lead to valuable outcomes. However, this assumption is not robust to model mis-specification (e.g., in the case of programmer errors). We demonstrate this by presenting some Supervised POMDP scenarios in which errors in the parameterized reward function remove the incentive to follow shutdown commands. These difficulties parallel those discussed by Soares et al. 2015 in their paper on corrigibility. We argue that it is important to consider systems that follow shutdown commands under some weaker set of assumptions (e.g., that one small verified module is correctly implemented; as opposed to an entire prior probability distribution and/or parameterized reward function). We discuss some difficulties with simple ways to attempt to attain these sorts of guarantees in a value learning framework.",17.0
Deep TAMER: Interactive Agent Shaping in High-Dimensional State Spaces,abcf11a9af3d83f85c5fbfffc5901d416ca7a73f,"[{'authorId': '1938253', 'name': 'Garrett Warnell'}, {'authorId': '3436871', 'name': 'Nicholas R. Waytowich'}, {'authorId': '2194602', 'name': 'V. Lawhern'}, {'authorId': '144848112', 'name': 'P. Stone'}]",2017.0,AAAI Conference on Artificial Intelligence,['AGI Safety Literature Review'],1,"
 
 While recent advances in deep reinforcement learning have allowed autonomous learning agents to succeed at a variety of complex tasks, existing algorithms generally require a lot oftraining data. One way to increase the speed at which agent sare able to learn to perform tasks is by leveraging the input of human trainers. Although such input can take many forms, real-time, scalar-valued feedback is especially useful in situations where it proves difficult or impossible for humans to provide expert demonstrations. Previous approaches have shown the usefulness of human input provided in this fashion (e.g., the TAMER framework), but they have thus far not considered high-dimensional state spaces or employed the use of deep learning. In this paper, we do both: we propose DeepTAMER, an extension of the TAMER framework that leverages the representational power of deep neural networks inorder to learn complex tasks in just a short amount of time with a human trainer. We demonstrate Deep TAMER’s success by using it and just 15 minutes of human-provided feedback to train an agent that performs better than humans on the Atari game of Bowling - a task that has proven difficult for even state-of-the-art reinforcement learning methods.
 
",169.0
Synthesizing Robust Adversarial Examples,8dce99e33c6fceb3e79023f5894fdbe733c91e92,"[{'authorId': '38939786', 'name': 'Anish Athalye'}, {'authorId': '39468283', 'name': 'Logan Engstrom'}, {'authorId': '34562927', 'name': 'Andrew Ilyas'}, {'authorId': '2058062760', 'name': 'K. Kwok'}]",2017.0,International Conference on Machine Learning,"['AGI Safety Literature Review', 'AI Research Considerations for Human Existential Safety (ARCHES)']",2,"Standard methods for generating adversarial examples for neural networks do not consistently fool neural network classifiers in the physical world due to a combination of viewpoint shifts, camera noise, and other natural transformations, limiting their relevance to real-world systems. We demonstrate the existence of robust 3D adversarial objects, and we present the first algorithm for synthesizing examples that are adversarial over a chosen distribution of transformations. We synthesize two-dimensional adversarial images that are robust to noise, distortion, and affine transformation. We apply our algorithm to complex three-dimensional objects, using 3D-printing to manufacture the first physical adversarial objects. Our results demonstrate the existence of 3D adversarial objects in the physical world.",1168.0
Trial without Error: Towards Safe Reinforcement Learning via Human Intervention,30ff82cebce6fdc2957043c4085a426414474d78,"[{'authorId': '2058848938', 'name': 'W. Saunders'}, {'authorId': '144864359', 'name': 'Girish Sastry'}, {'authorId': '2214496', 'name': 'Andreas Stuhlmüller'}, {'authorId': '47107786', 'name': 'Owain Evans'}]",2017.0,Adaptive Agents and Multi-Agent Systems,"['AGI Safety Literature Review', 'Unsolved Problems in ML Safety', 'AI Research Considerations for Human Existential Safety (ARCHES)']",3,"AI systems are increasingly applied to complex tasks that involve interaction with humans. During training, such systems are potentially dangerous, as they haven't yet learned to avoid actions that could cause serious harm. How can an AI system explore and learn without making a single mistake that harms humans or otherwise causes serious damage? For model-free reinforcement learning, having a human ""in the loop"" and ready to intervene is currently the only way to prevent all catastrophes. We formalize human intervention for RL and show how to reduce the human labor required by training a supervised learner to imitate the human's intervention decisions. We evaluate this scheme on Atari games, with a Deep RL agent being overseen by a human for four hours. When the class of catastrophes is simple, we are able to prevent all catastrophes without affecting the agent's learning (whereas an RL baseline fails due to catastrophic forgetting). However, this scheme is less successful when catastrophes are more complex: it reduces but does not eliminate catastrophes and the supervised learner fails on adversarial examples found by the agent. Extrapolating to more challenging environments, we show that our implementation would not scale (due to the infeasible amount of human labor required). We outline extensions of the scheme that are necessary if we are to train model-free agents without a single catastrophe.",163.0
The Singularity May Be Near,cc466d95caccc551d2fb23909cf39cab8642d76b,"[{'authorId': '1976753', 'name': 'Roman V Yampolskiy'}]",2017.0,Inf.,['AGI Safety Literature Review'],1,"Toby Walsh in “The Singularity May Never Be Near” gives six arguments to support his point of view that technological singularity may happen, but that it is unlikely. In this paper, we provide analysis of each one of his arguments and arrive at similar conclusions, but with more weight given to the “likely to happen” prediction.",14.0
When Will AI Exceed Human Performance? Evidence from AI Experts,6b93cedfe768eb8b5ece92612aac9cc8e986d12a,"[{'authorId': '40582697', 'name': 'K. Grace'}, {'authorId': '3373139', 'name': 'J. Salvatier'}, {'authorId': '3198576', 'name': 'A. Dafoe'}, {'authorId': '1406217399', 'name': 'Baobao Zhang'}, {'authorId': '47107786', 'name': 'Owain Evans'}]",2017.0,Journal of Artificial Intelligence Research,"['AGI Safety Literature Review', 'AI Research Considerations for Human Existential Safety (ARCHES)', 'X-Risk Analysis for AI Research']",3,"
 
 
 
Advances in artificial intelligence (AI) will transform modern life by reshaping transportation, health, science, finance, and the military. To adapt public policy, we need to better anticipate these advances. Here we report the results from a large survey of machine learning researchers on their beliefs about progress in AI. Researchers predict AI will outperform humans in many activities in the next ten years, such as translating languages (by 2024), writing high-school essays (by 2026), driving a truck (by 2027), working in retail (by 2031), writing a bestselling book (by 2049), and working as a surgeon (by 2053). Researchers believe there is a 50% chance of AI outperforming humans in all tasks in 45 years and of automating all human jobs in 120 years, with Asian respondents expecting these dates much sooner than North Americans. These results will inform discussion amongst researchers and policymakers about anticipating and managing trends in AI. 
 
 
This article is part of the special track on AI and Society. 
 
 
 
 
 
",446.0
Moral Decision Making Frameworks for Artificial Intelligence,a3bbffdcc1c7c4cae66d6af373651389d94b7090,"[{'authorId': '1749906', 'name': 'V. Conitzer'}, {'authorId': '1422133662', 'name': 'Walter Sinnott-Armstrong'}, {'authorId': '2591514', 'name': 'Jana Schaich Borg'}, {'authorId': '66357890', 'name': 'Yuan Deng'}, {'authorId': '1409261055', 'name': 'Max F. Kramer'}]",2017.0,International Symposium on Artificial Intelligence and Mathematics,['AGI Safety Literature Review'],1,"
 
 The generality of decision and game theory has enabled domain-independent progress in AI research. For example, a better algorithm for finding good policies in (PO)MDPs can be instantly used in a variety of applications. But such a general theory is lacking when it comes to moral decision making. For AI applications with a moral component, are we then forced to build systems based on many ad-hoc rules? In this paper we discuss possible ways to avoid this conclusion.
 
",137.0
Recent trend in the cost of computing,,[],2018.0,url: https://aiimpacts. org/recent-trend-in-the-cost-of-computing/ (visited on 01/12/2018).,"['AGI Safety Literature Review', 'AGI Safety Literature Review', 'AGI Safety Literature Review', 'AGI Safety Literature Review', 'AGI Safety Literature Review', 'AGI Safety Literature Review', 'AGI Safety Literature Review', 'AGI Safety Literature Review', 'AGI Safety Literature Review', 'AGI Safety Literature Review', 'AGI Safety Literature Review', 'AGI Safety Literature Review', 'AGI Safety Literature Review', 'AGI Safety Literature Review', 'AGI Safety Literature Review', 'AGI Safety Literature Review', 'AGI Safety Literature Review', 'AGI Safety Literature Review', 'AGI Safety Literature Review', 'AGI Safety Literature Review', 'AGI Safety Literature Review', 'AGI Safety Literature Review', 'AGI Safety Literature Review', 'AGI Safety Literature Review', 'AGI Safety Literature Review', 'AGI Safety Literature Review', 'AGI Safety Literature Review', 'AGI Safety Literature Review', 'AGI Safety Literature Review', 'AGI Safety Literature Review', 'AGI Safety Literature Review', 'AGI Safety Literature Review', 'AGI Safety Literature Review', 'AGI Safety Literature Review', 'AGI Safety Literature Review', 'AGI Safety Literature Review', 'AGI Safety Literature Review', 'AGI Safety Literature Review', 'AGI Safety Literature Review', 'AGI Safety Literature Review', 'AGI Safety Literature Review', 'AGI Safety Literature Review', 'AGI Safety Literature Review', 'AGI Safety Literature Review', 'AGI Safety Literature Review', 'AGI Safety Literature Review', 'Unsolved Problems in ML Safety', 'Unsolved Problems in ML Safety', 'Unsolved Problems in ML Safety', 'Unsolved Problems in ML Safety', 'Unsolved Problems in ML Safety', 'Unsolved Problems in ML Safety', 'Unsolved Problems in ML Safety', 'Unsolved Problems in ML Safety', 'Unsolved Problems in ML Safety', 'Unsolved Problems in ML Safety', 'Unsolved Problems in ML Safety', 'Unsolved Problems in ML Safety', 'Unsolved Problems in ML Safety', 'Unsolved Problems in ML Safety', 'Unsolved Problems in ML Safety', 'Unsolved Problems in ML Safety', 'Unsolved Problems in ML Safety', 'Unsolved Problems in ML Safety', 'Unsolved Problems in ML Safety', 'Unsolved Problems in ML Safety', 'Unsolved Problems in ML Safety', 'Unsolved Problems in ML Safety', 'Unsolved Problems in ML Safety', 'Unsolved Problems in ML Safety', 'AI safety: state of the field through quantitative lens', 'AI safety: state of the field through quantitative lens', 'AI safety: state of the field through quantitative lens', 'AI safety: state of the field through quantitative lens', 'AI safety: state of the field through quantitative lens', 'AI safety: state of the field through quantitative lens', 'AI Research Considerations for Human Existential Safety (ARCHES)', 'AI Research Considerations for Human Existential Safety (ARCHES)', 'AI Research Considerations for Human Existential Safety (ARCHES)', 'AI Research Considerations for Human Existential Safety (ARCHES)', 'AI Research Considerations for Human Existential Safety (ARCHES)', 'AI Research Considerations for Human Existential Safety (ARCHES)', 'AI Research Considerations for Human Existential Safety (ARCHES)', 'AI Research Considerations for Human Existential Safety (ARCHES)', 'AI Research Considerations for Human Existential Safety (ARCHES)', 'AI Research Considerations for Human Existential Safety (ARCHES)', 'AI Research Considerations for Human Existential Safety (ARCHES)', 'AI Research Considerations for Human Existential Safety (ARCHES)', 'AI Research Considerations for Human Existential Safety (ARCHES)', 'AI Research Considerations for Human Existential Safety (ARCHES)', 'AI Research Considerations for Human Existential Safety (ARCHES)', 'AI Research Considerations for Human Existential Safety (ARCHES)', 'AI Research Considerations for Human Existential Safety (ARCHES)', 'AI Research Considerations for Human Existential Safety (ARCHES)', 'AI Research Considerations for Human Existential Safety (ARCHES)', 'AI Research Considerations for Human Existential Safety (ARCHES)', 'AI Research Considerations for Human Existential Safety (ARCHES)', 'AI Research Considerations for Human Existential Safety (ARCHES)', 'AI Research Considerations for Human Existential Safety (ARCHES)', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'Responses to catastrophic AGI risk: a survey', 'X-Risk Analysis for AI Research', 'X-Risk Analysis for AI Research', 'X-Risk Analysis for AI Research', 'X-Risk Analysis for AI Research', 'X-Risk Analysis for AI Research', 'X-Risk Analysis for AI Research', 'X-Risk Analysis for AI Research', 'X-Risk Analysis for AI Research', 'X-Risk Analysis for AI Research', 'X-Risk Analysis for AI Research', 'X-Risk Analysis for AI Research', 'X-Risk Analysis for AI Research', 'X-Risk Analysis for AI Research', 'X-Risk Analysis for AI Research', 'X-Risk Analysis for AI Research', 'X-Risk Analysis for AI Research', 'X-Risk Analysis for AI Research', 'X-Risk Analysis for AI Research', 'X-Risk Analysis for AI Research', 'X-Risk Analysis for AI Research', 'X-Risk Analysis for AI Research', 'Superintelligence: Paths, Dangers, Strategies', 'Superintelligence: Paths, Dangers, Strategies', 'Superintelligence: Paths, Dangers, Strategies', 'Superintelligence: Paths, Dangers, Strategies', 'Superintelligence: Paths, Dangers, Strategies', 'Superintelligence: Paths, Dangers, Strategies', 'Superintelligence: Paths, Dangers, Strategies', 'Superintelligence: Paths, Dangers, Strategies', 'Superintelligence: Paths, Dangers, Strategies', 'Superintelligence: Paths, Dangers, Strategies', 'Superintelligence: Paths, Dangers, Strategies', 'Superintelligence: Paths, Dangers, Strategies', 'Superintelligence: Paths, Dangers, Strategies', 'Superintelligence: Paths, Dangers, Strategies', 'Superintelligence: Paths, Dangers, Strategies', 'Superintelligence: Paths, Dangers, Strategies', 'Superintelligence: Paths, Dangers, Strategies', 'Superintelligence: Paths, Dangers, Strategies', 'Superintelligence: Paths, Dangers, Strategies', 'Superintelligence: Paths, Dangers, Strategies', 'Superintelligence: Paths, Dangers, Strategies', 'Superintelligence: Paths, Dangers, Strategies', 'Superintelligence: Paths, Dangers, Strategies', 'Superintelligence: Paths, Dangers, Strategies', 'Superintelligence: Paths, Dangers, Strategies', 'Superintelligence: Paths, Dangers, Strategies', 'Superintelligence: Paths, Dangers, Strategies', 'Superintelligence: Paths, Dangers, Strategies', 'Superintelligence: Paths, Dangers, Strategies', 'Superintelligence: Paths, Dangers, Strategies', 'Superintelligence: Paths, Dangers, Strategies', 'Superintelligence: Paths, Dangers, Strategies', 'Superintelligence: Paths, Dangers, Strategies', 'Superintelligence: Paths, Dangers, Strategies', 'Superintelligence: Paths, Dangers, Strategies', 'Superintelligence: Paths, Dangers, Strategies', 'Superintelligence: Paths, Dangers, Strategies', 'Superintelligence: Paths, Dangers, Strategies', 'Superintelligence: Paths, Dangers, Strategies', 'Superintelligence: Paths, Dangers, Strategies', 'Superintelligence: Paths, Dangers, Strategies', 'Superintelligence: Paths, Dangers, Strategies', 'Superintelligence: Paths, Dangers, Strategies', 'Superintelligence: Paths, Dangers, Strategies', 'Superintelligence: Paths, Dangers, Strategies', 'Superintelligence: Paths, Dangers, Strategies', 'Superintelligence: Paths, Dangers, Strategies', 'Superintelligence: Paths, Dangers, Strategies', 'Superintelligence: Paths, Dangers, Strategies', 'Superintelligence: Paths, Dangers, Strategies', 'Superintelligence: Paths, Dangers, Strategies', 'Superintelligence: Paths, Dangers, Strategies', 'Superintelligence: Paths, Dangers, Strategies', 'Superintelligence: Paths, Dangers, Strategies', 'Superintelligence: Paths, Dangers, Strategies', 'Superintelligence: Paths, Dangers, Strategies', 'Superintelligence: Paths, Dangers, Strategies', 'Superintelligence: Paths, Dangers, Strategies', 'Superintelligence: Paths, Dangers, Strategies', 'Superintelligence: Paths, Dangers, Strategies', 'Superintelligence: Paths, Dangers, Strategies', 'Superintelligence: Paths, Dangers, Strategies', 'Superintelligence: Paths, Dangers, Strategies', 'Superintelligence: Paths, Dangers, Strategies', 'Superintelligence: Paths, Dangers, Strategies', 'Superintelligence: Paths, Dangers, Strategies', 'Superintelligence: Paths, Dangers, Strategies', 'Superintelligence: Paths, Dangers, Strategies', 'Superintelligence: Paths, Dangers, Strategies', 'Superintelligence: Paths, Dangers, Strategies', 'Superintelligence: Paths, Dangers, Strategies', 'Superintelligence: Paths, Dangers, Strategies', 'Superintelligence: Paths, Dangers, Strategies', 'Superintelligence: Paths, Dangers, Strategies', 'Superintelligence: Paths, Dangers, Strategies', 'Superintelligence: Paths, Dangers, Strategies', 'Superintelligence: Paths, Dangers, Strategies', 'Superintelligence: Paths, Dangers, Strategies', 'Superintelligence: Paths, Dangers, Strategies', 'Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy', 'Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy', 'Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy', 'Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy', 'Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy', 'Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy', 'Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy', 'Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy', 'Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy', 'Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy', 'Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy', 'Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy', 'Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy', 'Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy', 'Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy', 'Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy', 'Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy', 'Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy', 'Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy', 'Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy', 'Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy', 'Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy', 'Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy', 'Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy', 'Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy', 'Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy', 'Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy', 'Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy', 'Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy', 'Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy', 'Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy', 'Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy', 'Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy', 'Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy', 'Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy', 'Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy', 'Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy', 'Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy', 'Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy', 'Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy', 'Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy', 'Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy', 'Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy', 'Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy', 'Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy', 'Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy', 'Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy', 'Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy', 'Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy', 'Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy', 'Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy', 'Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy', 'Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy', 'Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy', 'Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",418,,
Human-aligned artificial intelligence is a multiobjective problem,627fc90f9bc87faef48f817136a8b3699a146fc4,"[{'authorId': '1990124', 'name': 'P. Vamplew'}, {'authorId': '3327913', 'name': 'R. Dazeley'}, {'authorId': '2763108', 'name': 'Cameron Foale'}, {'authorId': '6461299', 'name': 'Sally Firmin'}, {'authorId': '143902574', 'name': 'Jane Mummery'}]",2018.0,Ethics and Information Technology,['AGI Safety Literature Review'],1,,66.0
Conceptual-Linguistic Superintelligence,ffe93a2441f699bea4ecda4033c76621ea71910d,"[{'authorId': '2643853', 'name': 'David J. Jilk'}]",2017.0,Informatica,['AGI Safety Literature Review'],1,"We argue that artificial intelligence capable of sustaining an uncontrolled intelligence explosion must have a conceptual-linguistic faculty with substantial functional similarity to the human faculty. We then argue for three subsidiary claims: first, that detecting the presence of such a faculty will be an important indicator of imminent superintelligence; second, that such a superintelligence will, in creating further increases in intelligence, both face and consider the same sorts of existential risks that humans face today; third, that such a superintelligence is likely to assess and question its own values, purposes, and drives.",5.0
The Technological Landscape Affecting Artificial General Intelligence and the Importance of Nanoscale Neural Probes,72b4c1f5734ad20bcc143f471a712913b0803cb8,"[{'authorId': '2849316', 'name': 'Daniel Eth'}]",2017.0,Informatica,['AGI Safety Literature Review'],1,"In this paper, we contrast three major pathways to human level AI, also known as artificial general intelligence (AGI), and we investigate how safety considerations compare between the three. The first pathway is de novo AGI (dnAGI), AGI built from the ground up. The second is Neuromorphic AGI (NAGI), AGI based loosely on the principles of the human brain. And third is Whole Brain Emulation (WBE), AGI built by emulating a particular human brain, in silico. Bostrom has previously argued that NAGI is the least safe form of the three. NAGI would be messier than dnAGI and therefore harder to align to arbitrary values. Additionally, NAGI would not intrinsically possess safeguards found in the human brain – such as compassion – while WBE would. In this paper, we argue that getting WBE first would be preferable to getting dnAGI first. While the introduction of WBE would likely be followed by a later transition to the less-constrained and therefore more-powerful dnAGI, the creation of dnAGI would likely be less dangerous if accomplished by WBEs than if done simply by biological humans, for a variety of reasons. One major reason is that the higher intelligence and quicker speed of thinking in the WBEs compared to biological humans could increase the chances of traversing the path through dnAGI safely. We additionally investigate the major technological trends leading to these three types of AGI, and we find these trends to be: traditional AI research, computational hardware, nanotechnology research, nanoscale neural probes, and neuroscience. In particular, we find that WBE is unlikely to be achieved without nanoscale neural probes, since much of the information processing in the brain occurs on the subcellular level (i.e., the nanoscale). For this reason, we argue that nanoscale neural probes could improve safety by favoring WBE over NAGI.",4.0
Superintelligence As a Cause or Cure For Risks of Astronomical Suffering,d8ea9bd0258bacf6f30bd0399ee7208aea891e44,"[{'authorId': '2821562', 'name': 'Kaj Sotala'}, {'authorId': '134301056', 'name': 'Lukas Gloor'}]",2017.0,Informatica,['AGI Safety Literature Review'],1,"Discussions about the possible consequences of creating superintelligence have included the possibility of existential risk , often understood mainly as the risk of human extinction. We argue that suffering risks (s-risks) , where an adverse outcome would bring about severe suffering on an astronomical scale, are risks of a comparable severity and probability as risks of extinction. Preventing them is the common interest of many different value systems. Furthermore, we argue that in the same way as superintelligent AI both contributes to existential risk but can also help prevent it, superintelligent AI can both be a suffering risk or help avoid it. Some types of work aimed at making superintelligent AI safe will also help prevent suffering risks, and there may also be a class of safeguards for AI that helps specifically against s-risks.",31.0
A Berkeley View of Systems Challenges for AI,da12b03ad7cee8030b5e4c87b25edaae706d7106,"[{'authorId': '144467753', 'name': 'I. Stoica'}, {'authorId': '143711382', 'name': 'D. Song'}, {'authorId': '144963510', 'name': 'R. A. Popa'}, {'authorId': '1701130', 'name': 'D. Patterson'}, {'authorId': '143884206', 'name': 'Michael W. Mahoney'}, {'authorId': '38793222', 'name': 'R. Katz'}, {'authorId': '1687701', 'name': 'A. Joseph'}, {'authorId': '1694621', 'name': 'Michael I. Jordan'}, {'authorId': '1695576', 'name': 'J. Hellerstein'}, {'authorId': '49988044', 'name': 'Joseph E. Gonzalez'}, {'authorId': '144344283', 'name': 'Ken Goldberg'}, {'authorId': '38565890', 'name': 'A. Ghodsi'}, {'authorId': '4604030', 'name': 'David Culler'}, {'authorId': '1689992', 'name': 'P. Abbeel'}]",2017.0,arXiv.org,['AGI Safety Literature Review'],1,"With the increasing commoditization of computer vision, speech recognition and machine translation systems and the widespread deployment of learning-based back-end technologies such as digital advertising and intelligent infrastructures, AI (Artificial Intelligence) has moved from research labs to production. These changes have been made possible by unprecedented levels of data and computation, by methodological advances in machine learning, by innovations in systems software and architectures, and by the broad accessibility of these technologies. 
The next generation of AI systems promises to accelerate these developments and increasingly impact our lives via frequent interactions and making (often mission-critical) decisions on our behalf, often in highly personalized contexts. Realizing this promise, however, raises daunting challenges. In particular, we need AI systems that make timely and safe decisions in unpredictable environments, that are robust against sophisticated adversaries, and that can process ever increasing amounts of data across organizations and individuals without compromising confidentiality. These challenges will be exacerbated by the end of the Moore's Law, which will constrain the amount of data these technologies can store and process. In this paper, we propose several open research directions in systems, architectures, and security that can address these challenges and help unlock AI's potential to improve lives and society.",171.0
Impossibility of deducing preferences and rationality from human policy,11ca969b9c4f02bda7ce42f1bdfa491be4beceb1,"[{'authorId': '2054678912', 'name': 'S. Armstrong'}, {'authorId': '32777162', 'name': 'S. Mindermann'}]",2017.0,NIPS 2018,['AGI Safety Literature Review'],1,"Inverse reinforcement learning (IRL) attempts to infer human rewards or preferences from observed behavior. However, human planning systematically deviates from rationality. Though there has been some IRL work which assumes humans are noisily rational, there has been little analysis of the general problem of inferring the reward of a human of unknown rationality. The observed behavior can, in principle, be decomposed into two components: a reward function and a planning algorithm that maps reward function to policy. Both of these variables have to be inferred from behaviour. This paper presents a ""No Free Lunch"" theorem in this area, showing that, without making `normative' assumptions beyond the data, nothing about the human reward function can be deduced from human behaviour. Unlike most No Free Lunch theorems, this cannot be alleviated by regularising with simplicity assumptions. The simplest hypotheses are generally degenerate. The paper will then sketch how one might begin to use normative assumptions to get around the problem, without which solving the general IRL problem is impossible. The reward function-planning algorithm formalism can also be used to encode what it means for an agent to manipulate or override human preferences.",16.0
Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm,38fb1902c6a2ab4f767d4532b28a92473ea737aa,"[{'authorId': '145824029', 'name': 'David Silver'}, {'authorId': '2067208983', 'name': 'T. Hubert'}, {'authorId': '4337102', 'name': 'Julian Schrittwieser'}, {'authorId': '2460849', 'name': 'Ioannis Antonoglou'}, {'authorId': '40227832', 'name': 'Matthew Lai'}, {'authorId': '35099444', 'name': 'A. Guez'}, {'authorId': '1975889', 'name': 'Marc Lanctot'}, {'authorId': '2175946', 'name': 'L. Sifre'}, {'authorId': '2106164', 'name': 'D. Kumaran'}, {'authorId': '1686971', 'name': 'T. Graepel'}, {'authorId': '2542999', 'name': 'T. Lillicrap'}, {'authorId': '34838386', 'name': 'K. Simonyan'}, {'authorId': '48987704', 'name': 'D. Hassabis'}]",2017.0,arXiv.org,['AGI Safety Literature Review'],1,"The game of chess is the most widely-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. In contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go, by tabula rasa reinforcement learning from games of self-play. In this paper, we generalise this approach into a single AlphaZero algorithm that can achieve, tabula rasa, superhuman performance in many challenging domains. Starting from random play, and given no domain knowledge except the game rules, AlphaZero achieved within 24 hours a superhuman level of play in the games of chess and shogi (Japanese chess) as well as Go, and convincingly defeated a world-champion program in each case.",1207.0
Implementation of Moral Uncertainty in Intelligent Machines,93db0fdd2e54551adb476faf3c4c6c2b840268f8,"[{'authorId': '30556862', 'name': 'Kyle Bogosian'}]",2017.0,Minds and Machines,['AGI Safety Literature Review'],1,,29.0
AI Safety Gridworlds,d09bec5af4eef5038e48b26b6c14098f95997114,"[{'authorId': '2990741', 'name': 'J. Leike'}, {'authorId': '26890260', 'name': 'Miljan Martic'}, {'authorId': '2578985', 'name': 'Victoria Krakovna'}, {'authorId': '145981974', 'name': 'Pedro A. Ortega'}, {'authorId': '1868196', 'name': 'Tom Everitt'}, {'authorId': '8455031', 'name': 'Andrew Lefrancq'}, {'authorId': '1749270', 'name': 'Laurent Orseau'}, {'authorId': '34313265', 'name': 'S. Legg'}]",2017.0,arXiv.org,"['AGI Safety Literature Review', 'AI safety: state of the field through quantitative lens', 'AI Research Considerations for Human Existential Safety (ARCHES)']",3,"We present a suite of reinforcement learning environments illustrating various safety properties of intelligent agents. These problems include safe interruptibility, avoiding side effects, absent supervisor, reward gaming, safe exploration, as well as robustness to self-modification, distributional shift, and adversaries. To measure compliance with the intended safe behavior, we equip each environment with a performance function that is hidden from the agent. This allows us to categorize AI safety problems into robustness and specification problems, depending on whether the performance function corresponds to the observed reward function. We evaluate A2C and Rainbow, two recent deep reinforcement learning agents, on our environments and show that they are not able to solve them satisfactorily.",163.0
Good and safe uses of AI Oracles,4cbc5632026377476d55d8d851f4ba217b0b2615,"[{'authorId': '2054678912', 'name': 'S. Armstrong'}]",2017.0,arXiv.org,['AGI Safety Literature Review'],1,"It is possible that powerful and potentially dangerous artificial intelligence (AI) might be developed in the future. An Oracle is a design which aims to restrain the impact of a potentially dangerous AI by restricting the agent to no actions besides answering questions. Unfortunately, most Oracles will be motivated to gain more control over the world by manipulating users through the content of their answers, and Oracles of potentially high intelligence might be very successful at this \citep{DBLP:journals/corr/AlfonsecaCACAR16}. In this paper we present two designs for Oracles which, even under pessimistic assumptions, will not manipulate their users into releasing them and yet will still be incentivised to provide their users with helpful answers. The first design is the counterfactual Oracle -- which choses its answer as if it expected nobody to ever read it. The second design is the low-bandwidth Oracle -- which is limited by the quantity of information it can transmit.",22.0
"A Survey of Artificial General Intelligence Projects for Ethics, Risk, and Policy",8e6a7ecdb5a4b419d31b6b2225f07c56d4f71910,"[{'authorId': '2097800', 'name': 'S. Baum'}]",2017.0,,['AGI Safety Literature Review'],1,"Artificial general intelligence (AGI) is AI that can reason across a wide range of domains. It has long been considered the “grand dream” or “holy grail” of AI. It also poses major issues of ethics, risk, and policy due to its potential to transform society: if AGI is built, it could either help solve the world’s problems or cause major catastrophe, possibly even human extinction. This paper presents the first-ever survey of active AGI R&D projects in terms of ethics, risk, and policy. A thorough search identifies 45 projects of diverse sizes, nationalities, ethical goals, and other attributes. Most projects are either academic or corporate. The academic projects tend to express goals of advancing knowledge and are less likely to be active on AGI safety issues. The corporate projects tend to express goals of benefiting humanity and are more likely to be active on safety. Most projects are based in the US, and almost all are in either the US or a US ally, including all of the larger projects. This geographic concentration could simplify policymaking, though most projects publish open-source code, enabling contributions from anywhere in the world. These and other findings of the survey offer an empirical basis for the study of AGI R&D and a guide for policy and other action.",53.0
Inverse Reward Design,59094d64844ee21e32560fb08db6d53cc3af0c51,"[{'authorId': '1397904824', 'name': 'Dylan Hadfield-Menell'}, {'authorId': '3458938', 'name': 'S. Milli'}, {'authorId': '1689992', 'name': 'P. Abbeel'}, {'authorId': '145107462', 'name': 'Stuart J. Russell'}, {'authorId': '2745001', 'name': 'A. Dragan'}]",2017.0,NIPS,['AGI Safety Literature Review'],1,"Autonomous agents optimize the reward function we give them. What they don't know is how hard it is for us to design a reward function that actually captures what we want. When designing the reward, we might think of some specific training scenarios, and make sure that the reward will lead to the right behavior in those scenarios. Inevitably, agents encounter new scenarios (e.g., new types of terrain) where optimizing that same reward may lead to undesired behavior. Our insight is that reward functions are merely observations about what the designer actually wants, and that they should be interpreted in the context in which they were designed. We introduce inverse reward design (IRD) as the problem of inferring the true objective based on the designed reward and the training MDP. We introduce approximate methods for solving IRD problems, and use their solution to plan risk-averse behavior in test MDPs. Empirical results suggest that this approach can help alleviate negative side effects of misspecified reward functions and mitigate reward hacking.",260.0
How feasible is the rapid development of artificial superintelligence?,933c21f7886b6d4a91d51af7c7b0b8b215001cc3,"[{'authorId': '2821562', 'name': 'Kaj Sotala'}]",2017.0,,['AGI Safety Literature Review'],1,"What kinds of fundamental limits are there in how capable artificial intelligence (AI) systems might become? Two questions in particular are of interest: (1) How much more capable could AI become relative to humans, and (2) how easily could superhuman capability be acquired? To answer these questions, we will consider the literature on human expertise and intelligence, discuss its relevance for AI, and consider how AI could improve on humans in two major aspects of thought and expertise, namely simulation and pattern recognition. We find that although there are very real limits to prediction, it seems like AI could still substantially improve on human intelligence.",10.0
Mastering the game of Go without human knowledge,c27db32efa8137cbf654902f8f728f338e55cd1c,"[{'authorId': '145824029', 'name': 'David Silver'}, {'authorId': '4337102', 'name': 'Julian Schrittwieser'}, {'authorId': '34838386', 'name': 'K. Simonyan'}, {'authorId': '2460849', 'name': 'Ioannis Antonoglou'}, {'authorId': '1885349', 'name': 'Aja Huang'}, {'authorId': '35099444', 'name': 'A. Guez'}, {'authorId': '2067208983', 'name': 'T. Hubert'}, {'authorId': '2067596385', 'name': 'Lucas baker'}, {'authorId': '40227832', 'name': 'Matthew Lai'}, {'authorId': '34848283', 'name': 'A. Bolton'}, {'authorId': '2275897', 'name': 'Yutian Chen'}, {'authorId': '2542999', 'name': 'T. Lillicrap'}, {'authorId': '2059412590', 'name': 'Fan Hui'}, {'authorId': '2175946', 'name': 'L. Sifre'}, {'authorId': '47568983', 'name': 'George van den Driessche'}, {'authorId': '1686971', 'name': 'T. Graepel'}, {'authorId': '48987704', 'name': 'D. Hassabis'}]",2017.0,Nature,['AGI Safety Literature Review'],1,,7124.0
Growing the artificial intelligence industry in the UK,7f904b3896c21b984f2058fc7c5efb673097bf7f,"[{'authorId': '143786158', 'name': 'W. Hall'}, {'authorId': '94426391', 'name': 'J. Pesenti'}]",2017.0,,['AGI Safety Literature Review'],1,,135.0
Functional Decision Theory: A New Theory of Instrumental Rationality,61c368138b0211323e8773174ac3132122da07ef,"[{'authorId': '2542795', 'name': 'Eliezer Yudkowsky'}, {'authorId': '1719968', 'name': 'N. Soares'}]",2017.0,arXiv.org,['AGI Safety Literature Review'],1,"This paper describes and motivates a new decision theory known as functional decision theory (FDT), as distinct from causal decision theory and evidential decision theory. Functional decision theorists hold that the normative principle for action is to treat one's decision as the output of a fixed mathematical function that answers the question, ""Which output of this very function would yield the best outcome?"" Adhering to this principle delivers a number of benefits, including the ability to maximize wealth in an array of traditional decision-theoretic and game-theoretic problems where CDT and EDT perform poorly. Using one simple and coherent decision rule, functional decision theorists (for example) achieve more utility than CDT on Newcomb's problem, more utility than EDT on the smoking lesion problem, and more utility than both in Parfit's hitchhiker problem. In this paper, we define FDT, explore its prescriptions in a number of different decision problems, compare it to CDT and EDT, and give philosophical justifications for FDT as a normative theory of decision-making.",41.0
Thinking Fast and Slow,8165d6217a2f623f7d9e613c791e94102921cd3b,"[{'authorId': '48360624', 'name': 'Gautam Goel'}, {'authorId': '3255594', 'name': 'Niangjun Chen'}, {'authorId': '1706859', 'name': 'A. Wierman'}]",2017.0,Sigmetrics Performance Evaluation Review,['AGI Safety Literature Review'],1,"Many real-world control systems, such as the smart grid and software defined networks, have decentralized components that react quickly using local information and centralized components that react slowly using a more global view. This work seeks to provide a theoretical framework for how to design controllers that are decomposed across timescales in this way. The framework is analogous to how the network utility maximization framework uses optimization decomposition to distribute a global control problem across independent controllers, each of which solves a local problem; except our goal is to decompose a global problem temporally, extracting a timescale separation. Our results highlight that decomposition of a multi-timescale controller into a fast timescale, reactive controller and a slow timescale, predictive controller can be near-optimal in a strong sense. In particular, we exhibit such a design, named Multi-timescale Reflexive Predictive Control (MRPC), which maintains a per-timestep cost within a constant factor of the offline optimal in an adversarial setting.",4155.0
Learning a commonsense moral theory,2ff51d3371cd8be38e47ea188fb8db3c91160af2,"[{'authorId': '1390033240', 'name': 'Max Kleiman-Weiner'}, {'authorId': '2276622', 'name': 'R. Saxe'}, {'authorId': '1763295', 'name': 'J. Tenenbaum'}]",2017.0,Cognition,['AGI Safety Literature Review'],1,,53.0
Analyzing Hidden Representations in End-to-End Automatic Speech Recognition Systems,2f72a8bfcf198471255607b4fcb7e420a73b3400,"[{'authorId': '2083259', 'name': 'Yonatan Belinkov'}, {'authorId': '145898106', 'name': 'James R. Glass'}]",2017.0,NIPS,['AGI Safety Literature Review'],1,"Neural models have become ubiquitous in automatic speech recognition systems. While neural networks are typically used as acoustic models in more complex systems, recent studies have explored end-to-end speech recognition systems based on neural networks, which can be trained to directly predict text from input acoustic features. Although such systems are conceptually elegant and simpler than traditional systems, it is less obvious how to interpret the trained models. In this work, we analyze the speech representations learned by a deep end-to-end model that is based on convolutional and recurrent layers, and trained with a connectionist temporal classification (CTC) loss. We use a pre-trained model to generate frame-level features which are given to a classifier that is trained on frame classification into phones. We evaluate representations from different layers of the deep model and compare their quality for predicting phone labels. Our experiments shed light on important aspects of the end-to-end model such as layer depth, model complexity, and other design choices.",62.0
A Game-Theoretic Analysis of the Off-Switch Game,002050388cdb39d6413ce48230496e8ccc08df04,"[{'authorId': '22708544', 'name': 'Tobias Wängberg'}, {'authorId': '22708729', 'name': 'Mikael Böörs'}, {'authorId': '22574075', 'name': 'Elliot Catt'}, {'authorId': '1868196', 'name': 'Tom Everitt'}, {'authorId': '144154444', 'name': 'Marcus Hutter'}]",2017.0,Artificial General Intelligence,['AGI Safety Literature Review'],1,,9.0
"Robust Computer Algebra, Theorem Proving, and Oracle AI",daccf0057ec212399f53894be6ff5f33376cf541,"[{'authorId': '3451324', 'name': 'G. Sarma'}, {'authorId': '2934247', 'name': 'Nick J. Hay'}]",2017.0,Informatica,['AGI Safety Literature Review'],1,"In the context of superintelligent AI systems, the term “oracle” has two meanings. One refers to modular systems queried for domain-specific tasks. Another usage, referring to a class of systems which may be useful for addressing the value alignment and AI control problems, is a superintelligent AI system that only answers questions. The aim of this manuscript is to survey contemporary research problems related to oracles which align with long-term research goals of AI safety. We examine existing question answering systems and argue that their high degree of architectural heterogeneity makes them poor candidates for rigorous analysis as oracles. On the other hand, we identify computer algebra systems (CASs) as being primitive examples of domain-specific oracles for mathematics and argue that efforts to integrate computer algebra systems with theorem provers, systems which have largely been developed independent of one another, provide a concrete set of problems related to the notion of provable safety that has emerged in the AI safety community. We review approaches to interfacing CASs with theorem provers, describe well-defined architectural deficiencies that have been identified with CASs, and suggest possible lines of research and practical software projects for scientists interested in AI safety.",2.0
Robust Physical-World Attacks on Deep Learning Models,d295a620fc10a7a656dc693e1b1bf668d1508a8e,"[{'authorId': '22229139', 'name': 'I. Evtimov'}, {'authorId': '1825256', 'name': 'Kevin Eykholt'}, {'authorId': '35064352', 'name': 'Earlence Fernandes'}, {'authorId': '1769675', 'name': 'Tadayoshi Kohno'}, {'authorId': '2165245296', 'name': 'Bo Li'}, {'authorId': '49428285', 'name': 'Atul Prakash'}, {'authorId': '145416145', 'name': 'Amir Rahmati'}, {'authorId': '143711382', 'name': 'D. Song'}]",2017.0,,['AGI Safety Literature Review'],1,"Recent studies show that the state-of-the-art deep neural networks (DNNs) are vulnerable to adversarial examples, resulting from small-magnitude perturbations added to the input. Given that that emerging physical systems are using DNNs in safety-critical situations, adversarial examples could mislead these systems and cause dangerous situations.Therefore, understanding adversarial examples in the physical world is an important step towards developing resilient learning algorithms. We propose a general attack algorithm,Robust Physical Perturbations (RP2), to generate robust visual adversarial perturbations under different physical conditions. Using the real-world case of road sign classification, we show that adversarial examples generated using RP2 achieve high targeted misclassification rates against standard-architecture road sign classifiers in the physical world under various environmental conditions, including viewpoints. Due to the current lack of a standardized testing method, we propose a two-stage evaluation methodology for robust physical adversarial examples consisting of lab and field tests. Using this methodology, we evaluate the efficacy of physical adversarial manipulations on real objects. Witha perturbation in the form of only black and white stickers,we attack a real stop sign, causing targeted misclassification in 100% of the images obtained in lab settings, and in 84.8%of the captured video frames obtained on a moving vehicle(field test) for the target classifier.",511.0
A Formal Approach to the Problem of Logical Non-Omniscience,ea7fe17b98426eb2400dfe8c973f3a69afbe96be,"[{'authorId': '1740494', 'name': 'Scott Garrabrant'}, {'authorId': '1409258422', 'name': 'Tsvi Benson-Tilsen'}, {'authorId': '2651789', 'name': 'Andrew Critch'}, {'authorId': '1719968', 'name': 'N. Soares'}, {'authorId': '144364160', 'name': 'Jessica Taylor'}]",2017.0,Theoretical Aspects of Rationality and Knowledge,['AGI Safety Literature Review'],1,"We present the logical induction criterion for computable algorithms that assign probabilities to every logical statement in a given formal language, and refine those probabilities over time. The criterion is motivated by a series of stock trading analogies. Roughly speaking, each logical sentence phi is associated with a stock that is worth $1 per share if phi is true and nothing otherwise, and we interpret the belief-state of a logically uncertain reasoner as a set of market prices, where pt_N(phi)=50% means that on day N, shares of phi may be bought or sold from the reasoner for 50%. A market is then called a logical inductor if (very roughly) there is no polynomial-time computable trading strategy with finite risk tolerance that earns unbounded profits in that market over time. We then describe how this single criterion implies a number of desirable properties of bounded reasoners; for example, logical inductors outpace their underlying deductive process, perform universal empirical induction given enough time to think, and place strong trust in their own reasoning process.",8.0
Anthropomorphic reasoning about neuromorphic AGI safety,a51a7b9163546eb1476c5a62a03e1ddf4256859c,"[{'authorId': '2643853', 'name': 'David J. Jilk'}, {'authorId': '34884847', 'name': 'Seth Herd'}, {'authorId': '144940159', 'name': 'S. Read'}, {'authorId': '1390067049', 'name': 'R. O’Reilly'}]",2017.0,Journal of experimental and theoretical artificial intelligence (Print),['AGI Safety Literature Review'],1,"Abstract One candidate approach to creating artificial general intelligence (AGI) is to imitate the essential computations of human cognition. This process is sometimes called ‘reverse-engineering the brain’ and the end product called ‘neuromorphic.’ We argue that, unlike with other approaches to AGI, anthropomorphic reasoning about behaviour and safety concerns is appropriate and crucial in a neuromorphic context. Using such reasoning, we offer some initial ideas to make neuromorphic AGI safer. In particular, we explore how basic drives that promote social interaction may be essential to the development of cognitive capabilities as well as serving as a focal point for human-friendly outcomes.",5.0
A causal framework for explaining the predictions of black-box sequence-to-sequence models,6b5cb359a6a6c2a3d79ade3a825eb3e9a65f3a89,"[{'authorId': '1390096054', 'name': 'David Alvarez-Melis'}, {'authorId': '35132120', 'name': 'T. Jaakkola'}]",2017.0,Conference on Empirical Methods in Natural Language Processing,['AGI Safety Literature Review'],1,"We interpret the predictions of any black-box structured input-structured output model around a specific input-output pair. Our method returns an “explanation” consisting of groups of input-output tokens that are causally related. These dependencies are inferred by querying the model with perturbed inputs, generating a graph over tokens from the responses, and solving a partitioning problem to select the most relevant components. We focus the general approach on sequence-to-sequence problems, adopting a variational autoencoder to yield meaningful input perturbations. We test our method across several NLP sequence generation tasks.",163.0
Deep Reinforcement Learning from Human Preferences,5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd,"[{'authorId': '145791315', 'name': 'P. Christiano'}, {'authorId': '2990741', 'name': 'J. Leike'}, {'authorId': '31035595', 'name': 'Tom B. Brown'}, {'authorId': '26890260', 'name': 'Miljan Martic'}, {'authorId': '34313265', 'name': 'S. Legg'}, {'authorId': '2698777', 'name': 'Dario Amodei'}]",2017.0,NIPS,"['AGI Safety Literature Review', 'AI Research Considerations for Human Existential Safety (ARCHES)']",2,"For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than one percent of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any that have been previously learned from human feedback.",772.0
Low Impact Artificial Intelligences,be7853734c9d396c52be2d7cdfdcb7b024a16860,"[{'authorId': '2054678912', 'name': 'S. Armstrong'}, {'authorId': '16304453', 'name': 'B. Levinstein'}]",2017.0,arXiv.org,['AGI Safety Literature Review'],1,"There are many goals for an AI that could become dangerous if the AI becomes superintelligent or otherwise powerful. Much work on the AI control problem has been focused on constructing AI goals that are safe even for such AIs. This paper looks at an alternative approach: defining a general concept of `low impact'. The aim is to ensure that a powerful AI which implements low impact will not modify the world extensively, even if it is given a simple or dangerous goal. The paper proposes various ways of defining and grounding low impact, and discusses methods for ensuring that the AI can still be allowed to have a (desired) impact despite the restriction. The end of the paper addresses known issues with this approach and avenues for future research.",30.0
Should Robots be Obedient?,0f9e4431e844cece73223f521e6d8e4a812b0ef1,"[{'authorId': '3458938', 'name': 'S. Milli'}, {'authorId': '1397904824', 'name': 'Dylan Hadfield-Menell'}, {'authorId': '2745001', 'name': 'A. Dragan'}, {'authorId': '145107462', 'name': 'Stuart J. Russell'}]",2017.0,International Joint Conference on Artificial Intelligence,"['AGI Safety Literature Review', 'AI Research Considerations for Human Existential Safety (ARCHES)']",2,"Intuitively, obedience -- following the order that a human gives -- seems like a good property for a robot to have. But, we humans are not perfect and we may give orders that are not best aligned to our preferences. We show that when a human is not perfectly rational then a robot that tries to infer and act according to the human's underlying preferences can always perform better than a robot that simply follows the human's literal order. Thus, there is a tradeoff between the obedience of a robot and the value it can attain for its owner. We investigate how this tradeoff is impacted by the way the robot infers the human's preferences, showing that some methods err more on the side of obedience than others. We then analyze how performance degrades when the robot has a misspecified model of the features that the human cares about or the level of rationality of the human. Finally, we study how robots can start detecting such model misspecification. Overall, our work suggests that there might be a middle ground in which robots intelligently decide when to obey human orders, but err on the side of obedience.",40.0
Combating Deep Reinforcement Learning's Sisyphean Curse with Intrinsic Fear,5e0f2f82b4a28d59f1aa8b8ffe497790de1cdf9d,"[{'authorId': '32219137', 'name': 'Zachary Chase Lipton'}, {'authorId': '1800422', 'name': 'Jianfeng Gao'}, {'authorId': '47681372', 'name': 'Lihong Li'}, {'authorId': '1720246', 'name': 'Jianshu Chen'}, {'authorId': '144718788', 'name': 'L. Deng'}]",2017.0,,['AGI Safety Literature Review'],1,"To use deep reinforcement learning in the wild, we might hope for an agent that can avoid catastrophic mistakes. Unfortunately, even in simple environments, the popular deep Q-network (DQN) algorithm is doomed by a Sisyphean curse. Owing to the use of function approximation, these agents eventually forget experiences as they become exceedingly unlikely under a new policy. Consequently, for as long as they continue to train, DQNs may periodically relive catastrophic mistakes. Many real-world environments where people might be injured exhibit a special structure. We know a priori that catastrophes are not only bad, but that agents need not ever get near to a catastrophe state. In this paper, we exploit this structure to learn a reward-shaping that accelerates learning and guards oscillating policies against repeated catastrophes. First, we demonstrate unacceptable performance of DQNs on two toy problems. We then introduce intrinsic fear, a new method that mitigates these problems by avoiding dangerous states. Our approach incorporates a second model trained via supervised learning to predict the probability of catastrophe within a short number of steps. This score then acts to penalize the Q-learning objective, shaping the reward function away from catastrophic states.",15.0
Reluplex: An Efficient SMT Solver for Verifying Deep Neural Networks,b0dc598adda48acab590f95a5985fcc7abf2aca9,"[{'authorId': '40348902', 'name': 'Guy Katz'}, {'authorId': '1680661', 'name': 'Clark W. Barrett'}, {'authorId': '1699040', 'name': 'D. Dill'}, {'authorId': '145533911', 'name': 'Kyle D. Julian'}, {'authorId': '2275756', 'name': 'Mykel J. Kochenderfer'}]",2017.0,International Conference on Computer Aided Verification,"['AGI Safety Literature Review', 'AI Research Considerations for Human Existential Safety (ARCHES)']",2,,1414.0
Stoic Ethics for Artificial Agents,84336629aa32259e3ec46beb231de546ef2a0ee0,"[{'authorId': '2358515', 'name': 'Gabriel Murray'}]",2017.0,Canadian Conference on AI,['AGI Safety Literature Review'],1,,6.0
"Superintelligence: Paths, Dangers, Strategies",e0fe9b2f77288bc5e6f778611a49e62e98231f8c,"[{'authorId': '145155783', 'name': 'C. Robert'}]",2017.0,,"['AGI Safety Literature Review', 'AI Research Considerations for Human Existential Safety (ARCHES)']",2,,707.0
The Singularity May Never Be Near,77e08e2b73a36c5f25b82c968b5b09d4e1ac34af,"[{'authorId': '1733716', 'name': 'T. Walsh'}]",2016.0,The AI Magazine,['AGI Safety Literature Review'],1,"There is both much optimism and pessimism around artificial intelligence (AI) today. The optimists are investing millions of dollars, and even in some cases billions of dollars into AI. The pessimists, on the other hand, predict that AI will end many things: jobs, warfare, and even the human race. Both the optimists and the pessimists often appeal to the idea of a technological singularity, a point in time where machine intelligence starts to run away, and a new, more intelligent species starts to inhabit the earth. If the optimists are right, this will be a moment that fundamentally changes our economy and our society. If the pessimists are right, this will be a moment that also fundamentally changes our economy and our society. It is therefore very worthwhile spending some time deciding if either of them might be right.",19.0
Open ended intelligence: the individuation of intelligent agents,1db570be1ec45b3a07f329b75a8f8dfa6388f543,"[{'authorId': '39960619', 'name': 'David R. Weinbaum'}, {'authorId': '2297381', 'name': 'V. Veitas'}]",2015.0,Journal of experimental and theoretical artificial intelligence (Print),['AGI Safety Literature Review'],1,"Artificial general intelligence is a field of research aiming to distil the principles of intelligence that operate independently of a specific problem domain and utilise these principles in order to synthesise systems capable of performing any intellectual task a human being is capable of and beyond. While “narrow” artificial intelligence which focuses on solving specific problems such as speech recognition, text comprehension, visual pattern recognition and robotic motion has shown impressive breakthroughs lately, understanding general intelligence remains elusive. We propose a paradigm shift from intelligence perceived as a competence of individual agents defined in relation to an a priori given problem domain or a goal, to intelligence perceived as a formative process of self-organisation. We call this process open-ended intelligence. Starting with a brief introduction of the current conceptual approach, we expose a number of serious limitations that are traced back to the ontological roots of the concept of intelligence. Open-ended intelligence is then developed as an abstraction of the process of human cognitive development, so its application can be extended to general agents and systems. We introduce and discuss three facets of the idea: the philosophical concept of individuation, sense-making and the individuation of general cognitive agents. We further show how open-ended intelligence can be framed in terms of a distributed, self-organising network of interacting elements and how such process is scalable. The framework highlights an important relation between coordination and intelligence and a new understanding of values.",31.0
Agent Foundations for Aligning Machine Intelligence with Human Interests: A Technical Research Agenda,88fbf0a6ab0ce2329e3c9aca0d6f80cf405e207a,"[{'authorId': '1719968', 'name': 'N. Soares'}, {'authorId': '3442559', 'name': 'Benya Fallenstein'}]",2017.0,,"['AGI Safety Literature Review', 'AI safety: state of the field through quantitative lens']",2,,46.0
Nonparametric General Reinforcement Learning,7e66171492a39a28cbcefd7309fc8863d3954f0a,"[{'authorId': '2990741', 'name': 'J. Leike'}]",2016.0,arXiv.org,['AGI Safety Literature Review'],1,"Reinforcement learning (RL) problems are often phrased in terms of Markov decision processes (MDPs). In this thesis we go beyond MDPs and consider RL in environments that are non-Markovian, non-ergodic and only partially observable. Our focus is not on practical algorithms, but rather on the fundamental underlying problems: How do we balance exploration and exploitation? How do we explore optimally? When is an agent optimal? We follow the nonparametric realizable paradigm. 
We establish negative results on Bayesian RL agents, in particular AIXI. We show that unlucky or adversarial choices of the prior cause the agent to misbehave drastically. Therefore Legg-Hutter intelligence and balanced Pareto optimality, which depend crucially on the choice of the prior, are entirely subjective. Moreover, in the class of all computable environments every policy is Pareto optimal. This undermines all existing optimality properties for AIXI. However, there are Bayesian approaches to general RL that satisfy objective optimality guarantees: We prove that Thompson sampling is asymptotically optimal in stochastic environments in the sense that its value converges to the value of the optimal policy. We connect asymptotic optimality to regret given a recoverability assumption on the environment that allows the agent to recover from mistakes. Hence Thompson sampling achieves sublinear regret in these environments. 
Our results culminate in a formal solution to the grain of truth problem: A Bayesian agent acting in a multi-agent environment learns to predict the other agents' policies if its prior assigns positive probability to them (the prior contains a grain of truth). We construct a large but limit computable class containing a grain of truth and show that agents based on Thompson sampling over this class converge to play Nash equilibria in arbitrary unknown computable multi-agent environments.",18.0
Stifling artificial intelligence: Human perils,cff5cc26544f7ac057da867d7335ce3c4b9af461,"[{'authorId': '70271263', 'name': 'G. Gürkaynak'}, {'authorId': '24315433', 'name': 'Ilay Yilmaz'}, {'authorId': '117949642', 'name': 'Gunes Haksever'}]",2016.0,Computer Law and Security Review,['AGI Safety Literature Review'],1,,70.0
Logical Induction,1e390c5807fed93b2d89b708adbfabfc165b0299,"[{'authorId': '1740494', 'name': 'Scott Garrabrant'}, {'authorId': '1409258422', 'name': 'Tsvi Benson-Tilsen'}, {'authorId': '2651789', 'name': 'Andrew Critch'}, {'authorId': '1719968', 'name': 'N. Soares'}, {'authorId': '144364160', 'name': 'Jessica Taylor'}]",2016.0,Electron. Colloquium Comput. Complex.,"['AGI Safety Literature Review', 'AI Research Considerations for Human Existential Safety (ARCHES)']",2,"We present a computable algorithm that assigns probabilities to every logical statement in a given formal language, and refines those probabilities over time. For instance, if the language is Peano arithmetic, it assigns probabilities to all arithmetical statements, including claims about the twin prime conjecture, the outputs of long-running computations, and its own probabilities. We show that our algorithm, an instance of what we call a logical inductor, satisfies a number of intuitive desiderata, including: (1) it learns to predict patterns of truth and falsehood in logical statements, often long before having the resources to evaluate the statements, so long as the patterns can be written down in polynomial time; (2) it learns to use appropriate statistical summaries to predict sequences of statements whose truth values appear pseudorandom; and (3) it learns to have accurate beliefs about its own current beliefs, in a manner that avoids the standard paradoxes of self-reference. For example, if a given computer program only ever produces outputs in a certain range, a logical inductor learns this fact in a timely manner; and if late digits in the decimal expansion of π are difficult to predict, then a logical inductor learns to assign ≈ 10% probability to “the nth digit of π is a 7” for large n. Logical inductors also learn to trust their future beliefs more than their current beliefs, and their beliefs are coherent in the limit (whenever φ → ψ, P∞(φ) ≤ P∞(ψ), and so on); and logical inductors strictly dominate the universal semimeasure in the limit. These properties and many others all follow from a single logical induction criterion, which is motivated by a series of stock trading analogies. Roughly speaking, each logical sentence φ is associated with a stock that is worth $1 per share if φ is true and nothing otherwise, and we interpret the belief-state of a logically uncertain reasoner as a set of market prices, where Pn(φ) = 50% means that on day n, shares of φ may be bought or sold from the reasoner for 50¢. The logical induction criterion says (very roughly) that there should not be any polynomial-time computable trading strategy with finite risk tolerance that earns unbounded profits in that market over time. This criterion bears strong resemblance to the “no Dutch book” criteria that support both expected utility theory (von Neumann and Morgenstern 1944) and Bayesian probability theory (Ramsey 1931; de Finetti 1937).",24.0
A Formal Solution to the Grain of Truth Problem,cd868884ad4dfaf12b5b8e140d696ff0b31f5dcb,"[{'authorId': '2990741', 'name': 'J. Leike'}, {'authorId': '144364160', 'name': 'Jessica Taylor'}, {'authorId': '3442559', 'name': 'Benya Fallenstein'}]",2016.0,Conference on Uncertainty in Artificial Intelligence,['AGI Safety Literature Review'],1,"A Bayesian agent acting in a multi-agent environment learns to predict the other agents' policies if its prior assigns positive probability to them (in other words, its prior contains a \emph{grain of truth}). Finding a reasonably large class of policies that contains the Bayes-optimal policies with respect to this class is known as the \emph{grain of truth problem}. Only small classes are known to have a grain of truth and the literature contains several related impossibility results. In this paper we present a formal and general solution to the full grain of truth problem: we construct a class of policies that contains all computable policies as well as Bayes-optimal policies for every lower semicomputable prior over the class. When the environment is unknown, Bayes-optimal agents may fail to act optimally even asymptotically. However, agents based on Thompson sampling converge to play {\epsilon}-Nash equilibria in arbitrary unknown computable multi-agent environments. While these results are purely theoretical, we show that they can be computationally approximated arbitrarily closely.",14.0
Safely Interruptible Agents,ac70bb2458f01a9e47fc1afe0dd478fb2feb8f50,"[{'authorId': '1749270', 'name': 'Laurent Orseau'}, {'authorId': '2054678912', 'name': 'S. Armstrong'}]",2016.0,Conference on Uncertainty in Artificial Intelligence,['AGI Safety Literature Review'],1,"Reinforcement learning agents interacting with a complex environment like the real world are unlikely to behave optimally all the time. If such an agent is operating in real-time under human supervision, now and then it may be necessary for a human operator to press the big red button to prevent the agent from continuing a harmful sequence of actions—harmful either for the agent or for the environment—and lead the agent into a safer situation. However, if the learning agent expects to receive rewards from this sequence, it may learn in the long run to avoid such interruptions, for example by disabling the red button— which is an undesirable outcome. This paper explores a way to make sure a learning agent will not learn to prevent (or seek!) being interrupted by the environment or a human operator. We provide a formal definition of safe interruptibility and exploit the off-policy learning property to prove that either some agents are already safely interruptible, like Q-learning, or can easily be made so, like Sarsa. We show that even ideal, uncomputable reinforcement learning agents for (deterministic) general computable environments can be made safely interruptible.",99.0
Concrete Problems in AI Safety,e86f71ca2948d17b003a5f068db1ecb2b77827f7,"[{'authorId': '2698777', 'name': 'Dario Amodei'}, {'authorId': '37232298', 'name': 'C. Olah'}, {'authorId': '5164568', 'name': 'J. Steinhardt'}, {'authorId': '145791315', 'name': 'P. Christiano'}, {'authorId': '47971768', 'name': 'J. Schulman'}, {'authorId': '30415265', 'name': 'Dandelion Mané'}]",2016.0,arXiv.org,"['AGI Safety Literature Review', 'Unsolved Problems in ML Safety', 'AI safety: state of the field through quantitative lens', 'AI Research Considerations for Human Existential Safety (ARCHES)']",4,"Rapid progress in machine learning and artificial intelligence (AI) has brought increasing attention to the potential impacts of AI technologies on society. In this paper we discuss one such potential impact: the problem of accidents in machine learning systems, defined as unintended and harmful behavior that may emerge from poor design of real-world AI systems. We present a list of five practical research problems related to accident risk, categorized according to whether the problem originates from having the wrong objective function (""avoiding side effects"" and ""avoiding reward hacking""), an objective function that is too expensive to evaluate frequently (""scalable supervision""), or undesirable behavior during the learning process (""safe exploration"" and ""distributional shift""). We review previous work in these areas as well as suggesting research directions with a focus on relevance to cutting-edge AI systems. Finally, we consider the high-level question of how to think most productively about the safety of forward-looking applications of AI.",1503.0
Rationalizing Neural Predictions,467d5d8fc766e73bfd3e9415f75479823f92c2f7,"[{'authorId': '49986267', 'name': 'Tao Lei'}, {'authorId': '1741283', 'name': 'R. Barzilay'}, {'authorId': '35132120', 'name': 'T. Jaakkola'}]",2016.0,Conference on Empirical Methods in Natural Language Processing,['AGI Safety Literature Review'],1,"Prediction without justification has limited applicability. As a remedy, we learn to extract pieces of input text as justifications -- rationales -- that are tailored to be short and coherent, yet sufficient for making the same prediction. Our approach combines two modular components, generator and encoder, which are trained to operate well together. The generator specifies a distribution over text fragments as candidate rationales and these are passed through the encoder for prediction. Rationales are never given during training. Instead, the model is regularized by desiderata for rationales. We evaluate the approach on multi-aspect sentiment analysis against manually annotated test cases. Our approach outperforms attention-based baseline by a significant margin. We also successfully illustrate the method on the question retrieval task.",639.0
Death and Suicide in Universal Artificial Intelligence,d9c6562a197222cf0e054cda6c943a1b990845b5,"[{'authorId': '2110111518', 'name': 'Jarryd Martin'}, {'authorId': '1868196', 'name': 'Tom Everitt'}, {'authorId': '144154444', 'name': 'Marcus Hutter'}]",2016.0,Artificial General Intelligence,['AGI Safety Literature Review'],1,,18.0
Cooperative Inverse Reinforcement Learning,1e6abd43fcb157fde4d4ddc3ac8787ae45dbf777,"[{'authorId': '1397904824', 'name': 'Dylan Hadfield-Menell'}, {'authorId': '145107462', 'name': 'Stuart J. Russell'}, {'authorId': '1689992', 'name': 'P. Abbeel'}, {'authorId': '2745001', 'name': 'A. Dragan'}]",2016.0,NIPS,"['AGI Safety Literature Review', 'Unsolved Problems in ML Safety', 'AI Research Considerations for Human Existential Safety (ARCHES)']",3,"For an autonomous system to be helpful to humans and to pose no unwarranted risks, it needs to align its values with those of the humans in its environment in such a way that its actions contribute to the maximization of value for the humans. We propose a formal definition of the value alignment problem as cooperative inverse reinforcement learning (CIRL). A CIRL problem is a cooperative, partial-information game with two agents, human and robot; both are rewarded according to the human's reward function, but the robot does not initially know what this is. In contrast to classical IRL, where the human is assumed to act optimally in isolation, optimal CIRL solutions produce behaviors such as active teaching, active learning, and communicative actions that are more effective in achieving value alignment. We show that computing optimal joint policies in CIRL games can be reduced to solving a POMDP, prove that optimality in isolation is suboptimal in CIRL, and derive an approximate CIRL algorithm.",449.0
Should We Fear Supersmart Robots?,64c7ba4096caaf3c577ff438f4313f0bc32a9588,"[{'authorId': '2055581993', 'name': 'Stuart Russell'}]",2016.0,Scientific American,['AGI Safety Literature Review'],1,,42.0
Self-Modification of Policy and Utility Function in Rational Agents,d72fd1af6d31ceb7a46b1cd70822549c05a83151,"[{'authorId': '1868196', 'name': 'Tom Everitt'}, {'authorId': '3393209', 'name': 'Daniel Filan'}, {'authorId': '2842216', 'name': 'Mayank Daswani'}, {'authorId': '144154444', 'name': 'Marcus Hutter'}]",2016.0,Artificial General Intelligence,['AGI Safety Literature Review'],1,,25.0
End to End Learning for Self-Driving Cars,0e3cc46583217ec81e87045a4f9ae3478a008227,"[{'authorId': '2065365958', 'name': 'Mariusz Bojarski'}, {'authorId': '47091489', 'name': 'D. Testa'}, {'authorId': '3393305', 'name': 'Daniel Dworakowski'}, {'authorId': '2372758', 'name': 'Bernhard Firner'}, {'authorId': '2286388', 'name': 'B. Flepp'}, {'authorId': '38774604', 'name': 'Prasoon Goyal'}, {'authorId': '2065130000', 'name': 'L. Jackel'}, {'authorId': '95743023', 'name': 'Mathew Monfort'}, {'authorId': '145636949', 'name': 'Urs Muller'}, {'authorId': '3393542', 'name': 'Jiakai Zhang'}, {'authorId': None, 'name': 'Xin Zhang'}, {'authorId': '2109914894', 'name': 'Jake Zhao'}, {'authorId': '3262926', 'name': 'Karol Zieba'}]",2016.0,arXiv.org,"['AGI Safety Literature Review', 'AI Research Considerations for Human Existential Safety (ARCHES)']",2,"We trained a convolutional neural network (CNN) to map raw pixels from a single front-facing camera directly to steering commands. This end-to-end approach proved surprisingly powerful. With minimum training data from humans the system learns to drive in traffic on local roads with or without lane markings and on highways. It also operates in areas with unclear visual guidance such as in parking lots and on unpaved roads. 
The system automatically learns internal representations of the necessary processing steps such as detecting useful road features with only the human steering angle as the training signal. We never explicitly trained it to detect, for example, the outline of roads. 
Compared to explicit decomposition of the problem, such as lane marking detection, path planning, and control, our end-to-end system optimizes all processing steps simultaneously. We argue that this will eventually lead to better performance and smaller systems. Better performance will result because the internal components self-optimize to maximize overall system performance, instead of optimizing human-selected intermediate criteria, e.g., lane detection. Such criteria understandably are selected for ease of human interpretation which doesn't automatically guarantee maximum system performance. Smaller networks are possible because the system learns to solve the problem with the minimal number of processing steps. 
We used an NVIDIA DevBox and Torch 7 for training and an NVIDIA DRIVE(TM) PX self-driving car computer also running Torch 7 for determining where to drive. The system operates at 30 frames per second (FPS).",3214.0
Defining Human Values for Value Learners,d19fd5a2a59d735986af101a4526e898cbdf41cd,"[{'authorId': '2821562', 'name': 'Kaj Sotala'}]",2016.0,"AAAI Workshop: AI, Ethics, and Society",['AGI Safety Literature Review'],1,"Hypothetical “value learning” AIs learn human values and then try to act according to those values. The design of such AIs, however, is hampered by the fact that there exists no satisfactory definition of what exactly human values are. After arguing that the standard concept of preference is insufficient as a definition, I draw on reinforcement learning theory, emotion research, and moral psychology to offer an alternative definition. In this definition, human values are conceptualized as mental representations that encode the brain’s value function (in the reinforcement learning sense) by being imbued with a context-sensitive affective gloss. I finish with a discussion of the implications that this hypothesis has on the design of value learners.",19.0
Quantilizers: A Safer Alternative to Maximizers for Limited Optimization,4e8ff3b4069a12a00196d62925bab8add7389742,"[{'authorId': '144364160', 'name': 'Jessica Taylor'}]",2016.0,"AAAI Workshop: AI, Ethics, and Society","['AGI Safety Literature Review', 'AI Research Considerations for Human Existential Safety (ARCHES)']",2,"In the field of AI, expected utility maximizers are commonly used as a model for idealized agents. However, expected utility maximization can lead to unintended solutions when the utility function does not quantify everything the operators care about: imagine, for example, an expected utility maximizer tasked with winning money on the stock market, which has no regard for whether it accidentally causes a market crash. Once AI systems become sufficiently intelligent and powerful, these unintended solutions could become quite dangerous. In this paper, we describe an alternative to expected utility maximization for powerful AI systems, which we call expected utility quantilization. This could allow the construction of AI systems that do not necessarily fall into strange and unanticipated shortcuts and edge cases in pursuit of their goals.",38.0
Reinforcement Learning as a Framework for Ethical Decision Making,dac3fc0cd4a43997f6fa606c4def04eb7c05dbf2,"[{'authorId': '152422014', 'name': 'David Abel'}, {'authorId': '2700008', 'name': 'J. MacGlashan'}, {'authorId': '144885169', 'name': 'M. Littman'}]",2016.0,"AAAI Workshop: AI, Ethics, and Society","['AGI Safety Literature Review', 'AI safety: state of the field through quantitative lens']",2,"Emerging AI systems will be making more and more decisions that impact the lives of humans in a significant way. It is essential, then, that these AI systems make decisions that take into account the desires, goals, and preferences of other people, while simultaneously learning about what those preferences are. In this work, we argue that the reinforcement-learning framework achieves the appropriate generality required to theorize about an idealized ethical artificial agent, and offers the proper foundations for grounding specific questions about ethical learning and decision making that can promote further scientific investigation. We define an idealized formalism for an ethical learner, and conduct experiments on two toy ethical dilemmas, demonstrating the soundness and flexibility of our approach. Lastly, we identify several critical challenges for future advancement in the area that can leverage our proposed framework.",93.0
Using Stories to Teach Human Values to Artificial Agents,33b53abdf2824b2cb0ee083c284000df4343a33e,"[{'authorId': '2757194', 'name': 'Mark O. Riedl'}, {'authorId': '35066258', 'name': 'Brent Harrison'}]",2016.0,"AAAI Workshop: AI, Ethics, and Society",['AGI Safety Literature Review'],1,"Value alignment is a property of an intelligent agent indicating that it can only pursue goals that are beneficial to humans. Successful value alignment should ensure that an artificial general intelligence cannot intentionally or unintentionally perform behaviors that adversely affect humans. This is problematic in practice since it is difficult to exhaustively enumerated by human programmers. In order for successful value alignment, we argue that values should be learned. In this paper, we hypothesize that an artificial intelligence that can read and understand stories can learn the values tacitly held by the culture from which the stories originate.We describe preliminary work on using stories to generate a value-aligned reward signal for reinforcement learning agents that prevents psychotic-appearing behavior.",72.0
Graying the black box: Understanding DQNs,7200969d70cf6f3fd343f48e97b8ebf7d563a584,"[{'authorId': '3331540', 'name': 'Tom Zahavy'}, {'authorId': '1405614659', 'name': 'Nir Ben-Zrihem'}, {'authorId': '1712535', 'name': 'Shie Mannor'}]",2016.0,International Conference on Machine Learning,['AGI Safety Literature Review'],1,"In recent years there is a growing interest in using deep representations for reinforcement learning. In this paper, we present a methodology and tools to analyze Deep Q-networks (DQNs) in a non-blind matter. Using our tools we reveal that the features learned by DQNs aggregate the state space in a hierarchical fashion, explaining its success. Moreover we are able to understand and describe the policies learned by DQNs for three different Atari2600 games and suggest ways to interpret, debug and optimize deep neural networks in reinforcement learning.",219.0
Asynchronous Methods for Deep Reinforcement Learning,69e76e16740ed69f4dc55361a3d319ac2f1293dd,"[{'authorId': '3255983', 'name': 'Volodymyr Mnih'}, {'authorId': '36045539', 'name': 'Adrià Puigdomènech Badia'}, {'authorId': '153583218', 'name': 'Mehdi Mirza'}, {'authorId': '1753223', 'name': 'A. Graves'}, {'authorId': '2542999', 'name': 'T. Lillicrap'}, {'authorId': '3367786', 'name': 'Tim Harley'}, {'authorId': '145824029', 'name': 'David Silver'}, {'authorId': '2645384', 'name': 'K. Kavukcuoglu'}]",2016.0,International Conference on Machine Learning,['AGI Safety Literature Review'],1,"We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.",6489.0
Mastering the game of Go with deep neural networks and tree search,846aedd869a00c09b40f1f1f35673cb22bc87490,"[{'authorId': '145824029', 'name': 'David Silver'}, {'authorId': '1885349', 'name': 'Aja Huang'}, {'authorId': '2772217', 'name': 'Chris J. Maddison'}, {'authorId': '35099444', 'name': 'A. Guez'}, {'authorId': '2175946', 'name': 'L. Sifre'}, {'authorId': '47568983', 'name': 'George van den Driessche'}, {'authorId': '4337102', 'name': 'Julian Schrittwieser'}, {'authorId': '2460849', 'name': 'Ioannis Antonoglou'}, {'authorId': '2749418', 'name': 'Vedavyas Panneershelvam'}, {'authorId': '1975889', 'name': 'Marc Lanctot'}, {'authorId': '48373216', 'name': 'S. Dieleman'}, {'authorId': '2401609', 'name': 'Dominik Grewe'}, {'authorId': '4111313', 'name': 'John Nham'}, {'authorId': '2583391', 'name': 'Nal Kalchbrenner'}, {'authorId': '1701686', 'name': 'Ilya Sutskever'}, {'authorId': '2542999', 'name': 'T. Lillicrap'}, {'authorId': '40662181', 'name': 'M. Leach'}, {'authorId': '2645384', 'name': 'K. Kavukcuoglu'}, {'authorId': '1686971', 'name': 'T. Graepel'}, {'authorId': '48987704', 'name': 'D. Hassabis'}]",2016.0,Nature,['AGI Safety Literature Review'],1,,13345.0
"Learning the Preferences of Ignorant, Inconsistent Agents",5260a706305c59c0fa2981bcdd86280c3c4a16b1,"[{'authorId': '47107786', 'name': 'Owain Evans'}, {'authorId': '2214496', 'name': 'Andreas Stuhlmüller'}, {'authorId': '144002017', 'name': 'Noah D. Goodman'}]",2015.0,AAAI Conference on Artificial Intelligence,"['AGI Safety Literature Review', 'AI Research Considerations for Human Existential Safety (ARCHES)']",2,"
 
 An important use of machine learning is to learn what people value. What posts or photos should a user be shown? Which jobs or activities would a person find rewarding? In each case, observations of people's past choices can inform our inferences about their likes and preferences. If we assume that choices are approximately optimal according to some utility function, we can treat preference inference as Bayesian inverse planning. That is, given a prior on utility functions and some observed choices, we invert an optimal decision-making process to infer a posterior distribution on utility functions. However, people often deviate from approximate optimality. They have false beliefs, their planning is sub-optimal, and their choices may be temporally inconsistent due to hyperbolic discounting and other biases. We demonstrate how to incorporate these deviations into algorithms for preference inference by constructing generative models of planning for agents who are subject to false beliefs and time inconsistency. We explore the inferences these models make about preferences, beliefs, and biases. We present a behavioral experiment in which human subjects perform preference inference given the same observations of choices as our model. Results show that human subjects (like our model) explain choices in terms of systematic deviations from optimal behavior and suggest that they take such deviations into account when inferring preferences.
 
",106.0
Universal Artificial Intelligence-Practical Agents and Fundamental Challenges,28f8fc73e8d4a85cbf991067e912fb73bc7a867b,"[{'authorId': '1868196', 'name': 'Tom Everitt'}, {'authorId': '144154444', 'name': 'Marcus Hutter'}]",2016.0,,['AGI Safety Literature Review'],1,"Foundational theories have contributed greatly to scientific progress in many fields. Examples include Zermelo-Fraenkel set theory in mathematics, and universal Turing machines in computer science. Universal Artificial Intelligence (UAI) is an increasingly well-studied foundational theory for artificial intelligence, based on ancient principles in the philosophy of science and modern developments in information and probability theory. Importantly, it refrains from making unrealistic Markov, ergodicity, or stationarity assumptions on the environment. UAI provides a theoretically optimal agent AIXI and principled ideas for constructing practical autonomous agents. The theory also makes it possible to establish formal results on the motivations of AI systems. Such results may greatly enhance the trustability of autonomous agents, and guide design choices towards more robust agent architectures and incentive schemes. Finally, UAI offers a deeper appreciation of fundamental problems such as the induction problem and the explorationexploitation dilemma.",5.0
Racing to the precipice: a model of artificial intelligence development,275db7bea43cf793e2c305437b80252132e54c72,"[{'authorId': '2054678912', 'name': 'S. Armstrong'}, {'authorId': '2193691', 'name': 'N. Bostrom'}, {'authorId': '3389522', 'name': 'Carl Shulman'}]",2016.0,Ai & Society,"['AGI Safety Literature Review', 'AI Research Considerations for Human Existential Safety (ARCHES)']",2,,103.0
Research Priorities for Robust and Beneficial Artificial Intelligence,da08d9d5e0c12da55f3f86ef994759d6dd29f639,"[{'authorId': '145107462', 'name': 'Stuart J. Russell'}, {'authorId': '40829712', 'name': 'Dan Dewey'}, {'authorId': '2011933', 'name': 'Max Tegmark'}]",2015.0,The AI Magazine,"['AGI Safety Literature Review', 'Unsolved Problems in ML Safety', 'AI safety: state of the field through quantitative lens', 'AI Research Considerations for Human Existential Safety (ARCHES)']",4,"Success in the quest for artificial intelligence has the potential to bring unprecedented benefits to humanity, and it is therefore worthwhile to investigate how to maximize these benefits while avoiding potential pitfalls. This article gives numerous examples (which should by no means be construed as an exhaustive list) of such worthwhile research aimed at ensuring that AI remains robust and beneficial.",516.0
Taxonomy of Pathways to Dangerous AI,869fd8cd1962c0495cc35bcb96c99baf8bd16b69,"[{'authorId': '1976753', 'name': 'Roman V Yampolskiy'}]",2015.0,arXiv.org,['AGI Safety Literature Review'],1,"In order to properly handle a dangerous Artificially Intelligent (AI) system it is important to understand how the system came to be in such a state. In popular culture (science fiction movies/books) AIs/Robots became self-aware and as a result rebel against humanity and decide to destroy it. While it is one possible scenario, it is probably the least likely path to appearance of dangerous AI. In this work, we survey, classify and analyze a number of circumstances, which might lead to arrival of malicious AI. To the best of our knowledge, this is the first attempt to systematically classify types of pathways leading to malevolent AI. Previous relevant work either surveyed specific goals/meta-rules which might lead to malevolent behavior in AIs (\""Ozkural, 2014) or reviewed specific undesirable behaviors AGIs can exhibit at different stages of its development (Alexey Turchin, July 10 2015, July 10, 2015).",17.0
Proof-Producing Reflection for HOL - With an Application to Model Polymorphism,4cf0fca3815a565978a2a42f72f07fd1111dd01f,"[{'authorId': '1825635', 'name': 'Benja Fallenstein'}, {'authorId': '2117776492', 'name': 'Ramana Kumar'}]",2015.0,International Conference on Interactive Theorem Proving,['AGI Safety Literature Review'],1,,10.0
Sequential Extensions of Causal and Evidential Decision Theory,b4e18e168460753f199db4a8de036f050aac355b,"[{'authorId': '1868196', 'name': 'Tom Everitt'}, {'authorId': '2990741', 'name': 'J. Leike'}, {'authorId': '144154444', 'name': 'Marcus Hutter'}]",2015.0,Algorithmic Decision Theory,['AGI Safety Literature Review'],1,,14.0
"Reasons without Persons: Rationality, Identity, and Time",aa12de47f01ca1bef3bee994bf86ac745b837303,"[{'authorId': '66129548', 'name': 'B. Hedden'}]",2015.0,,['AGI Safety Literature Review'],1,"Acknowledgements 1. Time-Slice Rationality 2. General Motivations 3. Against Diachronic Principles 4. Against Reflection Principles 5. The Diachronic Tragedy Argument 6. Options and Time-Slice Practical Rationality 7. Options and Diachronic Tragedy 8. Replacing Diachronic Principles 9. Replacing Reflection Principles 10. Doxastic Justification, Reasoning, and Evidence-Gathering 11. Rationality and the Subject's Point of View",51.0
Deep Learning,a4cec122a08216fe8a3bc19b22e78fbaea096256,"[{'authorId': '1688882', 'name': 'Yann LeCun'}, {'authorId': '1751762', 'name': 'Yoshua Bengio'}, {'authorId': '49853479', 'name': 'Geoffrey Hinton'}]",2015.0,Nature,['AGI Safety Literature Review'],1,,62194.0
An Empirical Evaluation of Deep Learning on Highway Driving,12806c298e01083a79db77927530367d85939907,"[{'authorId': '2570381', 'name': 'Brody Huval'}, {'authorId': '2156632012', 'name': 'Tao Wang'}, {'authorId': '34947630', 'name': 'S. Tandon'}, {'authorId': '2512747', 'name': 'Jeff Kiske'}, {'authorId': '1410015319', 'name': 'W. Song'}, {'authorId': '1896859', 'name': 'Joel Pazhayampallil'}, {'authorId': '1906895', 'name': 'M. Andriluka'}, {'authorId': '2706258', 'name': 'Pranav Rajpurkar'}, {'authorId': '2178889', 'name': 'Toki Migimatsu'}, {'authorId': '1405494927', 'name': 'Royce Cheng-Yue'}, {'authorId': '33831066', 'name': 'Fernando A. Mujica'}, {'authorId': '144638694', 'name': 'Adam Coates'}, {'authorId': '34699434', 'name': 'A. Ng'}]",2015.0,arXiv.org,['AGI Safety Literature Review'],1,"Numerous groups have applied a variety of deep learning techniques to computer vision problems in highway perception scenarios. In this paper, we presented a number of empirical evaluations of recent deep learning advances. Computer vision, combined with deep learning, has the potential to bring about a relatively inexpensive, robust solution to autonomous driving. To prepare deep learning for industry uptake and practical applications, neural networks will require large data sets that represent all possible driving environments and scenarios. We collect a large data set of highway data and apply deep learning and computer vision algorithms to problems such as car and lane detection. We show how existing convolutional neural networks (CNNs) can be used to perform lane and vehicle detection while running at frame rates required for a real-time system. Our results lend credence to the hypothesis that deep learning holds promise for autonomous driving.",545.0
Motivated Value Selection for Artificial Agents,a1011898c7ca77afc51a1f331511c6325fb2964c,"[{'authorId': '2054678912', 'name': 'S. Armstrong'}]",2015.0,AI and Ethics,['AGI Safety Literature Review'],1,"Coding values (or preferences) directly into an artificial agent is a very challenging task, while value selection (or value-learning, or value-loading) allows agents to learn values from their programmers, other humans or their environments in an interactive way. However, there is a conflict between agents learning their future values and following their current values, which motivates agents to manipulate the value selection process. This paper establishes the conditions under which motivated value selection is an issue for some types of agents, and presents an example of an `indifferent' agent that avoids it entirely. This poses and solves an issue which has not to the author's knowledge been formally addressed in the literature.",43.0
Ethical guidelines for a superintelligence,525e4f93157113c8b80b5ff608500ae288d63b3c,"[{'authorId': '144883814', 'name': 'E. Davis'}]",2015.0,Artificial Intelligence,['AGI Safety Literature Review'],1,,30.0
Human-level control through deep reinforcement learning,e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d,"[{'authorId': '3255983', 'name': 'Volodymyr Mnih'}, {'authorId': '2645384', 'name': 'K. Kavukcuoglu'}, {'authorId': '145824029', 'name': 'David Silver'}, {'authorId': '2228824', 'name': 'Andrei A. Rusu'}, {'authorId': '144056327', 'name': 'J. Veness'}, {'authorId': '1792298', 'name': 'Marc G. Bellemare'}, {'authorId': '1753223', 'name': 'A. Graves'}, {'authorId': '3137672', 'name': 'Martin A. Riedmiller'}, {'authorId': '145600108', 'name': 'A. Fidjeland'}, {'authorId': '2273072', 'name': 'Georg Ostrovski'}, {'authorId': '48348688', 'name': 'Stig Petersen'}, {'authorId': '50388928', 'name': 'Charlie Beattie'}, {'authorId': '49813280', 'name': 'Amir Sadik'}, {'authorId': '2460849', 'name': 'Ioannis Antonoglou'}, {'authorId': '143776287', 'name': 'Helen King'}, {'authorId': '2106164', 'name': 'D. Kumaran'}, {'authorId': '1688276', 'name': 'Daan Wierstra'}, {'authorId': '34313265', 'name': 'S. Legg'}, {'authorId': '48987704', 'name': 'D. Hassabis'}]",2015.0,Nature,['AGI Safety Literature Review'],1,,19830.0
Explaining and Harnessing Adversarial Examples,bee044c8e8903fb67523c1f8c105ab4718600cdb,"[{'authorId': '153440022', 'name': 'Ian J. Goodfellow'}, {'authorId': '1789737', 'name': 'Jonathon Shlens'}, {'authorId': '2574060', 'name': 'Christian Szegedy'}]",2014.0,International Conference on Learning Representations,"['AGI Safety Literature Review', 'AI Research Considerations for Human Existential Safety (ARCHES)']",2,"Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.",12789.0
A comprehensive survey on safe reinforcement learning,c0f2c4104ef6e36bb67022001179887e6600d24d,"[{'authorId': '2110194270', 'name': 'Javier García'}, {'authorId': '143901279', 'name': 'F. Fernández'}]",2015.0,Journal of machine learning research,"['AGI Safety Literature Review', 'AI safety: state of the field through quantitative lens']",2,"Safe Reinforcement Learning can be defined as the process of learning policies that maximize the expectation of the return in problems in which it is important to ensure reasonable system performance and/or respect safety constraints during the learning and/or deployment processes. We categorize and analyze two approaches of Safe Reinforcement Learning. The first is based on the modification of the optimality criterion, the classic discounted finite/infinite horizon, with a safety factor. The second is based on the modification of the exploration process through the incorporation of external knowledge or the guidance of a risk metric. We use the proposed classification to survey the existing literature, as well as suggesting future directions for Safe Reinforcement Learning.",1040.0
Vingean Reflection : Reliable Reasoning for Self-Improving Agents,77c7bbe446564bd3253bf720e4729a113b01b798,"[{'authorId': '1825635', 'name': 'Benja Fallenstein'}, {'authorId': '1719968', 'name': 'N. Soares'}]",2015.0,,"['AGI Safety Literature Review', 'AI Research Considerations for Human Existential Safety (ARCHES)']",2,"Today, human-level machine intelligence is in the domain of futurism, but there is every reason to expect that it will be developed eventually. Once artificial agents become able to improve themselves further, they may far surpass human intelligence, making it vitally important to ensure that the result of an “intelligence explosion” is aligned with human interests. In this paper, we discuss one aspect of this challenge: ensuring that the initial agent’s reasoning about its future versions is reliable, even if these future versions are far more intelligent than the current reasoner. We refer to reasoning of this sort as Vingean reflection. A self-improving agent must reason about the behavior of its smarter successors in abstract terms, since if it could predict their actions in detail, it would already be as smart as them. This is called the Vingean principle, and we argue that theoretical work on Vingean reflection should focus on formal models that reflect this principle. However, the framework of expected utility maximization, commonly used to model rational agents, fails to do so. We review a body of work which instead investigates agents that use formal proofs to reason about their successors. While it is unlikely that real-world agents would base their behavior entirely on formal proofs, this appears to be the best currently available formal model of abstract reasoning, and work in this setting may lead to insights applicable to more realistic approaches to Vingean",17.0
Questions of Reasoning Under Logical Uncertainty,3ffdc1834c9d210bc328d7414a709aa3b15a8fd0,"[{'authorId': '1719968', 'name': 'N. Soares'}, {'authorId': '1825635', 'name': 'Benja Fallenstein'}]",2015.0,,['AGI Safety Literature Review'],1,"A logically uncertain reasoner would be able to reason as if they know both a programming language and a program, without knowing what the program outputs. Most practical reasoning involves some logical uncertainty, but no satisfactory theory of reasoning under logical uncertainty yet exists. A better theory of reasoning under logical uncertainty is needed in order to develop the tools necessary to construct highly reliable artificial reasoners. This paper introduces the topic, discusses a number of historical results, and describes a number of open prob-",20.0
Aligning Superintelligence with Human Interests: A Technical Research Agenda,d8033a314493c8df3791912272ac4b58d3a7b8c2,"[{'authorId': '1719968', 'name': 'N. Soares'}, {'authorId': '1825635', 'name': 'Benja Fallenstein'}]",2015.0,,"['AGI Safety Literature Review', 'AI Research Considerations for Human Existential Safety (ARCHES)']",2,"The property that has given humans a dominant advantage over other species is not strength or speed, but intelligence. If progress in artificial intelligence continues unabated, AI systems will eventually exceed humans in general reasoning ability. A system that is “superintelligent” in the sense of being “smarter than the best human brains in practically every field” could have an enormous impact upon humanity (Bostrom 2014). Just as human intelligence has allowed us to develop tools and strategies for controlling our environment, a superintelligent system would likely be capable of developing its own tools and strategies for exerting control (Muehlhauser and Salamon 2012). In light of this potential, it is essential to use caution when developing AI systems that can exceed human levels of general intelligence, or that can facilitate the creation of such systems.",77.0
Problems of Self-reference in Self-improving Space-Time Embedded Intelligence,96a95e460b48eef02c507dceba33f74c151c4f70,"[{'authorId': '1825635', 'name': 'Benja Fallenstein'}, {'authorId': '1719968', 'name': 'N. Soares'}]",2014.0,Artificial General Intelligence,['AGI Safety Literature Review'],1,,23.0
Teleporting Universal Intelligent Agents,c287f55ff4445d9cfdeed1e5383566f64f300575,"[{'authorId': '1749270', 'name': 'Laurent Orseau'}]",2014.0,Artificial General Intelligence,"['AGI Safety Literature Review', 'AI Research Considerations for Human Existential Safety (ARCHES)']",2,,11.0
"The Multi-slot Framework: A Formal Model for Multiple, Copiable AIs",c2c7987e0d549b50aa90210b9c017cc91ef795fb,"[{'authorId': '1749270', 'name': 'Laurent Orseau'}]",2014.0,Artificial General Intelligence,"['AGI Safety Literature Review', 'AI Research Considerations for Human Existential Safety (ARCHES)']",2,,8.0
"Superintelligence: Paths, Dangers, Strategies",7bba95b3d145564025e26b49ca67f13f884f8560,"[{'authorId': '2193691', 'name': 'N. Bostrom'}]",2014.0,,"['AGI Safety Literature Review', 'Superintelligence: Paths, Dangers, Strategies', 'Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",3,"The human brain has some capabilities that the brains of other animals lack. It is to these distinctive capabilities that our species owes its dominant position. Other animals have stronger muscles or sharper claws, but we have cleverer brains. If machine brains one day come to surpass human brains in general intelligence, then this new superintelligence could become very powerful. As the fate of the gorillas now depends more on us humans than on the gorillas themselves, so the fate of our species then would come to depend on the actions of the machine superintelligence. But we have one advantage: we get to make the first move. Will it be possible to construct a seed AI or otherwise to engineer initial conditions so as to make an intelligence explosion survivable? How could one achieve a controlled detonation? To get closer to an answer to this question, we must make our way through a fascinating landscape of topics and considerations. Read the book and learn about oracles, genies, singletons; about boxing methods, tripwires, and mind crime; about humanity's cosmic endowment and differential technological development; indirect normativity, instrumental convergence, whole brain emulation and technology couplings; Malthusian economics and dystopian evolution; artificial intelligence, and biological cognitive enhancement, and collective intelligence. This profoundly ambitious and original book picks its way carefully through a vast tract of forbiddingly difficult intellectual terrain. Yet the writing is so lucid that it somehow makes it all seem easy. After an utterly engrossing journey that takes us to the frontiers of thinking about the human condition and the future of intelligent life, we find in Nick Bostrom's work nothing less than a reconceptualization of the essential task of our time.",611.0
Limitations and risks of machine ethics,27c3c7c96f3252890b5af1da82831f75ce0ccc68,"[{'authorId': '35167962', 'name': 'Miles Brundage'}]",2014.0,Journal of experimental and theoretical artificial intelligence (Print),['AGI Safety Literature Review'],1,"Many authors have proposed constraining the behaviour of intelligent systems with ‘machine ethics’ to ensure positive social outcomes from the development of such systems. This paper critically analyses the prospects for machine ethics, identifying several inherent limitations. While machine ethics may increase the probability of ethical behaviour in some situations, it cannot guarantee it due to the nature of ethics, the computational limitations of computational agents and the complexity of the world. In addition, machine ethics, even if it were to be ‘solved’ at a technical level, would be insufficient to ensure positive social outcomes from intelligent systems.",74.0
Intriguing properties of neural networks,d891dc72cbd40ffaeefdc79f2e7afe1e530a23ad,"[{'authorId': '2574060', 'name': 'Christian Szegedy'}, {'authorId': '2563432', 'name': 'Wojciech Zaremba'}, {'authorId': '1701686', 'name': 'Ilya Sutskever'}, {'authorId': '143627859', 'name': 'Joan Bruna'}, {'authorId': '1761978', 'name': 'D. Erhan'}, {'authorId': '153440022', 'name': 'Ian J. Goodfellow'}, {'authorId': '2276554', 'name': 'R. Fergus'}]",2013.0,International Conference on Learning Representations,"['AGI Safety Literature Review', 'Unsolved Problems in ML Safety', 'AI Research Considerations for Human Existential Safety (ARCHES)']",3,"Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. 
First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. 
Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.",10875.0
Responses to catastrophic AGI risk: a survey,fb6eefc6a42fbbf286476e435d87eacb99a9a047,"[{'authorId': '2821562', 'name': 'Kaj Sotala'}, {'authorId': '1976753', 'name': 'Roman V Yampolskiy'}]",2014.0,,"['AGI Safety Literature Review', 'AI safety: state of the field through quantitative lens', 'Responses to catastrophic AGI risk: a survey']",3,"Many researchers have argued that humanity will create artificial general intelligence (AGI) within the next twenty to one hundred years. It has been suggested that AGI may inflict serious damage to human well-being on a global scale (‘catastrophic risk’). After summarizing the arguments for why AGI may pose such a risk, we review the fieldʼs proposed responses to AGI risk. We consider societal proposals, proposals for external constraints on AGI behaviors and proposals for creating AGIs that are safe due to their internal design.",103.0
Program Equilibrium in the Prisoner ’ s Dilemma via Löb ’ s Theorem,cd93af8a92ae504e8bf7227073fc8de4a3133580,"[{'authorId': '2254026', 'name': 'Patrick LaVictoire'}, {'authorId': '1825635', 'name': 'Benja Fallenstein'}, {'authorId': '2542795', 'name': 'Eliezer Yudkowsky'}, {'authorId': '2034274', 'name': 'M. Bárász'}]",2014.0,,['AGI Safety Literature Review'],1,"Applications of game theory often neglect that real-world agents normally have some amount of out-of-band information about each other. We consider the limiting case of a one-shot Prisoner’s Dilemma between algorithms with readaccess to one anothers’ source code. Previous work has shown that cooperation is possible at a Nash equilibrium in this setting, but existing constructions require interacting agents to be identical or near-identical. We show that a natural class of agents are able to achieve mutual cooperation at Nash equilibrium without any prior coordination of this sort.",27.0
Existential Risk Prevention as Global Priority,ced289065723368bca48636edf71eeed50f40a39,"[{'authorId': '2193691', 'name': 'N. Bostrom'}]",2013.0,,"['AGI Safety Literature Review', 'AI Research Considerations for Human Existential Safety (ARCHES)']",2,"risks are those that threaten the entire future of humanity. Many theories of value imply that even relatively small reductions in net existential risk have enormous expected value. Despite their importance, issues surrounding human-extinction risks and related hazards remain poorly understood. In this article, I clarify the concept of existential risk and develop an improved classification scheme. I discuss the relation between existential risks and basic issues in axiology, and show how existential risk reduction (via the maxipok rule) can serve as a strongly action-guiding principle for utilitarian concerns. I also show how the notion of existential risk suggests a new way of thinking about the ideal of sustainability. Policy Implications • Existential risk is a concept that can focus long-term global efforts and sustainability concerns. • The biggest existential risks are anthropogenic and related to potential future technologies. • A moral case can be made that existential risk reduction is strictly more important than any other global public good. • Sustainability should be reconceptualised in dynamic terms, as aiming for a sustainable trajectory rather than a sus- tainable state. • Some small existential risks can be mitigated today directly (e.g. asteroids) or indirectly (by building resilience and reserves to increase survivability in a range of extreme scenarios) but it is more important to build capacity to improve humanity's ability to deal with the larger existential risks that will arise later in this century. This will require collective wisdom, technology foresight, and the ability when necessary to mobilise a strong global coordi- nated response to anticipated existential risks. • Perhaps the most cost-effective way to reduce existential risks today is to fund analysis of a wide range of existen- tial risks and potential mitigation strategies, with a long-term perspective.",323.0
Future Progress in Artificial Intelligence: A Survey of Expert Opinion,9fc93a7ae7fabaabe1923a5914a29761dead1551,"[{'authorId': '1807005', 'name': 'V. C. Müller'}, {'authorId': '2193691', 'name': 'N. Bostrom'}]",2013.0,Conference on Philosophy and Theory of Artificial Intelligence,"['AGI Safety Literature Review', 'Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",2,,408.0
Space-Time Embedded Intelligence,05f7820b0bf336c38725db58dc97915f357ab49f,"[{'authorId': '1749270', 'name': 'Laurent Orseau'}, {'authorId': '11023955', 'name': 'Mark B. Ring'}]",2012.0,Artificial General Intelligence,"['AGI Safety Literature Review', 'AI Research Considerations for Human Existential Safety (ARCHES)']",2,,51.0
Thinking Inside the Box: Controlling and Using an Oracle AI,6d78d67d4f7f5fe2e66933778ab1faf119d21547,"[{'authorId': '2054678912', 'name': 'S. Armstrong'}, {'authorId': '144816231', 'name': 'A. Sandberg'}, {'authorId': '2193691', 'name': 'N. Bostrom'}]",2012.0,Minds and Machines,"['AGI Safety Literature Review', 'Responses to catastrophic AGI risk: a survey']",2,,105.0
The Superintelligent Will: Motivation and Instrumental Rationality in Advanced Artificial Agents,2f80a4a0b37887ccd98d23b4a0a2b58bc33e0864,"[{'authorId': '2193691', 'name': 'N. Bostrom'}]",2012.0,Minds and Machines,['AGI Safety Literature Review'],1,,140.0
One Decade of Universal Artificial Intelligence,27bd55a21fb6b9cf0d2ea7f17ea4ea82654363c5,"[{'authorId': '144154444', 'name': 'Marcus Hutter'}]",2012.0,arXiv.org,['AGI Safety Literature Review'],1,"The first decade of this century has seen the nascency of the first mathematical theory of general artificial intelligence. This theory of Universal Artificial Intelligence (UAI) has made significant contributions to many theoretical, philosophical, and practical AI questions. In a series of papers culminating in book (Hutter, 2005), an exciting sound and complete mathematical model for a super intelligent agent (AIXI) has been developed and rigorously analyzed. While nowadays most AI researchers avoid discussing intelligence, the awardwinning PhD thesis (Legg, 2008) provided the philosophical embedding and investigated the UAI-based universal measure of rational intelligence, which is formal, objective and non-anthropocentric. Recently, effective approximations of AIXI have been derived and experimentally investigated in JAIR paper (Veness et al. 2011). This practical breakthrough has resulted in some impressive applications, finally muting earlier critique that UAI is only a theory. For the first time, without providing any domain knowledge, the same agent is able to self-adapt to a diverse range of interactive environments. For instance, AIXI is able to learn from scratch to play TicTacToe, Pacman, Kuhn Poker, and other games by trial and error, without even providing the rules of the games. These achievements give new hope that the grand goal of Artificial General Intelligence is not elusive. This article provides an informal overview of UAI in context. It attempts to gently introduce a very theoretical, formal, and mathematical subject, and discusses philosophical and technical ingredients, traits of intelligence, some social questions, and the past and future of UAI.",43.0
Can Intelligence Explode?,dba694f5007986ad31b7a8a47f4dea7a700a465d,"[{'authorId': '144154444', 'name': 'Marcus Hutter'}]",2012.0,arXiv.org,"['AGI Safety Literature Review', 'Responses to catastrophic AGI risk: a survey']",2,"The technological singularity refers to a hypothetical scenario in which technological advances virtually explode. The most popular scenario is the creation of super-intelligent algorithms that recursively create ever higher intelligences. It took many decades for these ideas to spread from science fiction to popular science magazines and finally to attract the attention of serious philosophers. David Chalmers’ (JCS 2010) article is the first comprehensive philosophical analysis of the singularity in a respected philosophy journal. The motivation of my article is to augment Chalmers’ and to discuss some issues not addressed by him, in particular what it could mean for intelligence to explode. In this course, I will (have to) provide a more careful treatment of what intelligence actually is, separate speed from intelligence explosion, compare what super-intelligent participants and classical human observers might experience and do, discuss immediate implications for the diversity and value of life, consider possible bounds on intelligence, and contemplate intelligences right at the singularity.",28.0
Model-based Utility Functions,6b2bf1efaa66c77677070f1c52701f0f7f2a3e15,"[{'authorId': '39109261', 'name': 'B. Hibbard'}]",2011.0,Journal of Artificial General Intelligence,"['AGI Safety Literature Review', 'Responses to catastrophic AGI risk: a survey']",2,"Abstract Orseau and Ring, as well as Dewey, have recently described problems, including self-delusion, with the behavior of agents using various definitions of utility functions. An agent's utility function is defined in terms of the agent's history of interactions with its environment. This paper argues, via two examples, that the behavior problems can be avoided by formulating the utility function in two steps: 1) inferring a model of the environment from interactions, and 2) computing utility as a function of the environment model. Basing a utility function on a model that the agent must learn implies that the utility function must initially be expressed in terms of specifications to be matched to structures in the learned model. These specifications constitute prior assumptions about the environment so this approach will not work with arbitrary environments. But the approach should work for agents designed by humans to act in the physical world. The paper also addresses the issue of self-modifying agents and shows that if provided with the possibility to modify their utility functions agents will not choose to do so, under some usual assumptions.",46.0
Universal knowledge-seeking agents,c3b25d419f30873420f0bc159e9499edda9c0dbe,"[{'authorId': '1749270', 'name': 'Laurent Orseau'}]",2011.0,Theoretical Computer Science,['AGI Safety Literature Review'],1,,28.0
"Delusion, Survival, and Intelligent Agents",d8a200f7e0ef5659d544b4f5306251deaf781d1b,"[{'authorId': '11023955', 'name': 'Mark B. Ring'}, {'authorId': '1749270', 'name': 'Laurent Orseau'}]",2011.0,Artificial General Intelligence,"['AGI Safety Literature Review', 'Responses to catastrophic AGI risk: a survey']",2,,76.0
Self-Modification and Mortality in Artificial Agents,35553d2ddcd983b1e16a4d78726c857ab4ad3d5f,"[{'authorId': '1749270', 'name': 'Laurent Orseau'}, {'authorId': '11023955', 'name': 'Mark B. Ring'}]",2011.0,Artificial General Intelligence,"['AGI Safety Literature Review', 'Responses to catastrophic AGI risk: a survey']",2,,50.0
How long until human-level AI? Results from an expert assessment,17863bc49f8337b1646ad203ab12d716b8e90e48,"[{'authorId': '2097800', 'name': 'S. Baum'}, {'authorId': '1738080', 'name': 'B. Goertzel'}, {'authorId': '49961666', 'name': 'T. Goertzel'}]",2011.0,,"['AGI Safety Literature Review', 'Responses to catastrophic AGI risk: a survey']",2,,54.0
The Singularity: a Philosophical Analysis,3185fd49b5e7ef39ac25faba3429b8ede069364f,"[{'authorId': '2072252', 'name': 'D. Chalmers'}]",2010.0,,['AGI Safety Literature Review'],1,"What happens when machines become more intelligent than humans? One view is that this event will be followed by an explosion to ever-greater levels of intelligence, as each generation of machines creates more intelligent machines in turn. This intelligence explosion is now often known as the “singularity”. The basic argument here was set out by the statistician I. J. Good in his 1965 article “Speculations Concerning the First Ultraintelligent Machine”:",339.0
Inverse Reinforcement Learning in Partially Observable Environments,6a69ed251b91894bb760785e9e6dcfc3b293e8ba,"[{'authorId': '2056578', 'name': 'Jaedeug Choi'}, {'authorId': '1741330', 'name': 'Kee-Eung Kim'}]",2009.0,International Joint Conference on Artificial Intelligence,['AGI Safety Literature Review'],1,"Inverse reinforcement learning (IRL) is the problem of recovering the underlying reward function from the behavior of an expert. Most of the existing IRL algorithms assume that the environment is modeled as a Markov decision process (MDP), although it is desirable to handle partially observable settings in order to handle more realistic scenarios. In this paper, we present IRL algorithms for partially observable environments that can be modeled as a partially observable Markov decision process (POMDP). We deal with two cases according to the representation of the given expert's behavior, namely the case in which the expert's policy is explicitly given, and the case in which the expert's trajectories are available instead. The IRL in POMDPs poses a greater challenge than in MDPs since it is not only ill-posed due to the nature of IRL, but also computationally intractable due to the hardness in solving POMDPs. To overcome these obstacles, we present algorithms that exploit some of the classical results from the POMDP literature. Experimental results on several benchmark POMDP domains show that our work is useful for partially observable settings.",147.0
"An ""Ethical"" Game-Theoretic Solution Concept for Two-Player Perfect-Information Games",f7868c45efa321644ec6c9cfd0b01496fb5f70e1,"[{'authorId': '2433553', 'name': 'Joshua Letchford'}, {'authorId': '1749906', 'name': 'V. Conitzer'}, {'authorId': '143809582', 'name': 'K. Jain'}]",2008.0,Workshop on Internet and Network Economics,['AGI Safety Literature Review'],1,,14.0
Thinking From Inside the Box,348ea6366ff06686238bc1b339788d13c1fa03f0,"[{'authorId': '48035276', 'name': 'M. Swaminathan'}]",2008.0,Seminars in Cardiothoracic and Vascular Anesthesia,['AGI Safety Literature Review'],1,"For years, management gurus have encouraged the philosophy of “thinking outside the box” to promote unconventional approaches to solve problems when conventional thinking is thought futile. However, unstructured and abstract flights of thought are not necessarily always productive. At times, innovative thinking from “inside the box” may be more desirable. 1 Physicians are largely unaccustomed to “thinking outside the box” with regard to perioperative anesthesia management. This is especially true in the current health care environment that values and emphasizes protocols and frowns on any deviations from them. Whereas regulatory issues, practice guidelines, national standards, and institutional protocols provide a structured approach and streamline everyday care, they, at the same time, act as constraints to limit out-of-the-box thinking. Therefore, as physicians we are “boxed in” and must think about the outside from the inside. Clinicians must consider advances and innovations that are increasingly available for patient care and incorporate them in our perioperative environment. Unconventional approaches have indeed been applied to develop new technologies and improve existing technology. This issue of Seminars in Cardiothoracic and Vascular Anesthesia includes articles that discuss innovations that promise to influence the care we provide to our patients both in the operating room and beyond.",118.0
Maximum Entropy Inverse Reinforcement Learning,11b6bdfe36c48b11367b27187da11d95892f0361,"[{'authorId': '1753269', 'name': 'Brian D. Ziebart'}, {'authorId': '34961461', 'name': 'Andrew L. Maas'}, {'authorId': '1756566', 'name': 'J. Bagnell'}, {'authorId': '144021446', 'name': 'A. Dey'}]",2008.0,AAAI Conference on Artificial Intelligence,['AGI Safety Literature Review'],1,"Recent research has shown the benefit of framing problems of imitation learning as solutions to Markov Decision Problems. This approach reduces learning to the problem of recovering a utility function that makes the behavior induced by a near-optimal policy closely mimic demonstrated behavior. In this work, we develop a probabilistic approach based on the principle of maximum entropy. Our approach provides a well-defined, globally normalized distribution over decision sequences, while providing the same performance guarantees as existing methods. 
 
We develop our technique in the context of modeling real-world navigation and driving behaviors where collected data is inherently noisy and imperfect. Our probabilistic approach enables modeling of route preferences as well as a powerful new approach to inferring destinations and routes based on partial trajectories.",2317.0
The Nature of Self-Improving Artificial Intelligence,4618cbdfd7dada7f61b706e4397d4e5952b5c9a0,"[{'authorId': '1808760', 'name': 'S. Omohundro'}]",2008.0,,"['AGI Safety Literature Review', 'Responses to catastrophic AGI risk: a survey']",2,"An electronic still camera indicates the available capacity of a recording medium and the number of frames remaining that may be photographed, as well as warning by stages in at least two forms, based on the available capacity of the recording medium. The electronic still camera includes a photo lens, mirror, CCD, signal executing circuit, A/D converter, buffer memory, compression circuit, CPU, memory card, and indicator device. A CPU reads in the gross capacity and the written-on portion from a memory card and calculates the available capacity. When the available capacity becomes small, the indicator device that indicates the available capacity or the number of frames remaining that can be photographed begins to flash the display slowly on and off. When the available capacity grows even smaller and is almost gone, the display flashes rapidly on and off. In this way, the photographer can easily detect how many more frames he or she may photograph on the memory card.",59.0
Universal Intelligence: A Definition of Machine Intelligence,8e8ec502208f29ee9f78ded19226578e027ecd16,"[{'authorId': '34313265', 'name': 'S. Legg'}, {'authorId': '144154444', 'name': 'Marcus Hutter'}]",2007.0,Minds and Machines,['AGI Safety Literature Review'],1,,538.0
A Collection of Definitions of Intelligence,921328ec369ac8f30d719ec553dde0cb8b708ef4,"[{'authorId': '34313265', 'name': 'S. Legg'}, {'authorId': '144154444', 'name': 'Marcus Hutter'}]",2007.0,Artificial General Intelligence,"['AGI Safety Literature Review', 'Responses to catastrophic AGI risk: a survey']",2,"This chapter is a survey of a large number of informal definitions of “intelligence” that the authors have collected over the years. Naturally, compiling a complete list would be impossible as many definitions of intelligence are buried deep inside articles and books. Nevertheless, the 70 odd definitions presented here are, to the authors' knowledge, the largest and most well referenced collection there is.",410.0
An Application of Reinforcement Learning to Aerobatic Helicopter Flight,0bfbdafdfbcc268860fe54ae4d8f08d487bcc762,"[{'authorId': '1689992', 'name': 'P. Abbeel'}, {'authorId': '144638694', 'name': 'Adam Coates'}, {'authorId': '39100828', 'name': 'M. Quigley'}, {'authorId': '34699434', 'name': 'A. Ng'}]",2006.0,NIPS,['AGI Safety Literature Review'],1,"Autonomous helicopter flight is widely regarded to be a highly challenging control problem. This paper presents the first successful autonomous completion on a real RC helicopter of the following four aerobatic maneuvers: forward flip and sideways roll at low speed, tail-in funnel, and nose-in funnel. Our experimental results significantly extend the state of the art in autonomous helicopter flight. We used the following approach: First we had a pilot fly the helicopter to help us find a helicopter dynamics model and a reward (cost) function. Then we used a reinforcement learning (optimal control) algorithm to find a controller that is optimized for the resulting model and reward function. More specifically, we used differential dynamic programming (DDP), an extension of the linear quadratic regulator (LQR).",660.0
Artificial Intelligence as a Positive and Negative Factor in Global Risk,fcf7368061a544a09d16826eb4c5a8463ee5482e,"[{'authorId': '2542795', 'name': 'Eliezer Yudkowsky'}]",2006.0,,"['AGI Safety Literature Review', 'AI Research Considerations for Human Existential Safety (ARCHES)', 'Responses to catastrophic AGI risk: a survey', 'Superintelligence: Paths, Dangers, Strategies']",4,"By far the greatest danger of Artificial Intelligence is that people conclude too early that they understand it. Of course this problem is not limited to the field of AI. Jacques Monod wrote: ""A curious aspect of the theory of evolution is that everybody thinks he understands it."" (Monod 1974.) My father, a physicist, complained about people making up their own theories of physics; he wanted to know why people did not make up their own theories of chemistry. (Answer: They do.) Nonetheless the problem seems to be unusually acute in Artificial Intelligence. The field of AI has a reputation for making huge promises and then failing to deliver on them. Most observers conclude that AI is hard; as indeed it is. But the embarrassment does not stem from the difficulty. It is difficult to build a star from hydrogen, but the field of stellar astronomy does not have a terrible reputation for promising to build stars and then failing. The critical inference is not that AI is hard, but that, for some reason, it is very easy for people to think they know far more about Artificial Intelligence than they actually do.",358.0
Universal Artificial Intelligence,30a64bdf778b8f561af9ae589e822c2c800920b1,"[{'authorId': '1406347404', 'name': 'Dr. Marcus Hutter'}]",2004.0,Texts in Theoretical Computer Science. An EATCS Series,"['AGI Safety Literature Review', 'AI Research Considerations for Human Existential Safety (ARCHES)', 'Superintelligence: Paths, Dangers, Strategies']",3,,240.0
Reinforcement Learning: An Introduction,97efafdb4a3942ab3efba53ded7413199f79c054,"[{'authorId': '1699645', 'name': 'R. Sutton'}, {'authorId': '1730590', 'name': 'A. Barto'}]",2005.0,IEEE Transactions on Neural Networks,['AGI Safety Literature Review'],1,"Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives when interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the key ideas and algorithms of reinforcement learning. Their discussion ranges from the history of the field's intellectual foundations to the most recent developments and applications. The only necessary mathematical background is familiarity with elementary concepts of probability. The book is divided into three parts. Part I defines the reinforcement learning problem in terms of Markov decision processes. Part II provides basic solution methods: dynamic programming, Monte Carlo methods, and temporal-difference learning. Part III presents a unified view of the solution methods and incorporates artificial neural networks, eligibility traces, and planning; the two final chapters present case studies and consider the future of reinforcement learning.",37655.0
Reasoning with Limited Resources and Assigning Probabilities to Arithmetical Statements,cdc782c073de2bd35a5a5be575db4c59a88b358d,"[{'authorId': '2740556', 'name': 'H. Gaifman'}]",2004.0,Synthese,['AGI Safety Literature Review'],1,,62.0
Goedel Machines: Self-Referential Universal Problem Solvers Making Provably Optimal Self-Improvements,2e45fd58329558b0c9e73cbc1fb9f4f60dbb5ca7,"[{'authorId': '145341374', 'name': 'J. Schmidhuber'}]",2003.0,arXiv.org,['AGI Safety Literature Review'],1,"We present the first class of mathematically rigorous, general, fully self-referential, self-improving, optimally efficient problem solvers. Inspired by Kurt Goedel's celebrated self-referential formulas (1931), such a problem solver rewrites any part of its own code as soon as it has found a proof that the rewrite is useful, where the problem-dependent utility function and the hardware and the entire initial code are described by axioms encoded in an initial proof searcher which is also part of the initial code. The searcher systematically and efficiently tests computable proof techniques (programs whose outputs are proofs) until it finds a provably useful, computable self-rewrite. We show that such a self-rewrite is globally optimal - no local maxima! - since the code first had to prove that it is not useful to continue the proof search for alternative self-rewrites. Unlike previous non-self-referential methods based on hardwired proof searchers, ours not only boasts an optimal order of complexity but can optimally reduce any slowdowns hidden by the O()-notation, provided the utility of such speed-ups is provable at all.",50.0
Forecasting the Growth of Complexity and Change,5192ce4abae51ef90e514f2b21c68766939b6766,"[{'authorId': '65939980', 'name': 'T. Modis'}]",2002.0,,['AGI Safety Literature Review'],1,,64.0
The emotional dog and its rational tail: a social intuitionist approach to moral judgment.,1f60b37bc54ca27a5c378e65c9371ce8ff4183db,"[{'authorId': '2480714', 'name': 'J. Haidt'}]",2001.0,Psychology Review,['AGI Safety Literature Review'],1,"Research on moral judgment has been dominated by rationalist models, in which moral judgment is thought to be caused by moral reasoning. The author gives 4 reasons for considering the hypothesis that moral reasoning does not cause moral judgment; rather, moral reasoning is usually a post hoc construction, generated after a judgment has been reached. The social intuitionist model is presented as an alternative to rationalist models. The model is a social model in that it deemphasizes the private reasoning done by individuals and emphasizes instead the importance of social and cultural influences. The model is an intuitionist model in that it states that moral judgment is generally the result of quick, automatic evaluations (intuitions). The model is more consistent that rationalist models with recent findings in social, cultural, evolutionary, and biological psychology, as well as in anthropology and primatology.",6425.0
Pharmacokinetics of a novel formulation of ivermectin after administration to goats,b05b67aca720d0bc39bc9afad02a19f522c7a1bc,"[{'authorId': '34699434', 'name': 'A. Ng'}, {'authorId': '145107462', 'name': 'Stuart J. Russell'}]",2000.0,International Conference on Machine Learning,['AGI Safety Literature Review'],1,"Objective—To evaluate the pharmacokinetics of a novel commercial formulation of ivermectin after administration to goats.

Animals—6 healthy adult goats.

Procedure—Ivermectin (200 μg/kg) was initially administered IV to each goat, and plasma samples were obtained for 36 days. After a washout period of 3 weeks, each goat received a novel commercial formulation of ivermectin (200 μg/kg) by SC injection. Plasma samples were then obtained for 42 days. Drug concentrations were quantified by use of high-performance liquid chromatography with fluorescence detection.

Results—Pharmacokinetics of ivermectin after IV administration were best described by a 2-compartment open model; values for main compartmental variables included volume of distribution at a steady state (9.94 L/kg), clearance (1.54 L/kg/d), and area under the plasma concentration-time curve (AUC; 143 [ng•d]/mL). Values for the noncompartmental variables included mean residence time (7.37 days), AUC (153 [ng•d]/mL), and clearance (1.43 L/kg/d). After SC administration, noncompartmental pharmacokinetic analysis was conducted. Values of the variables calculated by use of this method included maximum plasma concentration (Cmax; 21.8 ng/mL), time to reach Cmax (3 days), and bioavailability (F; 91.8%).

Conclusions and Clinical Relevance—The commercial formulation used in this study is a good option to consider when administering ivermectin to goats because of the high absorption, which is characterized by high values of F. In addition, the values of Cmax and time to reach Cmax are higher than those reported by other investigators who used other routes of administration.",2738.0
Shadows of the mind - a search for the missing science of consciousness,15a93b1790e3025c107a56feb92ba2e15867dd08,"[{'authorId': '3110190', 'name': 'R. Penrose'}]",1994.0,,['AGI Safety Literature Review'],1,"From the Publisher: 
A New York Times bestseller when it appeared in 1989, Roger Penrose's The Emperor's New Mind was universally hailed as a marvelous survey of modern physics as well as a brilliant reflection on the human mind, offering a new perspective on the scientific landscape and a visionary glimpse of the possible future of science. Now, in Shadows of the Mind, Penrose offers another exhilarating look at modern science as he mounts an even more powerful attack on artificial intelligence. But perhaps more important, in this volume he points the way to a new science, one that may eventually explain the physical basis of the human mind. Penrose contends that some aspects of the human mind lie beyond computation. This is not a religious argument (that the mind is something other than physical) nor is it based on the brain's vast complexity (the weather is immensely complex, says Penrose, but it is still a computable thing, at least in theory). Instead, he provides powerful arguments to support his conclusion that there is something in the conscious activity of the brain that transcends computation - and will find no explanation in terms of present-day science. To illuminate what he believes this ""something"" might be, and to suggest where a new physics must proceed so that we may understand it, Penrose cuts a wide swathe through modern science, providing penetrating looks at everything from Turing computability and Godel's incompleteness, via Schrodinger's Cat and the Elitzur-Vaidman bomb-testing problem, to detailed microbiology. Of particular interest is Penrose's extensive examination of quantum mechanics, which introduces some new ideas that differ markedly from those advanced in The Emperor's New Mind, especially concerning the mysterious interface where classical and quantum physics meet. But perhaps the most interesting wrinkle in Shadows of the Mind is Penrose's excursion into microbiology, where he examines cytoskeletons and microtubules, minute substructures lying dee",409.0
The coming technological singularity: How to survive in the post-human era,3ab2d953596fea9131e622410c64b3d114a84f0c,"[{'authorId': '2012264', 'name': 'V. Vinge'}]",1993.0,,"['AGI Safety Literature Review', 'Responses to catastrophic AGI risk: a survey', 'Superintelligence: Paths, Dangers, Strategies']",3,"The acceleration of technological progress has been the central feature of this century. I argue in this paper that we are on the edge of change comparable to the rise of human life on Earth. The precise cause of this change is the imminent creation by technology of entities with greater than human intelligence. There are several means by which science may achieve this breakthrough (and this is another reason for having confidence that the event will occur): (1) the development of computers that are 'awake' and superhumanly intelligent (to date, most controversy in the area of AI relates to whether we can create human equivalence in a machine. But if the answer is 'yes, we can', then there is little doubt that beings more intelligent can be constructed shortly thereafter); (2) large computer networks (and their associated users) may 'wake up' as a superhumanly intelligent entity; (3) computer/human interfaces may become so intimate that users may reasonably be considered superhumanly intelligent; and (4) biological science may find ways to improve upon the natural human intellect. The first three possibilities depend in large part on improvements in computer hardware. Progress in computer hardware has followed an amazingly steady curve in the last few decades. Based largely on this trend, I believe that the creation of greater than human intelligence will occur during the next thirty years.",425.0
"Minds, brains, and programs",bfa035c0e723f8f540500db038ca6e26d599029d,"[{'authorId': '34493294', 'name': 'J. Searle'}]",1980.0,Behavioral and Brain Sciences,"['AGI Safety Literature Review', 'Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",2,"Abstract This article can be viewed as an attempt to explore the consequences of two propositions. (1) Intentionality in human beings (and animals) is a product of causal features of the brain. I assume this is an empirical fact about the actual causal relations between mental processes and brains. It says simply that certain brain processes are sufficient for intentionality. (2) Instantiating a computer program is never by itself a sufficient condition of intentionality. The main argument of this paper is directed at establishing this claim. The form of the argument is to show how a human agent could instantiate the program and still not have the relevant intentionality. These two propositions have the following consequences: (3) The explanation of how the brain produces intentionality cannot be that it does it by instantiating a computer program. This is a strict logical consequence of 1 and 2. (4) Any mechanism capable of producing intentionality must have causal powers equal to those of the brain. This is meant to be a trivial consequence of 1. (5) Any attempt literally to create intentionality artificially (strong AI) could not succeed just by designing programs but would have to duplicate the causal powers of the human brain. This follows from 2 and 4. “Could a machine think?” On the argument advanced here only a machine could think, and only very special kinds of machines, namely brains and machines with internal causal powers equivalent to those of brains. And that is why strong AI has little to tell us about thinking, since it is not about machines but about programs, and no program by itself is sufficient for thinking.",5061.0
Complexity-based induction systems: Comparisons and convergence theorems,0453028e68581624ec68cfec70214231da8dbce7,"[{'authorId': '1727567', 'name': 'R. Solomonoff'}]",1978.0,IEEE Transactions on Information Theory,['AGI Safety Literature Review'],1,"In 1964 the author proposed as an explication of {\em a priori} probability the probability measure induced on output strings by a universal Turing machine with unidirectional output tape and a randomly coded unidirectional input tape. Levin has shown that if tilde{P}'_{M}(x) is an unnormalized form of this measure, and P(x) is any computable probability measure on strings, x , then \tilde{P}'_{M}\geqCP(x) where C is a constant independent of x . The corresponding result for the normalized form of this measure, P'_{M} , is directly derivable from Willis' probability measures on nonuniversal machines. If the conditional probabilities of P'_{M} are used to approximate those of P , then the expected value of the total squared error in these conditional probabilities is bounded by -(1/2) \ln C . With this error criterion, and when used as the basis of a universal gambling scheme, P'_{M} is superior to Cover's measure b\ast . When H\ast\equiv -\log_{2} P'_{M} is used to define the entropy of a rmite sequence, the equation H\ast(x,y)= H\ast(x)+H^{\ast}_{x}(y) holds exactly, in contrast to Chaitin's entropy definition, which has a nonvanishing error term in this equation.",404.0
Чего не могут вычислительные машины: Критика искусственного разума. (What computers cant do: A critique of artificial reason),bdb94023768f2aae1ce7e7f83a79fa021f92d30b,"[{'authorId': '1414061107', 'name': 'Дрейфус Хьюберт'}, {'authorId': '1414061257', 'name': 'Перевод с английского Н.Родман. Общая редакция'}, {'authorId': '1414031867', 'name': 'послесловие и примечания Б.В.Бирюкова.'}]",1978.0,,['AGI Safety Literature Review'],1,,221.0
A Treatise of Human Nature,5b07c4016a6547e1211a172fe5ea95db5d463b16,"[{'authorId': '152806633', 'name': 'David Hume'}]",1970.0,,"['AGI Safety Literature Review', 'Unsolved Problems in ML Safety', 'X-Risk Analysis for AI Research']",3,"PART 1: INTRODUCTORY MATERIAL. How to Use this Book. List of Abbreviations. Editor's Introduction. Hume's Early years and Education. A Treatise of Human Nature. Book 1: Of the Understanding. Book 1 part 1: The Elements of the Mental World. Book 1 Part 2: The Ideas of Space and Time. Book 1 Part 3: Knowledge, Probability, Belief, and Causation. Book 1 Part 4: Forms of Scepticism. Book 2: Of the passions. Book 2 Part 1: The Indirect Passions of Pride and Humility. Book 2 Part 2: The Indirect Passions of Love and Hatred. Book 2 part 3: The Direct Passions and the Will. Book 3: Of Morals. Book 3 Part 1: The Source of Moral Distinctions. Book 3 Part 2: The Artificial Virtues. Book 3 Part 3: Natural Virtues and Natural Abilities. The Abstract and the Early Reception of the Treatise. Supplementary Reading. A Note on the Texts of this Edition. PART 2: THE TEXT. Advertisement. Introduction. Book 1: Of the Understanding. Part 1: Of ideas, their origin, composition, connexion, abstraction, etc.. Sect. 1: Of the origin of our ideas. Sect. 2: Division of the subject. Sect. 3: Of the ideas of the memory and imagination. Sect. 4: Of the connexion of association of ideas. Sect. 5. Of relations. Sect. 6 Of modes and substances. Sect. 7: Of abstract ideas. Part 2: Of ideas of space and time. Sect. 1: Of the infinite divisibility of our ideas of space and time. Sect. 2: Of the infinite divisibility of space and time. Sect. 3. Of the other qualities of our ideas of space and time. Sect. 4. Objections answered. Sect. 5: The same subject continued. Sect. 6: Of the idea of existence and of external existence. Part 3: of knowledge and probability. Sect. 1: Of knowledge. Sect. 2. Of probability and of the idea of cause and effect. Sect. 3: Why a cause is always necessary. Sect. 4: Of the component parts of our reasonings concerning cause and effect. Sect. 5: Of the impressions of the senses and memory. Section. 6: Of the inference from the impression to the idea. Sect. 7: Of the nature of the idea or belief. Sect. 8: Of the causes of belief. Sect. 9: Of the effects of other relations and other habits. Sect 10. Of the influence of belief. Sect. 11: Of the probability of chances. Sect. 12: Of the probability of causes. Sect. 13: Of unphilosophical probability. Sect. 14: Of the idea of necessary connexion. Sect. 15: Rules by which to judge of causes and effects. Sect. 16: Of the reason of animals. Part 4: Of the sceptical and other systems of philosophy. Sect. 1: Of scepticism with regard to reason. Sect. 2: Of scepticism with regard to the senses. Sect. 3. Of the ancient philosophy. Sect 4. Of the modern philosophy. Sect. 5: Of the immateriality of the soul. Sect. 6: Of personal identity. Sect. 7: Conclusion of this book. Book 2: Of the Passions. Part 1: Of pride and humility. Sect. 1: Division of the subject. Sect. 2: Of pride and humility their objects and causes. Sect. 3: Whence these objects and causes are derived. Sect. 4: Of the relations of impressions and ideas. Sect. 5: Of the influence of these relations on pride and humility. Sect. 6: Limitations of this system. Sect. 7: Of vice and virtue. Sect. 8: Of beauty and deformity. Sect. 9: Of external advantages and disadvantages. Sect. 10: Of property and riches. Sect. 11: Of the love of fame. Sect. 12: Of the pride and humility of animals. Part 2: Of love and hatred. Sect. 1: Of the objects and causes of love and hatred. Sect. 2: Experiments to confirm this system. Sect. 3: Difficulties solved. Sect. 4: Of the love of relations. Sect. 5: Of our esteem for the rich and powerful. Sect 6: Of benevolence and anger. Sect. 7: Of compassion. Sect. 8: Of malice and envy. Sect. 9: Of the mixture of benevolence and anger with compassion and malice. Sect. 10. Of respect and contempt. Sect. 11: Of the amorous passion, or love betwixt the sexes. Sect. 12: Of the love and hatred of animals. Part 3: Of the will and direct passions. Sect. 1: Of liberty and necessity. Sect. 2: The same subject continued. Sect. 3: Of the influencing motives of the will. Sect. 4: Of the causes of the violent passions. Sect. 5: Of the effects of custom. Sect. Of the influence of the imagination on passions. Sect. 7: Of contiguity and distance in space and time. Sect. 8: The same subject continued. Sect. 9: Of the direct passions. Sect. 10: Of curiosity, or the love of truth. Book 3: Of Morals. Advertisement. Part 1: Of virtue and vice in general. Sect. 1: Moral distinctions not derived from reason. Sect. 2: Moral distinctions derived from a moral sense. Part 2: Of justice and injustice. Sect. 1: Justice, whether a natural or artificial virtue?. Sect. 2: Of the origin of justice and property. Sect. 3: Of the rules, which determine property. Sect. 4: Of the transference of property by consent. Sect. 5: Of the obligation of promises. Sect. 6: Some farther reflections concerning justice and injustice. Sect. 7: Of the origin of government. Sect. 8: Of the source of allegiance. Sect. 9: Of the measures of allegiance. Sect. 10: Of the objects of allegiance. Sect. 11: Of the laws of nations. Sect. 12: Of chastity and modesty. Part 3: Of the other virtues and vices. Sect. 1: Of the origin of the natural virtues and vices. Sect. 2: Of greatness of mind. Sect. 3. Of goodness and benevolence. Sect. 4: Of natural abilities. Sect. 5: Some farther reflections concerning the natural virtues. Sect. 6: Conclusion of this book. Appendix. An Abstract of ... A Treatise of Human Nature. PART 3 SUPPLEMENTARY MATERIAL. Editors' Annotations. Annotations to the Treatise. Annotations to the Abstract. Glossary. References. Index",10359.0
Speculations Concerning the First Ultraintelligent Machine,d7d9d643a378b6fd69fff63d113f4eae1983adc8,"[{'authorId': '145179124', 'name': 'I. Good'}]",1965.0,Advances in Computing,"['AGI Safety Literature Review', 'AI Research Considerations for Human Existential Safety (ARCHES)', 'Responses to catastrophic AGI risk: a survey', 'Superintelligence: Paths, Dangers, Strategies']",4,,470.0
A Formal Theory of Inductive Inference. Part II,bef2ae523cd4447af687fae13bfbb606e4a4a5ca,"[{'authorId': '1727567', 'name': 'R. Solomonoff'}]",1964.0,Information and Control,['AGI Safety Literature Review'],1,,1615.0
A Formal Theory of Inductive Inference. Part I,40b5e2fa3eaae17886c066c9f107c8c865b4808b,"[{'authorId': '1727567', 'name': 'R. Solomonoff'}]",1964.0,Information and Control,['AGI Safety Literature Review'],1,,1385.0
"Minds, Machines and Gödel",bde3b731bf73ef6052e34c4465e57718c03b13f8,"[{'authorId': '117214427', 'name': 'J. Lucas'}]",1961.0,Philosophy,['AGI Safety Literature Review'],1,"Gödei's Theorem seems to me to prove that Mechanism is false, that is, that minds cannot be explained as machines. So also has it seemed to many other people: almost every mathematical logician I have put the matter to has confessed to similar thoughts, but has felt reluctant to commit himself definitely until he could see the whole argument set out, with all objections fully stated and properly met. This I attempt to do.",527.0
AGI Safety Literature Review,6b0b1ecca32809b1d5b70b978521a8c27bc98e25,"[{'authorId': '1868196', 'externalIds': {'DBLP': ['Tom Everitt'], 'ORCID': '0000-0003-1210-9866'}, 'url': 'https://www.semanticscholar.org/author/1868196', 'name': 'Tom Everitt', 'aliases': None, 'affiliations': ['DeepMind'], 'homepage': 'http://tomeveritt.se', 'paperCount': 40, 'citationCount': 955, 'hIndex': 14}, {'authorId': '1954561', 'externalIds': {'DBLP': ['Gary Lea']}, 'url': 'https://www.semanticscholar.org/author/1954561', 'name': 'G. Lea', 'aliases': ['G. Lea', 'Gary R Lea', 'Gary R. Lea', 'Gary Lea'], 'affiliations': [], 'homepage': None, 'paperCount': 15, 'citationCount': 168, 'hIndex': 4}, {'authorId': '144154444', 'externalIds': {'DBLP': ['Marcus Hutter'], 'ORCID': '0000-0002-3263-4097'}, 'url': 'https://www.semanticscholar.org/author/144154444', 'name': 'Marcus Hutter', 'aliases': ['M. Hutter'], 'affiliations': [], 'homepage': 'http://www.hutter1.net/', 'paperCount': 300, 'citationCount': 7548, 'hIndex': 39}]",2018.0,International Joint Conference on Artificial Intelligence,"['AGI Safety Literature Review', 'AI safety: state of the field through quantitative lens']",2,"The development of Artificial General Intelligence (AGI) promises to be a major event. Along with its many potential benefits, it also raises serious safety concerns. The intention of this paper is to provide an easily accessible and up-to-date collection of references for the emerging field of AGI safety. A significant number of safety problems for AGI have been identified. We list these, and survey recent research on solving them. We also cover works on how best to think of AGI from the limited knowledge we have today, predictions for when AGI will first be created, and what will happen after its creation. Finally, we review the current public policy on AGI.",79.0
Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets,6915f983d0d207ec9ce2bb12eefed86f3a14584c,"[{'authorId': '146162186', 'name': 'Alethea Power'}, {'authorId': '3080409', 'name': 'Yuri Burda'}, {'authorId': '144632352', 'name': 'Harrison Edwards'}, {'authorId': '7309979', 'name': 'I. Babuschkin'}, {'authorId': '40055795', 'name': 'Vedant Misra'}]",2022.0,arXiv.org,"['Unsolved Problems in ML Safety', 'X-Risk Analysis for AI Research']",2,"In this paper we propose to study generalization of neural networks on small algorithmically generated datasets. In this setting, questions about data efficiency, memorization, generalization, and speed of learning can be studied in great detail. In some situations we show that neural networks learn through a process of""grokking""a pattern in the data, improving generalization performance from random chance level to perfect generalization, and that this improvement in generalization can happen well past the point of overfitting. We also study generalization as a function of dataset size and find that smaller datasets require increasing amounts of optimization for generalization. We argue that these datasets provide a fertile ground for studying a poorly understood aspect of deep learning: generalization of overparametrized neural networks beyond memorization of the finite training dataset.",71.0
PixMix: Dreamlike Pictures Comprehensively Improve Safety Measures,fe34bca61e451a532f45c680c232cb78bdc558cf,"[{'authorId': '3422872', 'name': 'Dan Hendrycks'}, {'authorId': '1380103052', 'name': 'Andy Zou'}, {'authorId': '16787428', 'name': 'Mantas Mazeika'}, {'authorId': '2144484107', 'name': 'Leonard Tang'}, {'authorId': '143771569', 'name': 'Bo Li'}, {'authorId': '143711382', 'name': 'D. Song'}, {'authorId': '5164568', 'name': 'J. Steinhardt'}]",2021.0,Computer Vision and Pattern Recognition,['Unsolved Problems in ML Safety'],1,"In real-world applications of machine learning, reliable and safe systems must consider measures of performance beyond standard test set accuracy. These other goals include out-of-distribution (OOD) robustness, prediction consistency, resilience to adversaries, calibrated uncertainty estimates, and the ability to detect anomalous inputs. However, improving performance towards these goals is often a balancing act that today's methods cannot achieve without sacrificing performance on other safety axes. For instance, adversarial training improves adversarial robustness but sharply degrades other classifier performance metrics. Similarly, strong data augmentation and regularization techniques often improve OOD robustness but harm anomaly detection, raising the question of whether a Pareto improvement on all existing safety measures is possible. To meet this challenge, we design a new data augmentation strategy utilizing the natural structural complexity of pictures such as fractals, which outperforms numerous baselines, is near Pareto-optimal, and roundly improves safety measures.",38.0
Augmenting Decision Making via Interactive What-If Analysis,d2a93e6ff04f94b8cc98f73effd6e630ea38c242,"[{'authorId': '1666631409', 'name': 'Sneha Gathani'}, {'authorId': '51309767', 'name': 'Madelon Hulsebos'}, {'authorId': '2069289251', 'name': 'James Gale'}, {'authorId': '37810307', 'name': 'P. Haas'}, {'authorId': '121365865', 'name': 'cCaugatay Demiralp'}]",2021.0,Conference on Innovative Data Systems Research,['Unsolved Problems in ML Safety'],1,"The fundamental goal of business data analysis is to improve business decisions using data. Business users often make decisions to achieve key performance indicators (KPIs) such as increasing customer retention or sales, or decreasing costs. To discover the relationship between data attributes hypothesized to be drivers and those corresponding to KPIs of interest, business users currently need to perform lengthy exploratory analyses. This involves considering multitudes of combinations and scenarios and performing slicing, dicing, and transformations on the data accordingly, e.g., analyzing customer retention across quarters of the year or suggesting optimal media channels across strata of customers. However, the increasing complexity of datasets combined with the cognitive limitations of humans makes it challenging to carry over multiple hypotheses, even for simple datasets. Therefore mentally performing such analyses is hard. Existing commercial tools either provide partial solutions whose effectiveness remains unclear or fail to cater to business users altogether.Herewe argue for four functionalities that we believe are necessary to enable business users to interactively learn and reason about the relationships (functions) between sets of data attributes thereby facilitating data-driven decision making. We implement these functionalities in S YSTEM D, an interactive visual data analysis system enabling business users to experiment with the data by asking what-if questions. We evaluate the system through three business use cases: marketing mix modeling, customer retention analysis, and deal closing analysis, and report on feedback from multiple business users. Overall, business users find the S YSTEM D functionalities highly useful for quick testing and validation of their hypotheses around their KPIs of interest, clearly addressing their unmet analysis needs. Their feedback also suggests that the user experience design can be enhanced to further improve the understandability of these functionalities.",2.0
TruthfulQA: Measuring How Models Mimic Human Falsehoods,77d956cdab4508d569ae5741549b78e715fd0749,"[{'authorId': '48639938', 'name': 'Stephanie C. Lin'}, {'authorId': '2052366271', 'name': 'Jacob Hilton'}, {'authorId': '47107786', 'name': 'Owain Evans'}]",2021.0,Annual Meeting of the Association for Computational Linguistics,['Unsolved Problems in ML Safety'],1,"We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a T5-based model. The best model was truthful on 58% of questions, while human performance was 94%. Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans. The largest models were generally the least truthful. This contrasts with other NLP tasks, where performance improves with model size. However, this result is expected if false answers are learned from the training distribution. We suggest that scaling up models alone is less promising for improving truthfulness than fine-tuning using training objectives other than imitation of text from the web.",115.0
The Values Encoded in Machine Learning Research,61e06615f6cee5ed1ba7d22b801925b69a45653b,"[{'authorId': '8318698', 'name': 'A. Birhane'}, {'authorId': '13014201', 'name': 'Pratyusha Kalluri'}, {'authorId': '35540755', 'name': 'Dallas Card'}, {'authorId': '27377925', 'name': 'William Agnew'}, {'authorId': '1441101651', 'name': 'Ravit Dotan'}, {'authorId': '2115910347', 'name': 'Michelle Bao'}]",2021.0,"Conference on Fairness, Accountability and Transparency",['Unsolved Problems in ML Safety'],1,"Machine learning currently exerts an outsized influence on the world, increasingly affecting institutional practices and impacted communities. It is therefore critical that we question vague conceptions of the field as value-neutral or universally beneficial, and investigate what specific values the field is advancing. In this paper, we first introduce a method and annotation scheme for studying the values encoded in documents such as research papers. Applying the scheme, we analyze 100 highly cited machine learning papers published at premier machine learning conferences, ICML and NeurIPS. We annotate key features of papers which reveal their values: their justification for their choice of project, which attributes of their project they uplift, their consideration of potential negative consequences, and their institutional affiliations and funding sources. We find that few of the papers justify how their project connects to a societal need (15%) and far fewer discuss negative potential (1%). Through line-by-line content analysis, we identify 59 values that are uplifted in ML research, and, of these, we find that the papers most frequently justify and assess themselves based on Performance, Generalization, Quantitative evidence, Efficiency, Building on past work, and Novelty. We present extensive textual evidence and identify key themes in the definitions and operationalization of these values. Notably, we find systematic textual evidence that these top values are being defined and applied with assumptions and implications generally supporting the centralization of power. Finally, we find increasingly close ties between these highly cited papers and tech companies and elite universities.",88.0
Poisoning and Backdooring Contrastive Learning,6c50e8db8d44a3399a78adb8fab2d7f81a029c33,"[{'authorId': '2483738', 'name': 'Nicholas Carlini'}, {'authorId': '1763579', 'name': 'A. Terzis'}]",2021.0,International Conference on Learning Representations,['Unsolved Problems in ML Safety'],1,"Multimodal contrastive learning methods like CLIP train on noisy and uncurated training datasets. This is cheaper than labeling datasets manually, and even improves out-of-distribution robustness. We show that this practice makes backdoor and poisoning attacks a significant threat. By poisoning just 0.01% of a dataset (e.g., just 300 images of the 3 million-example Conceptual Captions dataset), we can cause the model to misclassify test images by overlaying a small patch. Targeted poisoning attacks, whereby the model misclassifies a particular test input with an adversarially-desired label, are even easier requiring control of 0.0001% of the dataset (e.g., just three out of the 3 million images). Our attacks call into question whether training on noisy and uncurated Internet scrapes is desirable.",52.0
Consistency Regularization for Adversarial Robustness,33ca8d34d226e47e0830b6eb73c06e0b85ae7ab7,"[{'authorId': '1750599181', 'name': 'Jihoon Tack'}, {'authorId': '2052088734', 'name': 'Sihyun Yu'}, {'authorId': '83125078', 'name': 'Jongheon Jeong'}, {'authorId': '2116505846', 'name': 'Minseong Kim'}, {'authorId': '2110796623', 'name': 'S. Hwang'}, {'authorId': '143720148', 'name': 'Jinwoo Shin'}]",2021.0,AAAI Conference on Artificial Intelligence,['Unsolved Problems in ML Safety'],1,"Adversarial training (AT) is currently one of the most successful methods to obtain the adversarial robustness of deep neural networks. However, the phenomenon of robust overfitting, i.e., the robustness starts to decrease significantly during AT, has been problematic, not only making practitioners consider a bag of tricks for a successful training, e.g., early stopping, but also incurring a significant generalization gap in the robustness. In this paper, we propose an effective regularization technique that prevents robust overfitting by optimizing an auxiliary `consistency' regularization loss during AT. Specifically, we discover that data augmentation is a quite effective tool to mitigate the overfitting in AT, and develop a regularization that forces the predictive distributions after attacking from two different augmentations of the same instance to be similar with each other. Our experimental results demonstrate that such a simple regularization technique brings significant improvements in the test robust accuracy of a wide range of AT methods. More remarkably, we also show that our method could significantly help the model to generalize its robustness against unseen adversaries, e.g., other types or larger perturbations compared to those used during training. Code is available at https://github.com/alinlab/consistency-adversarial.",18.0
What Would Jiminy Cricket Do? Towards Agents That Behave Morally,81d6dc0a54531874e26521082242158ad0f6da21,"[{'authorId': '3422872', 'name': 'Dan Hendrycks'}, {'authorId': '16787428', 'name': 'Mantas Mazeika'}, {'authorId': '1380103052', 'name': 'Andy Zou'}, {'authorId': '2109521852', 'name': 'Sahil Patel'}, {'authorId': '2125433744', 'name': 'Christine Zhu'}, {'authorId': '2142469675', 'name': 'Jesus Navarro'}, {'authorId': '143711382', 'name': 'D. Song'}, {'authorId': '2151288080', 'name': 'Bo Li'}, {'authorId': '5164568', 'name': 'J. Steinhardt'}]",2021.0,NeurIPS Datasets and Benchmarks,"['Unsolved Problems in ML Safety', 'X-Risk Analysis for AI Research']",2,"When making everyday decisions, people are guided by their conscience, an internal sense of right and wrong, to behave morally. By contrast, artificial agents may behave immorally when trained on environments that ignore moral concerns, such as violent video games. With the advent of generally capable agents that pretrain on many environments, mitigating inherited biases towards immoral behavior will become necessary. However, prior work on aligning agents with human values and morals focuses on small-scale settings lacking in semantic complexity. To enable research in larger, more realistic settings, we introduce Jiminy Cricket, an environment suite of 25 text-based adventure games with thousands of semantically rich, morally salient scenarios. Via dense annotations for every possible action, Jiminy Cricket environments robustly evaluate whether agents can act morally while maximizing reward. To improve moral behavior, we leverage language models with commonsense moral knowledge and develop strategies to mediate this knowledge into actions. In extensive experiments, we find that our artificial conscience approach can steer agents towards moral behavior without sacrificing performance.",16.0
Robustness and Generalization via Generative Adversarial Training,2344be99d7c7de8c70c485a79e52f2ddfd35bac8,"[{'authorId': '1973062', 'name': 'Omid Poursaeed'}, {'authorId': '2114746685', 'name': 'Tianxing Jiang'}, {'authorId': '2110162580', 'name': 'Harry Yang'}, {'authorId': '2067789287', 'name': 'S. Belongie'}, {'authorId': '38760573', 'name': 'Ser-Nam Lim'}]",2021.0,IEEE International Conference on Computer Vision,['Unsolved Problems in ML Safety'],1,"While deep neural networks have achieved remarkable success in various computer vision tasks, they often fail to generalize to new domains and subtle variations of input images. Several defenses have been proposed to improve the robustness against these variations. However, current defenses can only withstand the specific attack used in training, and the models often remain vulnerable to other input variations. Moreover, these methods often degrade performance of the model on clean images and do not generalize to out-of-domain samples. In this paper we present Generative Adversarial Training, an approach to simultaneously improve the model’s generalization to the test set and out-of-domain samples as well as its robustness to unseen adversarial attacks. Instead of altering a low-level pre-defined aspect of images, we generate a spectrum of low-level, mid-level and high-level changes using generative models with a disentangled latent space. Adversarial training with these examples enable the model to withstand a wide range of attacks by observing a variety of input alterations during training. We show that our approach not only improves performance of the model on clean images and out-of-domain samples but also makes it robust against unforeseen attacks and outperforms prior work. We validate effectiveness of our method by demonstrating results on various tasks such as classification, segmentation and object detection.",8.0
Towards Understanding the Generative Capability of Adversarially Robust Classifiers,8d95e99e14f6a588f92becc278717e9c87b84022,"[{'authorId': '2153095684', 'name': 'Yao Zhu'}, {'authorId': '2146394909', 'name': 'Jiacheng Ma'}, {'authorId': '2136769001', 'name': 'Jiacheng Sun'}, {'authorId': '2117987165', 'name': 'Zewei Chen'}, {'authorId': '3115947', 'name': 'Rongxin Jiang'}, {'authorId': '7718952', 'name': 'Zhenguo Li'}]",2021.0,IEEE International Conference on Computer Vision,['Unsolved Problems in ML Safety'],1,"Recently, some works found an interesting phenomenon that adversarially robust classifiers can generate good images comparable to generative models. We investigate this phenomenon from an energy perspective and provide a novel explanation. We reformulate adversarial example generation, adversarial training, and image generation in terms of an energy function. We find that adversarial training contributes to obtaining an energy function that is flat and has low energy around the real data, which is the key for generative capability. Based on our new understanding, we further propose a better adversarial training method, Joint Energy Adversarial Training (JEAT), which can generate high-quality images and achieve new state-of-the-art robustness under a wide range of attacks. The Inception Score of the images (CIFAR-10) generated by JEAT is 8.80, much better than original robust classifiers (7.50). In particular, we find that the robustness of JEAT is better than other hybrid models.",13.0
Program Synthesis with Large Language Models,a38e0f993e4805ba8a9beae4c275c91ffcec01df,"[{'authorId': '2058365883', 'name': 'Jacob Austin'}, {'authorId': '2624088', 'name': 'Augustus Odena'}, {'authorId': '51150953', 'name': 'Maxwell Nye'}, {'authorId': '40377863', 'name': 'Maarten Bosma'}, {'authorId': '47407464', 'name': 'H. Michalewski'}, {'authorId': '35363891', 'name': 'David Dohan'}, {'authorId': '122064392', 'name': 'Ellen Jiang'}, {'authorId': '145941081', 'name': 'Carrie J. Cai'}, {'authorId': '2053829286', 'name': 'Michael Terry'}, {'authorId': '1397917613', 'name': 'Quoc V. Le'}, {'authorId': '152549864', 'name': 'Charles Sutton'}]",2021.0,arXiv.org,['Unsolved Problems in ML Safety'],1,"This paper explores the limits of the current generation of large language models for program synthesis in general purpose programming languages. We evaluate a collection of such models (with between 244M and 137B parameters) on two new benchmarks, MBPP and MathQA-Python, in both the few-shot and fine-tuning regimes. Our benchmarks are designed to measure the ability of these models to synthesize short Python programs from natural language descriptions. The Mostly Basic Programming Problems (MBPP) dataset contains 974 programming tasks, designed to be solvable by entry-level programmers. The MathQA-Python dataset, a Python version of the MathQA benchmark, contains 23914 problems that evaluate the ability of the models to synthesize code from more complex text. On both datasets, we find that synthesis performance scales log-linearly with model size. Our largest models, even without finetuning on a code dataset, can synthesize solutions to 59.6 percent of the problems from MBPP using few-shot learning with a well-designed prompt. Fine-tuning on a held-out portion of the dataset improves performance by about 10 percentage points across most model sizes. On the MathQA-Python dataset, the largest fine-tuned model achieves 83.8 percent accuracy. Going further, we study the model's ability to engage in dialog about code, incorporating human feedback to improve its solutions. We find that natural language feedback from a human halves the error rate compared to the model's initial prediction. Additionally, we conduct an error analysis to shed light on where these models fall short and what types of programs are most difficult to generate. Finally, we explore the semantic grounding of these models by fine-tuning them to predict the results of program execution. We find that even our best models are generally unable to predict the output of a program given a specific input.",197.0
On the Opportunities and Risks of Foundation Models,4f68e07c6c3173480053fd52391851d6f80d651b,"[{'authorId': '150272855', 'name': 'Rishi Bommasani'}, {'authorId': '152951058', 'name': 'Drew A. Hudson'}, {'authorId': '3419364', 'name': 'E. Adeli'}, {'authorId': '2055661703', 'name': 'R. Altman'}, {'authorId': '47038321', 'name': 'Simran Arora'}, {'authorId': '1825243321', 'name': 'Sydney von Arx'}, {'authorId': '145879842', 'name': 'Michael S. Bernstein'}, {'authorId': '1775407', 'name': 'J. Bohg'}, {'authorId': '2691021', 'name': 'Antoine Bosselut'}, {'authorId': '2563117', 'name': 'Emma Brunskill'}, {'authorId': '2841157', 'name': 'E. Brynjolfsson'}, {'authorId': '8983218', 'name': 'S. Buch'}, {'authorId': '35540755', 'name': 'Dallas Card'}, {'authorId': '2119294311', 'name': 'Rodrigo Castellon'}, {'authorId': '22193324', 'name': 'Niladri S. Chatterji'}, {'authorId': '2111073657', 'name': 'Annie S. Chen'}, {'authorId': '1383066965', 'name': 'Kathleen A. Creel'}, {'authorId': '29827891', 'name': 'Jared Davis'}, {'authorId': '2123319430', 'name': 'Dora Demszky'}, {'authorId': '1872307', 'name': 'Chris Donahue'}, {'authorId': '2086066795', 'name': 'Moussa Doumbouya'}, {'authorId': '41152329', 'name': 'Esin Durmus'}, {'authorId': '2490652', 'name': 'S. Ermon'}, {'authorId': '3141819', 'name': 'J. Etchemendy'}, {'authorId': '10324691', 'name': 'Kawin Ethayarajh'}, {'authorId': '1435579960', 'name': 'L. Fei-Fei'}, {'authorId': '46881670', 'name': 'Chelsea Finn'}, {'authorId': '2066558041', 'name': 'Trevor Gale'}, {'authorId': '2059025041', 'name': 'Lauren E. Gillespie'}, {'authorId': '1822288', 'name': 'Karan Goel'}, {'authorId': '144002017', 'name': 'Noah D. Goodman'}, {'authorId': '69487116', 'name': 'S. Grossman'}, {'authorId': '2820009', 'name': 'Neel Guha'}, {'authorId': '2117567142', 'name': 'Tatsunori Hashimoto'}, {'authorId': '40068904', 'name': 'Peter Henderson'}, {'authorId': '145430120', 'name': 'John Hewitt'}, {'authorId': '2056459887', 'name': 'Daniel E. Ho'}, {'authorId': '2110641422', 'name': 'Jenny Hong'}, {'authorId': '32028215', 'name': 'Kyle Hsu'}, {'authorId': '30768523', 'name': 'Jing Huang'}, {'authorId': '8938047', 'name': 'Thomas F. Icard'}, {'authorId': '2116998535', 'name': 'Saahil Jain'}, {'authorId': '1746807', 'name': 'Dan Jurafsky'}, {'authorId': '13014201', 'name': 'Pratyusha Kalluri'}, {'authorId': '10737060', 'name': 'Siddharth Karamcheti'}, {'authorId': '12604592', 'name': 'G. Keeling'}, {'authorId': '3218675', 'name': 'Fereshte Khani'}, {'authorId': '144112155', 'name': 'O. Khattab'}, {'authorId': '2572525', 'name': 'Pang Wei Koh'}, {'authorId': '2053832474', 'name': 'M. Krass'}, {'authorId': '145237361', 'name': 'Ranjay Krishna'}, {'authorId': '82787161', 'name': 'Rohith Kuditipudi'}, {'authorId': '32423266', 'name': 'Ananya Kumar'}, {'authorId': '8759332', 'name': 'Faisal Ladhak'}, {'authorId': '49316195', 'name': 'Mina Lee'}, {'authorId': '2110585783', 'name': 'Tony Lee'}, {'authorId': '1702139', 'name': 'J. Leskovec'}, {'authorId': '2123318613', 'name': 'Isabelle Levent'}, {'authorId': '32551341', 'name': 'Xiang Lisa Li'}, {'authorId': '2145429039', 'name': 'Xuechen Li'}, {'authorId': '2114186424', 'name': 'Tengyu Ma'}, {'authorId': '47938601', 'name': 'Ali Malik'}, {'authorId': '144783904', 'name': 'Christopher D. Manning'}, {'authorId': '2247302', 'name': 'Suvir Mirchandani'}, {'authorId': '49688913', 'name': 'E. Mitchell'}, {'authorId': '2123319428', 'name': 'Zanele Munyikwa'}, {'authorId': '4734949', 'name': 'Suraj Nair'}, {'authorId': '1381444249', 'name': 'A. Narayan'}, {'authorId': '22252150', 'name': 'D. Narayanan'}, {'authorId': '51149693', 'name': 'Benjamin Newman'}, {'authorId': '21771052', 'name': 'Allen Nie'}, {'authorId': '9200530', 'name': 'Juan Carlos Niebles'}, {'authorId': '8791376', 'name': 'H. Nilforoshan'}, {'authorId': '2091807392', 'name': 'J. Nyarko'}, {'authorId': '2123320295', 'name': 'Giray Ogut'}, {'authorId': '4773175', 'name': 'Laurel J. Orr'}, {'authorId': '2085843465', 'name': 'Isabel Papadimitriou'}, {'authorId': '2116649486', 'name': 'J. Park'}, {'authorId': '2012749', 'name': 'C. Piech'}, {'authorId': '27729635', 'name': 'Eva Portelance'}, {'authorId': '144922861', 'name': 'Christopher Potts'}, {'authorId': '2655157', 'name': 'Aditi Raghunathan'}, {'authorId': '145523471', 'name': 'Robert Reich'}, {'authorId': '40046694', 'name': 'Hongyu Ren'}, {'authorId': '2047004093', 'name': 'Frieda Rong'}, {'authorId': '51285810', 'name': 'Yusuf H. Roohani'}, {'authorId': '123463839', 'name': 'Camilo Ruiz'}, {'authorId': '2004576551', 'name': 'Jack Ryan'}, {'authorId': '2061444681', 'name': ""Christopher R'e""}, {'authorId': '1779671', 'name': 'Dorsa Sadigh'}, {'authorId': '2389237', 'name': 'Shiori Sagawa'}, {'authorId': '50818255', 'name': 'Keshav Santhanam'}, {'authorId': '145117953', 'name': 'Andy Shih'}, {'authorId': '2093939303', 'name': 'K. Srinivasan'}, {'authorId': '88726969', 'name': 'Alex Tamkin'}, {'authorId': '46199305', 'name': 'Rohan Taori'}, {'authorId': '81623398', 'name': 'A. Thomas'}, {'authorId': '2444919', 'name': 'Florian Tramèr'}, {'authorId': '2155890009', 'name': 'Rose E. Wang'}, {'authorId': '2127971344', 'name': 'William Wang'}, {'authorId': '95528042', 'name': 'Bohan Wu'}, {'authorId': '2110435874', 'name': 'Jiajun Wu'}, {'authorId': '3374063', 'name': 'Yuhuai Wu'}, {'authorId': '46215055', 'name': 'Sang Michael Xie'}, {'authorId': '19168196', 'name': 'Michihiro Yasunaga'}, {'authorId': '145829303', 'name': 'Jiaxuan You'}, {'authorId': '143834867', 'name': 'M. Zaharia'}, {'authorId': '2124860797', 'name': 'Michael Zhang'}, {'authorId': '123437034', 'name': 'Tianyi Zhang'}, {'authorId': '88342435', 'name': 'Xikun Zhang'}, {'authorId': '49889860', 'name': 'Yuhui Zhang'}, {'authorId': '2118604716', 'name': 'Lucia Zheng'}, {'authorId': '3396547', 'name': 'Kaitlyn Zhou'}, {'authorId': '145419642', 'name': 'Percy Liang'}]",2021.0,arXiv.org,['Unsolved Problems in ML Safety'],1,"AI is undergoing a paradigm shift with the rise of models (e.g., BERT, DALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human interaction) and technical principles(e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities,and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and what they are even capable of due to their emergent properties. To tackle these questions, we believe much of the critical research on foundation models will require deep interdisciplinary collaboration commensurate with their fundamentally sociotechnical nature.",901.0
Triggering Failures: Out-Of-Distribution detection by learning from local adversarial attacks in Semantic Segmentation,393e0b8459eb1608b6b35d6057da4ddb09957555,"[{'authorId': '1400349847', 'name': 'Victor Besnier'}, {'authorId': '3056236', 'name': 'Andrei Bursuc'}, {'authorId': '145897899', 'name': 'David Picard'}, {'authorId': '50711387', 'name': 'Alexandre Briot'}]",2021.0,IEEE International Conference on Computer Vision,['Unsolved Problems in ML Safety'],1,"In this paper, we tackle the detection of out-of-distribution (OOD) objects in semantic segmentation. By analyzing the literature, we found that current methods are either accurate or fast but not both which limits their usability in real world applications. To get the best of both aspects, we propose to mitigate the common shortcomings by following four design principles: decoupling the OOD detection from the segmentation task, observing the entire segmentation network instead of just its output, generating training data for the OOD detector by leveraging blind spots in the segmentation network and focusing the generated data on localized regions in the image to simulate OOD objects. Our main contribution is a new OOD detection architecture called ObsNet associated with a dedicated training scheme based on Local Adversarial Attacks (LAA). We validate the soundness of our approach across numerous ablation studies. We also show it obtains top performances both in speed and accuracy when compared to ten recent methods of the literature on three different datasets.",20.0
What are you optimizing for? Aligning Recommender Systems with Human Values,ea69ce00936f5f60cce995cb88b8ba2339a007d0,"[{'authorId': '48756178', 'name': 'J. Stray'}, {'authorId': '2210865', 'name': 'Ivan Vendrov'}, {'authorId': '10115554', 'name': 'Jeremy Nixon'}, {'authorId': '2120820828', 'name': 'Steven Adler'}, {'authorId': '1397904824', 'name': 'Dylan Hadfield-Menell'}]",2021.0,arXiv.org,['Unsolved Problems in ML Safety'],1,"We describe cases where real recommender systems were modified in the service of various human values such as diversity, fairness, well-being, time well spent, and factual accuracy. From this we identify the current practice of values engineering: the creation of classifiers from humancreated data with value-based labels. This has worked in practice for a variety of issues, but problems are addressed one at a time, and users and other stakeholders have seldom been involved. Instead, we look to AI alignment work for approaches that could learn complex values directly from stakeholders, and identify four major directions: useful measures of alignment, participatory design and operation, interactive value learning, and informed deliberative judgments.",29.0
Conservative Objective Models for Effective Offline Model-Based Optimization,792249fdfb563263bb15adf659ff735630dab098,"[{'authorId': '1389987604', 'name': 'Brandon Trabucco'}, {'authorId': '1488785534', 'name': 'Aviral Kumar'}, {'authorId': '3468192', 'name': 'Xinyang Geng'}, {'authorId': '1736651', 'name': 'S. Levine'}]",2021.0,International Conference on Machine Learning,['Unsolved Problems in ML Safety'],1,"In this paper, we aim to solve data-driven modelbased optimization (MBO) problems, where the goal is to find a design input that maximizes an unknown objective function provided access to only a static dataset of inputs and their corresponding objective values. Such data-driven optimization procedures are the only practical methods in many real-world domains where active data collection is expensive (e.g., when optimizing over proteins) or dangerous (e.g., when optimizing over aircraft designs, actively evaluating malformed aircraft designs is unsafe). Typical methods for MBO that optimize the input against a learned model of the unknown score function are affected by erroneous overestimation in the learned model caused due to distributional shift, that drives the optimizer to low-scoring or invalid inputs. To overcome this, we propose conservative objective models (COMs), a method that learns a model of the objective function which lower bounds the actual value of the ground-truth objective on outof-distribution inputs and uses it for optimization. In practice, COMs outperform a number existing methods on a wide range of MBO problems, including optimizing controller parameters, robot morphologies, and superconducting materials.",32.0
Out-of-Distribution Dynamics Detection: RL-Relevant Benchmarks and Results,dc6ad02d48040c9171f7aca52f169469488c160b,"[{'authorId': '1739750274', 'name': 'Mohamad H. Danesh'}, {'authorId': '145841336', 'name': 'Alan Fern'}]",2021.0,arXiv.org,['Unsolved Problems in ML Safety'],1,"We study the problem of out-of-distribution dynamics (OODD) detection, which involves detecting when the dynamics of a temporal process change compared to the training-distribution dynamics. This is relevant to applications in control, reinforcement learning (RL), and multi-variate time-series, where changes to test time dynamics can impact the performance of learning con-trollers/predictors in unknown ways. This problem is particularly important in the context of deep RL, where learned controllers often overﬁt to the training environment. Currently, however, there is a lack of established OODD benchmarks for the types of environments commonly used in RL research. Our ﬁrst contribution is to design a set of OODD benchmarks derived from common RL environments with varying types and inten-sities of OODD. Our second contribution is to design a strong OODD baseline approach based on recurrent implicit quantile network (RIQN), which monitors autoregressive prediction errors for OODD detection. In addition to RIQN, we introduce and test three other simpler baselines. Our ﬁnal contribution is to evaluate our baseline approaches on the benchmarks to provide results for future comparison. Code are publicly available on GitHub 1 .",5.0
Evaluating Large Language Models Trained on Code,acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269,"[{'authorId': '2108828435', 'name': 'Mark Chen'}, {'authorId': '2065005836', 'name': 'Jerry Tworek'}, {'authorId': '35450887', 'name': 'Heewoo Jun'}, {'authorId': '153930486', 'name': 'Qiming Yuan'}, {'authorId': '2117715024', 'name': 'Henrique Ponde'}, {'authorId': '2053807409', 'name': 'Jared Kaplan'}, {'authorId': '144632352', 'name': 'Harrison Edwards'}, {'authorId': '51178856', 'name': 'Yura Burda'}, {'authorId': '2117706920', 'name': 'Nicholas Joseph'}, {'authorId': '2065151121', 'name': 'Greg Brockman'}, {'authorId': '2064770039', 'name': 'Alex Ray'}, {'authorId': '41158993', 'name': 'Raul Puri'}, {'authorId': '2064404342', 'name': 'Gretchen Krueger'}, {'authorId': '2136008481', 'name': 'Michael Petrov'}, {'authorId': '2103414', 'name': 'Heidy Khlaaf'}, {'authorId': '144864359', 'name': 'Girish Sastry'}, {'authorId': '2051714782', 'name': 'Pamela Mishkin'}, {'authorId': '1466431052', 'name': 'Brooke Chan'}, {'authorId': '145565184', 'name': 'Scott Gray'}, {'authorId': '39849748', 'name': 'Nick Ryder'}, {'authorId': '2068123790', 'name': 'Mikhail Pavlov'}, {'authorId': '146162186', 'name': 'Alethea Power'}, {'authorId': '40527594', 'name': 'Lukasz Kaiser'}, {'authorId': '2400764', 'name': 'Mohammad Bavarian'}, {'authorId': '2059411355', 'name': 'Clemens Winter'}, {'authorId': '2704719', 'name': 'Philippe Tillet'}, {'authorId': '9927844', 'name': 'F. Such'}, {'authorId': '80876468', 'name': 'D. Cummings'}, {'authorId': '3407285', 'name': 'Matthias Plappert'}, {'authorId': '2117714459', 'name': 'Fotios Chantzis'}, {'authorId': '2057742918', 'name': 'Elizabeth Barnes'}, {'authorId': '1404060687', 'name': 'Ariel Herbert-Voss'}, {'authorId': '39121861', 'name': 'William H. Guss'}, {'authorId': '38967461', 'name': 'Alex Nichol'}, {'authorId': '7309979', 'name': 'I. Babuschkin'}, {'authorId': '2054519183', 'name': 'S. Balaji'}, {'authorId': '150298413', 'name': 'Shantanu Jain'}, {'authorId': '153480842', 'name': 'A. Carr'}, {'authorId': '2990741', 'name': 'J. Leike'}, {'authorId': '3381809', 'name': 'Joshua Achiam'}, {'authorId': '40055795', 'name': 'Vedant Misra'}, {'authorId': '1404556973', 'name': 'Evan Morikawa'}, {'authorId': '38909097', 'name': 'Alec Radford'}, {'authorId': '3555117', 'name': 'M. Knight'}, {'authorId': '35167962', 'name': 'Miles Brundage'}, {'authorId': '2117715631', 'name': 'Mira Murati'}, {'authorId': '2059169400', 'name': 'Katie Mayer'}, {'authorId': '2930640', 'name': 'P. Welinder'}, {'authorId': '39593364', 'name': 'Bob McGrew'}, {'authorId': '2698777', 'name': 'Dario Amodei'}, {'authorId': '52238703', 'name': 'Sam McCandlish'}, {'authorId': '1701686', 'name': 'Ilya Sutskever'}, {'authorId': '2563432', 'name': 'Wojciech Zaremba'}]",2021.0,arXiv.org,"['Unsolved Problems in ML Safety', 'X-Risk Analysis for AI Research']",2,"We introduce Codex, a GPT language model fine-tuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8% of the problems, while GPT-3 solves 0% and GPT-J solves 11.4%. Furthermore, we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts. Using this method, we solve 70.2% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics.",733.0
Test-Time Adaptation to Distribution Shift by Confidence Maximization and Input Transformation,e3c867bc48b224f376f02f5ac31aae22978e7563,"[{'authorId': '29359383', 'name': 'Chaithanya Kumar Mummadi'}, {'authorId': '32038986', 'name': 'Robin Hutmacher'}, {'authorId': '2451538', 'name': 'K. Rambach'}, {'authorId': '3154212', 'name': 'Evgeny Levinkov'}, {'authorId': '1710872', 'name': 'T. Brox'}, {'authorId': '2708564', 'name': 'J. H. Metzen'}]",2021.0,arXiv.org,['Unsolved Problems in ML Safety'],1,"Deep neural networks often exhibit poor performance on data that is unlikely under the train-time data distribution, for instance data affected by corruptions. Previous works demonstrate that test-time adaptation to data shift, for instance using entropy minimization, effectively improves performance on such shifted distributions. This paper focuses on the fully test-time adaptation setting, where only unlabeled data from the target distribution is required. This allows adapting arbitrary pretrained networks. Specifically, we propose a novel loss that improves test-time adaptation by addressing both premature convergence and instability of entropy minimization. This is achieved by replacing the entropy by a non-saturating surrogate and adding a diversity regularizer based on batch-wise entropy maximization that prevents convergence to trivial collapsed solutions. Moreover, we propose to prepend an input transformation module to the network that can partially undo test-time distribution shifts. Surprisingly, this preprocessing can be learned solely using the fully test-time adaptation loss in an end-to-end fashion without any target domain labels or source domain data. We show that our approach outperforms previous work in improving the robustness of publicly available pretrained image classifiers to common corruptions on such challenging benchmarks as ImageNet-C.",30.0
Handcrafted Backdoors in Deep Neural Networks,721489ef0b26e7ef589b06f94eb35638b52a14f1,"[{'authorId': '2111053680', 'name': 'Sanghyun Hong'}, {'authorId': '2483738', 'name': 'Nicholas Carlini'}, {'authorId': '145714153', 'name': 'Alexey Kurakin'}]",2021.0,arXiv.org,['Unsolved Problems in ML Safety'],1,"When machine learning training is outsourced to third parties, backdoor attacks become practical as the third party who trains the model may act maliciously to inject hidden behaviors into the otherwise accurate model. Until now, the mechanism to inject backdoors has been limited to poisoning . We argue that a supply-chain attacker has more attack techniques available by introducing a handcrafted attack that directly manipulates a model’s weights. This direct modiﬁcation gives our attacker more degrees of freedom compared to poisoning, and we show it can be used to evade many backdoor detection or removal defenses effectively. Across four datasets and four network architectures our backdoor attacks maintain an attack success rate above 96%. Our results suggest that further research is needed for understanding the complete space of supply-chain backdoor attacks.",20.0
There Is No Turning Back: A Self-Supervised Approach for Reversibility-Aware Reinforcement Learning,f70d68fc66d06e067b6aee4d97cb3395a672cc71,"[{'authorId': '2007774239', 'name': 'Nathan Grinsztajn'}, {'authorId': '151047979', 'name': 'Johan Ferret'}, {'authorId': '1721354', 'name': 'O. Pietquin'}, {'authorId': '34682317', 'name': 'P. Preux'}, {'authorId': '1737555', 'name': 'M. Geist'}]",2021.0,Neural Information Processing Systems,['Unsolved Problems in ML Safety'],1,"We propose to learn to distinguish reversible from irreversible actions for better informed decision-making in Reinforcement Learning (RL). From theoretical considerations, we show that approximate reversibility can be learned through a simple surrogate task: ranking randomly sampled trajectory events in chronological order. Intuitively, pairs of events that are always observed in the same order are likely to be separated by an irreversible sequence of actions. Conveniently, learning the temporal order of events can be done in a fully self-supervised way, which we use to estimate the reversibility of actions from experience, without any priors. We propose two different strategies that incorporate reversibility in RL agents, one strategy for exploration (RAE) and one strategy for control (RAC). We demonstrate the potential of reversibility-aware agents in several environments, including the challenging Sokoban game. In synthetic tasks, we show that we can learn control policies that never fail and reduce to zero the side-effects of interactions, even without access to the reward function.",9.0
Concealed Data Poisoning Attacks on NLP Models,7571ed4cf1bbdcf891b576a0da12c910b1f0c72f,"[{'authorId': '145217343', 'name': 'Eric Wallace'}, {'authorId': '145914976', 'name': 'Tony Zhao'}, {'authorId': '144588144', 'name': 'Shi Feng'}, {'authorId': '34650964', 'name': 'Sameer Singh'}]",2021.0,North American Chapter of the Association for Computational Linguistics,['Unsolved Problems in ML Safety'],1,"Adversarial attacks alter NLP model predictions by perturbing test-time inputs. However, it is much less understood whether, and how, predictions can be manipulated with small, concealed changes to the training data. In this work, we develop a new data poisoning attack that allows an adversary to control model predictions whenever a desired trigger phrase is present in the input. For instance, we insert 50 poison examples into a sentiment model’s training set that causes the model to frequently predict Positive whenever the input contains “James Bond”. Crucially, we craft these poison examples using a gradient-based procedure so that they do not mention the trigger phrase. We also apply our poison attack to language modeling (“Apple iPhone” triggers negative generations) and machine translation (“iced coffee” mistranslated as “hot coffee”). We conclude by proposing three defenses that can mitigate our attack at some cost in prediction accuracy or extra human annotation.",49.0
Fighting Gradients with Gradients: Dynamic Defenses against Adversarial Attacks,b301da25b3d432e8293ca6c4ae27fed4f2689c03,"[{'authorId': '2118962158', 'name': 'Dequan Wang'}, {'authorId': '2067404069', 'name': 'An Ju'}, {'authorId': '1782282', 'name': 'Evan Shelhamer'}, {'authorId': '2065515758', 'name': 'David A. Wagner'}, {'authorId': '1753210', 'name': 'Trevor Darrell'}]",2021.0,arXiv.org,['Unsolved Problems in ML Safety'],1,"Adversarial attacks optimize against models to defeat defenses. Existing defenses are static, and stay the same once trained, even while attacks change. We argue that models should fight back, and optimize their defenses against attacks at test time. We propose dynamic defenses, to adapt the model and input during testing, by defensive entropy minimization (dent). Dent alters testing, but not training, for compatibility with existing models and train-time defenses. Dent improves the robustness of adversarially-trained defenses and nominally-trained models against white-box, black-box, and adaptive attacks on CIFAR-10/100 and ImageNet. In particular, dent boosts state-ofthe-art defenses by 20+ points absolute against AutoAttack on CIFAR-10 at ∞ = 8/255.",13.0
Emerging Properties in Self-Supervised Vision Transformers,ad4a0938c48e61b7827869e4ac3baffd0aefab35,"[{'authorId': '2062862676', 'name': 'Mathilde Caron'}, {'authorId': '2113243762', 'name': 'Hugo Touvron'}, {'authorId': '1806773', 'name': 'Ishan Misra'}, {'authorId': '2065248680', 'name': ""Herv'e J'egou""}, {'authorId': '2599292', 'name': 'J. Mairal'}, {'authorId': '2329288', 'name': 'Piotr Bojanowski'}, {'authorId': '2319608', 'name': 'Armand Joulin'}]",2021.0,IEEE International Conference on Computer Vision,['Unsolved Problems in ML Safety'],1,"In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) [16] that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder [26], multi-crop training [9], and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base.",1428.0
IMAGINE: Image Synthesis by Image-Guided Model Inversion,d7a40ebebd8c00ff853b64ebecad366477bf0adb,"[{'authorId': '2118951664', 'name': 'Pei Wang'}, {'authorId': '152998391', 'name': 'Yijun Li'}, {'authorId': '2108542429', 'name': 'Krishna Kumar Singh'}, {'authorId': '2054975', 'name': 'Jingwan Lu'}, {'authorId': '1699559', 'name': 'N. Vasconcelos'}]",2021.0,Computer Vision and Pattern Recognition,['Unsolved Problems in ML Safety'],1,"We introduce an inversion based method, denoted as IMAge-Guided model INvErsion (IMAGINE), to generate high-quality and diverse images from only a single training sample. We leverage the knowledge of image semantics from a pre-trained classifier to achieve plausible generations via matching multi-level feature representations in the classifier, associated with adversarial training with an external discriminator. IMAGINE enables the synthesis procedure to simultaneously 1) enforce semantic specificity constraints during the synthesis, 2) produce realistic images without generator training, and 3) give users intuitive control over the generation process. With extensive experimental results, we demonstrate qualitatively and quantitatively that IMAGINE performs favorably against state-of-the-art GAN-based and inversion-based methods, across three different image domains (i.e., objects, scenes, and textures).",15.0
Alignment of Language Agents,49f905eb03958c7cfae52ac759ea8978b8b2a6ea,"[{'authorId': '40947466', 'name': 'Z. Kenton'}, {'authorId': '1868196', 'name': 'Tom Everitt'}, {'authorId': '51932191', 'name': 'Laura Weidinger'}, {'authorId': '116589025', 'name': 'Iason Gabriel'}, {'authorId': '148305440', 'name': 'Vladimir Mikulik'}, {'authorId': '2060655766', 'name': 'G. Irving'}]",2021.0,arXiv.org,['Unsolved Problems in ML Safety'],1,"For artificial intelligence to be beneficial to humans the behaviour of AI agents needs to be aligned with what humans want. In this paper we discuss some behavioural issues for language agents, arising from accidental misspecification by the system designer. We highlight some ways that misspecification can occur and discuss some behavioural issues that could arise from misspecification, including deceptive or manipulative language, and review some approaches for avoiding these issues.",36.0
On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜,6d9727f1f058614cada3fe296eeebd8ec4fc512a,"[{'authorId': '2471699', 'name': 'Emily M. Bender'}, {'authorId': '2076288', 'name': 'Timnit Gebru'}, {'authorId': '1584940075', 'name': 'Angelina McMillan-Major'}, {'authorId': '2051526200', 'name': 'Shmargaret Shmitchell'}]",2021.0,"Conference on Fairness, Accountability and Transparency",['Unsolved Problems in ML Safety'],1,"The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.",1091.0
Fixing Data Augmentation to Improve Adversarial Robustness,762752eb9a9a92b028026b17c46d50474ddf3f06,"[{'authorId': '8478422', 'name': 'Sylvestre-Alvise Rebuffi'}, {'authorId': '2071666', 'name': 'Sven Gowal'}, {'authorId': '2792016', 'name': 'D. A. Calian'}, {'authorId': '3205302', 'name': 'Florian Stimberg'}, {'authorId': '8792285', 'name': 'Olivia Wiles'}, {'authorId': '2554720', 'name': 'Timothy A. Mann'}]",2021.0,arXiv.org,['Unsolved Problems in ML Safety'],1,"Adversarial training suffers from robust overfitting, a phenomenon where the robust test accuracy starts to decrease during training. In this paper, we focus on both heuristics-driven and data-driven augmentations as a means to reduce robust overfitting. First, we demonstrate that, contrary to previous findings, when combined with model weight averaging, data augmentation can significantly boost robust accuracy. Second, we explore how state-of-the-art generative models can be leveraged to artificially increase the size of the training set and further improve adversarial robustness. Finally, we evaluate our approach on CIFAR-10 against $\ell_\infty$ and $\ell_2$ norm-bounded perturbations of size $\epsilon = 8/255$ and $\epsilon = 128/255$, respectively. We show large absolute improvements of +7.06% and +5.88% in robust accuracy compared to previous state-of-the-art methods. In particular, against $\ell_\infty$ norm-bounded perturbations of size $\epsilon = 8/255$, our model reaches 64.20% robust accuracy without using any external data, beating most prior works that use external data.",111.0
Learning Transferable Visual Models From Natural Language Supervision,6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4,"[{'authorId': '38909097', 'name': 'Alec Radford'}, {'authorId': '2110935237', 'name': 'Jong Wook Kim'}, {'authorId': '2004021329', 'name': 'Chris Hallacy'}, {'authorId': '1992922591', 'name': 'A. Ramesh'}, {'authorId': '40087786', 'name': 'Gabriel Goh'}, {'authorId': '144517868', 'name': 'Sandhini Agarwal'}, {'authorId': '144864359', 'name': 'Girish Sastry'}, {'authorId': '119609682', 'name': 'Amanda Askell'}, {'authorId': '2051714782', 'name': 'Pamela Mishkin'}, {'authorId': '2115193883', 'name': 'Jack Clark'}, {'authorId': '2064404342', 'name': 'Gretchen Krueger'}, {'authorId': '1701686', 'name': 'Ilya Sutskever'}]",2021.0,International Conference on Machine Learning,['Unsolved Problems in ML Safety'],1,"State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.",4488.0
Measuring and Improving Consistency in Pretrained Language Models,73b6de24eb0e5f6ff4f9c3bdd9257f4554faca19,"[{'authorId': '51131518', 'name': 'Yanai Elazar'}, {'authorId': '9529535', 'name': 'Nora Kassner'}, {'authorId': '51432464', 'name': 'Shauli Ravfogel'}, {'authorId': '3023068', 'name': 'Abhilasha Ravichander'}, {'authorId': '144547315', 'name': 'E. Hovy'}, {'authorId': '144418438', 'name': 'Hinrich Schütze'}, {'authorId': '79775260', 'name': 'Yoav Goldberg'}]",2021.0,Transactions of the Association for Computational Linguistics,['Unsolved Problems in ML Safety'],1,"Abstract Consistency of a model—that is, the invariance of its behavior under meaning-preserving alternations in its input—is a highly desirable property in natural language processing. In this paper we study the question: Are Pretrained Language Models (PLMs) consistent with respect to factual knowledge? To this end, we create ParaRel🤘, a high-quality resource of cloze-style query English paraphrases. It contains a total of 328 paraphrases for 38 relations. Using ParaRel🤘, we show that the consistency of all PLMs we experiment with is poor— though with high variance between relations. Our analysis of the representational spaces of PLMs suggests that they have a poor structure and are currently not suitable for representing knowledge robustly. Finally, we propose a method for improving model consistency and experimentally demonstrate its effectiveness.1",78.0
WILDS: A Benchmark of in-the-Wild Distribution Shifts,40848b41ed8c9c255ecd8a920006877691b52d03,"[{'authorId': '48797476', 'name': 'P. W. Koh'}, {'authorId': '2389237', 'name': 'Shiori Sagawa'}, {'authorId': '66977188', 'name': 'H. Marklund'}, {'authorId': '46215055', 'name': 'Sang Michael Xie'}, {'authorId': '2634261', 'name': 'Marvin Zhang'}, {'authorId': '1693411', 'name': 'Akshay Balsubramani'}, {'authorId': '2146241852', 'name': 'Weihua Hu'}, {'authorId': '19168196', 'name': 'Michihiro Yasunaga'}, {'authorId': '2069956950', 'name': 'Richard L. Phillips'}, {'authorId': '31937047', 'name': 'Sara Beery'}, {'authorId': '1702139', 'name': 'J. Leskovec'}, {'authorId': '2844479', 'name': 'A. Kundaje'}, {'authorId': '145192191', 'name': 'E. Pierson'}, {'authorId': '1736651', 'name': 'S. Levine'}, {'authorId': '46881670', 'name': 'Chelsea Finn'}, {'authorId': '145419642', 'name': 'Percy Liang'}]",2020.0,International Conference on Machine Learning,['Unsolved Problems in ML Safety'],1,"Distribution shifts can cause significant degradation in a broad range of machine learning (ML) systems deployed in the wild. However, many widely-used datasets in the ML community today were not designed for evaluating distribution shifts. These datasets typically have training and test sets drawn from the same distribution, and prior work on retrofitting them with distribution shifts has generally relied on artificial shifts that need not represent the kinds of shifts encountered in the wild. In this paper, we present WILDS, a benchmark of in-the-wild distribution shifts spanning diverse data modalities and applications, from tumor identification to wildlife monitoring to poverty mapping. WILDS builds on top of recent data collection efforts by domain experts in these applications and provides a unified collection of datasets with evaluation metrics and train/test splits that are representative of real-world distribution shifts. These datasets reflect distribution shifts arising from training and testing on different hospitals, cameras, countries, time periods, demographics, molecular scaffolds, etc., all of which cause substantial performance drops in our baseline models. Finally, we survey other applications that would be promising additions to the benchmark but for which we did not manage to find appropriate datasets; we discuss their associated challenges and detail datasets and shifts where we did not see an appreciable performance drop. By unifying datasets from a variety of application areas and making them accessible to the ML community, we hope to encourage the development of general-purpose methods that are anchored to real-world distribution shifts and that work well across different applications and problem settings. Data loaders, default models, and leaderboards are available at this https URL.",547.0
RobustBench: a standardized adversarial robustness benchmark,2aab97e35c43d961d645e650808d5b052ec180ab,"[{'authorId': '39171784', 'name': 'Francesco Croce'}, {'authorId': '47669224', 'name': 'Maksym Andriushchenko'}, {'authorId': '3482535', 'name': 'Vikash Sehwag'}, {'authorId': '2175939276', 'name': 'Edoardo Debenedetti'}, {'authorId': '2175939276', 'name': 'Edoardo Debenedetti'}, {'authorId': '116491218', 'name': 'M. Chiang'}, {'authorId': '143615345', 'name': 'Prateek Mittal'}, {'authorId': '143610806', 'name': 'Matthias Hein'}]",2020.0,NeurIPS Datasets and Benchmarks,['Unsolved Problems in ML Safety'],1,"Evaluation of adversarial robustness is often error-prone leading to overestimation of the true robustness of models. While adaptive attacks designed for a particular defense are a way out of this, there are only approximate guidelines on how to perform them. Moreover, adaptive evaluations are highly customized for particular models, which makes it difficult to compare different defenses. Our goal is to establish a standardized benchmark of adversarial robustness, which as accurately as possible reflects the robustness of the considered models within a reasonable computational budget. This requires to impose some restrictions on the admitted models to rule out defenses that only make gradient-based attacks ineffective without improving actual robustness. We evaluate robustness of models for our benchmark with AutoAttack, an ensemble of white- and black-box attacks which was recently shown in a large-scale study to improve almost all robustness evaluations compared to the original publications. Our leaderboard, hosted at this http URL, aims at reflecting the current state of the art on a set of well-defined tasks in $\ell_\infty$- and $\ell_2$-threat models with possible extensions in the future. Additionally, we open-source the library this http URL that provides unified access to state-of-the-art robust models to facilitate their downstream applications. Finally, based on the collected models, we analyze general trends in $\ell_p$-robustness and its impact on other tasks such as robustness to various distribution shifts and out-of-distribution detection.",247.0
Network intrusion detection system: A systematic study of machine learning and deep learning approaches,c42c29c793f3b8518a313d03fa101b1201a8ad14,"[{'authorId': '48583070', 'name': 'Zeeshan Ahmad'}, {'authorId': '39891028', 'name': 'A. Khan'}, {'authorId': '3078761', 'name': 'W. Cheah'}, {'authorId': '145580217', 'name': 'J. Abdullah'}, {'authorId': '143679712', 'name': 'Farhan Ahmad'}]",2020.0,Transactions on Emerging Telecommunications Technologies,['Unsolved Problems in ML Safety'],1,"The rapid advances in the internet and communication fields have resulted in a huge increase in the network size and the corresponding data. As a result, many novel attacks are being generated and have posed challenges for network security to accurately detect intrusions. Furthermore, the presence of the intruders with the aim to launch various attacks within the network cannot be ignored. An intrusion detection system (IDS) is one such tool that prevents the network from possible intrusions by inspecting the network traffic, to ensure its confidentiality, integrity, and availability. Despite enormous efforts by the researchers, IDS still faces challenges in improving detection accuracy while reducing false alarm rates and in detecting novel intrusions. Recently, machine learning (ML) and deep learning (DL)‐based IDS systems are being deployed as potential solutions to detect intrusions across the network in an efficient manner. This article first clarifies the concept of IDS and then provides the taxonomy based on the notable ML and DL techniques adopted in designing network‐based IDS (NIDS) systems. A comprehensive review of the recent NIDS‐based articles is provided by discussing the strengths and limitations of the proposed solutions. Then, recent trends and advancements of ML and DL‐based NIDS are provided in terms of the proposed methodology, evaluation metrics, and dataset selection. Using the shortcomings of the proposed methods, we highlighted various research challenges and provided the future scope for the research in improving ML and DL‐based NIDS.",210.0
GeDi: Generative Discriminator Guided Sequence Generation,07bcda1dff9bb696ea9cbc69303eee8bd3d85bd6,"[{'authorId': '9340968', 'name': 'Ben Krause'}, {'authorId': '144049726', 'name': 'Akhilesh Deepak Gotmare'}, {'authorId': '143775536', 'name': 'Bryan McCann'}, {'authorId': '2844898', 'name': 'N. Keskar'}, {'authorId': '2708940', 'name': 'Shafiq R. Joty'}, {'authorId': '2166511', 'name': 'R. Socher'}, {'authorId': '8937909', 'name': 'Nazneen Rajani'}]",2020.0,Conference on Empirical Methods in Natural Language Processing,['Unsolved Problems in ML Safety'],1,"While large-scale language models (LMs) are able to imitate the distribution of natural language well enough to generate realistic text, it is difficult to control which regions of the distribution they generate. This is especially problematic because datasets used for training large LMs usually contain significant toxicity, hate, bias, and negativity. We propose GeDi as an efficient method for using smaller LMs as generative discriminators to guide generation from large LMs to make them safer and more controllable. GeDi guides generation at each step by computing classification probabilities for all possible next tokens via Bayes rule by normalizing over two class-conditional distributions; one conditioned on the desired attribute, or control code, and another conditioned on the undesired attribute, or anti control code. We find that GeDi gives stronger controllability than the state of the art method while also achieving generation speeds more than 30 times faster. Additionally, training GeDi on only four topics allows us to controllably generate new topics zero-shot from just a keyword, unlocking a new capability that previous controllable generation methods do not have. Lastly, we show that GeDi can make GPT-2 (1.5B parameters) significantly less toxic without sacrificing linguistic quality, making it by far the most practical existing method for detoxifying large language models while maintaining a fast generation speed.",168.0
Measuring Massive Multitask Language Understanding,10bb7e2c54b947fa50e7bb65b0b5c700fe998044,"[{'authorId': '3422872', 'name': 'Dan Hendrycks'}, {'authorId': '90909974', 'name': 'Collin Burns'}, {'authorId': '104444594', 'name': 'Steven Basart'}, {'authorId': '1380103052', 'name': 'Andy Zou'}, {'authorId': '16787428', 'name': 'Mantas Mazeika'}, {'authorId': '143711382', 'name': 'D. Song'}, {'authorId': '5164568', 'name': 'J. Steinhardt'}]",2020.0,International Conference on Learning Representations,['Unsolved Problems in ML Safety'],1,"We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings.",125.0
Aligning AI With Shared Human Values,65906e6027246ae9e4ecd18d6e019a24505c842e,"[{'authorId': '3422872', 'name': 'Dan Hendrycks'}, {'authorId': '90909974', 'name': 'Collin Burns'}, {'authorId': '104444594', 'name': 'Steven Basart'}, {'authorId': '2651789', 'name': 'Andrew Critch'}, {'authorId': '46276596', 'name': 'J. Li'}, {'authorId': '143711382', 'name': 'D. Song'}, {'authorId': '5164568', 'name': 'J. Steinhardt'}]",2020.0,International Conference on Learning Representations,"['Unsolved Problems in ML Safety', 'X-Risk Analysis for AI Research']",2,"We show how to assess a language model's knowledge of basic concepts of morality. We introduce the ETHICS dataset, a new benchmark that spans concepts in justice, well-being, duties, virtues, and commonsense morality. Models predict widespread moral judgments about diverse text scenarios. This requires connecting physical and social world knowledge to value judgements, a capability that may enable us to steer chatbot outputs or eventually regularize open-ended reinforcement learning agents. With the ETHICS dataset, we find that current language models have a promising but incomplete understanding of basic ethical knowledge. Our work shows that progress can be made on machine ethics today, and it provides a steppingstone toward AI that is aligned with human values.",97.0
Trojaning Language Models for Fun and Profit,11fe33206746251656698bf5188fc622aea7fc21,"[{'authorId': None, 'name': 'Xinyang Zhang'}, {'authorId': '1852415', 'name': 'Zheng Zhang'}, {'authorId': '2155389584', 'name': 'Ting Wang'}]",2020.0,European Symposium on Security and Privacy,['Unsolved Problems in ML Safety'],1,"Recent years have witnessed the emergence of a new paradigm of building natural language processing (NLP) systems: general-purpose, pre-trained language models (LMs) are composed with simple downstream models and fine-tuned for a variety of NLP tasks. This paradigm shift significantly simplifies the system development cycles. However, as many LMs are provided by untrusted third parties, their lack of standardization or regulation entails profound security implications, which are largely unexplored. To bridge this gap, this work studies the security threats posed by malicious LMs to NLP systems. Specifically, we present TrojanLM, a new class of trojaning attacks in which maliciously crafted LMs trigger host NLP systems to malfunction in a highly predictable manner. By empirically studying three state-of-the-art LMs (BERT, GPT-2, XLNet) in a range of security-critical NLP tasks (toxic comment detection, question answering, text completion) as well as user studies on crowdsourcing platforms, we demonstrate that TrojanLM possesses the following properties: (i) flexibility - the adversary is able to flexibly define logical combinations (e.g., ‘and’, ‘or’, ‘xor’) of arbitrary words as triggers, (ii) efficacy - the host systems misbehave as desired by the adversary with high probability when “trigger” -embedded inputs are present, (iii) specificity - the trojan LMs function indistinguishably from their benign counterparts on clean inputs, and (iv) fluency - the trigger-embedded inputs appear as fluent natural language and highly relevant to their surrounding contexts. We provide analytical justification for the practicality of TrojanLM, and further discuss potential countermeasures and their challenges, which lead to several promising research directions.",54.0
You Autocomplete Me: Poisoning Vulnerabilities in Neural Code Completion,1ec69f1a1a9d4ff5c5bc70db0e5087157b620570,"[{'authorId': '39347554', 'name': 'R. Schuster'}, {'authorId': '3469125', 'name': 'Congzheng Song'}, {'authorId': '2337345', 'name': 'Eran Tromer'}, {'authorId': '1723945', 'name': 'Vitaly Shmatikov'}]",2020.0,USENIX Security Symposium,['Unsolved Problems in ML Safety'],1,"Code autocompletion is an integral feature of modern code editors and IDEs. The latest generation of autocompleters uses neural language models, trained on public open-source code repositories, to suggest likely (not just statically feasible) completions given the current context. 
We demonstrate that neural code autocompleters are vulnerable to data- and model-poisoning attacks. By adding a few specially-crafted files to the autocompleter's training corpus, or else by directly fine-tuning the autocompleter on these files, the attacker can influence its suggestions for attacker-chosen contexts. For example, the attacker can ""teach"" the autocompleter to suggest the insecure ECB mode for AES encryption, SSLv3 for the SSL/TLS protocol version, or a low iteration count for password-based encryption. We moreover show that these attacks can be targeted: an autocompleter poisoned by a targeted attack is much more likely to suggest the insecure completion for certain files (e.g., those from a specific repo). 
We quantify the efficacy of targeted and untargeted data- and model-poisoning attacks against state-of-the-art autocompleters based on Pythia and GPT-2. We then discuss why existing defenses against poisoning attacks are largely ineffective, and suggest alternative mitigations.",53.0
The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization,022622e024890d6e044ac50e2da6b44c59bdf418,"[{'authorId': '3422872', 'name': 'Dan Hendrycks'}, {'authorId': '104444594', 'name': 'Steven Basart'}, {'authorId': '52227748', 'name': 'Norman Mu'}, {'authorId': '148070327', 'name': 'Saurav Kadavath'}, {'authorId': '2112315803', 'name': 'Frank Wang'}, {'authorId': '1779776376', 'name': 'Evan Dorundo'}, {'authorId': '2060225103', 'name': 'Rahul Desai'}, {'authorId': '8791781', 'name': 'Tyler Lixuan Zhu'}, {'authorId': '51887463', 'name': 'Samyak Parajuli'}, {'authorId': '2107683161', 'name': 'Mike Guo'}, {'authorId': '143711382', 'name': 'D. Song'}, {'authorId': '5164568', 'name': 'J. Steinhardt'}, {'authorId': '2058362', 'name': 'J. Gilmer'}]",2020.0,IEEE International Conference on Computer Vision,"['Unsolved Problems in ML Safety', 'X-Risk Analysis for AI Research']",2,"We introduce four new real-world distribution shift datasets consisting of changes in image style, image blurriness, geographic location, camera operation, and more. With our new datasets, we take stock of previously proposed methods for improving out-of-distribution robustness and put them to the test. We find that using larger models and artificial data augmentations can improve robustness on real-world distribution shifts, contrary to claims in prior work. We find improvements in artificial robustness benchmarks can transfer to real-world distribution shifts, contrary to claims in prior work. Motivated by our observation that data augmentations can help with real-world distribution shifts, we also introduce a new data augmentation method which advances the state-of-the-art and outperforms models pre-trained with 1000× more labeled data. Overall we find that some methods consistently help with distribution shifts in texture and local image statistics, but these methods do not help with some other distribution shifts like geographic changes. Our results show that future research must study multiple distribution shifts simultaneously, as we demonstrate that no evaluated method consistently improves robustness.",645.0
Perceptual Adversarial Robustness: Defense Against Unseen Threat Models,473a854a939eca4bf39420ff496f8e24e223d460,"[{'authorId': '114339785', 'name': 'Cassidy Laidlaw'}, {'authorId': '144190575', 'name': 'Sahil Singla'}, {'authorId': '34389431', 'name': 'S. Feizi'}]",2020.0,International Conference on Learning Representations,['Unsolved Problems in ML Safety'],1,"We present adversarial attacks and defenses for the perceptual adversarial threat model: the set of all perturbations to natural images which can mislead a classifier but are imperceptible to human eyes. The perceptual threat model is broad and encompasses $L_2$, $L_\infty$, spatial, and many other existing adversarial threat models. However, it is difficult to determine if an arbitrary perturbation is imperceptible without humans in the loop. To solve this issue, we propose to use a {\it neural perceptual distance}, an approximation of the true perceptual distance between images using internal activations of neural networks. In particular, we use the Learned Perceptual Image Patch Similarity (LPIPS) distance. We then propose the {\it neural perceptual threat model} that includes adversarial examples with a bounded neural perceptual distance to natural images. Under the neural perceptual threat model, we develop two novel perceptual adversarial attacks to find any imperceptible perturbations to images which can fool a classifier. Through an extensive perceptual study, we show that the LPIPS distance correlates well with human judgements of perceptibility of adversarial examples, validating our threat model. Because the LPIPS threat model is very broad, we find that Perceptual Adversarial Training (PAT) against a perceptual attack gives robustness against many other types of adversarial attacks. We test PAT on CIFAR-10 and ImageNet-100 against 12 types of adversarial attacks and find that, for each attack, PAT achieves close to the accuracy of adversarial training against just that perturbation type. That is, PAT generalizes well to unforeseen perturbation types. This is vital in sensitive applications where a particular threat model cannot be assumed, and to the best of our knowledge, PAT is the first adversarial defense with this property.",98.0
Neural Ensemble Search for Uncertainty Estimation and Dataset Shift,53ca11e0393ab21b6021eb6cf8ab9d3d8eef4081,"[{'authorId': '1749331444', 'name': 'Sheheryar Zaidi'}, {'authorId': '51109984', 'name': 'Arber Zela'}, {'authorId': '2501244', 'name': 'T. Elsken'}, {'authorId': '1569695293', 'name': 'Chris C. Holmes'}, {'authorId': '144661829', 'name': 'F. Hutter'}, {'authorId': '1725303', 'name': 'Y. Teh'}]",2020.0,Neural Information Processing Systems,['Unsolved Problems in ML Safety'],1,"Ensembles of neural networks achieve superior performance compared to standalone networks in terms of accuracy, uncertainty calibration and robustness to dataset shift. Deep ensembles, a state-of-the-art method for uncertainty estimation, only ensemble random initializations of a fixed architecture. Instead, we propose two methods for automatically constructing ensembles with varying architectures, which implicitly trade-off individual architectures’ strengths against the ensemble’s diversity and exploit architectural variation as a source of diversity. On a variety of classification tasks and modern architecture search spaces, we show that the resulting ensembles outperform deep ensembles not only in terms of accuracy but also uncertainty calibration and robustness to dataset shift. Our further analysis and ablation studies provide evidence of higher ensemble diversity due to architectural variation, resulting in ensembles that can outperform deep ensembles, even when having weaker average base learners. To foster reproducibility, our code is available: https://github.com/automl/nes",40.0
Reinforcement Learning Under Moral Uncertainty,239fa440de7bf63d38ce62f60caa1a51467930be,"[{'authorId': '66821245', 'name': 'Adrien Ecoffet'}, {'authorId': '39799304', 'name': 'J. Lehman'}]",2020.0,International Conference on Machine Learning,['Unsolved Problems in ML Safety'],1,"An ambitious goal for artificial intelligence is to create agents that behave ethically: The capacity to abide by human moral norms would greatly expand the context in which autonomous agents could be practically and safely deployed. While ethical agents could be trained through reinforcement, by rewarding correct behavior under a specific moral theory (e.g. utilitarianism), there remains widespread disagreement (both societally and among moral philosophers) about the nature of morality and what ethical theory (if any) is objectively correct. Acknowledging such disagreement, recent work in moral philosophy proposes that ethical behavior requires acting under moral uncertainty, i.e. to take into account when acting that one's credence is split across several plausible ethical theories. Inspired by such work, this paper proposes a formalism that translates such insights to the field of reinforcement learning. Demonstrating the formalism's potential, we then train agents in simple environments to act under moral uncertainty, highlighting how such uncertainty can help curb extreme behavior from commitment to single theories. The overall aim is to draw productive connections from the fields of moral philosophy and machine ethics to that of machine learning, to inspire further research by highlighting a spectrum of machine learning research questions relevant to training ethically capable reinforcement learning agents.",17.0
Blind Backdoors in Deep Learning Models,6f46322243a8318a9712bedf6a218e2df85c64fb,"[{'authorId': '36103467', 'name': 'Eugene Bagdasaryan'}, {'authorId': '1723945', 'name': 'Vitaly Shmatikov'}]",2020.0,USENIX Security Symposium,['Unsolved Problems in ML Safety'],1,"We investigate a new method for injecting backdoors into machine learning models, based on poisoning the loss-value computation in the model-training code. We use it to demonstrate new classes of backdoors strictly more powerful than those in prior literature: single-pixel and physical backdoors in ImageNet models, backdoors that switch the model to a covert, privacy-violating task, and backdoors that do not require inference-time input modifications. 
Our attack is \emph{blind}: the attacker cannot modify the training data, nor observe the execution of his code, nor access the resulting model. Blind backdoor training uses multi-objective optimization to achieve high accuracy on both the main and backdoor tasks. Finally, we show how the blind attack can evade all known defenses, and propose new ones.",126.0
ForecastQA: A Question Answering Challenge for Event Forecasting with Temporal Text Data,30602e3382df3abedb5f225b55b7efce8580f74d,"[{'authorId': '8844876', 'name': 'Woojeong Jin'}, {'authorId': '2109621632', 'name': 'Suji Kim'}, {'authorId': '1885282', 'name': 'Rahul Khanna'}, {'authorId': '2115475530', 'name': 'Dong-Ho Lee'}, {'authorId': '2775559', 'name': 'Fred Morstatter'}, {'authorId': '143728483', 'name': 'A. Galstyan'}, {'authorId': '1384550891', 'name': 'Xiang Ren'}]",2020.0,Annual Meeting of the Association for Computational Linguistics,['Unsolved Problems in ML Safety'],1,"Event forecasting is a challenging, yet important task, as humans seek to constantly plan for the future. Existing automated forecasting studies rely mostly on structured data, such as time-series or event-based knowledge graphs, to help predict future events. In this work, we aim to formulate a task, construct a dataset, and provide benchmarks for developing methods for event forecasting with large volumes of unstructured text data. To simulate the forecasting scenario on temporal news documents, we formulate the problem as a restricted-domain, multiple-choice, question-answering (QA) task. Unlike existing QA tasks, our task limits accessible information, and thus a model has to make a forecasting judgement. To showcase the usefulness of this task formulation, we introduce ForecastQA, a question-answering dataset consisting of 10,392 event forecasting questions, which have been collected and verified via crowdsourcing efforts. We present our experiments on ForecastQA using BERTbased models and find that our best model achieves 61.0% accuracy on the dataset, which still lags behind human performance by about 19%. We hope ForecastQA will support future research efforts in bridging this gap.",13.0
Stop-and-Go: Exploring Backdoor Attacks on Deep Reinforcement Learning-Based Traffic Congestion Control Systems,378d1f030d7790939681f36a7b1bf4938d662213,"[{'authorId': '2144333833', 'name': 'Yue Wang'}, {'authorId': '2129762897', 'name': 'Esha Sarkar'}, {'authorId': '2154826355', 'name': 'Wenqing Li'}, {'authorId': '1686192', 'name': 'M. Maniatakos'}, {'authorId': '40963588', 'name': 'S. E. Jabari'}]",2020.0,IEEE Transactions on Information Forensics and Security,['Unsolved Problems in ML Safety'],1,"Recent work has shown that the introduction of autonomous vehicles (AVs) in traffic could help reduce traffic jams. Deep reinforcement learning methods demonstrate good performance in complex control problems, including autonomous vehicle control, and have been used in state-of-the-art AV controllers. However, deep neural networks (DNNs) render automated driving vulnerable to machine learning-based attacks. In this work, we explore the backdooring/trojanning of DRL-based AV controllers. We develop a trigger design methodology that is based on well-established principles of traffic physics. The malicious actions include vehicle deceleration and acceleration to cause stop-and-go traffic waves to emerge (congestion attacks) or AV acceleration resulting in the AV crashing into the vehicle in front (insurance attack). We test our attack on single-lane and two-lane circuits. Our experimental results show that the backdoored model does not compromise normal operation performance, with the maximum decrease in cumulative rewards being 1%. Still, it can be maliciously activated to cause a crash or congestion when the corresponding triggers appear.",30.0
Optimal Policies Tend To Seek Power,46d4452eb041e33f1e58eab64ec8cf5af534b6ff,"[{'authorId': '49277224', 'name': 'A. M. Turner'}, {'authorId': '150194767', 'name': 'Logan Smith'}, {'authorId': '40947489', 'name': 'Rohin Shah'}, {'authorId': '2651789', 'name': 'Andrew Critch'}, {'authorId': '1729906', 'name': 'Prasad Tadepalli'}]",2019.0,Neural Information Processing Systems,"['Unsolved Problems in ML Safety', 'X-Risk Analysis for AI Research']",2,"Some researchers have speculated that capable reinforcement learning agents are often incentivized to seek resources and power in pursuit of their objectives. While seeking power in order to optimize a misspecified objective, agents might be incentivized to behave in undesirable ways, including rationally preventing deactivation and correction. Others have voiced skepticism: human power-seeking instincts seem idiosyncratic, and these urges need not be present in reinforcement learning agents. We formalize a notion of power within the context of Markov decision processes. With respect to a class of neutral reward function distributions, we provide sufficient conditions for when optimal policies tend to seek power over the environment.",20.0
Natural Adversarial Examples,45557cc70cd6989ab6b03e5aeb787e34299099f7,"[{'authorId': '3422872', 'name': 'Dan Hendrycks'}, {'authorId': '2074109526', 'name': 'Kevin Zhao'}, {'authorId': '104444594', 'name': 'Steven Basart'}, {'authorId': '5164568', 'name': 'J. Steinhardt'}, {'authorId': '143711382', 'name': 'D. Song'}]",2019.0,Computer Vision and Pattern Recognition,['Unsolved Problems in ML Safety'],1,"We introduce two challenging datasets that reliably cause machine learning model performance to substantially degrade. The datasets are collected with a simple adversarial filtration technique to create datasets with limited spurious cues. Our datasets’ real-world, unmodified examples transfer to various unseen models reliably, demonstrating that computer vision models have shared weaknesses. The first dataset is called IMAGENET-A and is like the ImageNet test set, but it is far more challenging for existing models. We also curate an adversarial out-of-distribution detection dataset called IMAGENET-O, which is the first out-of-distribution detection dataset created for ImageNet models. On IMAGENET-A a DenseNet-121 obtains around 2% accuracy, an accuracy drop of approximately 90%, and its out-of-distribution detection performance on IMAGENET-O is near random chance levels. We find that existing data augmentation techniques hardly boost performance, and using other public training datasets provides improvements that are limited. However, we find that improvements to computer vision architectures provide a promising path towards robust models.",539.0
An Empirical Cybersecurity Evaluation of GitHub Copilot's Code Contributions,3f97c2067cde9377e50b3160bbd7982c94abd88a,"[{'authorId': '3437933', 'name': 'H. Pearce'}, {'authorId': '2124891963', 'name': 'Baleegh Ahmad'}, {'authorId': '143645422', 'name': 'Benjamin Tan'}, {'authorId': '1398683279', 'name': 'Brendan Dolan-Gavitt'}, {'authorId': '1707355', 'name': 'R. Karri'}]",2021.0,arXiv.org,['Unsolved Problems in ML Safety'],1,"—There is burgeoning interest in designing AI-based systems to assist humans in designing computing systems, including tools that automatically generate computer code. The most notable of these comes in the form of the ﬁrst self-described ‘AI pair programmer’, GitHub Copilot, a language model trained over open-source GitHub code. However, code often contains bugs—and so, given the vast quantity of unvetted code that Copilot has processed, it is certain that the language model will have learned from exploitable, buggy code. This raises concerns on the security of Copilot’s code contributions. In this work, we systematically investigate the prevalence and conditions that can cause GitHub Copilot to recommend insecure code. To perform this analysis we prompt Copilot to generate code in scenarios relevant to high-risk CWEs (e.g. those from MITRE’s “Top 25” list). We explore Copilot’s performance on three distinct code generation axes—examining how it performs given diversity of weaknesses, diversity of prompts, and diversity of domains. In total, we produce 89 different scenarios for Copilot to complete, producing 1,692 programs. Of these, we found approximately 40% to be vulnerable.",24.0
The Parliamentary Approach to Moral Uncertainty,d08c5e14496ff7eaeb44c0d1499b48c6b4c81108,"[{'authorId': '32391889', 'name': 'T. Newberry'}, {'authorId': '46277517', 'name': 'Toby Ord'}]",2021.0,,"['Unsolved Problems in ML Safety', 'X-Risk Analysis for AI Research']",2,"We introduce a novel approach to the problem of decision-making under moral uncertainty, based on an analogy to a parliament. The appropriate choice under moral uncertainty is the one that would be reached by a parliament comprised of delegates representing the interests of each moral theory, who number in proportion to your credence in that theory. We present what we see as the best specific approach of this kind (based on proportional chances voting), and also show how the parliamentary approach can be used as a general framework for thinking about moral uncertainty, where extant approaches to addressing moral uncertainty correspond to parliaments with different rules and procedures.",5.0
Objective Robustness in Deep Reinforcement Learning,d801ea85c8d0c80aaf2499a040b22173b3b7ce26,"[{'authorId': '2106415442', 'name': 'Jack Koch'}, {'authorId': '2106415649', 'name': 'L. Langosco'}, {'authorId': '147132443', 'name': 'J. Pfau'}, {'authorId': '2118971168', 'name': 'James Le'}, {'authorId': '2106414883', 'name': 'Lee D. Sharkey'}]",2021.0,arXiv.org,['Unsolved Problems in ML Safety'],1,"We study objective robustness failures, a type of out-of-distribution robustness failure in reinforcement learning (RL). Objective robustness failures occur when an RL agent retains its capabilities off-distribution yet pursues the wrong objective. We provide the first explicit empirical demonstrations of objective robustness failures and argue that this type of failure is critical to address.",6.0
Tent: Fully Test-Time Adaptation by Entropy Minimization,180c78b132f6369a384d22a9529551d86c8788d3,"[{'authorId': '2118962158', 'name': 'Dequan Wang'}, {'authorId': '1782282', 'name': 'Evan Shelhamer'}, {'authorId': '1821387', 'name': 'Shaoteng Liu'}, {'authorId': '1708655', 'name': 'B. Olshausen'}, {'authorId': '1753210', 'name': 'Trevor Darrell'}]",2021.0,International Conference on Learning Representations,['Unsolved Problems in ML Safety'],1,"A model must adapt itself to generalize to new and different data during testing. In this setting of fully test-time adaptation the model has only the test data and its own parameters. We propose to adapt by test entropy minimization (tent1): we optimize the model for confidence as measured by the entropy of its predictions. Our method estimates normalization statistics and optimizes channel-wise affine transformations to update online on each batch. Tent reduces generalization error for image classification on corrupted ImageNet and CIFAR-10/100 and reaches a new state-of-the-art error on ImageNet-C. Tent handles source-free domain adaptation on digit recognition from SVHN to MNIST/MNIST-M/USPS, on semantic segmentation from GTA to Cityscapes, and on the VisDA-C benchmark. These results are achieved in one epoch of test-time optimization without altering training.",284.0
Localized Calibration: Metrics and Recalibration,1ca5b84e579e8fb363fc7e7eac79adda83fd6df0,"[{'authorId': '39109324', 'name': 'Rachel Luo'}, {'authorId': '31553879', 'name': 'Aadyot Bhatnagar'}, {'authorId': '46507194', 'name': 'Haiquan Wang'}, {'authorId': '2228109', 'name': 'Caiming Xiong'}, {'authorId': '1702137', 'name': 'S. Savarese'}, {'authorId': '1491626939', 'name': 'Yu Bai'}, {'authorId': '3303970', 'name': 'Shengjia Zhao'}, {'authorId': '2490652', 'name': 'S. Ermon'}]",2021.0,arXiv.org,['Unsolved Problems in ML Safety'],1,"Probabilistic classiﬁers output conﬁdence scores along with their predictions, and these conﬁdence scores must be well-calibrated (i.e. reﬂect the true probability of an event) to be meaningful and useful for downstream tasks. However, existing metrics for measuring calibration are insuﬃcient. Commonly used metrics such as the expected calibration error (ECE) only measure global trends, making them ineﬀective for measuring the calibration of a particular sample or subgroup. At the other end of the spectrum, a fully individualized calibration error is in general in-tractable to estimate from ﬁnite samples. In this work, we propose the local calibration error (LCE), a ﬁne-grained calibration metric that spans the gap between fully global and fully individualized calibration. The LCE leverages learned features to automatically capture rich subgroups, and it measures the calibration error around each individual example via a similarity function. We then introduce a localized recalibration method, LoRe, that improves the LCE better than existing recalibration methods. Finally, we show that applying our recalibration method improves decision-making on downstream tasks.",4.0
Open Problems in Cooperative AI,2a1573cfa29a426c695e2caf6de0167a12b788ef,"[{'authorId': '3198576', 'name': 'A. Dafoe'}, {'authorId': '37591038', 'name': 'Edward Hughes'}, {'authorId': '1698412', 'name': 'Yoram Bachrach'}, {'authorId': '2059467919', 'name': 'Tantum Collins'}, {'authorId': '47610458', 'name': 'Kevin R. McKee'}, {'authorId': '1700356', 'name': 'Joel Z. Leibo'}, {'authorId': '144652297', 'name': 'K. Larson'}, {'authorId': '1686971', 'name': 'T. Graepel'}]",2020.0,arXiv.org,"['Unsolved Problems in ML Safety', 'X-Risk Analysis for AI Research']",2,"Problems of cooperation--in which agents seek ways to jointly improve their welfare--are ubiquitous and important. They can be found at scales ranging from our daily routines--such as driving on highways, scheduling meetings, and working collaboratively--to our global challenges--such as peace, commerce, and pandemic preparedness. Arguably, the success of the human species is rooted in our ability to cooperate. Since machines powered by artificial intelligence are playing an ever greater role in our lives, it will be important to equip them with the capabilities necessary to cooperate and to foster cooperation. 
We see an opportunity for the field of artificial intelligence to explicitly focus effort on this class of problems, which we term Cooperative AI. The objective of this research would be to study the many aspects of the problems of cooperation and to innovate in AI to contribute to solving these problems. Central goals include building machine agents with the capabilities needed for cooperation, building tools to foster cooperation in populations of (machine and/or human) agents, and otherwise conducting AI research for insight relevant to problems of cooperation. This research integrates ongoing work on multi-agent systems, game theory and social choice, human-machine interaction and alignment, natural-language processing, and the construction of social tools and platforms. However, Cooperative AI is not the union of these existing areas, but rather an independent bet about the productivity of specific kinds of conversations that involve these and other areas. We see opportunity to more explicitly focus on the problem of cooperation, to construct unified theory and vocabulary, and to build bridges with adjacent communities working on cooperation, including in the natural, social, and behavioural sciences.",88.0
SOREL-20M: A Large Scale Benchmark Dataset for Malicious PE Detection,27c13bbac7db0557a6ab776e8c0f3ffaf57c39c0,"[{'authorId': '2584869', 'name': 'Richard E. Harang'}, {'authorId': '39886114', 'name': 'Ethan M. Rudd'}]",2020.0,arXiv.org,['Unsolved Problems in ML Safety'],1,"In this paper we describe the SOREL-20M (Sophos/ReversingLabs-20 Million) dataset: a large-scale dataset consisting of nearly 20 million files with pre-extracted features and metadata, high-quality labels derived from multiple sources, information about vendor detections of the malware samples at the time of collection, and additional ``tags'' related to each malware sample to serve as additional targets. In addition to features and metadata, we also provide approximately 10 million ``disarmed'' malware samples -- samples with both the optional\_headers.subsystem and file\_header.machine flags set to zero -- that may be used for further exploration of features and detection strategies. We also provide Python code to interact with the data and features, as well as baseline neural network and gradient boosted decision tree models and their results, with full training and evaluation code, to serve as a starting point for further experimentation.",45.0
Aligning AI Optimization to Community Well-Being,9315602c64288987462ea7a04c128692bd8f8c03,"[{'authorId': '48756178', 'name': 'J. Stray'}]",2020.0,International Journal of Community Well-Being,['Unsolved Problems in ML Safety'],1,,24.0
Destructive Cyber Operations and Machine Learning,db8518539beabc45d2caf0a575df2e9cd8ea0c53,"[{'authorId': '2088789768', 'name': 'Dakota Cary'}, {'authorId': '89162234', 'name': 'Daniel Cebul'}]",2020.0,,"['Unsolved Problems in ML Safety', 'X-Risk Analysis for AI Research']",2,"Machine learning may provide cyber attackers with the means to execute more effective and more destructive attacks against industrial control systems. As new ML tools are developed, CSET discusses the ways in which attackers may deploy these tools and the most effective avenues for industrial system defenders to respond.",3.0
Automating Cyber Attacks,d6a55f8fcf6eb4de5f2ef34062add22108a58336,"[{'authorId': '46517469', 'name': 'B. Buchanan'}, {'authorId': '2234367', 'name': 'John D. Bansemer'}, {'authorId': '2088789768', 'name': 'Dakota Cary'}, {'authorId': '2092612385', 'name': 'Jack Lucas'}, {'authorId': '1518759089', 'name': 'Micah Musser'}]",2020.0,,"['Unsolved Problems in ML Safety', 'X-Risk Analysis for AI Research']",2,"Based on an in-depth analysis of artificial intelligence and machine learning systems, the authors consider the future of applying such systems to cyber attacks, and what strategies attackers are likely or less likely to use. As nuanced, complex, and overhyped as machine learning is, they argue, it remains too important to ignore.",8.0
Formalizing IMO Problems and Solutions in Isabelle/HOL,c357c52ae9cc22c0cefc3b43012dce5674487bbb,"[{'authorId': '3004149', 'name': 'Filip Marić'}, {'authorId': '2007665482', 'name': 'Sana Stojanovic-Durdevic'}]",2020.0,ThEdu@IJCAR,['Unsolved Problems in ML Safety'],1,"The International Mathematical Olympiad (IMO) is perhaps the most celebrated mental competition in the world and as such is among the greatest grand challenges for Artificial Intelligence (AI). The IMO Grand Challenge, recently formulated, requires to build an AI that can win a gold medal in the competition. We present some initial steps that could help to tackle this goal by creating a public repository of mechanically checked solutions of IMO Problems in the interactive theorem prover Isabelle/HOL. This repository is actively maintained by students of the Faculty of Mathematics, University of Belgrade, Serbia within the course ""Introduction to Interactive Theorem Proving"".",2.0
Scaling Laws for Autoregressive Generative Modeling,3efbcfeeb0ea1051a71101d3318da4411081f0b8,"[{'authorId': '103143311', 'name': 'T. Henighan'}, {'authorId': '152724169', 'name': 'J. Kaplan'}, {'authorId': '102493084', 'name': 'Mor Katz'}, {'authorId': '2108828435', 'name': 'Mark Chen'}, {'authorId': '144239765', 'name': 'Christopher Hesse'}, {'authorId': '2119404536', 'name': 'Jacob Jackson'}, {'authorId': '35450887', 'name': 'Heewoo Jun'}, {'authorId': '31035595', 'name': 'Tom B. Brown'}, {'authorId': '6515819', 'name': 'Prafulla Dhariwal'}, {'authorId': '145565184', 'name': 'Scott Gray'}, {'authorId': '2004021329', 'name': 'Chris Hallacy'}, {'authorId': '2056658938', 'name': 'Benjamin Mann'}, {'authorId': '38909097', 'name': 'Alec Radford'}, {'authorId': '1992922591', 'name': 'A. Ramesh'}, {'authorId': '39849748', 'name': 'Nick Ryder'}, {'authorId': '2052152920', 'name': 'Daniel M. Ziegler'}, {'authorId': '47971768', 'name': 'J. Schulman'}, {'authorId': '2698777', 'name': 'Dario Amodei'}, {'authorId': '52238703', 'name': 'Sam McCandlish'}]",2020.0,arXiv.org,['Unsolved Problems in ML Safety'],1,"We identify empirical scaling laws for the cross-entropy loss in four domains: generative image modeling, video modeling, multimodal image$\leftrightarrow$text models, and mathematical problem solving. In all cases autoregressive Transformers smoothly improve in performance as model size and compute budgets increase, following a power-law plus constant scaling law. The optimal model size also depends on the compute budget through a power-law, with exponents that are nearly universal across all data domains. 
The cross-entropy loss has an information theoretic interpretation as $S($True$) + D_{\mathrm{KL}}($True$||$Model$)$, and the empirical scaling laws suggest a prediction for both the true data distribution's entropy and the KL divergence between the true and model distributions. With this interpretation, billion-parameter Transformers are nearly perfect models of the YFCC100M image distribution downsampled to an $8\times 8$ resolution, and we can forecast the model size needed to achieve any given reducible loss (ie $D_{\mathrm{KL}}$) in nats/image for other resolutions. 
We find a number of additional scaling laws in specific domains: (a) we identify a scaling relation for the mutual information between captions and images in multimodal models, and show how to answer the question ""Is a picture worth a thousand words?""; (b) in the case of mathematical problem solving, we identify scaling laws for model performance when extrapolating beyond the training distribution; (c) we finetune generative image models for ImageNet classification and find smooth scaling of the classification loss and error rate, even as the generative loss levels off. Taken together, these results strengthen the case that scaling laws have important implications for neural network performance, including on downstream tasks.",145.0
Avoiding Side Effects By Considering Future Tasks,532d734c9d622cc6dd6aa6cb6b4dce032ad591aa,"[{'authorId': '2578985', 'name': 'Victoria Krakovna'}, {'authorId': '1749270', 'name': 'Laurent Orseau'}, {'authorId': '2047148926', 'name': 'Richard Ngo'}, {'authorId': '26890260', 'name': 'Miljan Martic'}, {'authorId': '34313265', 'name': 'S. Legg'}]",2020.0,Neural Information Processing Systems,['Unsolved Problems in ML Safety'],1,"Designing reward functions is difficult: the designer has to specify what to do (what it means to complete the task) as well as what not to do (side effects that should be avoided while completing the task). To alleviate the burden on the reward designer, we propose an algorithm to automatically generate an auxiliary reward function that penalizes side effects. This auxiliary objective rewards the ability to complete possible future tasks, which decreases if the agent causes side effects during the current task. The future task reward can also give the agent an incentive to interfere with events in the environment that make future tasks less achievable, such as irreversible actions by other agents. To avoid this interference incentive, we introduce a baseline policy that represents a default course of action (such as doing nothing), and use it to filter out future tasks that are not achievable by default. We formally define interference incentives and show that the future task approach with a baseline policy avoids these incentives in the deterministic case. Using gridworld environments that test for side effects and interference, we show that our method avoids interference and is more effective for avoiding side effects than the common approach of penalizing irreversible actions.",19.0
Hidden Incentives for Auto-Induced Distributional Shift,1f95b66969fc6c4c2efbd80439df7a3801baa1f8,"[{'authorId': '145055042', 'name': 'David Krueger'}, {'authorId': '3422058', 'name': 'Tegan Maharaj'}, {'authorId': '2990741', 'name': 'J. Leike'}]",2020.0,arXiv.org,['Unsolved Problems in ML Safety'],1,"Decisions made by machine learning systems have increasing influence on the world, yet it is common for machine learning algorithms to assume that no such influence exists. An example is the use of the i.i.d. assumption in content recommendation. In fact, the (choice of) content displayed can change users' perceptions and preferences, or even drive them away, causing a shift in the distribution of users. We introduce the term auto-induced distributional shift (ADS) to describe the phenomenon of an algorithm causing a change in the distribution of its own inputs. Our goal is to ensure that machine learning systems do not leverage ADS to increase performance when doing so could be undesirable. We demonstrate that changes to the learning algorithm, such as the introduction of meta-learning, can cause hidden incentives for auto-induced distributional shift (HI-ADS) to be revealed. To address this issue, we introduce `unit tests' and a mitigation strategy for HI-ADS, as well as a toy environment for modelling real-world issues with HI-ADS in content recommendation, where we demonstrate that strong meta-learners achieve gains in performance via ADS. We show meta-learning and Q-learning both sometimes fail unit tests, but pass when using our mitigation strategy.",23.0
Certifiably Adversarially Robust Detection of Out-of-Distribution Data,0930f7234a861c34050685070e117c295ebb184a,"[{'authorId': '52215534', 'name': 'Julian Bitterwolf'}, {'authorId': '38150565', 'name': 'Alexander Meinke'}, {'authorId': '143610806', 'name': 'Matthias Hein'}]",2020.0,Neural Information Processing Systems,['Unsolved Problems in ML Safety'],1,"Deep neural networks are known to be overconfident when applied to out-of-distribution (OOD) inputs which clearly do not belong to any class. This is a problem in safety-critical applications since a reliable assessment of the uncertainty of a classifier is a key property, allowing the system to trigger human intervention or to transfer into a safe state. In this paper, we aim for certifiable worst case guarantees for OOD detection by enforcing not only low confidence at the OOD point but also in an $l_\infty$-ball around it. For this purpose, we use interval bound propagation (IBP) to upper bound the maximal confidence in the $l_\infty$-ball and minimize this upper bound during training time. We show that non-trivial bounds on the confidence for OOD data generalizing beyond the OOD dataset seen at training time are possible. Moreover, in contrast to certified adversarial robustness which typically comes with significant loss in prediction performance, certified guarantees for worst case OOD detection are possible without much loss in accuracy.",38.0
CSI: Novelty Detection via Contrastive Learning on Distributionally Shifted Instances,e1bb329621de73d08c47beae9b5439a1c244eb1a,"[{'authorId': '1750599181', 'name': 'Jihoon Tack'}, {'authorId': '9962692', 'name': 'Sangwoo Mo'}, {'authorId': '83125078', 'name': 'Jongheon Jeong'}, {'authorId': '143720148', 'name': 'Jinwoo Shin'}]",2020.0,Neural Information Processing Systems,['Unsolved Problems in ML Safety'],1,"Novelty detection, i.e., identifying whether a given sample is drawn from outside the training distribution, is essential for reliable machine learning. To this end, there have been many attempts at learning a representation well-suited for novelty detection and designing a score based on such representation. In this paper, we propose a simple, yet effective method named contrasting shifted instances (CSI), inspired by the recent success on contrastive learning of visual representations. Specifically, in addition to contrasting a given sample with other instances as in conventional contrastive learning methods, our training scheme contrasts the sample with distributionally-shifted augmentations of itself. Based on this, we propose a new detection score that is specific to the proposed training scheme. Our experiments demonstrate the superiority of our method under various novelty detection scenarios, including unlabeled one-class, unlabeled multi-class and labeled multi-class settings, with various image benchmark datasets.",286.0
"It Takes Two to Lie: One to Lie, and One to Listen",45204e83165e0bb7ee081d1a077c3b7fb899c5e4,"[{'authorId': '21317593', 'name': 'Denis Peskov'}, {'authorId': '2055923908', 'name': 'Benny Cheng'}, {'authorId': '143718836', 'name': 'Ahmed Elgohary'}, {'authorId': '40080808', 'name': 'Joe Barrow'}, {'authorId': '1388368997', 'name': 'Cristian Danescu-Niculescu-Mizil'}, {'authorId': '1389036863', 'name': 'Jordan L. Boyd-Graber'}]",2020.0,Annual Meeting of the Association for Computational Linguistics,['Unsolved Problems in ML Safety'],1,"Trust is implicit in many online text conversations—striking up new friendships, or asking for tech support. But trust can be betrayed through deception. We study the language and dynamics of deception in the negotiation-based game Diplomacy, where seven players compete for world domination by forging and breaking alliances with each other. Our study with players from the Diplomacy community gathers 17,289 messages annotated by the sender for their intended truthfulness and by the receiver for their perceived truthfulness. Unlike existing datasets, this captures deception in long-lasting relationships, where the interlocutors strategically combine truth with lies to advance objectives. A model that uses power dynamics and conversational contexts can predict when a lie occurs nearly as well as human players.",17.0
Smooth Adversarial Training,a2a349218b7889425c005880cc4b16b0a9e54dd8,"[{'authorId': '3011497', 'name': 'Cihang Xie'}, {'authorId': '120805419', 'name': 'Mingxing Tan'}, {'authorId': '40206014', 'name': 'Boqing Gong'}, {'authorId': '145081362', 'name': 'A. Yuille'}, {'authorId': '2827616', 'name': 'Quoc V. Le'}]",2020.0,arXiv.org,['Unsolved Problems in ML Safety'],1,"It is commonly believed that networks cannot be both accurate and robust, that gaining robustness means losing accuracy. It is also generally believed that, unless making networks larger, network architectural elements would otherwise matter little in improving adversarial robustness. Here we present evidence to challenge these common beliefs by a careful study about adversarial training. Our key observation is that the widely-used ReLU activation function significantly weakens adversarial training due to its non-smooth nature. Hence we propose smooth adversarial training (SAT), in which we replace ReLU with its smooth approximations to strengthen adversarial training. The purpose of smooth activation functions in SAT is to allow it to find harder adversarial examples and compute better gradient updates during adversarial training. Compared to standard adversarial training, SAT improves adversarial robustness for ""free"", i.e., no drop in accuracy and no increase in computational cost. For example, without introducing additional computations, SAT significantly enhances ResNet-50's robustness from 33.0% to 42.3%, while also improving accuracy by 0.9% on ImageNet. SAT also works well with larger networks: it helps EfficientNet-L1 to achieve 82.2% accuracy and 58.6% robustness on ImageNet, outperforming the previous state-of-the-art defense by 9.5% for accuracy and 11.6% for robustness.",104.0
Unsupervised Translation of Programming Languages,91b21ce88d97ce37a70c8c5d725c35b5ba466638,"[{'authorId': '114952298', 'name': 'Marie-Anne Lachaux'}, {'authorId': '3361236', 'name': 'Baptiste Rozière'}, {'authorId': '3294314', 'name': 'L. Chanussot'}, {'authorId': '1830914', 'name': 'Guillaume Lample'}]",2020.0,Neural Information Processing Systems,['Unsolved Problems in ML Safety'],1,"A transcompiler, also known as source-to-source translator, is a system that converts source code from a high-level programming language (such as C++ or Python) to another. Transcompilers are primarily used for interoperability, and to port codebases written in an obsolete or deprecated language (e.g. COBOL, Python 2) to a modern one. They typically rely on handcrafted rewrite rules, applied to the source code abstract syntax tree. Unfortunately, the resulting translations often lack readability, fail to respect the target language conventions, and require manual modifications in order to work properly. The overall translation process is timeconsuming and requires expertise in both the source and target languages, making code-translation projects expensive. Although neural models significantly outperform their rule-based counterparts in the context of natural language translation, their applications to transcompilation have been limited due to the scarcity of parallel data in this domain. In this paper, we propose to leverage recent approaches in unsupervised machine translation to train a fully unsupervised neural transcompiler. We train our model on source code from open source GitHub projects, and show that it can translate functions between C++, Java, and Python with high accuracy. Our method relies exclusively on monolingual source code, requires no expertise in the source or target languages, and can easily be generalized to other programming languages. We also build and release a test set composed of 852 parallel functions, along with unit tests to check the correctness of translations. We show that our model outperforms rule-based commercial baselines by a significant margin.",172.0
Avoiding Side Effects in Complex Environments,b6f028e8611417d813ed7939f2434e3fb8c1d641,"[{'authorId': '49277224', 'name': 'A. M. Turner'}, {'authorId': '13002147', 'name': 'Neale Ratzlaff'}, {'authorId': '1729906', 'name': 'Prasad Tadepalli'}]",2020.0,Neural Information Processing Systems,['Unsolved Problems in ML Safety'],1,"Reward function specification can be difficult, even in simple environments. Realistic environments contain millions of states. Rewarding the agent for making a widget may be easy, but penalizing the multitude of possible negative side effects is hard. In toy environments, Attainable Utility Preservation (AUP) avoids side effects by penalizing shifts in the ability to achieve randomly generated goals. We scale this approach to large, randomly generated environments based on Conway's Game of Life. By preserving optimal value for a single randomly generated reward function, AUP incurs modest overhead, completes the specified task, and avoids side effects.",21.0
AI Research Considerations for Human Existential Safety (ARCHES),a9c46dfd9a24c754a67386e02424ad68b1f4ab3b,"[{'authorId': '2651789', 'name': 'Andrew Critch'}, {'authorId': '145055042', 'name': 'David Krueger'}]",2020.0,arXiv.org,"['Unsolved Problems in ML Safety', 'AI Research Considerations for Human Existential Safety (ARCHES)']",2,"Framed in positive terms, this report examines how technical AI research might be steered in a manner that is more attentive to humanity's long-term prospects for survival as a species. In negative terms, we ask what existential risks humanity might face from AI development in the next century, and by what principles contemporary technical research might be directed to address those risks. 
A key property of hypothetical AI technologies is introduced, called \emph{prepotence}, which is useful for delineating a variety of potential existential risks from artificial intelligence, even as AI paradigms might shift. A set of \auxref{dirtot} contemporary research \directions are then examined for their potential benefit to existential safety. Each research direction is explained with a scenario-driven motivation, and examples of existing work from which to build. The research directions present their own risks and benefits to society that could occur at various scales of impact, and in particular are not guaranteed to benefit existential safety if major developments in them are deployed without adequate forethought and oversight. As such, each direction is accompanied by a consideration of potentially negative side effects.",21.0
Language Models are Few-Shot Learners,6b85b63579a916f705a8e10a49bd8d849d91b1fc,"[{'authorId': '31035595', 'name': 'Tom B. Brown'}, {'authorId': '2056658938', 'name': 'Benjamin Mann'}, {'authorId': '39849748', 'name': 'Nick Ryder'}, {'authorId': '2065894334', 'name': 'Melanie Subbiah'}, {'authorId': '152724169', 'name': 'J. Kaplan'}, {'authorId': '6515819', 'name': 'Prafulla Dhariwal'}, {'authorId': '2072676', 'name': 'Arvind Neelakantan'}, {'authorId': '67311962', 'name': 'Pranav Shyam'}, {'authorId': '144864359', 'name': 'Girish Sastry'}, {'authorId': '119609682', 'name': 'Amanda Askell'}, {'authorId': '144517868', 'name': 'Sandhini Agarwal'}, {'authorId': '1404060687', 'name': 'Ariel Herbert-Voss'}, {'authorId': '2064404342', 'name': 'Gretchen Krueger'}, {'authorId': '103143311', 'name': 'T. Henighan'}, {'authorId': '48422824', 'name': 'Rewon Child'}, {'authorId': '1992922591', 'name': 'A. Ramesh'}, {'authorId': '2052152920', 'name': 'Daniel M. Ziegler'}, {'authorId': '49387725', 'name': 'Jeff Wu'}, {'authorId': '2059411355', 'name': 'Clemens Winter'}, {'authorId': '144239765', 'name': 'Christopher Hesse'}, {'authorId': '2108828435', 'name': 'Mark Chen'}, {'authorId': '2064673055', 'name': 'Eric Sigler'}, {'authorId': '1380985420', 'name': 'Mateusz Litwin'}, {'authorId': '145565184', 'name': 'Scott Gray'}, {'authorId': '1490681878', 'name': 'Benjamin Chess'}, {'authorId': '2115193883', 'name': 'Jack Clark'}, {'authorId': '133740015', 'name': 'Christopher Berner'}, {'authorId': '52238703', 'name': 'Sam McCandlish'}, {'authorId': '38909097', 'name': 'Alec Radford'}, {'authorId': '1701686', 'name': 'Ilya Sutskever'}, {'authorId': '2698777', 'name': 'Dario Amodei'}]",2020.0,Neural Information Processing Systems,['Unsolved Problems in ML Safety'],1,"Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",9479.0
Identifying Statistical Bias in Dataset Replication,96351ffc9bcb9e579b93937b28e3cf3827a00184,"[{'authorId': '39468283', 'name': 'Logan Engstrom'}, {'authorId': '34562927', 'name': 'Andrew Ilyas'}, {'authorId': '2852106', 'name': 'Shibani Santurkar'}, {'authorId': '2754804', 'name': 'Dimitris Tsipras'}, {'authorId': '5164568', 'name': 'J. Steinhardt'}, {'authorId': '143826246', 'name': 'A. Madry'}]",2020.0,International Conference on Machine Learning,['Unsolved Problems in ML Safety'],1,"Dataset replication is a useful tool for assessing whether improvements in test accuracy on a specific benchmark correspond to improvements in models' ability to generalize reliably. In this work, we present unintuitive yet significant ways in which standard approaches to dataset replication introduce statistical bias, skewing the resulting observations. We study ImageNet-v2, a replication of the ImageNet dataset on which models exhibit a significant (11-14%) drop in accuracy, even after controlling for a standard human-in-the-loop measure of data quality. We show that after correcting for the identified statistical bias, only an estimated $3.6\% \pm 1.5\%$ of the original $11.7\% \pm 1.0\%$ accuracy drop remains unaccounted for. We conclude with concrete recommendations for recognizing and avoiding bias in dataset replication. Code for our study is publicly available at this http URL .",36.0
Toward Trustworthy AI Development: Mechanisms for Supporting Verifiable Claims,62c3142956d54db158d190ce691e3c13e7897412,"[{'authorId': '35167962', 'name': 'Miles Brundage'}, {'authorId': '49344883', 'name': 'S. Avin'}, {'authorId': '2116439939', 'name': 'Jasmine Wang'}, {'authorId': '36729401', 'name': 'Haydn Belfield'}, {'authorId': '2064404342', 'name': 'Gretchen Krueger'}, {'authorId': '40051700', 'name': 'Gillian K. Hadfield'}, {'authorId': '2103414', 'name': 'Heidy Khlaaf'}, {'authorId': '46477761', 'name': 'Jingying Yang'}, {'authorId': '48625835', 'name': 'H. Toner'}, {'authorId': '145891577', 'name': 'Ruth Fong'}, {'authorId': '3422058', 'name': 'Tegan Maharaj'}, {'authorId': '48797476', 'name': 'P. W. Koh'}, {'authorId': '50237813', 'name': 'Sara Hooker'}, {'authorId': '52152632', 'name': 'J. Leung'}, {'authorId': '145994651', 'name': 'Andrew Trask'}, {'authorId': '7533718', 'name': 'Emma Bluemke'}, {'authorId': '1632943234', 'name': 'Jonathan Lebensbold'}, {'authorId': '1435765036', 'name': ""Cullen O'Keefe""}, {'authorId': '13643850', 'name': 'Mark Koren'}, {'authorId': '81672773', 'name': 'T. Ryffel'}, {'authorId': '1490759620', 'name': 'J. Rubinovitz'}, {'authorId': '1632969493', 'name': 'T. Besiroglu'}, {'authorId': '113528137', 'name': 'F. Carugati'}, {'authorId': '2115193883', 'name': 'Jack Clark'}, {'authorId': '2654106', 'name': 'P. Eckersley'}, {'authorId': '1632944342', 'name': 'Sarah de Haas'}, {'authorId': '16543015', 'name': 'Maritza L. Johnson'}, {'authorId': '153774873', 'name': 'B. Laurie'}, {'authorId': '67147139', 'name': 'A. Ingerman'}, {'authorId': '49302440', 'name': 'Igor Krawczuk'}, {'authorId': '119609682', 'name': 'Amanda Askell'}, {'authorId': '3038395', 'name': 'Rosario Cammarota'}, {'authorId': '1809110', 'name': 'A. Lohn'}, {'authorId': '145055042', 'name': 'David Krueger'}, {'authorId': '1397118333', 'name': 'C. Stix'}, {'authorId': '153322217', 'name': 'Peter Henderson'}, {'authorId': '47428129', 'name': 'L. Graham'}, {'authorId': '50728331', 'name': 'Carina E. A. Prunkl'}, {'authorId': '2114362965', 'name': 'Bianca Martin'}, {'authorId': '40437610', 'name': 'Elizabeth Seger'}, {'authorId': '2447168', 'name': 'Noa Zilberman'}, {'authorId': '1632943165', 'name': ""Se'an 'O h'Eigeartaigh""}, {'authorId': '114363092', 'name': 'F. Kroeger'}, {'authorId': '144864359', 'name': 'Girish Sastry'}, {'authorId': '2059975474', 'name': 'R. Kagan'}, {'authorId': '145689461', 'name': 'Adrian Weller'}, {'authorId': '66139470', 'name': 'Brian Tse'}, {'authorId': '2057742918', 'name': 'Elizabeth Barnes'}, {'authorId': '3198576', 'name': 'A. Dafoe'}, {'authorId': '35681920', 'name': 'P. Scharre'}, {'authorId': '1404060687', 'name': 'Ariel Herbert-Voss'}, {'authorId': '1632969714', 'name': 'Martijn Rasser'}, {'authorId': '2462516', 'name': 'Shagun Sodhani'}, {'authorId': '152629250', 'name': 'Carrick Flynn'}, {'authorId': '50708404', 'name': 'T. Gilbert'}, {'authorId': '51020531', 'name': 'Lisa Dyer'}, {'authorId': '2111179979', 'name': 'Saif Khan'}, {'authorId': '1751762', 'name': 'Yoshua Bengio'}, {'authorId': '1486494220', 'name': 'Markus Anderljung'}]",2020.0,arXiv.org,['Unsolved Problems in ML Safety'],1,"With the recent wave of progress in artificial intelligence (AI) has come a growing awareness of the large-scale impacts of AI systems, and recognition that existing regulations and norms in industry and academia are insufficient to ensure responsible AI development. In order for AI developers to earn trust from system users, customers, civil society, governments, and other stakeholders that they are building AI responsibly, they will need to make verifiable claims to which they can be held accountable. Those outside of a given organization also need effective means of scrutinizing such claims. This report suggests various steps that different stakeholders can take to improve the verifiability of claims made about AI systems and their associated development processes, with a focus on providing evidence about the safety, security, fairness, and privacy protection of AI systems. We analyze ten mechanisms for this purpose--spanning institutions, software, and hardware--and make recommendations aimed at implementing, exploring, or improving those mechanisms.",174.0
Adversarial Weight Perturbation Helps Robust Generalization,574e8fb91ee0e089f4cadb4145302f97f6793bdf,"[{'authorId': '1492154834', 'name': 'Dongxian Wu'}, {'authorId': '3085483', 'name': 'Shutao Xia'}, {'authorId': '1919541', 'name': 'Yisen Wang'}]",2020.0,Neural Information Processing Systems,['Unsolved Problems in ML Safety'],1,"The study on improving the robustness of deep neural networks against adversarial examples grows rapidly in recent years. Among them, adversarial training is the most promising one, which flattens the input loss landscape (loss change with respect to input) via training on adversarially perturbed examples. However, how the widely used weight loss landscape (loss change with respect to weight) performs in adversarial training is rarely explored. In this paper, we investigate the weight loss landscape from a new perspective, and identify a clear correlation between the flatness of weight loss landscape and robust generalization gap. Several well-recognized adversarial training improvements, such as early stopping, designing new objective functions, or leveraging unlabeled data, all implicitly flatten the weight loss landscape. Based on these observations, we propose a simple yet effective Adversarial Weight Perturbation (AWP) to explicitly regularize the flatness of weight loss landscape, forming a double-perturbation mechanism in the adversarial training framework that adversarially perturbs both inputs and weights. Extensive experiments demonstrate that AWP indeed brings flatter weight loss landscape and can be easily incorporated into various existing adversarial training methods to further boost their adversarial robustness.",343.0
The TrojAI Software Framework: An OpenSource tool for Embedding Trojans into Deep Learning Models,41c883754ae1983e94bff6c3ec3d40306cffe06e,"[{'authorId': '38081739', 'name': 'Kiran Karra'}, {'authorId': '32339320', 'name': 'C. Ashcraft'}, {'authorId': '29880725', 'name': 'Neil Fendley'}]",2020.0,arXiv.org,['Unsolved Problems in ML Safety'],1,"In this paper, we introduce the TrojAI software framework, an open source set of Python tools capable of generating triggered (poisoned) datasets and associated deep learning (DL) models with trojans at scale. We utilize the developed framework to generate a large set of trojaned MNIST classifiers, as well as demonstrate the capability to produce a trojaned reinforcement-learning model using vector observations. Results on MNIST show that the nature of the trigger, training batch size, and dataset poisoning percentage all affect successful embedding of trojans. We test Neural Cleanse against the trojaned MNIST models and successfully detect anomalies in the trained models approximately $18\%$ of the time. Our experiments and workflow indicate that the TrojAI software framework will enable researchers to easily understand the effects of various configurations of the dataset and training hyperparameters on the generated trojaned deep learning model, and can be used to rapidly and comprehensively test new trojan detection methods.",21.0
On Adaptive Attacks to Adversarial Example Defenses,58c143069444c7dff4be53531a47efefc40be497,"[{'authorId': '2444919', 'name': 'Florian Tramèr'}, {'authorId': '2483738', 'name': 'Nicholas Carlini'}, {'authorId': '40634590', 'name': 'Wieland Brendel'}, {'authorId': '143826246', 'name': 'A. Madry'}]",2020.0,Neural Information Processing Systems,['Unsolved Problems in ML Safety'],1,"Adaptive attacks have (rightfully) become the de facto standard for evaluating defenses to adversarial examples. We find, however, that typical adaptive evaluations are incomplete. We demonstrate that thirteen defenses recently published at ICLR, ICML and NeurIPS---and chosen for illustrative and pedagogical purposes---can be circumvented despite attempting to perform evaluations using adaptive attacks. While prior evaluation papers focused mainly on the end result---showing that a defense was ineffective---this paper focuses on laying out the methodology and the approach necessary to perform an adaptive attack. We hope that these analyses will serve as guidance on how to properly perform adaptive attacks against defenses to adversarial examples, and thus will allow the community to make further progress in building more robust models.",534.0
"Statistical Consequences of Fat Tails: Real World Preasymptotics, Epistemology, and Applications",18fbc133e3fc8e0d9dd5c5164bf20ce196770798,"[{'authorId': '1928817', 'name': 'N. Taleb'}]",2020.0,,"['Unsolved Problems in ML Safety', 'X-Risk Analysis for AI Research']",2,"The book investigates the misapplication of conventional statistical techniques to fat tailed distributions and looks for remedies, when possible. Switching from thin tailed to fat tailed distributions requires more than ""changing the color of the dress"". Traditional asymptotics deal mainly with either n=1 or $n=\infty$, and the real world is in between, under of the ""laws of the medium numbers"" --which vary widely across specific distributions. Both the law of large numbers and the generalized central limit mechanisms operate in highly idiosyncratic ways outside the standard Gaussian or Levy-Stable basins of convergence. A few examples: + The sample mean is rarely in line with the population mean, with effect on ""naive empiricism"", but can be sometimes be estimated via parametric methods. + The ""empirical distribution"" is rarely empirical. + Parameter uncertainty has compounding effects on statistical metrics. + Dimension reduction (principal components) fails. + Inequality estimators (GINI or quantile contributions) are not additive and produce wrong results. + Many ""biases"" found in psychology become entirely rational under more sophisticated probability distributions + Most of the failures of financial economics, econometrics, and behavioral economics can be attributed to using the wrong distributions. This book, the first volume of the Technical Incerto, weaves a narrative around published journal articles.",43.0
"Artificial Intelligence, Values, and Alignment",7aa70e2c12c8ba2dcc828893adb8bb56e3766726,"[{'authorId': '116589025', 'name': 'Iason Gabriel'}]",2020.0,Minds and Machines,['Unsolved Problems in ML Safety'],1,,148.0
Dreaming to Distill: Data-Free Knowledge Transfer via DeepInversion,0dd89a3b906e9a90a47834dcc766a0b9967d10d1,"[{'authorId': '1989015', 'name': 'Hongxu Yin'}, {'authorId': '2824500', 'name': 'Pavlo Molchanov'}, {'authorId': '49969902', 'name': 'Zhizhong Li'}, {'authorId': '2974008', 'name': 'J. Álvarez'}, {'authorId': '36508529', 'name': 'Arun Mallya'}, {'authorId': '2433269', 'name': 'Derek Hoiem'}, {'authorId': '144874163', 'name': 'N. Jha'}, {'authorId': '1690538', 'name': 'J. Kautz'}]",2019.0,Computer Vision and Pattern Recognition,['Unsolved Problems in ML Safety'],1,"We introduce DeepInversion, a new method for synthesizing images from the image distribution used to train a deep neural network. We ``invert'' a trained network (teacher) to synthesize class-conditional input images starting from random noise, without using any additional information about the training dataset. Keeping the teacher fixed, our method optimizes the input while regularizing the distribution of intermediate feature maps using information stored in the batch normalization layers of the teacher. Further, we improve the diversity of synthesized images using Adaptive DeepInversion, which maximizes the Jensen-Shannon divergence between the teacher and student network logits. The resulting synthesized images from networks trained on the CIFAR-10 and ImageNet datasets demonstrate high fidelity and degree of realism, and help enable a new breed of data-free applications - ones that do not require any real images or labeled data. We demonstrate the applicability of our proposed method to three tasks of immense practical importance - (i) data-free network pruning, (ii) data-free knowledge transfer, and (iii) data-free continual learning.",271.0
AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty,02b1607af35b48f0bd716367caf6a7428b969369,"[{'authorId': '3422872', 'name': 'Dan Hendrycks'}, {'authorId': '52227748', 'name': 'Norman Mu'}, {'authorId': '8132903', 'name': 'E. D. Cubuk'}, {'authorId': '2368067', 'name': 'Barret Zoph'}, {'authorId': '2058362', 'name': 'J. Gilmer'}, {'authorId': '40627523', 'name': 'Balaji Lakshminarayanan'}]",2019.0,International Conference on Learning Representations,['Unsolved Problems in ML Safety'],1,"Modern deep neural networks can achieve high accuracy when the training distribution and test distribution are identically distributed, but this assumption is frequently violated in practice. When the train and test distributions are mismatched, accuracy can plummet. Currently there are few techniques that improve robustness to unforeseen data shifts encountered during deployment. In this work, we propose a technique to improve the robustness and uncertainty estimates of image classifiers. We propose AugMix, a data processing technique that is simple to implement, adds limited computational overhead, and helps models withstand unforeseen corruptions. AugMix significantly improves robustness and uncertainty measures on challenging image classification benchmarks, closing the gap between previous methods and the best possible performance in some cases by more than half.",678.0
SafeLife 1.0: Exploring Side Effects in Complex Environments,058897b987f563da332a48dfecf4dda0367c3d74,"[{'authorId': '2064084601', 'name': 'Carroll L. Wainwright'}, {'authorId': '2654106', 'name': 'P. Eckersley'}]",2019.0,SafeAI@AAAI,['Unsolved Problems in ML Safety'],1,"We present SafeLife, a publicly available reinforcement learning environment that tests the safety of reinforcement learning agents. It contains complex, dynamic, tunable, procedurally generated levels with many opportunities for unsafe behavior. Agents are graded both on their ability to maximize their explicit reward and on their ability to operate safely without unnecessary side effects. We train agents to maximize rewards using proximal policy optimization and score them on a suite of benchmark levels. The resulting agents are performant but not safe---they tend to cause large side effects in their environments---but they form a baseline against which future safety research can be measured.",11.0
Intriguing Properties of Adversarial ML Attacks in the Problem Space,bf0deef9c078697a5e2063d51c2088848f9ea8a2,"[{'authorId': '2185446', 'name': 'Fabio Pierazzi'}, {'authorId': '51122933', 'name': 'Feargus Pendlebury'}, {'authorId': '3447275', 'name': 'Jacopo Cortellazzi'}, {'authorId': '2189170', 'name': 'L. Cavallaro'}]",2019.0,IEEE Symposium on Security and Privacy,['Unsolved Problems in ML Safety'],1,"Recent research efforts on adversarial ML have investigated problem-space attacks, focusing on the generation of real evasive objects in domains where, unlike images, there is no clear inverse mapping to the feature space (e.g., software). However, the design, comparison, and real-world implications of problem-space attacks remain underexplored.This paper makes two major contributions. First, we propose a novel formalization for adversarial ML evasion attacks in the problem-space, which includes the definition of a comprehensive set of constraints on available transformations, preserved semantics, robustness to preprocessing, and plausibility. We shed light on the relationship between feature space and problem space, and we introduce the concept of side-effect features as the byproduct of the inverse feature-mapping problem. This enables us to define and prove necessary and sufficient conditions for the existence of problem-space attacks. We further demonstrate the expressive power of our formalization by using it to describe several attacks from related literature across different domains.Second, building on our formalization, we propose a novel problem-space attack on Android malware that overcomes past limitations. Experiments on a dataset with 170K Android apps from 2017 and 2018 show the practical feasibility of evading a state-of-the-art malware classifier along with its hardened version. Our results demonstrate that ""adversarial-malware as a service"" is a realistic threat, as we automatically generate thousands of realistic and inconspicuous adversarial applications at scale, where on average it takes only a few minutes to generate an adversarial app. Yet, out of the 1600+ papers on adversarial ML published in the past six years, roughly 40 focus on malware [15]—and many remain only in the feature space.Our formalization of problem-space attacks paves the way to more principled research in this domain. We responsibly release the code and dataset of our novel attack to other researchers, to encourage future work on defenses in the problem space.",141.0
Confidence-Calibrated Adversarial Training: Generalizing to Unseen Attacks,604dc3c7ad3736e58d7fd8a5839f8d8ba63e63b6,"[{'authorId': '145177088', 'name': 'David Stutz'}, {'authorId': '143610806', 'name': 'Matthias Hein'}, {'authorId': '48920094', 'name': 'B. Schiele'}]",2019.0,International Conference on Machine Learning,['Unsolved Problems in ML Safety'],1,"Adversarial training yields robust models against a specific threat model, e.g., $L_\infty$ adversarial examples. Typically robustness does not generalize to previously unseen threat models, e.g., other $L_p$ norms, or larger perturbations. Our confidence-calibrated adversarial training (CCAT) tackles this problem by biasing the model towards low confidence predictions on adversarial examples. By allowing to reject examples with low confidence, robustness generalizes beyond the threat model employed during training. CCAT, trained only on $L_\infty$ adversarial examples, increases robustness against larger $L_\infty$, $L_2$, $L_1$ and $L_0$ attacks, adversarial frames, distal adversarial examples and corrupted examples and yields better clean accuracy compared to adversarial training. For thorough evaluation we developed novel white- and black-box attacks directly attacking CCAT by maximizing confidence. For each threat model, we use $7$ attacks with up to $50$ restarts and $5000$ iterations and report worst-case robust test error, extended to our confidence-thresholded setting, across all attacks.",80.0
Neutaint: Efficient Dynamic Taint Analysis with Neural Networks,290142398d893cd3da83f821463478bf9f309a71,"[{'authorId': '38468935', 'name': 'Dongdong She'}, {'authorId': '3325283', 'name': 'Yizheng Chen'}, {'authorId': '31631000', 'name': 'Baishakhi Ray'}, {'authorId': '39400201', 'name': 'S. Jana'}]",2019.0,IEEE Symposium on Security and Privacy,['Unsolved Problems in ML Safety'],1,"Dynamic taint analysis (DTA) is widely used by various applications to track information flow during runtime execution. Existing DTA techniques use rule-based taint-propagation, which is neither accurate (i.e., high false positive rate) nor efficient (i.e., large runtime overhead). It is hard to specify taint rules for each operation while covering all corner cases correctly. Moreover, the overtaint and undertaint errors can accumulate during the propagation of taint information across multiple operations. Finally, rule-based propagation requires each operation to be inspected before applying the appropriate rules resulting in prohibitive performance overhead on large real-world applications.In this work, we propose Neutaint, a novel end-to-end approach to track information flow using neural program embeddings. The neural program embeddings model the target’s programs computations taking place between taint sources and sinks, which automatically learns the information flow by observing a diverse set of execution traces. To perform lightweight and precise information flow analysis, we utilize saliency maps to reason about most influential sources for different sinks. Neutaint constructs two saliency maps, a popular machine learning approach to influence analysis, to summarize both coarse-grained and fine-grained information flow in the neural program embeddings.We compare Neutaint with 3 state-of-the-art dynamic taint analysis tools. The evaluation results show that Neutaint can achieve 68% accuracy, on average, which is 10% improvement while reducing 40× runtime overhead over the second-best taint tool Libdft on 6 real world programs. Neutaint also achieves 61% more edge coverage when used for taint-guided fuzzing indicating the effectiveness of the identified influential bytes. We also evaluate Neutaint’s ability to detect real world software attacks. The results show that Neutaint can successfully detect different types of vulnerabilities including buffer/heap/integer overflows, division by zero, etc. Lastly, Neutaint can detect 98.7% of total flows, the highest among all taint analysis tools.",32.0
Adversarial Policies: Attacking Deep Reinforcement Learning,6ff50528f3d7c72772f8c0e3f8398f9dd8e06575,"[{'authorId': '34594377', 'name': 'A. Gleave'}, {'authorId': '95358337', 'name': 'Michael Dennis'}, {'authorId': '2065893046', 'name': 'Neel Kant'}, {'authorId': '48984579', 'name': 'Cody Wild'}, {'authorId': '1736651', 'name': 'S. Levine'}, {'authorId': '145107462', 'name': 'Stuart J. Russell'}]",2019.0,International Conference on Learning Representations,"['Unsolved Problems in ML Safety', 'X-Risk Analysis for AI Research']",2,"Deep reinforcement learning (RL) policies are known to be vulnerable to adversarial perturbations to their observations, similar to adversarial examples for classifiers. However, an attacker is not usually able to directly modify another agent's observations. This might lead one to wonder: is it possible to attack an RL agent simply by choosing an adversarial policy acting in a multi-agent environment so as to create natural observations that are adversarial? We demonstrate the existence of adversarial policies in zero-sum games between simulated humanoid robots with proprioceptive observations, against state-of-the-art victims trained via self-play to be robust to opponents. The adversarial policies reliably win against the victims but generate seemingly random and uncoordinated behavior. We find that these policies are more successful in high-dimensional environments, and induce substantially different activations in the victim policy network than when the victim plays against a normal opponent. Videos are available at this https URL.",210.0
Detecting and Characterizing Lateral Phishing at Scale,eb901db319736a9b4f3a2be86735df393ef569bd,"[{'authorId': '119958711', 'name': 'Grant Ho'}, {'authorId': '2717484', 'name': 'Asaf Cidon'}, {'authorId': '2064122', 'name': 'Lior Gavish'}, {'authorId': '1381199611', 'name': 'M. Schweighauser'}, {'authorId': '1744800', 'name': 'V. Paxson'}, {'authorId': '1727599', 'name': 'S. Savage'}, {'authorId': '1739245', 'name': 'G. Voelker'}, {'authorId': '145394689', 'name': 'D. Wagner'}]",2019.0,USENIX Security Symposium,['Unsolved Problems in ML Safety'],1,"Author(s): Ho, G; Cidon, A; Gavish, L; Schweighauser, M; Paxson, V; Savage, S; Voelker, GM; Wagner, D | Abstract: © 2019 by The USENIX Association. All rights reserved. We present the first large-scale characterization of lateral phishing attacks, based on a dataset of 113 million employee-sent emails from 92 enterprise organizations. In a lateral phishing attack, adversaries leverage a compromised enterprise account to send phishing emails to other users, benefit-ting from both the implicit trust and the information in the hijacked user's account. We develop a classifier that finds hundreds of real-world lateral phishing emails, while generating under four false positives per every one-million employee-sent emails. Drawing on the attacks we detect, as well as a corpus of user-reported incidents, we quantify the scale of lateral phishing, identify several thematic content and recipient targeting strategies that attackers follow, illuminate two types of sophisticated behaviors that attackers exhibit, and estimate the success rate of these attacks. Collectively, these results expand our mental models of the 'enterprise attacker' and shed light on the current state of enterprise phishing attacks.",58.0
Verified Uncertainty Calibration,b8333963567e9ef65c059466c03b0fbee3eaec6a,"[{'authorId': '32423266', 'name': 'Ananya Kumar'}, {'authorId': '145419642', 'name': 'Percy Liang'}, {'authorId': '1901958', 'name': 'Tengyu Ma'}]",2019.0,Neural Information Processing Systems,['Unsolved Problems in ML Safety'],1,"Applications such as weather forecasting and personalized medicine demand models that output calibrated probability estimates---those representative of the true likelihood of a prediction. Most models are not calibrated out of the box but are recalibrated by post-processing model outputs. We find in this work that popular recalibration methods like Platt scaling and temperature scaling are (i) less calibrated than reported, and (ii) current techniques cannot estimate how miscalibrated they are. An alternative method, histogram binning, has measurable calibration error but is sample inefficient---it requires $O(B/\epsilon^2)$ samples, compared to $O(1/\epsilon^2)$ for scaling methods, where $B$ is the number of distinct probabilities the model can output. To get the best of both worlds, we introduce the scaling-binning calibrator, which first fits a parametric function to reduce variance and then bins the function values to actually ensure calibration. This requires only $O(1/\epsilon^2 + B)$ samples. Next, we show that we can estimate a model's calibration error more accurately using an estimator from the meteorological community---or equivalently measure its calibration error with fewer samples ($O(\sqrt{B})$ instead of $O(B)$). We validate our approach with multiclass calibration experiments on CIFAR-10 and ImageNet, where we obtain a 35% lower calibration error than histogram binning and, unlike scaling methods, guarantees on true calibration. In these experiments, we also estimate the calibration error and ECE more accurately than the commonly used plugin estimators. We implement all these methods in a Python library: this https URL",198.0
The Vulnerable World Hypothesis,41918a9ed66135691d741d0285c5ef24ea9597ff,"[{'authorId': '2193691', 'name': 'N. Bostrom'}]",2019.0,Global Policy,"['Unsolved Problems in ML Safety', 'AI Research Considerations for Human Existential Safety (ARCHES)', 'X-Risk Analysis for AI Research']",3,"Scientific and technological progress might change people’s capabilities or incentives in ways that would destabilize civilization. For example, advances in DIY biohacking tools might make it easy for anybody with basic training in biology to kill millions; novel military technologies could trigger arms races in which whoever strikes first has a decisive advantage; or some economically advantageous process may be invented that produces disastrous negative global externalities that are hard to regulate. This paper introduces the concept of a vulnerable world: roughly, one in which there is some level of technological development at which civilization almost certainly gets devastated by default, i.e. unless it has exited the ‘semi-anarchic default condition’. Several counterfactual historical and speculative future vulnerabilities are analyzed and arranged into a typology. A general ability to stabilize a vulnerable world would require greatly amplified capacities for preventive policing and global governance. The vulnerable world hypothesis thus offers a new perspective from which to evaluate the risk-benefit balance of developments towards ubiquitous surveillance or a unipolar world order. Policy Implications • Technology policy should not unquestioningly assume that all technological progress is beneficial, or that complete scientific openness is always best, or that the world has the capacity to manage any potential downside of a technology after it is invented. • Some areas, such as synthetic biology, could produce a discovery that suddenly democratizes mass destruction, e.g. by empowering individuals to kill hundreds of millions of people using readily available materials. In order for civilization to have a general capacity to deal with “black ball” inventions of this type, it would need a system of ubiquitous real-time worldwide surveillance. In some scenarios, such a system would need to be in place before the technology is invented. • Partial protection against a limited set of possible black balls is obtainable through more targeted interventions. For example, biorisk might be mitigated by means of background checks and monitoring of personnel in some types of biolab, by discouraging DIY biohacking (e.g. through licencing requirements), and by restructuring the biotech sector to limit access to some cutting-edge instrumentation and information. Rather than allow anybody to buy their own DNA synthesis machine, DNA synthesis could be provided as a service by a small number of closely monitored providers. • Another, subtler, type of black ball would be one that strengthens incentives for harmful use—e.g. a military technology that makes wars more destructive while giving a greater advantage to the side that strikes first. Like a squirrel who uses the times of plenty to store up nuts for the winter, we should use times of relative peace to build stronger mechanisms for resolving international disputes. Is there a black ball in the urn of possible inventions? One way of looking at human creativity is as a process of pulling balls out of a giant urn. The balls represent possible ideas, discoveries, technological inventions. Over the course of history, we have extracted a great many balls – mostly white (beneficial) but also various shades of gray (moderately harmful ones and mixed blessings). The cumulative effect on the human condition has so far been overwhelmingly positive, and may be much better still in the future (Bostrom, 2008). The global population has grown about three orders of magnitude over the last ten thousand years, and in the last two centuries per capita income, standards of living, and life expectancy have also risen. What we haven’t extracted, so far, is a black ball: a technology that invariably or by default destroys the civilization that invents it. The reason is not that we have been particularly careful or wise in our technology policy. We have just been lucky. It does not appear that any human civilization has been destroyed – as opposed to transformed – by its own inventions. We do have examples of civilizations being destroyed by inventions made elsewhere. For example, the European inventions that enabled transoceanic travel and force projection could be regarded as a black-ball event for the indigenous populations of the Americas, Australia, Tasmania, and some other places. The extinction of archaic hominid populations, such as the Neanderthals and the Denisovans, was probably facilitated by the technological superiority of Homo sapiens. But thus far, it seems, we have seen no sufficiently auto-destructive invention to count as a black ball for humanity. What if there is a black ball in the urn? If scientific and technological research continues, we will eventually reach it and pull it out. Our civilization has a considerable ability to Global Policy (2019) 10:4 doi: 10.1111/1758-5899.12718 © 2019 The Authors. Global Policy published by Durham University and John Wiley & Sons Ltd. This is an open access article under the terms of the Creative Commons Attribution-NonCommercial License, which permits use, distribution and reproduction in any medium, provided the original work is properly cited and is not used for commercial purposes. Global Policy Volume 10 . Issue 4 . November 2019 455",80.0
Beyond temperature scaling: Obtaining well-calibrated multiclass probabilities with Dirichlet calibration,d4691aef27ae3c768b90c34ca5d8521d202eb47c,"[{'authorId': '66368615', 'name': 'Meelis Kull'}, {'authorId': '1399591467', 'name': 'Miquel Perello-Nieto'}, {'authorId': '2728242', 'name': 'Markus Kängsepp'}, {'authorId': '3024963', 'name': 'T. S. Filho'}, {'authorId': '2112980202', 'name': 'Hao Song'}, {'authorId': '47840704', 'name': 'Peter A. Flach'}]",2019.0,Neural Information Processing Systems,['Unsolved Problems in ML Safety'],1,"Class probabilities predicted by most multiclass classifiers are uncalibrated, often tending towards over-confidence. With neural networks, calibration can be improved by temperature scaling, a method to learn a single corrective multiplicative factor for inputs to the last softmax layer. On non-neural models the existing methods apply binary calibration in a pairwise or one-vs-rest fashion. We propose a natively multiclass calibration method applicable to classifiers from any model class, derived from Dirichlet distributions and generalising the beta calibration method from binary classification. It is easily implemented with neural nets since it is equivalent to log-transforming the uncalibrated probabilities, followed by one linear layer and softmax. Experiments demonstrate improved probabilistic predictions according to multiple measures (confidence-ECE, classwise-ECE, log-loss, Brier score) across a wide range of datasets and classifiers. Parameters of the learned Dirichlet calibration map provide insights to the biases in the uncalibrated model.",199.0
Testing Robustness Against Unforeseen Adversaries,1c5b068ce6dff86bf152312b99f3360456a00faf,"[{'authorId': '35342489', 'name': 'Daniel Kang'}, {'authorId': '2116961690', 'name': 'Yi Sun'}, {'authorId': '3422872', 'name': 'Dan Hendrycks'}, {'authorId': '31035595', 'name': 'Tom B. Brown'}, {'authorId': '5164568', 'name': 'J. Steinhardt'}]",2019.0,arXiv.org,['Unsolved Problems in ML Safety'],1,"Most existing adversarial defenses only measure robustness to L_p adversarial attacks. Not only are adversaries unlikely to exclusively create small L_p perturbations, adversaries are unlikely to remain fixed. Adversaries adapt and evolve their attacks; hence adversarial defenses must be robust to a broad range of unforeseen attacks. We address this discrepancy between research and reality by proposing a new evaluation framework called ImageNet-UA. Our framework enables the research community to test ImageNet model robustness against attacks not encountered during training. To create ImageNet-UA's diverse attack suite, we introduce a total of four novel adversarial attacks. We also demonstrate that, in comparison to ImageNet-UA, prevailing L_inf robustness assessments give a narrow account of model robustness. By evaluating current defenses with ImageNet-UA, we find they provide little robustness to unforeseen attacks. We hope the greater variety and realism of ImageNet-UA enables development of more robust defenses which can generalize beyond attacks seen during training.",99.0
A 20-Year Community Roadmap for Artificial Intelligence Research in the US,6eb2b5f6e6985e5a2aa56c4843b02b8a644644ad,"[{'authorId': '145526918', 'name': 'Y. Gil'}, {'authorId': '1744679', 'name': 'B. Selman'}]",2019.0,arXiv.org,['Unsolved Problems in ML Safety'],1,"Decades of research in artificial intelligence (AI) have produced formidable technologies that are providing immense benefit to industry, government, and society. AI systems can now translate across multiple languages, identify objects in images and video, streamline manufacturing processes, and control cars. The deployment of AI systems has not only created a trillion-dollar industry that is projected to quadruple in three years, but has also exposed the need to make AI systems fair, explainable, trustworthy, and secure. Future AI systems will rightfully be expected to reason effectively about the world in which they (and people) operate, handling complex tasks and responsibilities effectively and ethically, engaging in meaningful communication, and improving their awareness through experience. 
Achieving the full potential of AI technologies poses research challenges that require a radical transformation of the AI research enterprise, facilitated by significant and sustained investment. These are the major recommendations of a recent community effort coordinated by the Computing Community Consortium and the Association for the Advancement of Artificial Intelligence to formulate a Roadmap for AI research and development over the next two decades.",38.0
Stateful Detection of Black-Box Adversarial Attacks,c8b2fcff3d9b26aeb2dd2967f0a0773d65bea126,"[{'authorId': '2110910255', 'name': 'Steven Chen'}, {'authorId': '2483738', 'name': 'Nicholas Carlini'}, {'authorId': '145394689', 'name': 'D. Wagner'}]",2019.0,arXiv.org,['Unsolved Problems in ML Safety'],1,"The problem of adversarial examples, evasion attacks on machine learning classifiers, has proven extremely difficult to solve. This is true even in the black-box threat model, as is the case in many practical settings. Here, the classifier is hosted as a remote service and the adversary does not have direct access to the model parameters. This paper argues that in such settings, defenders have a larger space of actions than previously studied. Specifically, we deviate from the implicit assumption made by prior work that a defense must be a stateless function that operates on individual examples, and evaluate the space of stateful defenses. We develop a defense designed to detect the process of generating adversarial examples. By keeping a history of the past queries, a defender can try to identify when a sequence of queries appears to be for the purpose of generating an adversarial example. We then introduce query blinding, a new class of attacks designed to bypass defenses that rely on such a defense approach. We believe that expanding the study of adversarial examples from stateless classifiers to stateful systems is not only more realistic for many black-box settings, but also gives the defender a much-needed advantage in responding to the adversary.",64.0
Using Self-Supervised Learning Can Improve Model Robustness and Uncertainty,db787640c9b42416ff8d7015546e667e58267177,"[{'authorId': '3422872', 'name': 'Dan Hendrycks'}, {'authorId': '16787428', 'name': 'Mantas Mazeika'}, {'authorId': '148070327', 'name': 'Saurav Kadavath'}, {'authorId': '143711382', 'name': 'D. Song'}]",2019.0,Neural Information Processing Systems,"['Unsolved Problems in ML Safety', 'X-Risk Analysis for AI Research']",2,"Self-supervision provides effective representations for downstream tasks without requiring labels. However, existing approaches lag behind fully supervised training and are often not thought beneficial beyond obviating the need for annotations. We find that self-supervision can benefit robustness in a variety of ways, including robustness to adversarial examples, label corruption, and common input corruptions. Additionally, self-supervision greatly benefits out-of-distribution detection on difficult, near-distribution outliers, so much so that it exceeds the performance of fully supervised methods. These results demonstrate the promise of self-supervision for improving robustness and uncertainty estimation and establish these tasks as new axes of evaluation for future self-supervised learning research.",587.0
Can You Trust Your Model's Uncertainty? Evaluating Predictive Uncertainty Under Dataset Shift,1eb7f46b1a0a7df823194d86543e5554aa21021a,"[{'authorId': '2067138676', 'name': 'Yaniv Ovadia'}, {'authorId': '35105647', 'name': 'Emily Fertig'}, {'authorId': '92095972', 'name': 'Jie Jessie Ren'}, {'authorId': '81408931', 'name': 'Zachary Nado'}, {'authorId': '1733143', 'name': 'D. Sculley'}, {'authorId': '2388416', 'name': 'S. Nowozin'}, {'authorId': '2403637', 'name': 'Joshua V. Dillon'}, {'authorId': '40627523', 'name': 'Balaji Lakshminarayanan'}, {'authorId': '144108062', 'name': 'Jasper Snoek'}]",2019.0,Neural Information Processing Systems,['Unsolved Problems in ML Safety'],1,"Modern machine learning methods including deep learning have achieved great success in predictive accuracy for supervised learning tasks, but may still fall short in giving useful estimates of their predictive {\em uncertainty}. Quantifying uncertainty is especially critical in real-world settings, which often involve input distributions that are shifted from the training distribution due to a variety of factors including sample bias and non-stationarity. In such settings, well calibrated uncertainty estimates convey information about when a model's output should (or should not) be trusted. Many probabilistic deep learning methods, including Bayesian-and non-Bayesian methods, have been proposed in the literature for quantifying predictive uncertainty, but to our knowledge there has not previously been a rigorous large-scale empirical comparison of these methods under dataset shift. We present a large-scale benchmark of existing state-of-the-art methods on classification problems and investigate the effect of dataset shift on accuracy and calibration. We find that traditional post-hoc calibration does indeed fall short, as do several other previous methods. However, some methods that marginalize over models give surprisingly strong results across a broad spectrum of tasks.",972.0
Risks from Learned Optimization in Advanced Machine Learning Systems,7ee12d3bf8e0ce20d281b4550e39a1ee53839452,"[{'authorId': '146614650', 'name': 'Evan Hubinger'}, {'authorId': '146400709', 'name': 'Chris van Merwijk'}, {'authorId': '148305440', 'name': 'Vladimir Mikulik'}, {'authorId': '147156275', 'name': 'Joar Skalse'}, {'authorId': '1740494', 'name': 'Scott Garrabrant'}]",2019.0,arXiv.org,"['Unsolved Problems in ML Safety', 'AI safety: state of the field through quantitative lens', 'X-Risk Analysis for AI Research']",3,"We analyze the type of learned optimization that occurs when a learned model (such as a neural network) is itself an optimizer - a situation we refer to as mesa-optimization, a neologism we introduce in this paper. We believe that the possibility of mesa-optimization raises two important questions for the safety and transparency of advanced machine learning systems. First, under what circumstances will learned models be optimizers, including when they should not be? Second, when a learned model is an optimizer, what will its objective be - how will it differ from the loss function it was trained under - and how can it be aligned? In this paper, we provide an in-depth analysis of these two primary questions and provide an overview of topics for future research.",48.0
Unlabeled Data Improves Adversarial Robustness,b3f1aa12dde233aaf543bb9ccb27213c494e0fd5,"[{'authorId': '2444742', 'name': 'Y. Carmon'}, {'authorId': '2655157', 'name': 'Aditi Raghunathan'}, {'authorId': '152772922', 'name': 'Ludwig Schmidt'}, {'authorId': '145419642', 'name': 'Percy Liang'}, {'authorId': '1734693', 'name': 'John C. Duchi'}]",2019.0,Neural Information Processing Systems,['Unsolved Problems in ML Safety'],1,"We demonstrate, theoretically and empirically, that adversarial robustness can significantly benefit from semisupervised learning. Theoretically, we revisit the simple Gaussian model of Schmidt et al. that shows a sample complexity gap between standard and robust classification. We prove that unlabeled data bridges this gap: a simple semisupervised learning procedure (self-training) achieves high robust accuracy using the same number of labels required for achieving high standard accuracy. Empirically, we augment CIFAR-10 with 500K unlabeled images sourced from 80 Million Tiny Images and use robust self-training to outperform state-of-the-art robust accuracies by over 5 points in (i) $\ell_\infty$ robustness against several strong attacks via adversarial training and (ii) certified $\ell_2$ and $\ell_\infty$ robustness via randomized smoothing. On SVHN, adding the dataset's own extra training set with the labels removed provides gains of 4 to 10 points, within 1 point of the gain from using the extra labels.",518.0
NeuFuzz: Efficient Fuzzing With Deep Neural Network,5479c0e59e71bc09b504206fdeaeb7dc9c7b5e9e,"[{'authorId': '2108700830', 'name': 'Yunchao Wang'}, {'authorId': '48551776', 'name': 'Zehui Wu'}, {'authorId': '2087762275', 'name': 'Qiang Wei'}, {'authorId': '19592980', 'name': 'Qingxian Wang'}]",2019.0,IEEE Access,['Unsolved Problems in ML Safety'],1,"Coverage-guided graybox fuzzing is one of the most popular and effective techniques for discovering vulnerabilities due to its nature of high speed and scalability. However, the existing techniques generally focus on code coverage but not on vulnerable code. These techniques aim to cover as many paths as possible rather than to explore paths that are more likely to be vulnerable. When selecting the seeds to test, the existing fuzzers usually treat all seed inputs equally, ignoring the fact that paths exercised by different seed inputs are not equally vulnerable. This results in wasting time testing uninteresting paths rather than vulnerable paths, thus reducing the efficiency of vulnerability detection. In this paper, we present a solution, NeuFuzz, using the deep neural network to guide intelligent seed selection during graybox fuzzing to alleviate the aforementioned limitation. In particular, the deep neural network is used to learn the hidden vulnerability pattern from a large number of vulnerable and clean program paths to train a prediction model to classify whether paths are vulnerable. The fuzzer then prioritizes seed inputs that are capable of covering the likely to be vulnerable paths and assigns more mutation energy (i.e., the number of inputs to be generated) to these seeds. We implemented a prototype of NeuFuzz based on an existing fuzzer PTfuzz and evaluated it on two different test suites: LAVA-M and nine real-world applications. The experimental results showed that NeuFuzz can find more vulnerabilities than the existing fuzzers in less time. We have found 28 new security bugs in these applications, 21 of which have been assigned as CVE IDs.",28.0
Certified Adversarial Robustness via Randomized Smoothing,f7f73185e3975bb62a3c42b2ba6bd4db57fee8ed,"[{'authorId': '39951470', 'name': 'Jeremy M. Cohen'}, {'authorId': '49686853', 'name': 'Elan Rosenfeld'}, {'authorId': '145116464', 'name': 'J. Z. Kolter'}]",2019.0,International Conference on Machine Learning,['Unsolved Problems in ML Safety'],1,"We show how to turn any classifier that classifies well under Gaussian noise into a new classifier that is certifiably robust to adversarial perturbations under the $\ell_2$ norm. This ""randomized smoothing"" technique has been proposed recently in the literature, but existing guarantees are loose. We prove a tight robustness guarantee in $\ell_2$ norm for smoothing with Gaussian noise. We use randomized smoothing to obtain an ImageNet classifier with e.g. a certified top-1 accuracy of 49% under adversarial perturbations with $\ell_2$ norm less than 0.5 (=127/255). No certified defense has been shown feasible on ImageNet except for smoothing. On smaller-scale datasets where competing approaches to certified $\ell_2$ robustness are viable, smoothing delivers higher certified accuracies. Our strong empirical results suggest that randomized smoothing is a promising direction for future research into adversarially robust classification. Code and models are available at this http URL.",1228.0
Using Pre-Training Can Improve Model Robustness and Uncertainty,aa5741c74b7fac10680c1cfbdd49d9ffb5751a68,"[{'authorId': '3422872', 'name': 'Dan Hendrycks'}, {'authorId': '3436470', 'name': 'Kimin Lee'}, {'authorId': '16787428', 'name': 'Mantas Mazeika'}]",2019.0,International Conference on Machine Learning,"['Unsolved Problems in ML Safety', 'X-Risk Analysis for AI Research']",2,"He et al. (2018) have called into question the utility of pre-training by showing that training from scratch can often yield similar performance to pre-training. We show that although pre-training may not improve performance on traditional classification metrics, it improves model robustness and uncertainty estimates. Through extensive experiments on adversarial examples, label corruption, class imbalance, out-of-distribution detection, and confidence calibration, we demonstrate large gains from pre-training and complementary effects with task-specific methods. We introduce adversarial pre-training and show approximately a 10% absolute improvement over the previous state-of-the-art in adversarial robustness. In some cases, using pre-training without task-specific methods also surpasses the state-of-the-art, highlighting the need for pre-training when evaluating future methods on robustness and uncertainty tasks.",467.0
Theoretically Principled Trade-off between Robustness and Accuracy,6c405d4b5dc41a86be05acd59c06ed19daf01d14,"[{'authorId': '40975176', 'name': 'Hongyang Zhang'}, {'authorId': '29001000', 'name': 'Yaodong Yu'}, {'authorId': '2784735', 'name': 'Jiantao Jiao'}, {'authorId': '143977260', 'name': 'E. Xing'}, {'authorId': '1701847', 'name': 'L. Ghaoui'}, {'authorId': '1694621', 'name': 'Michael I. Jordan'}]",2019.0,International Conference on Machine Learning,['Unsolved Problems in ML Safety'],1,"We identify a trade-off between robustness and accuracy that serves as a guiding principle in the design of defenses against adversarial examples. Although this problem has been widely studied empirically, much remains unknown concerning the theory underlying this trade-off. In this work, we decompose the prediction error for adversarial examples (robust error) as the sum of the natural (classification) error and boundary error, and provide a differentiable upper bound using the theory of classification-calibrated loss, which is shown to be the tightest possible upper bound uniform over all probability distributions and measurable predictors. Inspired by our theoretical analysis, we also design a new defense method, TRADES, to trade adversarial robustness off against accuracy. Our proposed algorithm performs well experimentally in real-world datasets. The methodology is the foundation of our entry to the NeurIPS 2018 Adversarial Vision Challenge in which we won the 1st place out of ~2,000 submissions, surpassing the runner-up approach by $11.41\%$ in terms of mean $\ell_2$ perturbation distance.",1483.0
The Age of Surveillance Capitalism: The Fight for a Human Future at the New Frontier of Power,f95ddf7f2ac8f3cef3d808205e0606c394ca03a3,"[{'authorId': '2684210', 'name': 'Shoshana Zuboff'}]",2019.0,,['Unsolved Problems in ML Safety'],1,"Society is at a turning point. The heady optimism that accompanied the advent of the Internet has gone, replaced with a deep unease as technology, capitalism and an unequal society combine to create the perfect storm. Tech companies are gathering our information online and selling it to the highest bidder, whether government or retailer. In this world of surveillance capitalism, profit depends not only on predicting but modifying our online behaviour. How will this fusion of capitalism and the digital shape the values that define our future?",2013.0
Building Jiminy Cricket: An Architecture for Moral Agreements Among Stakeholders,8dd1a9c2dddcf4f4010ea861107847545c9146c1,"[{'authorId': '35041184', 'name': 'B. Liao'}, {'authorId': '2013035', 'name': 'M. Slavkovik'}, {'authorId': '69574688', 'name': 'Leendert van der Torre'}]",2018.0,"AAAI/ACM Conference on AI, Ethics, and Society",['Unsolved Problems in ML Safety'],1,"An autonomous system is constructed by a manufacturer, operates in a society subject to norms and laws, and is interacting with end-users. We address the challenge of how the moral values and views of all stakeholders can be integrated and reflected in the moral behavior of the autonomous system. We propose an artificial moral agent architecture that uses techniques from normative systems and formal argumentation to reach moral agreements among stakeholders. We show how our architecture can be used not only for ethical practical reasoning and collaborative decision-making, but also for the explanation of such moral behavior.",20.0
Quantifying Generalization in Reinforcement Learning,ef2bc452812d6005ab0a66af6c3f97b6b0ba837e,"[{'authorId': '6062736', 'name': 'Karl Cobbe'}, {'authorId': '2067138712', 'name': 'Oleg Klimov'}, {'authorId': '144239765', 'name': 'Christopher Hesse'}, {'authorId': '2111274998', 'name': 'Taehoon Kim'}, {'authorId': '47971768', 'name': 'J. Schulman'}]",2018.0,International Conference on Machine Learning,['Unsolved Problems in ML Safety'],1,"In this paper, we investigate the problem of overfitting in deep reinforcement learning. Among the most common benchmarks in RL, it is customary to use the same environments for both training and testing. This practice offers relatively little insight into an agent's ability to generalize. We address this issue by using procedurally generated environments to construct distinct training and test sets. Most notably, we introduce a new environment called CoinRun, designed as a benchmark for generalization in RL. Using CoinRun, we find that agents overfit to surprisingly large training sets. We then show that deeper convolutional architectures improve generalization, as do methods traditionally found in supervised learning, including L2 regularization, dropout, data augmentation and batch normalization.",442.0
Exploring Adversarial Examples in Malware Detection,d5d7ce70a6618393176f254d5617b3bb070003cd,"[{'authorId': '31765629', 'name': 'Octavian Suciu'}, {'authorId': '2591222', 'name': 'Scott E. Coull'}, {'authorId': '33674253', 'name': 'Jeffrey Johns'}]",2018.0,2019 IEEE Security and Privacy Workshops (SPW),['Unsolved Problems in ML Safety'],1,"The convolutional neural network (CNN) architecture is increasingly being applied to new domains, such as malware detection, where it is able to learn malicious behavior from raw bytes extracted from executables. These architectures reach impressive performance with no feature engineering effort involved, but their robustness against active attackers is yet to be understood. Such malware detectors could face a new attack vector in the form of adversarial interference with the classification model. Existing evasion attacks intended to cause misclassification on test-time instances, which have been extensively studied for image classifiers, are not applicable because of the input semantics that prevents arbitrary changes to the binaries. This paper explores the area of adversarial examples for malware detection. By training an existing model on a production-scale dataset, we show that some previous attacks are less effective than initially reported, while simultaneously highlighting architectural weaknesses that facilitate new attack strategies for malware classification. Finally, we explore how generalizable different attack strategies are, the trade-offs when aiming to increase their effectiveness, and the transferability of single-step attacks.",125.0
Preferences Implicit in the State of the World,5d60613c15de600dc321788b1953bf49f8eced50,"[{'authorId': '40947489', 'name': 'Rohin Shah'}, {'authorId': '52510051', 'name': 'Dmitrii Krasheninnikov'}, {'authorId': '2113340449', 'name': 'Jordan Alexander'}, {'authorId': '1689992', 'name': 'P. Abbeel'}, {'authorId': '2745001', 'name': 'A. Dragan'}]",2018.0,International Conference on Learning Representations,['Unsolved Problems in ML Safety'],1,"Reinforcement learning (RL) agents optimize only the features specified in a reward function and are indifferent to anything left out inadvertently. This means that we must not only specify what to do, but also the much larger space of what not to do. It is easy to forget these preferences, since these preferences are already satisfied in our environment. This motivates our key insight: when a robot is deployed in an environment that humans act in, the state of the environment is already optimized for what humans want. We can therefore use this implicit preference information from the state to fill in the blanks. We develop an algorithm based on Maximum Causal Entropy IRL and use it to evaluate the idea in a suite of proof-of-concept environments designed to show its properties. We find that information from the initial state can be used to infer both side effects that should be avoided as well as preferences for how the environment should be organized. Our code can be found at this https URL.",42.0
Deep Anomaly Detection with Outlier Exposure,6cf1d69e447e9687dbd2d92572f44bddbabd8192,"[{'authorId': '3422872', 'name': 'Dan Hendrycks'}, {'authorId': '16787428', 'name': 'Mantas Mazeika'}, {'authorId': '144299726', 'name': 'Thomas G. Dietterich'}]",2018.0,International Conference on Learning Representations,"['Unsolved Problems in ML Safety', 'X-Risk Analysis for AI Research']",2,"It is important to detect anomalous inputs when deploying machine learning systems. The use of larger and more complex inputs in deep learning magnifies the difficulty of distinguishing between anomalous and in-distribution examples. At the same time, diverse image and text data are available in enormous quantities. We propose leveraging these data to improve deep anomaly detection by training anomaly detectors against an auxiliary dataset of outliers, an approach we call Outlier Exposure (OE). This enables anomaly detectors to generalize and detect unseen anomalies. In extensive experiments on natural language processing and small- and large-scale vision tasks, we find that Outlier Exposure significantly improves detection performance. We also observe that cutting-edge generative models trained on CIFAR-10 may assign higher likelihoods to SVHN images than to CIFAR-10 images; we use OE to mitigate this issue. We also analyze the flexibility and robustness of Outlier Exposure, and identify characteristics of the auxiliary dataset that improve performance.",856.0
NEUZZ: Efficient Fuzzing with Neural Program Smoothing,51652bfba5204fc4476849bffa2295187267c356,"[{'authorId': '38468935', 'name': 'Dongdong She'}, {'authorId': '40428350', 'name': 'Kexin Pei'}, {'authorId': '32486555', 'name': 'Dave Epstein'}, {'authorId': '152211006', 'name': 'Junfeng Yang'}, {'authorId': '31631000', 'name': 'Baishakhi Ray'}, {'authorId': '39400201', 'name': 'S. Jana'}]",2018.0,IEEE Symposium on Security and Privacy,['Unsolved Problems in ML Safety'],1,"Fuzzing has become the de facto standard technique for finding software vulnerabilities. However, even state-of-the-art fuzzers are not very efficient at finding hard-to-trigger software bugs. Most popular fuzzers use evolutionary guidance to generate inputs that can trigger different bugs. Such evolutionary algorithms, while fast and simple to implement, often get stuck in fruitless sequences of random mutations. Gradient-guided optimization presents a promising alternative to evolutionary guidance. Gradient-guided techniques have been shown to significantly outperform evolutionary algorithms at solving high-dimensional structured optimization problems in domains like machine learning by efficiently utilizing gradients or higher-order derivatives of the underlying function. However, gradient-guided approaches are not directly applicable to fuzzing as real-world program behaviors contain many discontinuities, plateaus, and ridges where the gradient-based methods often get stuck. We observe that this problem can be addressed by creating a smooth surrogate function approximating the target program’s discrete branching behavior. In this paper, we propose a novel program smoothing technique using surrogate neural network models that can incrementally learn smooth approximations of a complex, real-world program's branching behaviors. We further demonstrate that such neural network models can be used together with gradient-guided input generation schemes to significantly increase the efficiency of the fuzzing process. Our extensive evaluations demonstrate that NEUZZ significantly outperforms 10 state-of-the-art graybox fuzzers on 10 popular real-world programs both at finding new bugs and achieving higher edge coverage. NEUZZ found 31 previously unknown bugs (including two CVEs) that other fuzzers failed to find in 10 real-world programs and achieved 3X more edge coverage than all of the tested graybox fuzzers over 24 hour runs. Furthermore, NEUZZ also outperformed existing fuzzers on both LAVA-M and DARPA CGC bug datasets.",120.0
Benchmarking Neural Network Robustness to Common Corruptions and Perturbations,5b8013f4425b6b889a514e95fe6355a83c64d1ea,"[{'authorId': '3422872', 'name': 'Dan Hendrycks'}, {'authorId': '144299726', 'name': 'Thomas G. Dietterich'}]",2018.0,International Conference on Learning Representations,['Unsolved Problems in ML Safety'],1,"In this paper we establish rigorous benchmarks for image classifier robustness. Our first benchmark, ImageNet-C, standardizes and expands the corruption robustness topic, while showing which classifiers are preferable in safety-critical applications. Then we propose a new dataset called ImageNet-P which enables researchers to benchmark a classifier's robustness to common perturbations. Unlike recent robustness research, this benchmark evaluates performance on common corruptions and perturbations not worst-case adversarial perturbations. We find that there are negligible changes in relative corruption robustness from AlexNet classifiers to ResNet classifiers. Afterward we discover ways to enhance corruption and perturbation robustness. We even find that a bypassed adversarial defense provides substantial common perturbation robustness. Together our benchmarks may aid future work toward networks that robustly generalize.",1802.0
Certified Robustness to Adversarial Examples with Differential Privacy,3e86a51d1f2051ab8f448b66c6dcc17924d17cfa,"[{'authorId': '2574478', 'name': 'Mathias Lécuyer'}, {'authorId': '2501382', 'name': 'Vaggelis Atlidakis'}, {'authorId': '1972091', 'name': 'Roxana Geambasu'}, {'authorId': '143724861', 'name': 'Daniel J. Hsu'}, {'authorId': '39400201', 'name': 'S. Jana'}]",2018.0,IEEE Symposium on Security and Privacy,['Unsolved Problems in ML Safety'],1,"Adversarial examples that fool machine learning models, particularly deep neural networks, have been a topic of intense research interest, with attacks and defenses being developed in a tight back-and-forth. Most past defenses are best effort and have been shown to be vulnerable to sophisticated attacks. Recently a set of certified defenses have been introduced, which provide guarantees of robustness to norm-bounded attacks. However these defenses either do not scale to large datasets or are limited in the types of models they can support. This paper presents the first certified defense that both scales to large networks and datasets (such as Google’s Inception network for ImageNet) and applies broadly to arbitrary model types. Our defense, called PixelDP, is based on a novel connection between robustness against adversarial examples and differential privacy, a cryptographically-inspired privacy formalism, that provides a rigorous, generic, and flexible foundation for defense.",671.0
Sequential Feature Explanations for Anomaly Detection,4c729c317045bcf2a85cfc3157225e61c3cb6c1a,"[{'authorId': '1952207', 'name': 'Md Amran Siddiqui'}, {'authorId': '145841336', 'name': 'Alan Fern'}, {'authorId': '144299726', 'name': 'Thomas G. Dietterich'}, {'authorId': '37535697', 'name': 'Weng-Keen Wong'}]",2015.0,ACM Transactions on Knowledge Discovery from Data,['Unsolved Problems in ML Safety'],1,"In many applications, an anomaly detection system presents the most anomalous data instance to a human analyst, who then must determine whether the instance is truly of interest (e.g., a threat in a security setting). Unfortunately, most anomaly detectors provide no explanation about why an instance was considered anomalous, leaving the analyst with no guidance about where to begin the investigation. To address this issue, we study the problems of computing and evaluating sequential feature explanations (SFEs) for anomaly detectors. An SFE of an anomaly is a sequence of features, which are presented to the analyst one at a time (in order) until the information contained in the highlighted features is enough for the analyst to make a confident judgement about the anomaly. Since analyst effort is related to the amount of information that they consider in an investigation, an explanation’s quality is related to the number of features that must be revealed to attain confidence. In this article, we first formulate the problem of optimizing SFEs for a particular density-based anomaly detector. We then present both greedy algorithms and an optimal algorithm, based on branch-and-bound search, for optimizing SFEs. Finally, we provide a large scale quantitative evaluation of these algorithms using a novel framework for evaluating explanations. The results show that our algorithms are quite effective and that our best greedy algorithm is competitive with optimal solutions.",37.0
Benchmarking Safe Exploration in Deep Reinforcement Learning,4d0f6a6ffcd6ab04732ff76420fd9f8a7bb649c3,"[{'authorId': '3381809', 'name': 'Joshua Achiam'}, {'authorId': '2698777', 'name': 'Dario Amodei'}]",2019.0,,['Unsolved Problems in ML Safety'],1,"Reinforcement learning (RL) agents need to explore their environments in order to learn optimal policies by trial and error. In many environments, safety is a critical concern and certain errors are unacceptable: for example, robotics systems that interact with humans should never cause injury to the humans while exploring. While it is currently typical to train RL agents mostly or entirely in simulation, where safety concerns are minimal, we anticipate that challenges in simulating the complexities of the real world (such as human-AI interactions) will cause a shift towards training RL agents directly in the real world, where safety concerns are paramount. Consequently we take the position that safe exploration should be viewed as a critical focus area for RL research, and in this work we make three contributions to advance the study of safe exploration. First, building on a wide range of prior work on safe reinforcement learning, we propose to standardize constrained RL as the main formalism for safe exploration. Second, we present the Safety Gym benchmark suite, a new slate of high-dimensional continuous control environments for measuring research progress on constrained RL. Finally, we benchmark several constrained deep RL algorithms on Safety Gym environments to establish baselines that future work can build on.",229.0
Robust artificial intelligence and robust human organizations,9d0bd60fbf4eb6e82e5b622911207caeb0dcf0a1,"[{'authorId': '144299726', 'name': 'Thomas G. Dietterich'}]",2018.0,Frontiers of Computer Science,"['Unsolved Problems in ML Safety', 'AI Research Considerations for Human Existential Safety (ARCHES)']",2,,22.0
Scalable agent alignment via reward modeling: a research direction,c6f913e4baa7f2c85363c0625c87003ad3b3a14c,"[{'authorId': '2990741', 'name': 'J. Leike'}, {'authorId': '145055042', 'name': 'David Krueger'}, {'authorId': '1868196', 'name': 'Tom Everitt'}, {'authorId': '26890260', 'name': 'Miljan Martic'}, {'authorId': '51965508', 'name': 'Vishal Maini'}, {'authorId': '34313265', 'name': 'S. Legg'}]",2018.0,arXiv.org,"['Unsolved Problems in ML Safety', 'AI safety: state of the field through quantitative lens', 'AI Research Considerations for Human Existential Safety (ARCHES)']",3,"One obstacle to applying reinforcement learning algorithms to real-world problems is the lack of suitable reward functions. Designing such reward functions is difficult in part because the user only has an implicit understanding of the task objective. This gives rise to the agent alignment problem: how do we create agents that behave in accordance with the user's intentions? We outline a high-level research direction to solve the agent alignment problem centered around reward modeling: learning a reward function from interaction with the user and optimizing the learned reward function with reinforcement learning. We discuss the key challenges we expect to face when scaling reward modeling to complex and general domains, concrete approaches to mitigate these challenges, and ways to establish trust in the resulting agents.",144.0
Hallucinations in Neural Machine Translation,97685859d4bcbc3b893425e6cb8fda8e9c15cfcb,"[{'authorId': '3844009', 'name': 'Katherine Lee'}, {'authorId': '2345617', 'name': 'Orhan Firat'}, {'authorId': '2078528337', 'name': 'Ashish Agarwal'}, {'authorId': '4424752', 'name': 'C. Fannjiang'}, {'authorId': '3089810', 'name': 'David Sussillo'}]",2018.0,,['Unsolved Problems in ML Safety'],1,"Neural machine translation (NMT) systems have reached state of the art perfor-mance in translating text and are in wide deployment. Yet little is understood about how these systems function or break. Here we show that NMT systems are susceptible to producing highly pathological translations that are completely untethered from the source material, which we term hallucinations . Such pathological translations are problematic because they are are deeply disturbing of user trust and easy to ﬁnd with a simple search. We describe a method to generate hallucinations and show that many common variations of the NMT architecture are susceptible to them. We study a variety of approaches to reduce the frequency of hallucinations, including data augmentation, dynamical systems and regularization techniques, showing that data augmentation signiﬁcantly reduces hallucination frequency. Finally, we analyze networks that produce hallucinations and show that there are signatures in the attention matrix as well as in the hidden states of the decoder.",73.0
Characterizing Adversarial Examples Based on Spatial Consistency Information for Semantic Segmentation,74dbcc09a3456ddacf5cece640b84045ebdf6be1,"[{'authorId': '2723309', 'name': 'Chaowei Xiao'}, {'authorId': '38343750', 'name': 'Ruizhi Deng'}, {'authorId': '2165245296', 'name': 'Bo Li'}, {'authorId': '1807197', 'name': 'F. Yu'}, {'authorId': '39037167', 'name': 'M. Liu'}, {'authorId': '143711382', 'name': 'D. Song'}]",2018.0,European Conference on Computer Vision,['Unsolved Problems in ML Safety'],1,,77.0
Motivating the Rules of the Game for Adversarial Example Research,18063ed998c99bfef92fad8418610b97f863d878,"[{'authorId': '2058362', 'name': 'J. Gilmer'}, {'authorId': '1722180', 'name': 'Ryan P. Adams'}, {'authorId': '153440022', 'name': 'Ian J. Goodfellow'}, {'authorId': '2055790316', 'name': 'David G. Andersen'}, {'authorId': '35188630', 'name': 'George E. Dahl'}]",2018.0,arXiv.org,['Unsolved Problems in ML Safety'],1,"Advances in machine learning have led to broad deployment of systems with impressive performance on important problems. Nonetheless, these systems can be induced to make errors on data that are surprisingly similar to examples the learned system handles correctly. The existence of these errors raises a variety of questions about out-of-sample generalization and whether bad actors might use such examples to abuse deployed systems. As a result of these security concerns, there has been a flurry of recent papers proposing algorithms to defend against such malicious perturbations of correctly handled examples. It is unclear how such misclassifications represent a different kind of security problem than other errors, or even other attacker-produced examples that have no specific relationship to an uncorrupted input. In this paper, we argue that adversarial example defense papers have, to date, mostly considered abstract, toy games that do not relate to any specific security concern. Furthermore, defense papers have not yet precisely described all the abilities and limitations of attackers that would be relevant in practical security. Towards this end, we establish a taxonomy of motivations, constraints, and abilities for more plausible adversaries. Finally, we provide a series of recommendations outlining a path forward for future work to more clearly articulate the threat model and perform more meaningful evaluation.",192.0
Accurate Uncertainties for Deep Learning Using Calibrated Regression,6f55f2234a002c69001df8ef9108cb86f1dfa506,"[{'authorId': '145712106', 'name': 'Volodymyr Kuleshov'}, {'authorId': '51130926', 'name': 'Nathan Fenner'}, {'authorId': '2490652', 'name': 'S. Ermon'}]",2018.0,International Conference on Machine Learning,"['Unsolved Problems in ML Safety', 'AI Research Considerations for Human Existential Safety (ARCHES)']",2,"Methods for reasoning under uncertainty are a key building block of accurate and reliable machine learning systems. Bayesian methods provide a general framework to quantify uncertainty. However, because of model misspecification and the use of approximate inference, Bayesian uncertainty estimates are often inaccurate -- for example, a 90% credible interval may not contain the true outcome 90% of the time. Here, we propose a simple procedure for calibrating any regression algorithm; when applied to Bayesian and probabilistic models, it is guaranteed to produce calibrated uncertainty estimates given enough data. Our procedure is inspired by Platt scaling and extends previous work on classification. We evaluate this approach on Bayesian linear regression, feedforward, and recurrent neural networks, and find that it consistently outputs well-calibrated credible intervals while improving performance on time series forecasting and model-based reinforcement learning tasks.",391.0
Explaining Explanations: An Overview of Interpretability of Machine Learning,d7701e78e0bfc92b03a89582e80cfb751ac03f26,"[{'authorId': '145019478', 'name': 'Leilani H. Gilpin'}, {'authorId': '144159726', 'name': 'David Bau'}, {'authorId': '144002190', 'name': 'Ben Z. Yuan'}, {'authorId': '50397921', 'name': 'Ayesha Bajwa'}, {'authorId': '144417360', 'name': 'Michael A. Specter'}, {'authorId': '1735243', 'name': 'Lalana Kagal'}]",2018.0,International Conference on Data Science and Advanced Analytics,['Unsolved Problems in ML Safety'],1,"There has recently been a surge of work in explanatory artificial intelligence (XAI). This research area tackles the important problem that complex machines and algorithms often cannot provide insights into their behavior and thought processes. XAI allows users and parts of the internal system to be more transparent, providing explanations of their decisions in some level of detail. These explanations are important to ensure algorithmic fairness, identify potential bias/problems in the training data, and to ensure that the algorithms perform as expected. However, explanations produced by these systems is neither standardized nor systematically assessed. In an effort to create best practices and identify open challenges, we describe foundational concepts of explainability and show how they can be used to classify existing literature. We discuss why current approaches to explanatory methods especially for deep neural networks are insufficient. Finally, based on our survey, we conclude with suggested future research directions for explanatory artificial intelligence.",1196.0
Poison Frogs! Targeted Clean-Label Poisoning Attacks on Neural Networks,c6d4ef1a98b1b9e5875b79efd3ef2a73403a0884,"[{'authorId': '3246287', 'name': 'Ali Shafahi'}, {'authorId': '2107221138', 'name': 'W. R. Huang'}, {'authorId': '40465379', 'name': 'Mahyar Najibi'}, {'authorId': '31765629', 'name': 'Octavian Suciu'}, {'authorId': '1746575', 'name': 'Christoph Studer'}, {'authorId': '3343194', 'name': 'Tudor Dumitras'}, {'authorId': '1962083', 'name': 'T. Goldstein'}]",2018.0,Neural Information Processing Systems,['Unsolved Problems in ML Safety'],1,"Data poisoning is an attack on machine learning models wherein the attacker adds examples to the training set to manipulate the behavior of the model at test time. This paper explores poisoning attacks on neural nets. The proposed attacks use ""clean-labels""; they don't require the attacker to have any control over the labeling of training data. They are also targeted; they control the behavior of the classifier on a $\textit{specific}$ test instance without degrading overall classifier performance. For example, an attacker could add a seemingly innocuous image (that is properly labeled) to a training set for a face recognition engine, and control the identity of a chosen person at test time. Because the attacker does not need to control the labeling function, poisons could be entered into the training set simply by leaving them on the web and waiting for them to be scraped by a data collection bot. 
We present an optimization-based method for crafting poisons, and show that just one single poison image can control classifier behavior when transfer learning is used. For full end-to-end training, we present a ""watermarking"" strategy that makes poisoning reliable using multiple ($\approx$50) poisoned training instances. We demonstrate our method by generating poisoned frog images from the CIFAR dataset and using them to manipulate image classifiers.",650.0
Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples,651adaa058f821a890f2c5d1053d69eb481a8352,"[{'authorId': '38939786', 'name': 'Anish Athalye'}, {'authorId': '2483738', 'name': 'Nicholas Carlini'}, {'authorId': '145394689', 'name': 'D. Wagner'}]",2018.0,International Conference on Machine Learning,['Unsolved Problems in ML Safety'],1,"We identify obfuscated gradients, a kind of gradient masking, as a phenomenon that leads to a false sense of security in defenses against adversarial examples. While defenses that cause obfuscated gradients appear to defeat iterative optimization-based attacks, we find defenses relying on this effect can be circumvented. We describe characteristic behaviors of defenses exhibiting the effect, and for each of the three types of obfuscated gradients we discover, we develop attack techniques to overcome it. In a case study, examining non-certified white-box-secure defenses at ICLR 2018, we find obfuscated gradients are a common occurrence, with 7 of 9 defenses relying on obfuscated gradients. Our new attacks successfully circumvent 6 completely, and 1 partially, in the original threat model each paper considers.",2487.0
Certified Defenses against Adversarial Examples,966e3c7a65ec75a6359b55c0cecaf3896d318432,"[{'authorId': '2655157', 'name': 'Aditi Raghunathan'}, {'authorId': '5164568', 'name': 'J. Steinhardt'}, {'authorId': '145419642', 'name': 'Percy Liang'}]",2018.0,International Conference on Learning Representations,['Unsolved Problems in ML Safety'],1,"While neural networks have achieved high accuracy on standard image classification benchmarks, their accuracy drops to nearly zero in the presence of small adversarial perturbations to test inputs. Defenses based on regularization and adversarial training have been proposed, but often followed by new, stronger attacks that defeat these defenses. Can we somehow end this arms race? In this work, we study this problem for neural networks with one hidden layer. We first propose a method based on a semidefinite relaxation that outputs a certificate that for a given network and test input, no attack can force the error to exceed a certain value. Second, as this certificate is differentiable, we jointly optimize it with the network parameters, providing an adaptive regularizer that encourages robustness against all attacks. On MNIST, our approach produces a network and a certificate that no attack that perturbs each pixel by at most \epsilon = 0.1 can cause more than 35% test error.",802.0
Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models,1b225474e7a5794f98cdfbde8b12ccbc56799409,"[{'authorId': '40634590', 'name': 'Wieland Brendel'}, {'authorId': '19237612', 'name': 'Jonas Rauber'}, {'authorId': '1731199', 'name': 'M. Bethge'}]",2017.0,International Conference on Learning Representations,['Unsolved Problems in ML Safety'],1,"Many machine learning algorithms are vulnerable to almost imperceptible perturbations of their inputs. So far it was unclear how much risk adversarial perturbations carry for the safety of real-world machine learning applications because most methods used to generate such perturbations rely either on detailed model information (gradient-based attacks) or on confidence scores such as class probabilities (score-based attacks), neither of which are available in most real-world scenarios. In many such cases one currently needs to retreat to transfer-based attacks which rely on cumbersome substitute models, need access to the training data and can be defended against. Here we emphasise the importance of attacks which solely rely on the final model decision. Such decision-based attacks are (1) applicable to real-world black-box models such as autonomous cars, (2) need less knowledge and are easier to apply than transfer-based attacks and (3) are more robust to simple defences than gradient- or score-based attacks. Previous attacks in this category were limited to simple models or simple datasets. Here we introduce the Boundary Attack, a decision-based attack that starts from a large adversarial perturbation and then seeks to reduce the perturbation while staying adversarial. The attack is conceptually simple, requires close to no hyperparameter tuning, does not rely on substitute models and is competitive with the best gradient-based attacks in standard computer vision tasks like ImageNet. We apply the attack on two black-box algorithms from Clarifai.com. The Boundary Attack in particular and the class of decision-based attacks in general open new avenues to study the robustness of machine learning models and raise new questions regarding the safety of deployed machine learning systems. An implementation of the attack is available as part of Foolbox at this https URL .",875.0
Towards Deep Learning Models Resistant to Adversarial Attacks,7aa38b85fa8cba64d6a4010543f6695dbf5f1386,"[{'authorId': '143826246', 'name': 'A. Madry'}, {'authorId': '17775913', 'name': 'Aleksandar Makelov'}, {'authorId': '152772922', 'name': 'Ludwig Schmidt'}, {'authorId': '2754804', 'name': 'Dimitris Tsipras'}, {'authorId': '2869958', 'name': 'Adrian Vladu'}]",2017.0,International Conference on Learning Representations,"['Unsolved Problems in ML Safety', 'X-Risk Analysis for AI Research']",2,"Recent work has demonstrated that deep neural networks are vulnerable to adversarial examples---inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. Code and pre-trained models are available at this https URL and this https URL.",7271.0
Ensemble Adversarial Training: Attacks and Defenses,136dee73f203df2f4831994bf4f0c0a4ad2e764e,"[{'authorId': '2444919', 'name': 'Florian Tramèr'}, {'authorId': '145714153', 'name': 'Alexey Kurakin'}, {'authorId': '1967156', 'name': 'Nicolas Papernot'}, {'authorId': '1752788', 'name': 'D. Boneh'}, {'authorId': '144061974', 'name': 'P. Mcdaniel'}]",2017.0,International Conference on Learning Representations,['Unsolved Problems in ML Safety'],1,"Adversarial examples are perturbed inputs designed to fool machine learning models. Adversarial training injects such examples into training data to increase robustness. To scale this technique to large datasets, perturbations are crafted using fast single-step methods that maximize a linear approximation of the model's loss. We show that this form of adversarial training converges to a degenerate global minimum, wherein small curvature artifacts near the data points obfuscate a linear approximation of the loss. The model thus learns to generate weak perturbations, rather than defend against strong ones. As a result, we find that adversarial training remains vulnerable to black-box attacks, where we transfer perturbations computed on undefended models, as well as to a powerful novel single-step attack that escapes the non-smooth vicinity of the input data via a small random step. We further introduce Ensemble Adversarial Training, a technique that augments training data with perturbations transferred from other models. On ImageNet, Ensemble Adversarial Training yields models with strong robustness to black-box attacks. In particular, our most robust model won the first round of the NIPS 2017 competition on Defenses against Adversarial Attacks. However, subsequent work found that more elaborate black-box attacks could significantly enhance transferability and reduce the accuracy of our models.",2055.0
A Rotation and a Translation Suffice: Fooling CNNs with Simple Transformations,afa0d49c1399c752d6f4665d75ecec640c000468,"[{'authorId': '39468283', 'name': 'Logan Engstrom'}, {'authorId': '2754804', 'name': 'Dimitris Tsipras'}, {'authorId': '152772922', 'name': 'Ludwig Schmidt'}, {'authorId': '143826246', 'name': 'A. Madry'}]",2017.0,arXiv.org,['Unsolved Problems in ML Safety'],1,"Recent work has shown that neural network-based vision classifiers exhibit a significant vulnerability to misclassifications caused by imperceptible but adversarial perturbations of their inputs. These perturbations, however, are purely pixel-wise and built out of loss function gradients of either the attacked model or its surrogate. As a result, they tend to look pretty artificial and contrived. This might suggest that vulnerability to misclassification of slight input perturbations can only arise in a truly adversarial setting and thus is unlikely to be a problem in more benign contexts. 
In this paper, we provide evidence that such a belief might be incorrect. To this end, we show that neural networks are already vulnerable to significantly simpler - and more likely to occur naturally - transformations of the inputs. Specifically, we demonstrate that rotations and translations alone suffice to significantly degrade the classification performance of neural network-based vision models across a spectrum of datasets. This remains to be the case even when these models are trained using appropriate data augmentation and are already robust against the canonical, pixel-wise perturbations. Also, finding such ""fooling"" transformation does not even require having any special access to the model or its surrogate - just trying out a small number of random rotation and translation combinations already has a significant effect. These findings suggest that our current neural network-based vision models might not be as reliable as we tend to assume.",311.0
"Deep Learning Scaling is Predictable, Empirically",a1c922be467d1c0c64b963e65dae41778b81b2a0,"[{'authorId': '3130228', 'name': 'J. Hestness'}, {'authorId': '46617804', 'name': 'Sharan Narang'}, {'authorId': '2774880', 'name': 'Newsha Ardalani'}, {'authorId': '2040049', 'name': 'G. Diamos'}, {'authorId': '35450887', 'name': 'Heewoo Jun'}, {'authorId': '7880519', 'name': 'Hassan Kianinejad'}, {'authorId': '8176660', 'name': 'Md. Mostofa Ali Patwary'}, {'authorId': '2152916796', 'name': 'Yang Yang'}, {'authorId': '2389316', 'name': 'Yanqi Zhou'}]",2017.0,arXiv.org,['Unsolved Problems in ML Safety'],1,"Deep learning (DL) creates impactful advances following a virtuous recipe: model architecture search, creating large training data sets, and scaling computation. It is widely believed that growing training sets and models should improve accuracy and result in better products. As DL application domains grow, we would like a deeper understanding of the relationships between training set size, computational scale, and model accuracy improvements to advance the state-of-the-art. 
This paper presents a large scale empirical characterization of generalization error and model size growth as training sets grow. We introduce a methodology for this measurement and test four machine learning domains: machine translation, language modeling, image processing, and speech recognition. Our empirical results show power-law generalization error scaling across a breadth of factors, resulting in power-law exponents---the ""steepness"" of the learning curve---yet to be explained by theoretical work. Further, model improvements only shift the error but do not appear to affect the power-law exponent. We also show that model size scales sublinearly with data size. These scaling relationships have significant implications on deep learning research, practice, and systems. They can assist model debugging, setting accuracy targets, and decisions about data set growth. They can also guide computing system design and underscore the importance of continued computational scaling.",363.0
CheXNet: Radiologist-Level Pneumonia Detection on Chest X-Rays with Deep Learning,31f10a6f602bef0306ac37322f84f6163c8a8ecb,"[{'authorId': '2706258', 'name': 'Pranav Rajpurkar'}, {'authorId': '46559852', 'name': 'J. Irvin'}, {'authorId': '29972904', 'name': 'Kaylie Zhu'}, {'authorId': '145951921', 'name': 'Brandon Yang'}, {'authorId': '3776937', 'name': 'Hershel Mehta'}, {'authorId': '15069782', 'name': 'Tony Duan'}, {'authorId': '51235411', 'name': 'D. Ding'}, {'authorId': '30043065', 'name': 'Aarti Bagul'}, {'authorId': '2356307', 'name': 'C. Langlotz'}, {'authorId': '3474704', 'name': 'K. Shpanskaya'}, {'authorId': '4204731', 'name': 'M. Lungren'}, {'authorId': '34699434', 'name': 'A. Ng'}]",2017.0,arXiv.org,['Unsolved Problems in ML Safety'],1,"We develop an algorithm that can detect pneumonia from chest X-rays at a level exceeding practicing radiologists. Our algorithm, CheXNet, is a 121-layer convolutional neural network trained on ChestX-ray14, currently the largest publicly available chest X-ray dataset, containing over 100,000 frontal-view X-ray images with 14 diseases. Four practicing academic radiologists annotate a test set, on which we compare the performance of CheXNet to that of radiologists. We find that CheXNet exceeds average radiologist performance on the F1 metric. We extend CheXNet to detect all 14 diseases in ChestX-ray14 and achieve state of the art results on all 14 diseases.",1889.0
BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain,573fd2ce97c70bb29097e8efb28a27af791225ca,"[{'authorId': '2367353', 'name': 'Tianyu Gu'}, {'authorId': '1398683279', 'name': 'Brendan Dolan-Gavitt'}, {'authorId': '1696125', 'name': 'S. Garg'}]",2017.0,arXiv.org,['Unsolved Problems in ML Safety'],1,"Deep learning-based techniques have achieved state-of-the-art performance on a wide variety of recognition and classification tasks. However, these networks are typically computationally expensive to train, requiring weeks of computation on many GPUs; as a result, many users outsource the training procedure to the cloud or rely on pre-trained models that are then fine-tuned for a specific task. In this paper we show that outsourced training introduces new security risks: an adversary can create a maliciously trained network (a backdoored neural network, or a \emph{BadNet}) that has state-of-the-art performance on the user's training and validation samples, but behaves badly on specific attacker-chosen inputs. We first explore the properties of BadNets in a toy example, by creating a backdoored handwritten digit classifier. Next, we demonstrate backdoors in a more realistic scenario by creating a U.S. street sign classifier that identifies stop signs as speed limits when a special sticker is added to the stop sign; we then show in addition that the backdoor in our US street sign detector can persist even if the network is later retrained for another task and cause a drop in accuracy of {25}\% on average when the backdoor trigger is present. These results demonstrate that backdoors in neural networks are both powerful and---because the behavior of neural networks is difficult to explicate---stealthy. This work provides motivation for further research into techniques for verifying and inspecting neural networks, just as we have developed tools for verifying and debugging software.",920.0
On Calibration of Modern Neural Networks,d65ce2b8300541414bfe51d03906fca72e93523c,"[{'authorId': '144993411', 'name': 'Chuan Guo'}, {'authorId': '10804137', 'name': 'Geoff Pleiss'}, {'authorId': '2117103358', 'name': 'Yu Sun'}, {'authorId': '7446832', 'name': 'Kilian Q. Weinberger'}]",2017.0,International Conference on Machine Learning,"['Unsolved Problems in ML Safety', 'AI Research Considerations for Human Existential Safety (ARCHES)']",2,"Confidence calibration -- the problem of predicting probability estimates representative of the true correctness likelihood -- is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-of-the-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling -- a single-parameter variant of Platt Scaling -- is surprisingly effective at calibrating predictions.",3199.0
Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles,802168a81571dde28f5ddb94d84677bc007afa7b,"[{'authorId': '40627523', 'name': 'Balaji Lakshminarayanan'}, {'authorId': '1863250', 'name': 'A. Pritzel'}, {'authorId': '1723876', 'name': 'C. Blundell'}]",2016.0,NIPS,"['Unsolved Problems in ML Safety', 'AI Research Considerations for Human Existential Safety (ARCHES)']",2,"Deep neural networks (NNs) are powerful black box predictors that have recently achieved impressive performance on a wide spectrum of tasks. Quantifying predictive uncertainty in NNs is a challenging and yet unsolved problem. Bayesian NNs, which learn a distribution over weights, are currently the state-of-the-art for estimating predictive uncertainty; however these require significant modifications to the training procedure and are computationally expensive compared to standard (non-Bayesian) NNs. We propose an alternative to Bayesian NNs that is simple to implement, readily parallelizable, requires very little hyperparameter tuning, and yields high quality predictive uncertainty estimates. Through a series of experiments on classification and regression benchmarks, we demonstrate that our method produces well-calibrated uncertainty estimates which are as good or better than approximate Bayesian NNs. To assess robustness to dataset shift, we evaluate the predictive uncertainty on test examples from known and unknown distributions, and show that our method is able to express higher uncertainty on out-of-distribution examples. We demonstrate the scalability of our method by evaluating predictive uncertainty estimates on ImageNet.",3308.0
The Off-Switch Game,808dec0828a74fecab07a497c10cd93e3748a5e2,"[{'authorId': '1397904824', 'name': 'Dylan Hadfield-Menell'}, {'authorId': '2745001', 'name': 'A. Dragan'}, {'authorId': '1689992', 'name': 'P. Abbeel'}, {'authorId': '145107462', 'name': 'Stuart J. Russell'}]",2016.0,International Joint Conference on Artificial Intelligence,"['Unsolved Problems in ML Safety', 'AI Research Considerations for Human Existential Safety (ARCHES)', 'X-Risk Analysis for AI Research']",3,"It is clear that one of the primary tools we can use to mitigate the potential risk from a misbehaving AI system is the ability to turn the system off. As the capabilities of AI systems improve, it is important to ensure that such systems do not adopt subgoals that prevent a human from switching them off. This is a challenge because many formulations of rational agents create strong incentives for self-preservation. This is not caused by a built-in instinct, but because a rational agent will maximize expected utility and cannot achieve whatever objective it has been given if it is dead. Our goal is to study the incentives an agent has to allow itself to be switched off. We analyze a simple game between a human H and a robot R, where H can press R's off switch but R can disable the off switch. A traditional agent takes its reward function for granted: we show that such agents have an incentive to disable the off switch, except in the special case where H is perfectly rational. Our key insight is that for R to want to preserve its off switch, it needs to be uncertain about the utility associated with the outcome, and to treat H's actions as important observations about that utility. (R also has no incentive to switch itself off in this setting.) We conclude that giving machines an appropriate level of uncertainty about their objectives leads to safer designs, and we argue that this setting is a useful generalization of the classical AI paradigm of rational agents.",87.0
Membership Inference Attacks Against Machine Learning Models,f0dcc9aa31dc9b31b836bcac1b140c8c94a2982d,"[{'authorId': '2520493', 'name': 'R. Shokri'}, {'authorId': '34828439', 'name': 'Marco Stronati'}, {'authorId': '3469125', 'name': 'Congzheng Song'}, {'authorId': '1723945', 'name': 'Vitaly Shmatikov'}]",2016.0,IEEE Symposium on Security and Privacy,['Unsolved Problems in ML Safety'],1,"We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial ""machine learning as a service"" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies.",2282.0
A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks,6ff2a434578ff2746b9283e45abf296887f48a2d,"[{'authorId': '3422872', 'name': 'Dan Hendrycks'}, {'authorId': '1700980', 'name': 'Kevin Gimpel'}]",2016.0,International Conference on Learning Representations,['Unsolved Problems in ML Safety'],1,"We consider the two related problems of detecting if an example is misclassified or out-of-distribution. We present a simple baseline that utilizes probabilities from softmax distributions. Correctly classified examples tend to have greater maximum softmax probabilities than erroneously classified and out-of-distribution examples, allowing for their detection. We assess performance by defining several tasks in computer vision, natural language processing, and automatic speech recognition, showing the effectiveness of this baseline across all. We then show the baseline can sometimes be surpassed, demonstrating the room for future research on these underexplored detection tasks.",1877.0
Towards Evaluating the Robustness of Neural Networks,df40ce107a71b770c9d0354b78fdd8989da80d2f,"[{'authorId': '2483738', 'name': 'Nicholas Carlini'}, {'authorId': '145394689', 'name': 'D. Wagner'}]",2016.0,IEEE Symposium on Security and Privacy,['Unsolved Problems in ML Safety'],1,"Neural networks provide state-of-the-art results for most machine learning tasks. Unfortunately, neural networks are vulnerable to adversarial examples: given an input x and any target classification t, it is possible to find a new input x' that is similar to x but classified as t. This makes it difficult to apply neural networks in security-critical areas. Defensive distillation is a recently proposed approach that can take an arbitrary neural network, and increase its robustness, reducing the success rate of current attacks' ability to find adversarial examples from 95% to 0.5%.In this paper, we demonstrate that defensive distillation does not significantly increase the robustness of neural networks by introducing three new attack algorithms that are successful on both distilled and undistilled neural networks with 100% probability. Our attacks are tailored to three distance metrics used previously in the literature, and when compared to previous adversarial example generation algorithms, our attacks are often much more effective (and never worse). Furthermore, we propose using high-confidence adversarial examples in a simple transferability test we show can also be used to break defensive distillation. We hope our attacks will be used as a benchmark in future defense attempts to create neural networks that resist adversarial examples.",5898.0
Accessorize to a Crime: Real and Stealthy Attacks on State-of-the-Art Face Recognition,7f57e9939560562727344c1c987416285ef76cda,"[{'authorId': '36301492', 'name': 'Mahmood Sharif'}, {'authorId': '38181360', 'name': 'Sruti Bhagavatula'}, {'authorId': '41224057', 'name': 'Lujo Bauer'}, {'authorId': '1746214', 'name': 'M. Reiter'}]",2016.0,Conference on Computer and Communications Security,['Unsolved Problems in ML Safety'],1,"Machine learning is enabling a myriad innovations, including new algorithms for cancer diagnosis and self-driving cars. The broad use of machine learning makes it important to understand the extent to which machine-learning algorithms are subject to attack, particularly when used in applications where physical security or safety is at risk. In this paper, we focus on facial biometric systems, which are widely used in surveillance and access control. We define and investigate a novel class of attacks: attacks that are physically realizable and inconspicuous, and allow an attacker to evade recognition or impersonate another individual. We develop a systematic method to automatically generate such attacks, which are realized through printing a pair of eyeglass frames. When worn by the attacker whose image is supplied to a state-of-the-art face-recognition algorithm, the eyeglasses allow her to evade being recognized or to impersonate another individual. Our investigation focuses on white-box face-recognition systems, but we also demonstrate how similar techniques can be used in black-box scenarios, as well as to avoid face detection.",1226.0
Equality of Opportunity in Supervised Learning,d42b11ce90c9c69a20ed015b73dc33e0e4100a7b,"[{'authorId': '1775622', 'name': 'Moritz Hardt'}, {'authorId': '4989538', 'name': 'Eric Price'}, {'authorId': '1706280', 'name': 'Nathan Srebro'}]",2016.0,NIPS,"['Unsolved Problems in ML Safety', 'AI Research Considerations for Human Existential Safety (ARCHES)']",2,"We propose a criterion for discrimination against a specified sensitive attribute in supervised learning, where the goal is to predict some target based on available features. Assuming data about the predictor, target, and membership in the protected group are available, we show how to optimally adjust any learned predictor so as to remove discrimination according to our definition. Our framework also improves incentives by shifting the cost of poor classification from disadvantaged groups to the decision maker, who can respond by improving the classification accuracy. We enourage readers to consult the more complete manuscript on the arXiv.",2787.0
The Rationality Quotient: Toward a Test of Rational Thinking,e2998a76d43ea6c59e5332f7c1a5f0437ff647a2,"[{'authorId': '4248890', 'name': 'K. Stanovich'}, {'authorId': '36855520', 'name': 'R. F. West'}, {'authorId': '146673920', 'name': 'M. Toplak'}]",2016.0,,['Unsolved Problems in ML Safety'],1,"ing, and the perceptions of melody. Chapters 7 and 8 provide guidance on Baroque composition and part-writing, Chapters 9 and 10 explore embellishments and the sense ofmusic leading somewhere, and Chapters 11 through 14 present additional considerations for composition, such as the perception of harmony and how we analyze auditory scenes and experiencemusical texture. Chapter 15discusses how learning and experience influence sound perception, and Chapter 16 focuses on experiments on why music is pleasing. Chapter 17 comprehensively summarizes the book. For readers who tire of marching through each principle, Huron has incorporated an explicit roadmap for reading Voice Leading in Chapters 1 and 17. He also implements parallel formatting for ease of reading: each chapter starts with a concise plan, a tangible and common example to illustrate the ideas discussed, technical exploration of the concept with key ideas italicized, extensive background to explore the ideas, and an excellent summary in a chapter reprise. Although each chapter is dense with scientific information, and the information can be quite technical at times, the explanations are easy to grasp. At the end of each chapter it will be a pleasant surprise to discover how much you have learned. You do not have to be a musician or composer interested in the cognition of music to appreciate this book. For bioacousticians, the author’s navigation of human auditory perception invokes shadows of signal analysis, peripheral nervous system constraints, central nervous system processing, multicomponent signals, and signal composition that can be applied across the animal kingdom. Huron’s neuroethological approach to understanding the perception of music will bring new appreciation to consideration of the aesthetics of sound in other animals. Furthermore, the historical commentary on musical composition throughout anchors the volume within a social context. Perhaps for Huron’s next book he could consider how auditory perception in other animals contrasts with humans to provide context for how unique—or not—human perception of music really is. Overall, Voice Leading provides a framework not just for understanding why musical compositions are perceived the way they are (or which rules musicians should follow to meet specific goals), but paints a picture of the complexity of the neurophysiological and psychological aspects of the impressive human auditory system. Kasey Fowler-Finn, Biology, Saint Louis University, St. Louis, Missouri The Rationality Quotient: Toward a Test of Rational Thinking. By Keith E. Stanovich, Richard F. West, and Maggie E. Toplak. Cambridge (Massachusetts): MIT Press. $39.00. xvii + 459 p.; ill.; author and subject indexes. ISBN: 978-0-262-03484-5. 2016. The great Arthur Conan Doyle, author of the timeless Sherlock Holmes novels, was fooled into believing that fairies exist by two teenage girls armed with a camera and a few cardboard cutouts of fairies. How could the brilliant intellectual father of the hypercritical and discerning Sherlock Holmes take on such an extravagant conviction on such meager evidence? The reason, Stanovich et al. would tell us, is that—contrary to common belief—rationality has very little to do with intelligence. RQ (Rationality Quotient) is only weakly correlated with IQ. So what underlies rational thinking? This important question is at the core of The Rationality Quotient. In addition to providing a comprehensive overview of the major findings of four decades of research in the heuristics and biases tradition instigated by the great Kahneman and Tversky, this volume delivers a standardized test to assess rationality in individuals. The importance of the book lies both in the comprehensive overview of the research on (ir)rationality and the psychometric system it proposes to gauge rationality in individuals. For far too long, IQ has been getting all of the attention. Although IQ is an important metric and a good predictor of an individual’s occupational level and performance, RQ—as the authors point out—is both more encompassing and important. In an increasingly hostile cognitive environment (i.e., an environment that differs from the environment of evolutionary adaptedness to which our innate intuitive modes of reasoning are attuned) forming rational beliefs and taking rational decisions becomes ever more challenging. At the same time, in a world where we are constantly bombarded with informational snippets that diverge widely with respect to their trustworthiness, rationality becomes evermore important. The modern world, as the authors point out, puts a premium on rational thinking. But the importance of rational thinking on an individual level is overshadowed by its importance on a societal level. From overconfidence leading to war and financial crises to the affect heuristic making us overreact on terrorism and remain dangerously impassive to the threat of climate change, the woes of society are the result of a lack of rationality. One can only hope that putting RQ on the map will produce a Flynn effect as has been the case for IQ, where average IQ has been steadily on the rise since the test was first introduced in the beginning of the 20th century. NEW BIOLOGICAL BOOKS March 2018 43",143.0
Deep Learning with Differential Privacy,e9a986c8ff6c2f381d026fe014f6aaa865f34da7,"[{'authorId': '2057642721', 'name': 'Martín Abadi'}, {'authorId': '1396184193', 'name': 'Andy Chu'}, {'authorId': '153440022', 'name': 'Ian J. Goodfellow'}, {'authorId': '145057514', 'name': 'H. B. McMahan'}, {'authorId': '145591745', 'name': 'Ilya Mironov'}, {'authorId': '35210462', 'name': 'Kunal Talwar'}, {'authorId': '2152832173', 'name': 'Li Zhang'}]",2016.0,Conference on Computer and Communications Security,['Unsolved Problems in ML Safety'],1,"Machine learning techniques based on neural networks are achieving remarkable results in a wide variety of domains. Often, the training of models requires large, representative datasets, which may be crowdsourced and contain sensitive information. The models should not expose private information in these datasets. Addressing this goal, we develop new algorithmic techniques for learning and a refined analysis of privacy costs within the framework of differential privacy. Our implementation and experiments demonstrate that we can train deep neural networks with non-convex objectives, under a modest privacy budget, and at a manageable cost in software complexity, training efficiency, and model quality.",3485.0
Towards Open Set Deep Networks,d094fb0af5bc6a26fa9c27d638c4a3a0725d8b5c,"[{'authorId': '3274223', 'name': 'Abhijit Bendale'}, {'authorId': '32163276', 'name': 'T. Boult'}]",2015.0,Computer Vision and Pattern Recognition,['Unsolved Problems in ML Safety'],1,"Deep networks have produced significant gains for various visual recognition problems, leading to high impact academic and commercial applications. Recent work in deep networks highlighted that it is easy to generate images that humans would never classify as a particular object class, yet networks classify such images high confidence as that given class - deep network are easily fooled with images humans do not consider meaningful. The closed set nature of deep networks forces them to choose from one of the known classes leading to such artifacts. Recognition in the real world is open set, i.e. the recognition system should reject unknown/unseen classes at test time. We present a methodology to adapt deep networks for open set recognition, by introducing a new model layer, OpenMax, which estimates the probability of an input being from an unknown class. A key element of estimating the unknown probability is adapting Meta-Recognition concepts to the activation patterns in the penultimate layer of the network. Open-Max allows rejection of ""fooling"" and unrelated open set images presented to the system, OpenMax greatly reduces the number of obvious errors made by a deep network. We prove that the OpenMax concept provides bounded open space risk, thereby formally providing an open set recognition solution. We evaluate the resulting open set deep networks using pre-trained networks from the Caffe Model-zoo on ImageNet 2012 validation data, and thousands of fooling and open set images. The proposed OpenMax model significantly outperforms open set recognition accuracy of basic deep networks as well as deep networks with thresholding of SoftMax probabilities.",839.0
Hidden Technical Debt in Machine Learning Systems,1eb131a34fbb508a9dd8b646950c65901d6f1a5b,"[{'authorId': '1733143', 'name': 'D. Sculley'}, {'authorId': '144510728', 'name': 'Gary Holt'}, {'authorId': '145973657', 'name': 'D. Golovin'}, {'authorId': '143698521', 'name': 'Eugene Davydov'}, {'authorId': '2054375101', 'name': 'Todd Phillips'}, {'authorId': '49236095', 'name': 'D. Ebner'}, {'authorId': '2055477158', 'name': 'Vinay Chaudhary'}, {'authorId': '2114084357', 'name': 'Michael Young'}, {'authorId': '40169157', 'name': 'J. Crespo'}, {'authorId': '47019745', 'name': 'Dan Dennison'}]",2015.0,NIPS,['Unsolved Problems in ML Safety'],1,"Machine learning offers a fantastically powerful toolkit for building useful complex prediction systems quickly. This paper argues it is dangerous to think of these quick wins as coming for free. Using the software engineering framework of technical debt, we find it is common to incur massive ongoing maintenance costs in real-world ML systems. We explore several ML-specific risk factors to account for in system design. These include boundary erosion, entanglement, hidden feedback loops, undeclared consumers, data dependencies, configuration issues, changes in the external world, and a variety of system-level anti-patterns.",735.0
Principle 1: Preoccupation with Failure,9ff9f3f4a1dd536860927ff9d447339fe651c081,"[{'authorId': '2362309', 'name': 'K. Weick'}, {'authorId': '3321742', 'name': 'K. Sutcliffe'}]",2015.0,,['Unsolved Problems in ML Safety'],1,,1.0
Posterior calibration and exploratory analysis for natural language processing models,1aad4230be997ec83acd6b7c41d581eb8dd033ca,"[{'authorId': '144867624', 'name': 'Khanh Nguyen'}, {'authorId': '153724741', 'name': ""Brendan T. O'Connor""}]",2015.0,Conference on Empirical Methods in Natural Language Processing,['Unsolved Problems in ML Safety'],1,"Many models in natural language processing define probabilistic distributions over linguistic structures. We argue that (1) the quality of a model' s posterior distribution can and should be directly evaluated, as to whether probabilities correspond to empirical frequencies, and (2) NLP uncertainty can be projected not only to pipeline components, but also to exploratory data analysis, telling a user when to trust and not trust the NLP analysis. We present a method to analyze calibration, and apply it to compare the miscalibration of several commonly used models. We also contribute a coreference sampling algorithm that can create confidence intervals for a political event extraction task.",92.0
Recognizing Functions in Binaries with Neural Networks,103de5df41ed33f1c0bd1710f582d720cb05e6fd,"[{'authorId': '2802006', 'name': 'E. C. Shin'}, {'authorId': '143711382', 'name': 'D. Song'}, {'authorId': '2051440', 'name': 'R. Moazzezi'}]",2015.0,USENIX Security Symposium,['Unsolved Problems in ML Safety'],1,"Binary analysis facilitates many important applications like malware detection and automatically fixing vulnerable software. In this paper, we propose to apply artificial neural networks to solve important yet difficult problems in binary analysis. Specifically, we tackle the problem of function identification, a crucial first step in many binary analysis techniques. Although neural networks have undergone a renaissance in the past few years, achieving breakthrough results in multiple application domains such as visual object recognition, language modeling, and speech recognition, no researchers have yet attempted to apply these techniques to problems in binary analysis. Using a dataset from prior work, we show that recurrent neural networks can identify functions in binaries with greater accuracy and efficiency than the state-of-the-art machine-learning-based method. We can train the model an order of magnitude faster and evaluate it on binaries hundreds of times faster. Furthermore, it halves the error rate on six out of eight benchmarks, and performs comparably on the remaining two.",243.0
The Possibility of an Ongoing Moral Catastrophe,1f3f2f5e73d7a84a06bcd4a01a795aa23f4e05b5,"[{'authorId': '7427880', 'name': 'E. G. Williams'}]",2015.0,,['Unsolved Problems in ML Safety'],1,,18.0
Superforecasting: The Art and Science of Prediction,7fd0b7c04157ca1459ca16607de9ec139fd9ad51,"[{'authorId': '46645239', 'name': 'P. Tetlock'}, {'authorId': '49215963', 'name': 'Dan Gardner'}]",2015.0,,['Unsolved Problems in ML Safety'],1,"A New York Times Bestseller An Economist Best Book of 2015 ""The most important book on decision making since Daniel Kahneman's Thinking, Fast and Slow."" Jason Zweig, The Wall Street Journal Everyone would benefit from seeing further into the future, whether buying stocks, crafting policy, launching a new product, or simply planning the weeks meals. Unfortunately, people tend to be terrible forecasters. As Wharton professor Philip Tetlock showed in a landmark 2005 study, even experts predictions are only slightly better than chance. However, an important and underreported conclusion of that study was that some experts do have real foresight, and Tetlock has spent the past decade trying to figure out why. What makes some people so good? And can this talent be taught? In Superforecasting, Tetlock and coauthor Dan Gardner offer a masterwork on prediction, drawing on decades of research and the results of a massive, government-funded forecasting tournament. The Good Judgment Project involves tens of thousands of ordinary peopleincluding a Brooklyn filmmaker, a retired pipe installer, and a former ballroom dancerwho set out to forecast global events. Some of the volunteers have turned out to be astonishingly good. Theyve beaten other benchmarks, competitors, and prediction markets. Theyve even beaten the collective judgment of intelligence analysts with access to classified information. They are ""superforecasters."" In this groundbreaking and accessible book, Tetlock and Gardner show us how we can learn from this elite group. Weaving together stories of forecasting successes (the raid on Osama bin Ladens compound) and failures (the Bay of Pigs) and interviews with a range of high-level decision makers, from David Petraeus to Robert Rubin, they show that good forecasting doesnt require powerful computers or arcane methods. It involves gathering evidence from a variety of sources, thinking probabilistically, working in teams, keeping score, and being willing to admit error and change course. Superforecasting offers the first demonstrably effective way to improve our ability to predict the futurewhether in business, finance, politics, international affairs, or daily lifeand is destined to become a modern classic.",492.0
The Point of View of the Universe: Sidgwick and Contemporary Ethics,5946229e1baf3a2c73477e2e472a82058655018a,"[{'authorId': '1403052988', 'name': 'K. Lazari-Radek'}, {'authorId': '145814775', 'name': 'P. Singer'}]",2014.0,,['Unsolved Problems in ML Safety'],1,"Preface A Biographical Prologue 1. Introduction 2. Reason and Action 3. Intuition and the Morality of Common Sense 4. Justification in Ethics 5. The Axioms of Ethics 6. The Profoundest Problem of Ethics 7. The Origins of Ethics and the Unity of Practical Reason 8. Ultimate Good, Part I: Perfectionism and Desire Based Theory 9. Ultimate Good, Part II: Hedonism 10. Rules 11. Demandingness 12. Distribution Conclusion",86.0
On the Difference between Binary Prediction and True Exposure with Implications for Forecasting Tournaments and Decision Making Research,fca7a005ad47cfc77e17b37ca2ee6724275f382a,"[{'authorId': '1928817', 'name': 'N. Taleb'}, {'authorId': '46645239', 'name': 'P. Tetlock'}]",2013.0,,['Unsolved Problems in ML Safety'],1,"There are serious differences between predictions, bets, and exposures that have a yes/no type of payoff, the ""binaries"", and those that have varying payoffs, which we call the ""vanilla"". Real world exposures tend to belong to the vanilla category, and are poorly captured by binaries. Vanilla exposures are sensitive to Black Swan effects, model errors, and prediction problems, while the binaries are largely immune to them. The binaries are mathematically tractable, while the vanilla are much less so. Hedging vanilla exposures with binary bets can be disastrous -- and because of the human tendency to engage in attribute substitution when confronted by difficult questions, decision-makers and researchers often confuse the vanilla for the binary.",13.0
Antifragile: Things That Gain from Disorder,0f5db40de6d3a05476a3247195a2fe5a0a4da23c,"[{'authorId': '3189820', 'name': 'Chris Arney'}]",2013.0,,['Unsolved Problems in ML Safety'],1,"ANTIFRAGILE: THINGS THAT GAIN FROM DISORDER Nassim Nicholas Taleb Random House, New York, 2012, 519 pp. ISBN: 978-1-4000-6782-4Antifragile is a book about the structure and behavior of dynamic systems. To a mathematician, this is mainstream mathematics. However, Taleb, writing for a general audience, seems to try to hide the formal mathematics and obscure its usefulness by his focus on a more literary, historical, and philosophical presentation. Yet, in the end, enough mathematical evidence and explanation of Taleb's antifragile theory as well as examples make this book worthy of consideration. Taleb's theory is simple enough. He classifies systems by the nonlinearity (convexity and concavity) of their utility or health functions as either antifragile or fragile. While never producing a precise measure for this property for systems or entities, he does give myriad interesting examples of his theory in systems, items, and concepts from biology, human nature, natural science, information science, social science, business, literature, politics, and philosophy. Taleb's reason for inventing the term ""antifragile"" is interesting: ""Half of life - the interesting half of life - we don't have a name for."" (p. 33)It's a shame that Taleb does not take a more mathematical and scientific approach to his fragility concept, because many excellent points could be made and insights developed that could help to understand his examples. Taleb uses the word ""antifragile"" to describe systems that are the opposite of fragile, and therefore, usually, a property to celebrate, advocate, design and use to one's benefit. Systems and items that improve or gain from disorder and stress are antifragile to that disorder or stress. The nonlinearity of antifragility can come from many properties of the system: redundancy, complexity, volatility, randomness, and asymmetry, the author's interesting examples come from all areas of life. Some that are rich in flavor and insight include health, medicine, love, banking, traffic, research, decision making, education, ethics, government, religion, smoking, technology, and weightlifting. The author is careful to contrast the antifragility property with related but different concepts of robustness and resilience.This book contains seven sections (called books), a glossary, a bibliography, and two appendices. The first and most insightful table is a five-page Table of Triads - a classification of dozens of elements by their range of fragility (fragility on one end, antifragile on the other, and robustness in the middle - thus the term triad). For example, directed research is fragile, opportunistic research is robust, and stochastic tinkering is antifragile. Corporate employment is fragile, the dental profession is robust, and taxi driving can be considered antifragile. Bureaucrats are fragile and entrepreneurs are antifragile. In literature, e-readers are fragile, books are robust, and oral tradition is antifragile. Mother Nature is Taleb's prime example of antifragility - living things like ""a certain measure of randomness and disorder"", (p. …",662.0
Evasion Attacks against Machine Learning at Test Time,033c08ca48aaed2d5ab0a17d668d410538678ed8,"[{'authorId': '1684175', 'name': 'B. Biggio'}, {'authorId': '2338858', 'name': 'I. Corona'}, {'authorId': '3248803', 'name': 'Davide Maiorca'}, {'authorId': '39743720', 'name': 'Blaine Nelson'}, {'authorId': '2118348', 'name': 'Nedim Srndic'}, {'authorId': '1754215', 'name': 'P. Laskov'}, {'authorId': '1779484', 'name': 'G. Giacinto'}, {'authorId': '1710171', 'name': 'F. Roli'}]",2013.0,ECML/PKDD,['Unsolved Problems in ML Safety'],1,,1665.0
Facebook Use Predicts Declines in Subjective Well-Being in Young Adults,82e804cf2b65c774dd758e1ea2afad7e9ddd0df0,"[{'authorId': '2334455', 'name': 'Ethan Kross'}, {'authorId': '4698745', 'name': 'Philippe Verduyn'}, {'authorId': '2285466', 'name': 'E. Demiralp'}, {'authorId': '46979321', 'name': 'Jiyoung Park'}, {'authorId': '3100543', 'name': 'David S Lee'}, {'authorId': '5006249', 'name': 'Natalie J. Lin'}, {'authorId': '47885426', 'name': 'Holly Shablack'}, {'authorId': '1869771', 'name': 'J. Jonides'}, {'authorId': '6213128', 'name': 'O. Ybarra'}]",2013.0,PLoS ONE,"['Unsolved Problems in ML Safety', 'X-Risk Analysis for AI Research']",2,"Over 500 million people interact daily with Facebook. Yet, whether Facebook use influences subjective well-being over time is unknown. We addressed this issue using experience-sampling, the most reliable method for measuring in-vivo behavior and psychological experience. We text-messaged people five times per day for two-weeks to examine how Facebook use influences the two components of subjective well-being: how people feel moment-to-moment and how satisfied they are with their lives. Our results indicate that Facebook use predicts negative shifts on both of these variables over time. The more people used Facebook at one time point, the worse they felt the next time we text-messaged them; the more they used Facebook over two-weeks, the more their life satisfaction levels declined over time. Interacting with other people “directly” did not predict these negative outcomes. They were also not moderated by the size of people's Facebook networks, their perceived supportiveness, motivation for using Facebook, gender, loneliness, self-esteem, or depression. On the surface, Facebook provides an invaluable resource for fulfilling the basic human need for social connection. Rather than enhancing well-being, however, these findings suggest that Facebook may undermine it.",1133.0
Monitor alarm fatigue: an integrative review.,da8aa38ba087b0200b2f81f2e86e0e8036993579,"[{'authorId': '11335863', 'name': 'M. Cvach'}]",2012.0,Biomedical Instrumentation & Technology,['Unsolved Problems in ML Safety'],1,"Alarm fatigue is a national problem and the number one medical device technology hazard in 2012. The problem of alarm desensitization is multifaceted and related to a high false alarm rate, poor positive predictive value, lack of alarm standardization, and the number of alarming medical devices in hospitals today. This integrative review synthesizes research and non-research findings published between 1/1/2000 and 10/1/2011 using The Johns Hopkins Nursing Evidence-Based Practice model. Seventy-two articles were included. Research evidence was organized into five main themes: excessive alarms and effects on staff; nurse's response to alarms; alarm sounds and audibility; technology to reduce false alarms; and alarm notification systems. Non-research evidence was divided into two main themes: strategies to reduce alarm desensitization, and alarm priority and notification systems. Evidence-based practice recommendations and gaps in research are summarized.",475.0
Engineering a Safer World: Systems Thinking Applied to Safety,266fbab5bbba25c28d23efe34dff534e6be9522a,"[{'authorId': '1777378', 'name': 'N. Leveson'}]",2012.0,,"['Unsolved Problems in ML Safety', 'X-Risk Analysis for AI Research']",2,"A new approach to safety, based on systems thinking, that is more effective, less costly, and easier to use than current techniques.Engineering has experienced a technological revolution, but the basic engineering techniques applied in safety and reliability engineering, created in a simpler, analog world, have changed very little over the years. In this groundbreaking book, Nancy Leveson proposes a new approach to safety?more suited to today's complex, sociotechnical, software-intensive world?based on modern systems thinking and systems theory. Revisiting and updating ideas pioneered by 1950s aerospace engineers in their System Safety concept, and testing her new model extensively on real-world examples, Leveson has created a new approach to safety that is more effective, less expensive, and easier to use than current techniques.Arguing that traditional models of causality are inadequate, Leveson presents a new, extended model of causation (Systems-Theoretic Accident Model and Processes, or STAMP), then shows how the new model can be used to create techniques for system safety engineering, including accident analysis, hazard analysis, system design, safety in operations, and management of safety-critical systems. She applies the new techniques to real-world events including the friendly-fire loss of a U.S. Blackhawk helicopter in the first Gulf War; the Vioxx recall; the U.S. Navy SUBSAFE program; and the bacterial contamination of a public water supply in a Canadian town. Leveson's approach is relevant even beyond safety engineering, offering techniques for ?reengineering? any large sociotechnical system to improve safety and manage risk.",1533.0
Stuxnet: Dissecting a Cyberwarfare Weapon,72a3e02eccf9872c7080645d936b5564d356ecfa,"[{'authorId': '2066225316', 'name': 'Ralph Langner'}]",2011.0,IEEE Security and Privacy,['Unsolved Problems in ML Safety'],1,"Last year marked a turning point in the history of cybersecurity-the arrival of the first cyber warfare weapon ever, known as Stuxnet. Not only was Stuxnet much more complex than any other piece of malware seen before, it also followed a completely new approach that's no longer aligned with conven tional confidentiality, integrity, and availability thinking. Con trary to initial belief, Stuxnet wasn't about industrial espionage: it didn't steal, manipulate, or erase information. Rather, Stuxnet's goal was to physically destroy a military target-not just meta phorically, but literally. Let's see how this was done.",1405.0
The Flash Crash: The Impact of High Frequency Trading on an Electronic Market,f997411ca4c133dd790f78d7dc5f6936b5d221af,"[{'authorId': '8528698', 'name': 'A. Kirilenko'}, {'authorId': '98066902', 'name': 'M. Samadi'}, {'authorId': '144865543', 'name': 'A. Kyle'}, {'authorId': '118270631', 'name': 'Tugkan Tuzun'}]",2011.0,,['Unsolved Problems in ML Safety'],1,"We use E-mini S&P 500 futures market audit-trail data to compare the trading of High Frequency Traders and other traders during the Flash Crash of May 6, 2010 with the three prior trading days. On all 4 days, High Frequency Trader’s inventories rarely exceeded 3000 contracts, mean-reverted to 0 with a half-life of approximately two minutes, and had similar correlations with price changes over one second intervals. The Flash Crash was triggered by a 75,000 contract sell program. We conclude that the inventories of High Frequency Traders were too small to have caused or prevented the Flash Crash.",633.0
High income improves evaluation of life but not emotional well-being,e6f7e18374d79e3a03fe70f9a99d4614b01e8f19,"[{'authorId': '3683465', 'name': 'D. Kahneman'}, {'authorId': '3866028', 'name': 'A. Deaton'}]",2010.0,Proceedings of the National Academy of Sciences of the United States of America,['Unsolved Problems in ML Safety'],1,"Recent research has begun to distinguish two aspects of subjective well-being. Emotional well-being refers to the emotional quality of an individual's everyday experience—the frequency and intensity of experiences of joy, stress, sadness, anger, and affection that make one's life pleasant or unpleasant. Life evaluation refers to the thoughts that people have about their life when they think about it. We raise the question of whether money buys happiness, separately for these two aspects of well-being. We report an analysis of more than 450,000 responses to the Gallup-Healthways Well-Being Index, a daily survey of 1,000 US residents conducted by the Gallup Organization. We find that emotional well-being (measured by questions about emotional experiences yesterday) and life evaluation (measured by Cantril's Self-Anchoring Scale) have different correlates. Income and education are more closely related to life evaluation, but health, care giving, loneliness, and smoking are relatively stronger predictors of daily emotions. When plotted against log income, life evaluation rises steadily. Emotional well-being also rises with log income, but there is no further progress beyond an annual income of ~$75,000. Low income exacerbates the emotional pain associated with such misfortunes as divorce, ill health, and being alone. We conclude that high income buys life satisfaction but not happiness, and that low income is associated both with low life evaluation and low emotional well-being.",2369.0
Outside the Closed World: On Using Machine Learning for Network Intrusion Detection,8346b9a8e156d6e7a7012bcd47bc4f5d4be59e92,"[{'authorId': '1690799', 'name': 'Robin Sommer'}, {'authorId': '1744800', 'name': 'V. Paxson'}]",2010.0,IEEE Symposium on Security and Privacy,['Unsolved Problems in ML Safety'],1,"In network intrusion detection research, one popular strategy for finding attacks is monitoring a network's activity for anomalies: deviations from profiles of normality previously learned from benign traffic, typically identified using tools borrowed from the machine learning community. However, despite extensive academic research one finds a striking gap in terms of actual deployments of such systems: compared with other intrusion detection approaches, machine learning is rarely employed in operational ""real world"" settings. We examine the differences between the network intrusion detection problem and other areas where machine learning regularly finds much more success. Our main claim is that the task of finding attacks is fundamentally different from these other applications, making it significantly harder for the intrusion detection community to employ machine learning effectively. We support this claim by identifying challenges particular to network intrusion detection, and provide a set of guidelines meant to strengthen future research on anomaly detection.",1338.0
What the GDP Gets Wrong (Why Managers Should Care),e0951ace9136ef53d227bfbb1f3b98127d9fb5fa,"[{'authorId': '2841157', 'name': 'E. Brynjolfsson'}, {'authorId': '120858947', 'name': 'A. Saunders'}]",2009.0,,['Unsolved Problems in ML Safety'],1,"We see the influence of the information age everywhere, except in the GDP statistics. More people than ever are using Wikipedia, Facebook, Craigslist, Pandora, Hulu and Google. Thousands of new information goods and services are introduced each year. Yet, according to the official GDP statistics, the information sector (software, publishing, motion picture and sound recording, broadcasting, telecom, and information and data processing services) is about the same share of the economy as it was 25 years ago - about 4%. How is this possible? Don’t we have access to more information than ever before?The answer isn’t about quantity, it’s about price. The bits that comprise today’s information goods are supplanting the atoms that formed yesterday’s encyclopedias, movie theaters, music CDs and newspapers. Online information may be updated every minute of the day and accessible almost anywhere in the world, but its price is usually radically lower than that of its physical counterpart, if there even is a price.",24.0
Analysis of the 2007 Cyber Attacks Against Estonia from the Information Warfare Perspective,b63d7ca0eb68bb24e9b9abc5fab8b828e1f5ddcb,"[{'authorId': '2085463', 'name': 'Rain Ottis'}]",2008.0,,"['Unsolved Problems in ML Safety', 'X-Risk Analysis for AI Research']",2,"Following the relocation of a Soviet-era statue in Tallinn in April of 2007, Estonia fell under a politically motivated cyber attack campaign lasting twenty-two days. Perhaps the best known attacks were distributed denial of service attacks, resulting in temporary degradation or loss of service on many commercial and government servers. While most of the attacks targeted non-critical services like public websites and e-mail, others concentrated on more vital targets, such as online banking and DNS. At the time of this writing – more than six months after the cyber attacks – no organization or group has claimed responsibility for the cyber attacks, although some individuals have been linked with carrying them out. This paper will argue that the key to understanding the cyber attacks that took place against Estonia in 2007 lies with the analysis of an abundance of circumstantial evidence that ran parallel to the cyber attacks. These consisted of political, economic and information attacks on Estonia, as well as isolated cases of physical violence. Clear political signatures were even detected in the malicious network traffic. All told, it is clear that the cyber attacks were linked with the overall political conflict between Estonia and Russia. While some analysts have considered last year’s events in Estonia an international, grass roots, display of public opinion, there are some direct and many indirect indications of state support behind what can be best described as an information operation. By information operation, the author means the use of information and information technology to affect the decisions and actions of an opponent. The paper will give an overview of the major events and provide an analysis of the attacks from the information warfare perspective. The paper will also discuss some of the potential problems with using the Internet as a field of battle by lone hackers, terrorist groups and states. To a minor degree, the paper will also cover the difficulties associated with investigating and analyzing international cyber attacks. The objective of this paper is not to implicate a specific organization or entity, but to provide a wider view to the cyber attacks that were carried out against Estonia in the spring of 2007.",64.0
The Black Swan: The Impact of the Highly Improbable,bdabc13bc819f84356eaeb7b087855f5457d990d,"[{'authorId': '1928817', 'name': 'N. Taleb'}]",2007.0,,['Unsolved Problems in ML Safety'],1,"Nassim Nicholas Taleb's phenomenal international bestseller The Black Swan: The Impact of the Highly Improbable shows us how to stop trying to predict everything - and take advantage of uncertainty. What have the invention of the wheel, Pompeii, the Wall Street Crash, Harry Potter and the internet got in common? Why are all forecasters con-artists? What can Catherine the Great's lovers tell us about probability? Why should you never run for a train or read a newspaper? This book is all about Black Swans: the random events that underlie our lives, from bestsellers to world disasters. Their impact is huge; they're impossible to predict; yet after they happen we always try to rationalize them. ""Taleb is a bouncy and even exhilarating guide...I came to relish what he said, and even develop a sneaking affection for him as a person."" (Will Self, Independent on Sunday). ""He leaps like some superhero of the mind."" (Boyd Tonkin, Independent). ""Funny, quirky and thought-provoking...confirms his status as a guru for every would-be Damien Hirst, George Soros and aspirant despot."" (John Cornwell, Sunday Times). ""Idiosyncratically brilliant."" (Niall Ferguson, Sunday Telegraph). ""Great fun...brash, stubborn, entertaining, opinionated, curious, cajoling. "" (Stephen J. Dubner, Co-Author of Freakonomics).",3974.0
A history of internet security,f971fa0d0989e473130a223c3d1241454e3b4d22,"[{'authorId': '2432386', 'name': 'L. DeNardis'}]",2007.0,,['Unsolved Problems in ML Safety'],1,,16.0
Affective Forecasting,026450847572b1115b4b84ea47eea3c13c1adbb3,"[{'authorId': '144882054', 'name': 'T. Wilson'}, {'authorId': '1874113', 'name': 'D. Gilbert'}]",2005.0,,['Unsolved Problems in ML Safety'],1,"People base many decisions on affective forecasts, predictions about their emotional reactions to future events. They often display an impact bias, overestimating the intensity and duration of their emotional reactions to such events. One cause of the impact bias is focalism, the tendency to underestimate the extent to which other events will influence our thoughts and feelings. Another is people's failure to anticipate how quickly they will make sense of things that happen to them in a way that speeds emotional recovery. This is especially true when predicting reactions to negative events: People fail to anticipate how quickly they will cope psychologically with such events in ways that speed their recovery from them. Several implications are discussed, such as the tendency for people to attribute their unexpected resilience to external agents.",1433.0
"The Misbehavior of Markets: A Fractal View of Risk, Ruin, and Reward",2f82413d3e42b846175e560deabbb454ac015886,"[{'authorId': '46561670', 'name': 'B. Mandelbrot'}, {'authorId': '1790260', 'name': 'Richard L. Hudson'}]",2004.0,,['Unsolved Problems in ML Safety'],1,"This international bestseller, which foreshadowed a market crash, explains why it could happen again if we don’t act now. Fractal geometry is the mathematics of roughness: how to reduce the outline of a jagged leaf or static in a computer connection to a few simple mathematical properties. With his fractal tools, Mandelbrot has got to the bottom of how financial markets really work. He finds they have a shifting sense of time and wild behaviour that makes them volatile, dangerous - and beautiful. In his models, the complex gyrations of the FTSE 100 and exchange rates can be reduced to straightforward formulae that yield a much more accurate description of the risks involved.",488.0
A Brief History of Generative Models for Power Law and Lognormal Distributions,7cfa3ed908e28472a6b0aec6acae5d8df559eb7b,"[{'authorId': '1745699', 'name': 'M. Mitzenmacher'}]",2004.0,Internet Mathematics,['Unsolved Problems in ML Safety'],1,"Recently, I became interested in a current debate over whether file size distributions are best modelled by a power law distribution or a lognormal distribution. In trying to learn enough about these distributions to settle the question, I found a rich and long history, spanning many fields. Indeed, several recently proposed models from the computer science community have antecedents in work from decades ago. Here, I briefly survey some of this history, focusing on underlying generative models that lead to these distributions. One finding is that lognormal and power law distributions connect quite naturally, and hence, it is not surprising that lognormal distributions have arisen as a possible alternative to power law distributions across many fields.",1745.0
CAPABILITIES AS FUNDAMENTAL ENTITLEMENTS: SEN AND SOCIAL JUSTICE,90d9727a5a1df03df2af1e4f11eaad508146b5ca,"[{'authorId': '144779960', 'name': 'M. Nussbaum'}]",2003.0,,['Unsolved Problems in ML Safety'],1,"Amartya Sen has made a major contribution to the theory of social justice, and of gender justice, by arguing that capabilities are the relevant space of comparison when justice-related issues are considered. This article supports Sen's idea, arguing that capabilities supply guidance superior to that of utility and resources (the view's familiar opponents), but also to that of the social contract tradition, and at least some accounts of human rights. But I argue that capabilities can help us to construct a normative conception of social justice, with critical potential for gender issues, only if we specify a definite set of capabilities as the most important ones to protect. Sen's ""perspective of freedom"" is too vague. Some freedoms limit others; some freedoms are important, some trivial, some good, and some positively bad. Before the approach can offer a valuable normative gender perspective, we must make commitments about substance.",2137.0
Inequality and Violent Crime*,52af2f4cc17f2a16f1ea1dfa5bddf2e8d201bfcf,"[{'authorId': '100932279', 'name': 'Pablo Fajnzylber'}, {'authorId': '50979967', 'name': 'D. Lederman'}, {'authorId': '152368548', 'name': 'Norman V. Loayza'}]",2002.0,Journal law and economy,['Unsolved Problems in ML Safety'],1,"We investigate the robustness and causality of the link between income inequality and violent crime across countries. First, we study the correlation between the Gini index and homicide and robbery rates within and between countries. Second, we examine the partial correlation by considering other crime determinants. Third, we control for the endogeneity of inequality by isolating its exogenous impact on these crime rates. Fourth, we control for measurement error in crime rates by modeling it as both unobserved country effects and random noise. Finally, we examine the robustness of this partial correlation to alternative measures of inequality. The panel data consist of nonoverlapping 5‐year averages for 39 countries during 1965–95 for homicides and 37 countries during 1970–94 for robberies. Crime rates and inequality are positively correlated within countries and, particularly, between countries, and this correlation reflects causation from inequality to crime rates, even after controlling for other crime determinants.",1030.0
What drives tropical deforestation?: a meta-analysis of proximate and underlying causes of deforestation based on subnational case study evidence,9793a01f86aed20aa28a3778170f9069bc658192,"[{'authorId': '46594402', 'name': 'H. Geist'}, {'authorId': '1844258', 'name': 'E. Lambin'}]",2001.0,,['Unsolved Problems in ML Safety'],1,,749.0
"Risky business: safety regulations, risk compensation, and individual behavior",400d08a1ebace33672de6aeaf592660d332063bd,"[{'authorId': '5611592', 'name': 'J. Hedlund'}]",2000.0,Injury Prevention,['Unsolved Problems in ML Safety'],1,"Editors comment: We are proud to be able to bring to our readers this full text version of the Haddon Memorial Lecture delivered at the recent Fifth World Conference on Injury Prevention and Control in New Delhi, India. James Hedlund offers a brilliant review of one of the most important areas of debate in the entire field of injury control. This is the most complete, most perceptive, and well balanced appraisals of this complex issue I have ever read. Take the time to digest it completely. Our thanks to the Insurance Institute for Highway Safety for agreeing to permit us to publish it. 

Government regulations and industry practices constrain our behavior in many ways in an attempt to reduce injuries. Safety features are designed into products we use: cars now have airbags; medicine bottles have “childproof” caps. Laws require us to act in a safe manner: we must wear seat belts while driving and hard hats in construction areas. But do these measures influence our behavior in other ways? Risk compensation theory hypothesizes that they do, that we “use up” the additional safety though more risky actions.

This paper surveys risk compensation by reviewing its history, discussing its theoretical foundations, outlining evidence for and against its claims, and providing the author's own views. It concludes by discussing the relevance of risk compensation for injury prevention workers who seek to reduce unintentional injuries.

Injury prevention as a discipline began when injuries were understood to be both predictable and preventable. Most injuries are the unintended consequences of individual actions in a risky environment; they are not due to fate or to problem behavior. This understanding led to three fundamental injury prevention strategies, as described in the comprehensive report Injury in America 1:",300.0
An Application of Machine Learning to Anomaly Detection,701d2f7c45277e74cc1f047d7681010a233ecc66,"[{'authorId': '143673956', 'name': 'T. Lane'}]",1999.0,,['Unsolved Problems in ML Safety'],1,The anomaly detection problem has been widely studied in the computer security literature. In this paper we present a machine learning approach to anomaly detection. Our system builds user profiles based on command sequences and compares current input sequences to the profile using a similarity measure. The system must learn to classify current behavior as consistent or anomalous with past behavior using only positive examples of the account's valid user. Our empirical results demonstrate that this is a promising approach to distinguishing the legitamate user from an intruder.,233.0
‘Improving ratings’: audit in the British University system,5a984f47f4a20af09b6b1cdd42f47fe32cff97d8,"[{'authorId': '10183207', 'name': 'M. Strathern'}]",1997.0,Europaeum review,['Unsolved Problems in ML Safety'],1,"This paper gives an anthropological comment on what has been called the ‘audit explosion’, the proliferation of procedures for evaluating performance. In higher education the subject of audit (in this sense) is not so much the education of the students as the institutional provision for their education. British universities, as institutions, are increasingly subject to national scrutiny for teaching, research and administrative competence. In the wake of this scrutiny comes a new cultural apparatus of expectations and technologies. While the metaphor of financial auditing points to the important values of accountability, audit does more than monitor—it has a life of its own that jeopardizes the life it audits. The runaway character of assessment practices is analysed in terms of cultural practice. Higher education is intimately bound up with the origins of such practices, and is not just the latter day target of them. © 1997 by John Wiley & Sons, Ltd.",411.0
"The Third Industrial Revolution:: Technology, Productivity, and Income Inequality",0b5204d205643d14b06d1cb33eb816d16bd4d354,"[{'authorId': '2017289', 'name': 'Jeremy Greenwood'}]",1997.0,,['Unsolved Problems in ML Safety'],1,"The author examines periods of rapid technological change for coincidences of widening inequality and slowing productivity growth. He contends that while in the short run the introduction of technologies offers profits to investors and premiums for skilled workers, in the long run the rising tide of technological change lifts everybody's boat.",142.0
Failure mode and effect analysis : FMEA from theory to execution,7f318ef302c1b13a3436752defdefff759422e0d,"[{'authorId': '50060480', 'name': 'H. Schneider'}]",1996.0,,['Unsolved Problems in ML Safety'],1, ,1385.0
Programming Satan's Computer,3a47b96cda3453ef5f81c9da838a750aad339cbe,"[{'authorId': '144913176', 'name': 'Ross J. Anderson'}, {'authorId': '1703604', 'name': 'R. Needham'}]",1995.0,Computer Science Today,['Unsolved Problems in ML Safety'],1,,188.0
Smoking behavior of adolescents exposed to cigarette advertising.,727d45cf9bd24916afed5d18001ab07b0e7422f3,"[{'authorId': '6791511', 'name': 'G. Botvin'}, {'authorId': '145131443', 'name': 'C. Goldberg'}, {'authorId': '6322950', 'name': 'E. M. Botvin'}, {'authorId': '7573633', 'name': 'L. Dusenbury'}]",1993.0,Public health reports (1974),['Unsolved Problems in ML Safety'],1,"The purpose of the study was to explore the relationship between the exposure of adolescents in the seventh and eighth grades to cigarette advertising and their being smokers. A survey questionnaire given to 602 adolescents assessed their exposure to cigarette advertising and provided measures of their smoking behavior, demographic characteristics, and some psychosocial variables. The results indicated that exposure to cigarette advertising and having friends who smoked were predictive of current smoking status. Adolescents with high exposure to cigarette advertising were significantly more likely to be smokers, according to several measures of smoking behavior, than were those with low exposure to cigarette advertising. The findings extend previous research identifying factors that may play a role in the initiation and maintenance of smoking among adolescents.",112.0
Problems of Monetary Management: The UK Experience,0ae623749b30de53a39cf05813f5f3842e422c01,"[{'authorId': '83376155', 'name': 'C. Goodhart'}]",1984.0,,['Unsolved Problems in ML Safety'],1,,273.0
System Safety in Aircraft Acquisition,8bf4e1325c82ff510951c993b0974a14084fedef,"[{'authorId': '14771005', 'name': 'F. R. Frola'}, {'authorId': '2107320304', 'name': 'C. O. Miller'}]",1984.0,,"['Unsolved Problems in ML Safety', 'X-Risk Analysis for AI Research']",2,"Abstract : This report identifies management initiatives for strengthening the effectiveness of system safety in aircraft acquisition programs. More aggressive implementation of existing system safety policy can help reduce the mishap rate and the need for costly modification programs to correct safety deficiencies. Recommendations to OSD and the Military Departments include: (1) the need for continuing top management support; (2) staffing and funding for system safety efforts commensurate with responsibilities set forth in policy statements: (3) emphasis on the man-machine interface and the associated need for better coordination of system safety and human factors engineering activities; (4) improved methods for detecting system software hazards; (5) better utilization of advanced technology, including flight data recorders, ground proximity warning systems, and collision avoidance systems; (6) writing better contracts with respect to system safety tasks; and (7) more effective recruiting, training, and retention of system safety personnel.",18.0
The Methods of Ethics,3a7904ca8b44b0ebb92c13915183bef65da1aa4c,"[{'authorId': '115481530', 'name': 'G. J. Dalcourt'}]",1983.0,,"['Unsolved Problems in ML Safety', 'Superintelligence: Paths, Dangers, Strategies']",2,"The paper begins with an account of the intellectual background to Henry Sidgwick’s writing of his Methods of Ethics and an analysis of what Sidgwick meant by a ‘method’. His broad distinction between three main ethical theories – egoism, consequentialism, and deontology – is elucidated and accepted. Sidgwick’s different forms of intuitionism are explained, as are his criteria for testing the ‘certainty’ of a potentially self-evident belief. Section 3 discusses dogmatic intuitionism (common-sense morality systematized) and Sidgwick’s own view, in the light of his requirement for precision in ethics. The final section concerns the implications of Sidgwick’s position on disagreement for ethical theory. It is suggested that we have some knowledge in ethics, on which most converge, but not much. The paper concludes with a recommendation for a more eirenic and less dogmatic approach to philosophical ethics.",581.0
Integrative Complexity Theory and Forecasting International Crises,9cc06eafd18729f5f45effa2ea73a388db0b5a7c,"[{'authorId': '2053050108', 'name': 'Theodore D. Raphael'}]",1982.0,,['Unsolved Problems in ML Safety'],1,"Can integrative complexity theory be successfully employed to forecast international crises? To answer this question this study focuses on the conflict-prone relationship between the United States and USSR in Berlin, 1946 to 1962. Two hypotheses were tested. Hypothesis 1 predicted that significant declines in the cognitive complexity of U.S. and USSR foreign policy elites would be exhibited just prior to the onset of crises in June 1948 and June 1961. Hypothesis 2 predicted, however, that following the onset of crisis, the participants would exhibit patterns of rising complexity leading to peaceful conflict resolution. The results generally supported the hypotheses. First, significant declines in complexity occurred from 1946 to 1962 only in the two periods ending six months prior to the two major crises. Second, significant phase-to-phase increases in complexity were exhibited by the participants in both crises. The study provides initial evidence of the theory's utility for crisis forecasting.",57.0
"Utilitarianism, Economics, and Legal Theory",ce1697c07a83c8cfd51d031cf072f2bdc88b8e85,"[{'authorId': '96008731', 'name': 'E. Weinrib'}]",1980.0,,['Unsolved Problems in ML Safety'],1,,94.0
Systemantics: How Systems Work and Especially How They Fail,d6a70d0ec276d3ba74429b2e81312b5e37809a44,"[{'authorId': '153078714', 'name': 'J. Gall'}]",1977.0,,['Unsolved Problems in ML Safety'],1,,69.0
"THEORY OF THE FIRM: MANAGERIAL BEHAVIOR, AGENCY COSTS AND OWNERSHIP STRUCTURE",ae48c66d5e86722277c507f00f61256836b0ecee,"[{'authorId': '48109845', 'name': 'M. C. Jensen'}, {'authorId': '69007981', 'name': 'W. Meckling'}]",1976.0,,['Unsolved Problems in ML Safety'],1,,61638.0
A Theory of Justice,40b949055cb08461ecce261d2bd365e77b41f22e,"[{'authorId': '41186791', 'name': 'J. Rawls'}]",1971.0,Princeton Readings in Political Thought,"['Unsolved Problems in ML Safety', 'Superintelligence: Paths, Dangers, Strategies']",2,"John Rawls is Professor Emeritus at Harvard University. He is the author of the well-known and path breaking A Theory of Justice (Harvard, 1971) and the more recent work Political Liberalism (Columbia, 1996). These excerpts from A Theory of Justice provide a skeletal account of Rawls's project of using social contract theory to generate principles of justice for assigning basic rights and duties and determining the division of social benefits in a society. Rawls argues that the two principles that would be reached through an agreement in an original position of fairness and equality are 1) each person is to have an equal right to the most extensive basic liberty compatible with a similar liberty for others and 2) social and economic inequalities are to be arranged so that they are both a) reasonably expected to be to everyone's advantage; and b) attached to positions and offices open to all.",12268.0
Hedonic relativism and planning the good society,705b7748c08bfdd1808d76a6b10a37842a2482ef,"[{'authorId': '48718443', 'name': 'P. Brickman'}, {'authorId': '31766163', 'name': 'D. Campbell'}]",1971.0,,['Unsolved Problems in ML Safety'],1,,1534.0
Dysfunctional Consequences of Performance Measurements,3fa85e3758c9dce19408483a3b6ac5dcd6796a4f,"[{'authorId': '116833729', 'name': 'V. Ridgway'}]",1956.0,,['Unsolved Problems in ML Safety'],1,,348.0
Map-Colour Theorem,7d85e2a54f82f47054e110352da5ea782b05943e,"[{'authorId': '96665094', 'name': 'P. J. Heawood'}]",1949.0,,['Unsolved Problems in ML Safety'],1,,304.0
Unsolved Problems in ML Safety,05c2e1ee203be217f100d2da05bdcc52004f00b6,"[{'authorId': '3422872', 'externalIds': {'DBLP': ['Dan Hendrycks']}, 'url': 'https://www.semanticscholar.org/author/3422872', 'name': 'Dan Hendrycks', 'aliases': None, 'affiliations': ['UC Berkeley'], 'homepage': 'danhendrycks.com', 'paperCount': 49, 'citationCount': 11806, 'hIndex': 28}, {'authorId': '2483738', 'externalIds': {'DBLP': ['Nicholas Carlini']}, 'url': 'https://www.semanticscholar.org/author/2483738', 'name': 'Nicholas Carlini', 'aliases': None, 'affiliations': [], 'homepage': None, 'paperCount': 101, 'citationCount': 22448, 'hIndex': 40}, {'authorId': '47971768', 'externalIds': {'DBLP': ['John D. Schulman', 'John Schulman']}, 'url': 'https://www.semanticscholar.org/author/47971768', 'name': 'J. Schulman', 'aliases': ['J Schulman', 'John D. Schulman', 'John Schulman'], 'affiliations': [], 'homepage': None, 'paperCount': 68, 'citationCount': 38493, 'hIndex': 41}, {'authorId': '5164568', 'externalIds': {'DBLP': ['Jacob Steinhardt']}, 'url': 'https://www.semanticscholar.org/author/5164568', 'name': 'J. Steinhardt', 'aliases': ['J. Steinhardt', 'Jacob Steinhardt'], 'affiliations': [], 'homepage': None, 'paperCount': 98, 'citationCount': 7795, 'hIndex': 33}]",2021.0,arXiv.org,"['Unsolved Problems in ML Safety', 'X-Risk Analysis for AI Research']",2,"Machine learning (ML) systems are rapidly increasing in size, are acquiring new capabilities, and are increasingly deployed in high-stakes settings. As with other powerful technologies, safety for ML should be a leading research priority. In response to emerging safety challenges in ML, such as those introduced by recent large-scale models, we provide a new roadmap for ML Safety and refine the technical problems that the field needs to address. We present four problems ready for research, namely withstanding hazards (""Robustness""), identifying hazards (""Monitoring""), reducing inherent model hazards (""Alignment""), and reducing systemic hazards (""Systemic Safety""). Throughout, we clarify each problem's motivation and provide concrete research directions.",88.0
"Machine Learning Testing: Survey, Landscapes and Horizons",218062f45c15f39bc8f4fb2c930ddf20b5809b11,"[{'authorId': '51250527', 'name': 'J Zhang'}, {'authorId': '145836176', 'name': 'M. Harman'}, {'authorId': '2109704789', 'name': 'Lei Ma'}, {'authorId': '144440741', 'name': 'Yang Liu'}]",2019.0,IEEE Transactions on Software Engineering,['AI safety: state of the field through quantitative lens'],1,"This paper provides a comprehensive survey of techniques for testing machine learning systems; Machine Learning Testing (ML testing) research. It covers 144 papers on testing properties (e.g., correctness, robustness, and fairness), testing components (e.g., the data, learning program, and framework), testing workflow (e.g., test generation and test evaluation), and application scenarios (e.g., autonomous driving, machine translation). The paper also analyses trends concerning datasets, research trends, and research focus, concluding with research challenges and promising research directions in ML testing.",426.0
Fairness in Learning-Based Sequential Decision Algorithms: A Survey,5da5c07ab04d3ba005a1978426a6a4ab3fed18f4,"[{'authorId': '1845782244', 'name': 'Xueru Zhang'}, {'authorId': '39037167', 'name': 'M. Liu'}]",2020.0,Handbook of Reinforcement Learning and Control,['AI safety: state of the field through quantitative lens'],1,,28.0
"Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy",6a132499ca1dc0d23cfe7d5db841b819df63b51b,"[{'authorId': '1724490', 'name': 'Yogesh Kumar Dwivedi'}, {'authorId': '2066599148', 'name': 'Laurie Hughes'}, {'authorId': '3446873', 'name': 'Elvira Ismagilova'}, {'authorId': '91018104', 'name': 'G. Aarts'}, {'authorId': '2235927', 'name': 'C. Coombs'}, {'authorId': '2285182', 'name': 'Tom Crick'}, {'authorId': '144521858', 'name': 'Y. Duan'}, {'authorId': '1410968693', 'name': 'R. Dwivedi'}, {'authorId': '143698898', 'name': 'J. Edwards'}, {'authorId': '121714298', 'name': 'Aled Eirug'}, {'authorId': '26898704', 'name': 'Vassilis Galanos'}, {'authorId': '2937059', 'name': 'P. V. Ilavarasan'}, {'authorId': '50817520', 'name': 'M. Janssen'}, {'authorId': '2168272578', 'name': 'Paul Jones'}, {'authorId': '2733956', 'name': 'A. Kar'}, {'authorId': '41074539', 'name': 'Hatice Kizgin'}, {'authorId': '1418648774', 'name': 'Bianca Kronemann'}, {'authorId': '1696747', 'name': 'Banita Lal'}, {'authorId': '3094343', 'name': 'B. Lucini'}, {'authorId': '2575165', 'name': 'R. Medaglia'}, {'authorId': '1404797274', 'name': 'K. L. Meunier-FitzHugh'}, {'authorId': '1415234363', 'name': 'L. L. Meunier-FitzHugh'}, {'authorId': '47033159', 'name': 'S. Misra'}, {'authorId': '52563419', 'name': 'Emmanuel Mogaji'}, {'authorId': '2038468629', 'name': 'S. Sharma'}, {'authorId': '153278840', 'name': 'Jang B. Singh'}, {'authorId': '35098701', 'name': 'Vishnupriya Raghavan'}, {'authorId': '2775338', 'name': 'R. Raman'}, {'authorId': '1688149', 'name': 'N. Rana'}, {'authorId': '2032434', 'name': 'Spyridon Samothrakis'}, {'authorId': '66781543', 'name': 'Jak Spencer'}, {'authorId': '2191954', 'name': 'K. Tamilmani'}, {'authorId': '102927759', 'name': 'Annie Tubadji'}, {'authorId': '144340154', 'name': 'P. Walton'}, {'authorId': '2116399602', 'name': 'Michael D. Williams'}]",2019.0,International Journal of Information Management,"['AI safety: state of the field through quantitative lens', 'Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",2,,670.0
A Survey on Bias and Fairness in Machine Learning,0090023afc66cd2741568599057f4e82b566137c,"[{'authorId': '51997673', 'name': 'Ninareh Mehrabi'}, {'authorId': '2775559', 'name': 'Fred Morstatter'}, {'authorId': '51884035', 'name': 'N. Saxena'}, {'authorId': '1782658', 'name': 'Kristina Lerman'}, {'authorId': '143728483', 'name': 'A. Galstyan'}]",2019.0,ACM Computing Surveys,['AI safety: state of the field through quantitative lens'],1,"With the widespread use of artificial intelligence (AI) systems and applications in our everyday lives, accounting for fairness has gained significant importance in designing and engineering of such systems. AI systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that these decisions do not reflect discriminatory behavior toward certain groups or populations. More recently some work has been developed in traditional machine learning and deep learning that address such challenges in different subdomains. With the commercialization of these systems, researchers are becoming more aware of the biases that these applications can contain and are attempting to address them. In this survey, we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning researchers have defined to avoid the existing bias in AI systems. In addition to that, we examined different domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and ways they have tried to address them. There are still many future directions and solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will motivate researchers to tackle these issues in the near future by observing existing work in their respective fields.",1708.0
Extracting Relational Explanations From Deep Neural Networks: A Survey From a Neural-Symbolic Perspective,41c1fe3e822507c4f1e374f6a35c43a5c72d4899,"[{'authorId': '152446281', 'name': 'J. Townsend'}, {'authorId': '1403569489', 'name': 'Thomas Chaton'}, {'authorId': '144361306', 'name': 'J. Monteiro'}]",2020.0,IEEE Transactions on Neural Networks and Learning Systems,['AI safety: state of the field through quantitative lens'],1,"The term “explainable AI” refers to the goal of producing artificially intelligent agents that are capable of providing explanations for their decisions. Some models (e.g., rule-based systems) are designed to be explainable, while others are less explicit “black boxes” for which their reasoning remains a mystery. One example of the latter is the neural network, and over the past few decades, researchers in the field of neural-symbolic integration (NSI) have sought to extract relational knowledge from such networks. Extraction from deep neural networks, however, has remained a challenge until recent years in which many methods of extracting distinct, salient features from input or hidden feature spaces of deep neural networks have been proposed. Furthermore, methods of identifying relationships between these features have also emerged. This article presents examples of old and new developments in extracting relational explanations in order to argue that the latter have analogies in the former and, as such, can be described in terms of long-established taxonomies and frameworks presented in early neural-symbolic literature. We also outline potential future research directions that come to light from this refreshed perspective.",32.0
"Universals and variations in moral decisions made in 42 countries by 70,000 participants",260da071bd9238241ea783a1ced85ab9d4098a4c,"[{'authorId': '38568367', 'name': 'E. Awad'}, {'authorId': '2561756', 'name': 'S. Dsouza'}, {'authorId': '1999088', 'name': 'A. Shariff'}, {'authorId': '1705156', 'name': 'I. Rahwan'}, {'authorId': '3107309', 'name': 'Jean‐François Bonnefon'}]",2020.0,Proceedings of the National Academy of Sciences of the United States of America,['AI safety: state of the field through quantitative lens'],1,"Significance We report the largest cross-cultural study of moral preferences in sacrificial dilemmas, that is, the circumstances under which people find it acceptable to sacrifice one life to save several. On the basis of 70,000 responses to three dilemmas, collected in 10 languages and 42 countries, we document a universal qualitative pattern of preferences together with substantial country-level variations in the strength of these preferences. In particular, we document a strong association between low relational mobility (where people are more cautious about not alienating their current social partners) and the tendency to reject sacrifices for the greater good—which may be explained by the positive social signal sent by such a rejection. We make our dataset publicly available for researchers. When do people find it acceptable to sacrifice one life to save many? Cross-cultural studies suggested a complex pattern of universals and variations in the way people approach this question, but data were often based on small samples from a small number of countries outside of the Western world. Here we analyze responses to three sacrificial dilemmas by 70,000 participants in 10 languages and 42 countries. In every country, the three dilemmas displayed the same qualitative ordering of sacrifice acceptability, suggesting that this ordering is best explained by basic cognitive processes rather than cultural norms. The quantitative acceptability of each sacrifice, however, showed substantial country-level variations. We show that low relational mobility (where people are more cautious about not alienating their current social partners) is strongly associated with the rejection of sacrifices for the greater good (especially for Eastern countries), which may be explained by the signaling value of this rejection. We make our dataset fully available as a public resource for researchers studying universals and variations in human morality.",76.0
"Learning from Learning Machines: Optimisation, Rules, and Social Norms",080ec83e004f90e67405ee034b84134f978bb949,"[{'authorId': '46258927', 'name': 'Travis LaCroix'}, {'authorId': '1751762', 'name': 'Yoshua Bengio'}]",2019.0,arXiv.org,['AI safety: state of the field through quantitative lens'],1,"There is an analogy between machine learning systems and economic entities in that they are both adaptive, and their behaviour is specified in a more-or-less explicit way. It appears that the area of AI that is most analogous to the behaviour of economic entities is that of morally good decision-making, but it is an open question as to how precisely moral behaviour can be achieved in an AI system. This paper explores the analogy between these two complex systems, and we suggest that a clearer understanding of this apparent analogy may help us forward in both the socio-economic domain and the AI domain: known results in economics may help inform feasible solutions in AI safety, but also known results in AI may inform economic policy. If this claim is correct, then the recent successes of deep learning for AI suggest that more implicit specifications work better than explicit ones for solving such problems.",4.0
Artificial Moral Agents: A Survey of the Current Status,a737220807f6827913c573d9f9473ef410c672fd,"[{'authorId': '144875992', 'name': 'José-Antonio Cervantes'}, {'authorId': '2071038030', 'name': 'Sonia López'}, {'authorId': '40428623', 'name': 'Luis-Felipe Rodríguez'}, {'authorId': '145613399', 'name': 'Salvador Cervantes'}, {'authorId': '2071223193', 'name': 'Francisco Cervantes'}, {'authorId': '145054210', 'name': 'F. Ramos'}]",2019.0,Science and Engineering Ethics,['AI safety: state of the field through quantitative lens'],1,,60.0
"Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI",530a059cb48477ad1e3d4f8f4b153274c8997332,"[{'authorId': '1379511816', 'name': 'Alejandro Barredo Arrieta'}, {'authorId': '2058921025', 'name': 'Natalia Díaz Rodríguez'}, {'authorId': '9221552', 'name': 'J. Ser'}, {'authorId': '1379511786', 'name': 'Adrien Bennetot'}, {'authorId': '3030006', 'name': 'S. Tabik'}, {'authorId': '50449165', 'name': 'A. Barbado'}, {'authorId': '39558258', 'name': 'S. García'}, {'authorId': '1402195255', 'name': 'S. Gil-Lopez'}, {'authorId': '145337392', 'name': 'D. Molina'}, {'authorId': '2445552', 'name': 'Richard Benjamins'}, {'authorId': '2091924780', 'name': 'Raja Chatila'}, {'authorId': '2098723448', 'name': 'Francisco Herrera'}]",2019.0,Information Fusion,['AI safety: state of the field through quantitative lens'],1,,2687.0
Mapping the public debate on ethical concerns: algorithms in mainstream media,d0587699e216eb969f28a93f8c53ed47142c1ae2,"[{'authorId': '1807719', 'name': 'B. Barn'}]",2019.0,"Journal of Information, Communication and Ethics in Society",['AI safety: state of the field through quantitative lens'],1,"
Purpose
Algorithms are in the mainstream media news on an almost daily basis. Their context is invariably artificial intelligence (AI) and machine learning decision-making. In media articles, algorithms are described as powerful, autonomous actors that have a capability of producing actions that have consequences. Despite a tendency for deification, the prevailing critique of algorithms focuses on ethical concerns raised by decisions resulting from algorithmic processing. However, the purpose of this paper is to propose that the ethical concerns discussed are limited in scope and suggest that it is not clear what concerns dominate the debate.


Design/methodology/approach
The paper uses a systematic mapping study approach to review articles appearing in leading UK national papers from the perspective of the ethical concerns over a period of one year. The articles are categorised using a widely cited framework detailing a taxonomy of ethical concerns. The UK context is important because of UK public policy initiatives around AI.


Findings
The research presented in this paper contributes the first systematic mapping study of articles appearing in leading UK national papers from the perspective of widely accepted ethical concerns such as inscrutable evidence, misguided evidence, unfair outcomes and transformative effects.


Originality/value
The research presented in this paper contributes the first systematic mapping study of articles appearing in leading UK national papers from the perspective of the ethical concerns. The UK context is important because of UK public policy initiatives around AI. To review the media content from the perspective of ethical concerns, this paper uses the synthesised conceptual map of ethical concerns developed by Mittelstad et al. Given the dominance of that framework, this paper’s contribution is also an important instantiation and experimental validation of using that conceptual map.
",12.0
Adversarial Examples: Opportunities and Challenges,1879d6b29eee6efab8f6217a7a6f47ec04f25b3e,"[{'authorId': '50560753', 'name': 'Jiliang Zhang'}, {'authorId': '2116520733', 'name': 'Chen Li'}]",2018.0,IEEE Transactions on Neural Networks and Learning Systems,['AI safety: state of the field through quantitative lens'],1,"Deep neural networks (DNNs) have shown huge superiority over humans in image recognition, speech processing, autonomous vehicles, and medical diagnosis. However, recent studies indicate that DNNs are vulnerable to adversarial examples (AEs), which are designed by attackers to fool deep learning models. Different from real examples, AEs can mislead the model to predict incorrect outputs while hardly be distinguished by human eyes, therefore threaten security-critical deep-learning applications. In recent years, the generation and defense of AEs have become a research hotspot in the field of artificial intelligence (AI) security. This article reviews the latest research progress of AEs. First, we introduce the concept, cause, characteristics, and evaluation metrics of AEs, then give a survey on the state-of-the-art AE generation methods with the discussion of advantages and disadvantages. After that, we review the existing defenses and discuss their limitations. Finally, future research opportunities and challenges on AEs are prospected.",123.0
The Debate on the Ethics of AI in Health Care: a Reconstruction and Critical Review,600ac5a6810496536627b725079b4e5fd48c83da,"[{'authorId': '1751614630', 'name': 'J. Morley'}, {'authorId': '2081272022', 'name': 'C. Machado'}, {'authorId': '1483617437', 'name': 'C. Burr'}, {'authorId': '3011486', 'name': 'Josh Cowls'}, {'authorId': '2084659', 'name': 'M. Taddeo'}, {'authorId': '1982425', 'name': 'L. Floridi'}]",2019.0,Social Science Research Network,['AI safety: state of the field through quantitative lens'],1,"Healthcare systems across the globe are struggling with increasing costs and worsening outcomes. This presents those responsible for overseeing healthcare with a challenge. Increasingly, policymakers, politicians, clinical entrepreneurs and computer and data scientists argue that a key part of the solution will be ‘Artificial Intelligence’ (AI) – particularly Machine Learning (ML). This argument stems not from the belief that all healthcare needs will soon be taken care of by “robot doctors.” Instead, it is an argument that rests on the classic counterfactual definition of AI as an umbrella term for a range of techniques that can be used to make machines complete tasks in a way that would be considered intelligent were they to be completed by a human. Automation of this nature could offer great opportunities for the improvement of healthcare services and ultimately patients’ health by significantly improving human clinical capabilities in diagnosis, drug discovery, epidemiology, personalised medicine, and operational efficiency. However, if these AI solutions are to be embedded in clinical practice, then at least three issues need to be considered: the technical possibilities and limitations; the ethical, regulatory and legal framework; and the governance framework. In this article, we report on the results of a systematic analysis designed to provide a clear overview of the second of these elements: the ethical, regulatory and legal framework. We find that ethical issues arise at six levels of abstraction (individual, interpersonal, group, institutional, sectoral, and societal) and can be categorised as epistemic, normative, or overarching. We conclude by stressing how important it is that the ethical challenges raised by implementing AI in healthcare settings are tackled proactively rather than reactively and map the key considerations for policymakers to each of the ethical concerns highlighted.",26.0
The global landscape of AI ethics guidelines,35ebed28c967acbfe38fa757f4e7023d075beaeb,"[{'authorId': '3458650', 'name': 'Anna Jobin'}, {'authorId': '3453968', 'name': 'M. Ienca'}, {'authorId': '145104974', 'name': 'E. Vayena'}]",2019.0,Nature Machine Intelligence,['AI safety: state of the field through quantitative lens'],1,,886.0
The security of machine learning in an adversarial setting: A survey,164591c72517124262af7903fad4e2d1661c3d3e,"[{'authorId': '1524733684', 'name': 'Xianmin Wang'}, {'authorId': '2152907883', 'name': 'Jing Li'}, {'authorId': '2084690', 'name': 'Xiaohui Kuang'}, {'authorId': '2182387', 'name': 'Yu‐an Tan'}, {'authorId': '2118399204', 'name': 'Jin Li'}]",2019.0,J. Parallel Distributed Comput.,['AI safety: state of the field through quantitative lens'],1,,114.0
Global AI Ethics: A Review of the Social Impacts and Ethical Implications of Artificial Intelligence,b5344d0c1281b91c223837334ae20d457666ca20,"[{'authorId': '2122918159', 'name': 'Alexa Hagerty'}, {'authorId': '108375030', 'name': 'I. Rubinov'}]",2019.0,arXiv.org,['AI safety: state of the field through quantitative lens'],1,"The ethical implications and social impacts of artificial intelligence have become topics of compelling interest to industry, researchers in academia, and the public. However, current analyses of AI in a global context are biased toward perspectives held in the U.S., and limited by a lack of research, especially outside the U.S. and Western Europe. 
This article summarizes the key findings of a literature review of recent social science scholarship on the social impacts of AI and related technologies in five global regions. Our team of social science researchers reviewed more than 800 academic journal articles and monographs in over a dozen languages. 
Our review of the literature suggests that AI is likely to have markedly different social impacts depending on geographical setting. Likewise, perceptions and understandings of AI are likely to be profoundly shaped by local cultural and social context. 
Recent research in U.S. settings demonstrates that AI-driven technologies have a pattern of entrenching social divides and exacerbating social inequality, particularly among historically-marginalized groups. Our literature review indicates that this pattern exists on a global scale, and suggests that low- and middle-income countries may be more vulnerable to the negative social impacts of AI and less likely to benefit from the attendant gains. 
We call for rigorous ethnographic research to better understand the social impacts of AI around the world. Global, on-the-ground research is particularly critical to identify AI systems that may amplify social inequality in order to mitigate potential harms. Deeper understanding of the social impacts of AI in diverse social settings is a necessary precursor to the development, implementation, and monitoring of responsible and beneficial AI technologies, and forms the basis for meaningful regulation of these technologies.",34.0
Classification Schemas for Artificial Intelligence Failures,6ff7df5f32fca05658ebb10243b0a8a3831aba20,"[{'authorId': '40188913', 'name': 'P. Scott'}, {'authorId': '1976753', 'name': 'Roman V Yampolskiy'}]",2019.0,Delphi - Interdisciplinary Review of Emerging Technologies,['AI safety: state of the field through quantitative lens'],1,In this paper we examine historical failures of artificial intelligence (AI) and propose a classification scheme for categorizing future failures. By doing so we hope that (a) the responses to future failures can be improved through applying a systematic classification that can be used to simplify the choice of response and (b) future failures can be reduced through augmenting development lifecycles with targeted risk assessments.,13.0
Unexplainability and Incomprehensibility of Artificial Intelligence,d8749900eb28a92182cf11f53a4ae1842b2eb31b,"[{'authorId': '1976753', 'name': 'Roman V Yampolskiy'}]",2019.0,arXiv.org,['AI safety: state of the field through quantitative lens'],1,"Explainability and comprehensibility of AI are important requirements for intelligent systems deployed in real-world domains. Users want and frequently need to understand how decisions impacting them are made. Similarly it is important to understand how an intelligent system functions for safety and security reasons. In this paper, we describe two complementary impossibility results (Unexplainability and Incomprehensibility), essentially showing that advanced AIs would not be able to accurately explain some of their decisions and for the decisions they could explain people would not understand some of those explanations.",18.0
"From What to How: An Initial Review of Publicly Available AI Ethics Tools, Methods and Research to Translate Principles into Practices",7700d89c3a2c898daadd111372566f7146310f5a,"[{'authorId': '1751614630', 'name': 'J. Morley'}, {'authorId': '1982425', 'name': 'L. Floridi'}, {'authorId': '40901846', 'name': 'Libby Kinsey'}, {'authorId': '2600242', 'name': 'Anat Elhalal'}]",2019.0,Science and Engineering Ethics,['AI safety: state of the field through quantitative lens'],1,,276.0
An instrument to evaluate the maturity of bias governance capability in artificial intelligence projects,670e2331f78028f14ccfc94399eba72eccd53642,"[{'authorId': '121254591', 'name': 'D. Coates'}, {'authorId': '2111203189', 'name': 'Andrew Martin'}]",2019.0,IBM Journal of Research and Development,['AI safety: state of the field through quantitative lens'],1,"Artificial intelligence (AI) promises unprecedented contributions to both business and society, attracting a surge of interest from many organizations. However, there is evidence that bias is already prevalent in AI datasets and algorithms, which, albeit unintended, is considered to be unethical, suboptimal, unsustainable, and challenging to manage. It is believed that the governance of data and algorithmic bias must be deeply embedded in the values, mindsets, and procedures of AI software development teams, but currently there is a paucity of actionable mechanisms to help. In this paper, we describe a maturity framework based on ethical principles and best practices, which can be used to evaluate an organization's capability to govern bias. We also design, construct, validate, and test an original instrument for operationalizing the framework, which considers both technical and organizational aspects. The instrument has been developed and validated through a two-phase study involving field experts and academics. The framework and instrument are presented for ongoing evolution and utilization.",4.0
Explainability in human–agent systems,e0f7763c0da21ea3180165fa09be97cf5c62d40e,"[{'authorId': '40110198', 'name': 'A. Rosenfeld'}, {'authorId': '2817130', 'name': 'A. Richardson'}]",2019.0,Autonomous Agents and Multi-Agent Systems,['AI safety: state of the field through quantitative lens'],1,,129.0
Inside the Organization: Why Privacy and Security Engineering Is a Challenge for Engineers,be9a5b9a921f945840c6b49e3cd14ade8b7b53cc,"[{'authorId': '1681853', 'name': 'S. Spiekermann'}, {'authorId': '2402448', 'name': 'Jana Korunovska'}, {'authorId': '47085377', 'name': 'Marc Langheinrich'}]",2018.0,Proceedings of the IEEE,['AI safety: state of the field through quantitative lens'],1,"Machine ethics is a key challenge in times when digital systems play an increasing role in people’s lives. At the core of machine ethics is the handling of personal data and the security of machine operations. Yet, privacy and security engineering are a challenge in today’s business world where personal data markets, corporate deadlines, and a lack of perfectionism frame the context in which engineers need to work. Besides these organizational and market challenges, each engineer has his or her specific view on the importance of these values that can foster or inhibit taking them into consideration. We present the results of an empirical study of 124 engineers based on the Theory of Planned Behavior and Jonas’ Principle of Responsibility to understand the drivers and impediments of ethical system development as far as privacy and security engineering are concerned. We find that many engineers find the two values important, but do not enjoy working on them. We also find that many struggle with the organizational environment: They face a lack of time and autonomy that is necessary for building ethical systems, even at this basic level. Organizations’ privacy and security norms are often too weak or even oppose value-based design, putting engineers in conflict with their organizations. Our data indicate that it is largely engineers’ individually perceived responsibility as well as a few character traits that make a positive difference to ethical system development.",40.0
A Survey of Methods for Explaining Black Box Models,f7325d232c7ac7d2daaf6605377058db5b5b83cc,"[{'authorId': '1704327', 'name': 'Riccardo Guidotti'}, {'authorId': '2296962', 'name': 'A. Monreale'}, {'authorId': '1707206', 'name': 'F. Turini'}, {'authorId': '1693341', 'name': 'D. Pedreschi'}, {'authorId': '1685102', 'name': 'F. Giannotti'}]",2018.0,ACM Computing Surveys,['AI safety: state of the field through quantitative lens'],1,"In recent years, many accurate decision support systems have been constructed as black boxes, that is as systems that hide their internal logic to the user. This lack of explanation constitutes both a practical and an ethical issue. The literature reports many approaches aimed at overcoming this crucial weakness, sometimes at the cost of sacrificing accuracy for interpretability. The applications in which black box decision systems can be used are various, and each approach is typically developed to provide a solution for a specific problem and, as a consequence, it explicitly or implicitly delineates its own definition of interpretability and explanation. The aim of this article is to provide a classification of the main problems addressed in the literature with respect to the notion of explanation and the type of black box system. Given a problem definition, a black box type, and a desired explanation, this survey should help the researcher to find the proposals more useful for his own work. The proposed classification of approaches to open black box models should also be useful for putting the many research open questions in perspective.",2497.0
Adversarial Examples: Attacks and Defenses for Deep Learning,03a507a0876c7e1a26608358b1a9dd39f1eb08e0,"[{'authorId': '2115844647', 'name': 'Xiaoyong Yuan'}, {'authorId': '50462511', 'name': 'Pan He'}, {'authorId': '22317545', 'name': 'Qile Zhu'}, {'authorId': '2108673454', 'name': 'Xiaolin Li'}]",2017.0,IEEE Transactions on Neural Networks and Learning Systems,['AI safety: state of the field through quantitative lens'],1,"With rapid progress and significant successes in a wide spectrum of applications, deep learning is being applied in many safety-critical environments. However, deep neural networks (DNNs) have been recently found vulnerable to well-designed input samples called adversarial examples. Adversarial perturbations are imperceptible to human but can easily fool DNNs in the testing/deploying stage. The vulnerability to adversarial examples becomes one of the major risks for applying DNNs in safety-critical environments. Therefore, attacks and defenses on adversarial examples draw great attention. In this paper, we review recent findings on adversarial examples for DNNs, summarize the methods for generating adversarial examples, and propose a taxonomy of these methods. Under the taxonomy, applications for adversarial examples are investigated. We further elaborate on countermeasures for adversarial examples. In addition, three major challenges in adversarial examples and the potential solutions are discussed.",1182.0
"“Dave...I can assure you ...that it’s going to be all right ...” A Definition, Case for, and Survey of Algorithmic Assurances in Human-Autonomy Trust Relationships",12d4522b3f24a8a5c69f8a38f2c632d240cbda10,"[{'authorId': '2426676', 'name': 'Brett W. Israelsen'}, {'authorId': '143898375', 'name': 'N. Ahmed'}]",2017.0,ACM Computing Surveys,['AI safety: state of the field through quantitative lens'],1,"People who design, use, and are affected by autonomous artificially intelligent agents want to be able to trust such agents—that is, to know that these agents will perform correctly, to understand the reasoning behind their actions, and to know how to use them appropriately. Many techniques have been devised to assess and influence human trust in artificially intelligent agents. However, these approaches are typically ad hoc and have not been formally related to each other or to formal trust models. This article presents a survey of algorithmic assurances, i.e., programmed components of agent operation that are expressly designed to calibrate user trust in artificially intelligent agents. Algorithmic assurances are first formally defined and classified from the perspective of formally modeled human-artificially intelligent agent trust relationships. Building on these definitions, a synthesis of research across communities such as machine learning, human-computer interaction, robotics, e-commerce, and others reveals that assurance algorithms naturally fall along a spectrum in terms of their impact on an agent’s core functionality, with seven notable classes ranging from integral assurances (which impact an agent’s core functionality) to supplemental assurances (which have no direct effect on agent performance). Common approaches within each of these classes are identified and discussed; benefits and drawbacks of different approaches are also investigated.",55.0
Guidelines for Artificial Intelligence Containment,b0c820f9d7fa4d8c56a7887548834b7ba65ddfa5,"[{'authorId': '143727143', 'name': 'James Babcock'}, {'authorId': '50458432', 'name': 'János Kramár'}, {'authorId': '1976753', 'name': 'Roman V Yampolskiy'}]",2017.0,Next-Generation Ethics,['AI safety: state of the field through quantitative lens'],1,"With almost daily improvements in capabilities of artificial intelligence it is more important than ever to develop safety software for use by the AI research community. Building on our previous work on AI Containment Problem we propose a number of guidelines which should help AI safety researchers to develop reliable sandboxing software for intelligent programs of all levels. Such safety container software will make it possible to study and analyze intelligent artificial agent while maintaining certain level of safety against information leakage, social engineering attacks and cyberattacks from within the container.",26.0
Relevance of Ethical Guidelines for Artificial Intelligence - a Survey and Evaluation,98a6235dda76a6efe0d15202313c53354fe2606a,"[{'authorId': '14607628', 'name': 'L. Rothenberger'}, {'authorId': '144457676', 'name': 'Benjamin Fabian'}, {'authorId': '133873804', 'name': 'Elmar Arunov'}]",2019.0,European Conference on Information Systems,['AI safety: state of the field through quantitative lens'],1,"Ethics for artificial intelligence (AI) is a topic of growing practical relevance. Many people seem to believe that AI could render jobs obsolete in the future. Others wonder who is in charge for the actions of AI systems they encounter. Providing and prioritizing ethical guidelines for AI is therefore an important measure for providing safeguards and increasing the acceptance of this technology. The aim of this research is to survey ethical guidelines for the handling of AI in the ICT industry and evaluate them with respect to their relevance. For this goal, first, an overview of AI ethics is derived from the literature, with a focus on classical Western ethical theories. From this, a candidate set of important ethical guidelines is developed. Then, qualitative interviews with experts are conducted for in-depth feedback and ranking of these guidelines. Furthermore, an online survey is performed in order to more representatively weight the ethical guidelines in terms of importance among a broader audience. Combining both studies, a prioritization matrix is created using the weights from the experts and the survey participants in order to synthesize their votes. Based on this, a ranked catalogue of ethical guidelines for AI is created, and novel avenues for research on AI ethics are presented.",19.0
Transparency and Algorithmic Governance,68ab57501a72a1db65877b97bad3959da7f15c4a,"[{'authorId': '1740052', 'name': 'C. Coglianese'}, {'authorId': '2081973974', 'name': 'David Lehr'}]",2018.0,,['AI safety: state of the field through quantitative lens'],1,"Machine-learning algorithms are improving and automating important functions in medicine, transportation, and business. Government officials have also started to take notice of the accuracy and speed that such algorithms provide, increasingly relying on them to aid with consequential public-sector functions, including tax administration, regulatory oversight, and benefits administration. Despite machine-learning algorithms’ superior predictive power over conventional analytic tools, algorithmic forecasts are difficult to understand and explain. Machine learning’s “black-box” nature has thus raised concern: Can algorithmic governance be squared with legal principles of governmental transparency? We analyze this question and conclude that machine-learning algorithms’ relative inscrutability does not pose a legal barrier to their responsible use by governmental authorities. We distinguish between principles of “fishbowl transparency” and “reasoned transparency,” explaining how both are implicated by algorithmic governance but also showing that neither conception compels anything close to total transparency. Although machine learning’s black-box features distinctively implicate notions of reasoned transparency, legal demands for reason-giving can be satisfied by explaining an algorithm’s purpose, design, and basic functioning. Furthermore, new technical advances will only make machine-learning algorithms increasingly more explainable. Algorithmic governance can meet both legal and public demands for transparency while still enhancing accuracy, efficiency, and even potentially legitimacy in government.",40.0
The Moral Machine experiment,06e5770e74860bd2dcc95b5dbe3d1a411950906c,"[{'authorId': '38568367', 'name': 'E. Awad'}, {'authorId': '2561756', 'name': 'S. Dsouza'}, {'authorId': '2054541342', 'name': 'Richard Kim'}, {'authorId': '48189383', 'name': 'Jonathan F. Schulz'}, {'authorId': '2300422', 'name': 'J. Henrich'}, {'authorId': '1999088', 'name': 'A. Shariff'}, {'authorId': '3107309', 'name': 'Jean‐François Bonnefon'}, {'authorId': '1705156', 'name': 'I. Rahwan'}]",2018.0,Nature,"['AI safety: state of the field through quantitative lens', 'Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",2,,824.0
Supervising strong learners by amplifying weak experts,0052b31f07eda7737b5e0e2bf3803c3a32f3f728,"[{'authorId': '145791315', 'name': 'P. Christiano'}, {'authorId': '79384063', 'name': 'Buck Shlegeris'}, {'authorId': '2698777', 'name': 'Dario Amodei'}]",2018.0,arXiv.org,['AI safety: state of the field through quantitative lens'],1,"Many real world learning tasks involve complex or hard-to-specify objectives, and using an easier-to-specify proxy can lead to poor performance or misaligned behavior. One solution is to have humans provide a training signal by demonstrating or judging performance, but this approach fails if the task is too complicated for a human to directly evaluate. We propose Iterated Amplification, an alternative training strategy which progressively builds up a training signal for difficult problems by combining solutions to easier subproblems. Iterated Amplification is closely related to Expert Iteration (Anthony et al., 2017; Silver et al., 2017), except that it uses no external reward function. We present results in algorithmic environments, showing that Iterated Amplification can efficiently learn complex behaviors.",55.0
"Governing artificial intelligence: ethical, legal and technical opportunities and challenges",e95efbe0fbe496e61da17cedacbe4357cf9db57c,"[{'authorId': '7794798', 'name': 'Corinne Cath'}]",2018.0,"Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences",['AI safety: state of the field through quantitative lens'],1,"This paper is the introduction to the special issue entitled: ‘Governing artificial intelligence: ethical, legal and technical opportunities and challenges'. Artificial intelligence (AI) increasingly permeates every aspect of our society, from the critical, like urban infrastructure, law enforcement, banking, healthcare and humanitarian aid, to the mundane like dating. AI, including embodied AI in robotics and techniques like machine learning, can improve economic, social welfare and the exercise of human rights. Owing to the proliferation of AI in high-risk areas, the pressure is mounting to design and govern AI to be accountable, fair and transparent. How can this be achieved and through which frameworks? This is one of the central questions addressed in this special issue, in which eight authors present in-depth analyses of the ethical, legal-regulatory and technical challenges posed by developing governance regimes for AI systems. It also gives a brief overview of recent developments in AI governance, how much of the agenda for defining AI regulation, ethical frameworks and technical approaches is set, as well as providing some concrete suggestions to further the debate on AI governance. This article is part of the theme issue ‘Governing artificial intelligence: ethical, legal, and technical opportunities and challenges’.",184.0
Adversarial Attacks and Defences: A Survey,869fdb53a40290a3941fd6ab808835e9b5184d62,"[{'authorId': '2055809732', 'name': 'Anirban Chakraborty'}, {'authorId': '2109936241', 'name': 'Manaar Alam'}, {'authorId': '38592060', 'name': 'Vishal Dey'}, {'authorId': '144990091', 'name': 'A. Chattopadhyay'}, {'authorId': '1705579', 'name': 'Debdeep Mukhopadhyay'}]",2018.0,arXiv.org,['AI safety: state of the field through quantitative lens'],1,"Deep learning has emerged as a strong and efficient framework that can be applied to a broad spectrum of complex learning problems which were difficult to solve using the traditional machine learning techniques in the past. In the last few years, deep learning has advanced radically in such a way that it can surpass human-level performance on a number of tasks. As a consequence, deep learning is being extensively used in most of the recent day-to-day applications. However, security of deep learning systems are vulnerable to crafted adversarial examples, which may be imperceptible to the human eye, but can lead the model to misclassify the output. In recent times, different types of adversaries based on their threat model leverage these vulnerabilities to compromise a deep learning system where adversaries have high incentives. Hence, it is extremely important to provide robustness to deep learning algorithms against these adversaries. However, there are only a few strong countermeasures which can be used in all types of attack scenarios to design a robust deep learning system. In this paper, we attempt to provide a detailed discussion on different types of adversarial attacks with various threat models and also elaborate the efficiency and challenges of recent countermeasures against them.",429.0
Peeking Inside the Black-Box: A Survey on Explainable Artificial Intelligence (XAI),21dff47a4142445f83016da0819ffe6dd2947f66,"[{'authorId': '9139705', 'name': 'Amina Adadi'}, {'authorId': '50487490', 'name': 'M. Berrada'}]",2018.0,IEEE Access,"['AI safety: state of the field through quantitative lens', 'Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",2,"At the dawn of the fourth industrial revolution, we are witnessing a fast and widespread adoption of artificial intelligence (AI) in our daily life, which contributes to accelerating the shift towards a more algorithmic society. However, even with such unprecedented advancements, a key impediment to the use of AI-based systems is that they often lack transparency. Indeed, the black-box nature of these systems allows powerful predictions, but it cannot be directly explained. This issue has triggered a new debate on explainable AI (XAI). A research field holds substantial promise for improving trust and transparency of AI-based systems. It is recognized as the sine qua non for AI to continue making steady progress without disruption. This survey provides an entry point for interested researchers and practitioners to learn key aspects of the young and rapidly growing body of research related to XAI. Through the lens of the literature, we review the existing approaches regarding the topic, discuss trends surrounding its sphere, and present major research trajectories.",2224.0
Explainable artificial intelligence: A survey,d2d79513f32c4d09b6255b18514d7ad07ebf43fe,"[{'authorId': '2214998955', 'name': 'Filip Karlo Dosilovic'}, {'authorId': '35167635', 'name': 'Mario Brčič'}, {'authorId': '1791663', 'name': 'N. Hlupic'}]",2018.0,"International Convention on Information and Communication Technology, Electronics and Microelectronics",['AI safety: state of the field through quantitative lens'],1,"In the last decade, with availability of large datasets and more computing power, machine learning systems have achieved (super)human performance in a wide variety of tasks. Examples of this rapid development can be seen in image recognition, speech analysis, strategic game planning and many more. The problem with many state-of-the-art models is a lack of transparency and interpretability. The lack of thereof is a major drawback in many applications, e.g. healthcare and finance, where rationale for model's decision is a requirement for trust. In the light of these issues, explainable artificial intelligence (XAI) has become an area of interest in research community. This paper summarizes recent developments in XAI in supervised learning, starts a discussion on its connection with artificial general intelligence, and gives proposals for further research directions.",542.0
Asking 'Why' in AI: Explainability of intelligent systems - perspectives and challenges,434be8e804a79843377d1b8d94d1fcde2feb4ad2,"[{'authorId': '1762890', 'name': 'A. Preece'}]",2018.0,Intell. Syst. Account. Finance Manag.,"['AI safety: state of the field through quantitative lens', 'Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",2,"Recent rapid progress in machine learning (ML), particularly soâ€ called â€˜deep learningâ€™, has led to a resurgence in interest in explainability of artificial intelligence (AI) systems, reviving an area of research dating back to the 1970s. The aim of this article is to view current issues concerning MLâ€ based AI systems from the perspective of classical AI, showing that the fundamental problems are far from new, and arguing that elements of that earlier work offer routes to making progress towards explainable AI today.",86.0
Threat of Adversarial Attacks on Deep Learning in Computer Vision: A Survey,b514949ad8344071c0f342f182390d2d88bcc26d,"[{'authorId': '47398812', 'name': 'Naveed Akhtar'}, {'authorId': '1747500', 'name': 'A. Mian'}]",2018.0,IEEE Access,['AI safety: state of the field through quantitative lens'],1,"Deep learning is at the heart of the current rise of artificial intelligence. In the field of computer vision, it has become the workhorse for applications ranging from self-driving cars to surveillance and security. Whereas, deep neural networks have demonstrated phenomenal success (often beyond human capabilities) in solving complex problems, recent studies show that they are vulnerable to adversarial attacks in the form of subtle perturbations to inputs that lead a model to predict incorrect outputs. For images, such perturbations are often too small to be perceptible, yet they completely fool the deep learning models. Adversarial attacks pose a serious threat to the success of deep learning in practice. This fact has recently led to a large influx of contributions in this direction. This paper presents the first comprehensive survey on adversarial attacks on deep learning in computer vision. We review the works that design adversarial attacks, analyze the existence of such attacks and propose defenses against them. To emphasize that adversarial attacks are possible in practical conditions, we separately review the contributions that evaluate adversarial attacks in the real-world scenarios. Finally, drawing on the reviewed literature, we provide a broader outlook of this research direction.",1361.0
"Artificial Intelligence and the ‘Good Society’: the US, EU, and UK approach",cd95a81b10ebfc7d2f4dbc6751c3de05247a13ed,"[{'authorId': '7794798', 'name': 'Corinne Cath'}, {'authorId': '12806133', 'name': 'Sandra Wachter'}, {'authorId': '3127701', 'name': 'B. Mittelstadt'}, {'authorId': '2084659', 'name': 'M. Taddeo'}, {'authorId': '1982425', 'name': 'L. Floridi'}]",2016.0,Science and Engineering Ethics,['AI safety: state of the field through quantitative lens'],1,"In October 2016, the White House, the European Parliament, and the UK House of Commons each issued a report outlining their visions on how to prepare society for the widespread use of artificial intelligence (AI). In this article, we provide a comparative assessment of these three reports in order to facilitate the design of policies favourable to the development of a ‘good AI society’. To do so, we examine how each report addresses the following three topics: (a) the development of a ‘good AI society’; (b) the role and responsibility of the government, the private sector, and the research community (including academia) in pursuing such a development; and (c) where the recommendations to support such a development may be in need of improvement. Our analysis concludes that the reports address adequately various ethical, social, and economic topics, but come short of providing an overarching political vision and long-term strategy for the development of a ‘good AI society’. In order to contribute to fill this gap, in the conclusion we suggest a two-pronged approach.",290.0
BEYOND MAD ? : THE RACE FOR ARTIFICIAL GENERAL INTELLIGENCE,7371bb45f85d297fbad25dee15a6b7f089cd60df,"[{'authorId': '2058336709', 'name': 'Anand Ramamoorthy'}, {'authorId': '1976753', 'name': 'Roman V Yampolskiy'}]",2018.0,,['AI safety: state of the field through quantitative lens'],1,"Artificial intelligence research is a source of great technological advancement as well as ethical concern, as applied AI invades diverse aspects of human life. Yet true artificial general intelligence remains out of reach. Based on the history of deeply transformative technologies developed by multiple actors on the global stage and their consequences for global stability, we consider the possibility of artificial general intelligence arms races and propose solutions aimed at managing the development of such an intelligence without increasing the risks to global stability and humanity.",28.0
Value Alignment or Misalignment - What Will Keep Systems Accountable?,c7d0ff05ee7dd1e7996604e0b4e93701540a18bc,"[{'authorId': '2053868213', 'name': 'Thomas Arnold'}, {'authorId': '19185302', 'name': 'Daniel Kasenberg'}, {'authorId': '1793014', 'name': 'Matthias Scheutz'}]",2017.0,AAAI Workshops,['AI safety: state of the field through quantitative lens'],1,"Machine learning’s advances have led to new ideas about the feasibility and importance of machine ethics keeping pace, with increasing emphasis on safety, containment, and align- ment. This paper addresses a recent suggestion that inverse reinforcement learning (IRL) could be a means to so-called “value alignment.” We critically consider how such an approach can engage the social, norm-infused nature of ethical action and outline several features of ethical appraisal that go beyond simple models of behavior, including unavoidably temporal dimensions of norms and counterfactuals. We propose that a hybrid approach for computational architectures still offers the most promising avenue for machines acting in an ethical fashion.",79.0
The ethics of algorithms: Mapping the debate,6d77a03fbf6341674a52d9bf95c9e49f02ef1a74,"[{'authorId': '3127701', 'name': 'B. Mittelstadt'}, {'authorId': '1759175', 'name': 'P. Allo'}, {'authorId': '2084659', 'name': 'M. Taddeo'}, {'authorId': '12806133', 'name': 'Sandra Wachter'}, {'authorId': '1982425', 'name': 'L. Floridi'}]",2016.0,,['AI safety: state of the field through quantitative lens'],1,"In information societies, operations, decisions and choices previously left to humans are increasingly delegated to algorithms, which may advise, if not decide, about how data should be interpreted and what actions should be taken as a result. More and more often, algorithms mediate social processes, business transactions, governmental decisions, and how we perceive, understand, and interact among ourselves and with the environment. Gaps between the design and operation of algorithms and our understanding of their ethical implications can have severe consequences affecting individuals as well as groups and whole societies. This paper makes three contributions to clarify the ethical importance of algorithmic mediation. It provides a prescriptive map to organise the debate. It reviews the current discussion of ethical aspects of algorithms. And it assesses the available literature in order to identify areas requiring further work to develop the ethics of algorithms.",997.0
Smart Policies for Artificial Intelligence,937d6d4c34ad41edc6fb68e477cad23657b949e2,"[{'authorId': '35167962', 'name': 'Miles Brundage'}, {'authorId': '2055073817', 'name': 'Joanna J. Bryson'}]",2016.0,arXiv.org,['AI safety: state of the field through quantitative lens'],1,"We argue that there already exists de facto artificial intelligence policy - a patchwork of policies impacting the field of AI's development in myriad ways. The key question related to AI policy, then, is not whether AI should be governed at all, but how it is currently being governed, and how that governance might become more informed, integrated, effective, and anticipatory. We describe the main components of de facto AI policy and make some recommendations for how AI policy can be improved, drawing on lessons from other scientific and technological domains.",19.0
A comprehensive survey on safe reinforcement learning,3e168fe7891c27f64e19bfaf9d1bb039b0ab46a0,"[{'authorId': '1578860918', 'name': 'GarcíaJavier'}, {'authorId': '1578867297', 'name': 'FernándezFernando'}]",2015.0,,['AI safety: state of the field through quantitative lens'],1,Safe Reinforcement Learning can be defined as the process of learning policies that maximize the expectation of the return in problems in which it is important to ensure reasonable system performan...,32.0
Safety Engineering for Artificial General Intelligence,2b1f2b906c23444e38e95d40779377db6ac51bb6,"[{'authorId': '1976753', 'name': 'Roman V Yampolskiy'}, {'authorId': '49824819', 'name': 'Joshua Fox'}]",2012.0,,"['AI safety: state of the field through quantitative lens', 'Responses to catastrophic AGI risk: a survey']",2,,68.0
AI safety: state of the field through quantitative lens,2e5ab5250e524801e2efd24249d75fcbb80f2b99,"[{'authorId': '2066411673', 'externalIds': {}, 'url': 'https://www.semanticscholar.org/author/2066411673', 'name': 'Mislav Juric', 'aliases': None, 'affiliations': [], 'homepage': None, 'paperCount': 2, 'citationCount': 14, 'hIndex': 1}, {'authorId': '2064105646', 'externalIds': {}, 'url': 'https://www.semanticscholar.org/author/2064105646', 'name': 'A. Sandic', 'aliases': ['A. Sandic', 'Agneza Sandic'], 'affiliations': [], 'homepage': None, 'paperCount': 2, 'citationCount': 15, 'hIndex': 1}, {'authorId': '35167635', 'externalIds': {'DBLP': ['Mario Brcic']}, 'url': 'https://www.semanticscholar.org/author/35167635', 'name': 'Mario Brčič', 'aliases': ['Mario Brcic', 'Mario Brčić'], 'affiliations': [], 'homepage': None, 'paperCount': 45, 'citationCount': 667, 'hIndex': 7}]",2020.0,"International Convention on Information and Communication Technology, Electronics and Microelectronics",['AI safety: state of the field through quantitative lens'],1,"Last decade has seen major improvements in the performance of artificial intelligence which has driven wide-spread applications. Unforeseen effects of such massad-option has put the notion of AI safety into the public eye. AI safety is a relatively new field of research focused on techniques for building AI beneficial for humans. While there exist survey papers for the field of AI safety, there is a lack of a quantitative look at the research being conducted. The quantitative aspect gives a data-driven insight about the emerging trends, knowledge gaps and potential areas for future research. In this paper, bibliometric analysis of the literature finds significant increase in research activity since 2015. Also, the field is so new that most of the technical issues are open, including: explainability and its long-term utility, and value alignment which we have identified as the most important long-term research topic. Equally, there is a severe lack of research into concrete policies regarding AI. As we expect AI to be the one of the main driving forces of changes, AI safety is the field under which we need to decide the direction of humanity’s future.",14.0
It's Time to Do Something: Mitigating the Negative Impacts of Computing Through a Change to the Peer Review Process,5b14bbe1c41fe18c63dc3545b50bd36bb08991fd,"[{'authorId': '32980371', 'name': 'Brent J. Hecht'}, {'authorId': '2073552928', 'name': 'Lauren Wilcox'}, {'authorId': '1744846', 'name': 'Jeffrey P. Bigham'}, {'authorId': '2070910', 'name': 'Johannes Schöning'}, {'authorId': '1491348598', 'name': 'E. Hoque'}, {'authorId': '2053351156', 'name': 'J. Ernst'}, {'authorId': '3312309', 'name': 'Yonatan Bisk'}, {'authorId': '1794953', 'name': 'Luigi De Russis'}, {'authorId': '145422160', 'name': 'L. Yarosh'}, {'authorId': '2148258064', 'name': 'Bushra Anjum'}, {'authorId': '2075459', 'name': 'Danish Contractor'}, {'authorId': '2144310200', 'name': 'Cathy Wu'}]",2021.0,arXiv.org,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"The computing research community needs to work much harder to address the downsides of our innovations. Between the erosion of privacy, threats to democracy, and automation's effect on employment (among many other issues), we can no longer simply assume that our research will have a net positive impact on the world. While bending the arc of computing innovation towards societal benefit may at first seem intractable, we believe we can achieve substantial progress with a straightforward step: making a small change to the peer review process. As we explain below, we hypothesize that our recommended change will force computing researchers to more deeply consider the negative impacts of their work. We also expect that this change will incentivize research and policy that alleviates computing's negative impacts.",97.0
Active Reinforcement Learning: Observing Rewards at a Cost,a47f52b25ce1e56a03876d9c0fd7c45e63270eb4,"[{'authorId': '145055042', 'name': 'David Krueger'}, {'authorId': '2990741', 'name': 'J. Leike'}, {'authorId': '47107786', 'name': 'Owain Evans'}, {'authorId': '3373139', 'name': 'J. Salvatier'}]",2020.0,arXiv.org,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"Active reinforcement learning (ARL) is a variant on reinforcement learning where the agent does not observe the reward unless it chooses to pay a query cost c > 0. The central question of ARL is how to quantify the long-term value of reward information. Even in multi-armed bandits, computing the value of this information is intractable and we have to rely on heuristics. We propose and evaluate several heuristic approaches for ARL in multi-armed bandits and (tabular) Markov decision processes, and discuss and illustrate some challenging aspects of the ARL problem.",18.0
"A PARAMETRIC, RESOURCE-BOUNDED GENERALIZATION OF LÖB’S THEOREM, AND A ROBUST COOPERATION CRITERION FOR OPEN-SOURCE GAME THEORY",7c2f29205b297b7944aa4ea8f1cfd8b8e4ab3706,"[{'authorId': '2651789', 'name': 'Andrew Critch'}]",2019.0,Journal of Symbolic Logic (JSL),['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"Abstract This article presents two theorems: (1) a generalization of Löb’s Theorem that applies to formal proof systems operating with bounded computational resources, such as formal verification software or theorem provers, and (2) a theorem on the robust cooperation of agents that employ proofs about one another’s source code as unexploitable criteria for cooperation. The latter illustrates a capacity for outperforming classical Nash equilibria and correlated equilibria, attaining mutually cooperative program equilibrium in the Prisoner’s Dilemma while remaining unexploitable, i.e., sometimes achieving the outcome (Cooperate, Cooperate), and never receiving the outcome (Cooperate, Defect) as player 1.",17.0
Literal or Pedagogic Human? Analyzing Human Model Misspecification in Objective Learning,18786349b4b7cfc628a6813eb09eacca63977dc5,"[{'authorId': '3458938', 'name': 'S. Milli'}, {'authorId': '2745001', 'name': 'A. Dragan'}]",2019.0,Conference on Uncertainty in Artificial Intelligence,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"It is incredibly easy for a system designer to misspecify the objective for an autonomous system (""robot''), thus motivating the desire to have the robot learn the objective from human behavior instead. Recent work has suggested that people have an interest in the robot performing well, and will thus behave pedagogically, choosing actions that are informative to the robot. In turn, robots benefit from interpreting the behavior by accounting for this pedagogy. In this work, we focus on misspecification: we argue that robots might not know whether people are being pedagogic or literal and that it is important to ask which assumption is safer to make. We cast objective learning into the more general form of a common-payoff game between the robot and human, and prove that in any such game literal interpretation is more robust to misspecification. Experiments with human data support our theoretical results and point to the sensitivity of the pedagogic assumption.",13.0
Bridging near- and long-term concerns about AI,947368509b697ed76e4a7912f7f251e53e403592,"[{'authorId': '51129868', 'name': 'S. Cave'}, {'authorId': '2351755', 'name': 'Seán S. ÓhÉigeartaigh'}]",2019.0,Nature Machine Intelligence,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,,32.0
Hierarchical Game-Theoretic Planning for Autonomous Vehicles,fea36129092f7fc07faf4359eaf57c9a025942e4,"[{'authorId': '1843342', 'name': 'J. Fisac'}, {'authorId': '51489600', 'name': 'Eli Bronstein'}, {'authorId': '40250862', 'name': 'Elis Stefansson'}, {'authorId': '1779671', 'name': 'Dorsa Sadigh'}, {'authorId': '144797536', 'name': 'S. Sastry'}, {'authorId': '2745001', 'name': 'A. Dragan'}]",2018.0,IEEE International Conference on Robotics and Automation,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"The actions of an autonomous vehicle on the road affect and are affected by those of other drivers, whether overtaking, negotiating a merge, or avoiding an accident. This mutual dependence, best captured by dynamic game theory, creates a strong coupling between the vehicle’s planning and its predictions of other drivers’ behavior, and constitutes an open problem with direct implications on the safety and viability of autonomous driving technology. Unfortunately, dynamic games are too computationally demanding to meet the real-time constraints of autonomous driving in its continuous state and action space. In this paper, we introduce a novel game-theoretic trajectory planning algorithm for autonomous driving, that enables real-time performance by hierarchically decomposing the underlying dynamic game into a long-horizon “strategic” game with simplified dynamics and full information structure, and a short-horizon “tactical” game with full dynamics and a simplified information structure. The value of the strategic game is used to guide the tactical planning, implicitly extending the planning horizon, pushing the local trajectory optimization closer to global solutions, and, most importantly, quantitatively accounting for the autonomous vehicle and the human driver’s ability and incentives to influence each other. In addition, our approach admits non-deterministic models of human decision-making, rather than relying on perfectly rational predictions. Our results showcase richer, safer, and more effective autonomous behavior in comparison to existing techniques.",138.0
Noise Contrastive Priors for Functional Uncertainty,23a716105b548865451e72dda3397d0fa96d6293,"[{'authorId': '35006479', 'name': 'Danijar Hafner'}, {'authorId': '47497262', 'name': 'Dustin Tran'}, {'authorId': '2542999', 'name': 'T. Lillicrap'}, {'authorId': '17818078', 'name': 'A. Irpan'}, {'authorId': '2068894907', 'name': 'James Davidson'}]",2018.0,Conference on Uncertainty in Artificial Intelligence,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"Obtaining reliable uncertainty estimates of neural network predictions is a long standing challenge. Bayesian neural networks have been proposed as a solution, but it remains open how to specify their prior. In particular, the common practice of an independent normal prior in weight space imposes relatively weak constraints on the function posterior, allowing it to generalize in unforeseen ways on inputs outside of the training distribution. We propose noise contrastive priors (NCPs) to obtain reliable uncertainty estimates. The key idea is to train the model to output high uncertainty for data points outside of the training distribution. NCPs do so using an input prior, which adds noise to the inputs of the current mini batch, and an output prior, which is a wide distribution given these inputs. NCPs are compatible with any model that can output uncertainty estimates, are easy to scale, and yield reliable uncertainty estimates throughout training. Empirically, we show that NCPs prevent overfitting outside of the training distribution and result in uncertainty estimates that are useful for active learning. We demonstrate the scalability of our method on the flight delays data set, where we significantly improve upon previously published results.",72.0
Penalizing Side Effects using Stepwise Relative Reachability,0039dc54599ef609b3e7044f3b7cdfac354f79c2,"[{'authorId': '2578985', 'name': 'Victoria Krakovna'}, {'authorId': '1749270', 'name': 'Laurent Orseau'}, {'authorId': '26890260', 'name': 'Miljan Martic'}, {'authorId': '34313265', 'name': 'S. Legg'}]",2018.0,AISafety@IJCAI,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"How can we design safe reinforcement learning agents that avoid unnecessary disruptions to their environment? We show that current approaches to penalizing side effects can introduce bad incentives, e.g. to prevent any irreversible changes in the environment, including the actions of other agents. To isolate the source of such undesirable incentives, we break down side effects penalties into two components: a baseline state and a measure of deviation from this baseline state. We argue that some of these incentives arise from the choice of baseline, and others arise from the choice of deviation measure. We introduce a new variant of the stepwise inaction baseline and a new deviation measure based on relative reachability of states. The combination of these design choices avoids the given undesirable incentives, while simpler baselines and the unreachability measure fail. We demonstrate this empirically by comparing different combinations of baseline and deviation measure choices on a set of gridworld experiments designed to illustrate possible bad incentives.",36.0
One Pixel Attack for Fooling Deep Neural Networks,a6f835ca6e12245a835ab6074bc6ec2c3c60b85a,"[{'authorId': '1730754', 'name': 'Jiawei Su'}, {'authorId': '145197293', 'name': 'Danilo Vasconcellos Vargas'}, {'authorId': '145106127', 'name': 'K. Sakurai'}]",2017.0,IEEE Transactions on Evolutionary Computation,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"Recent research has revealed that the output of deep neural networks (DNNs) can be easily altered by adding relatively small perturbations to the input vector. In this paper, we analyze an attack in an extremely limited scenario where only one pixel can be modified. For that we propose a novel method for generating one-pixel adversarial perturbations based on differential evolution (DE). It requires less adversarial information (a black-box attack) and can fool more types of networks due to the inherent features of DE. The results show that 67.97% of the natural images in Kaggle CIFAR-10 test dataset and 16.04% of the ImageNet (ILSVRC 2012) test images can be perturbed to at least one target class by modifying just one pixel with 74.03% and 22.91% confidence on average. We also show the same vulnerability on the original CIFAR-10 dataset. Thus, the proposed attack explores a different take on adversarial machine learning in an extreme limited scenario, showing that current DNNs are also vulnerable to such low dimension attacks. Besides, we also illustrate an important application of DE (or broadly speaking, evolutionary computation) in the domain of adversarial machine learning: creating tools that can effectively generate low-cost adversarial attacks against neural networks for evaluating robustness.",1669.0
Reward learning from human preferences and demonstrations in Atari,325e1b7cec684c22bb3c2cf65205c77eaf55114f,"[{'authorId': '6675568', 'name': 'Borja Ibarz'}, {'authorId': '2990741', 'name': 'J. Leike'}, {'authorId': '3408089', 'name': 'Tobias Pohlen'}, {'authorId': '2060655766', 'name': 'G. Irving'}, {'authorId': '34313265', 'name': 'S. Legg'}, {'authorId': '2698777', 'name': 'Dario Amodei'}]",2018.0,Neural Information Processing Systems,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"To solve complex real-world problems with reinforcement learning, we cannot rely on manually specified reward functions. Instead, we can have humans communicate an objective to the agent directly. In this work, we combine two approaches to learning from human feedback: expert demonstrations and trajectory preferences. We train a deep neural network to model the reward function and use its predicted reward to train an DQN-based deep reinforcement learning agent on 9 Atari games. Our approach beats the imitation learning baseline in 7 games and achieves strictly superhuman performance on 2 games without using game rewards. Additionally, we investigate the goodness of fit of the reward model, present some reward hacking problems, and study the effects of noise in the human labels.",171.0
Scaling shared model governance via model splitting,60301daed5a2738e8cdca85a1ddf0a400f514d08,"[{'authorId': '26890260', 'name': 'Miljan Martic'}, {'authorId': '2990741', 'name': 'J. Leike'}, {'authorId': '145994651', 'name': 'Andrew Trask'}, {'authorId': '39357484', 'name': 'Matteo Hessel'}, {'authorId': '34313265', 'name': 'S. Legg'}, {'authorId': '143967473', 'name': 'Pushmeet Kohli'}]",2018.0,arXiv.org,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"Currently the only techniques for sharing governance of a deep learning model are homomorphic encryption and secure multiparty computation. Unfortunately, neither of these techniques is applicable to the training of large neural networks due to their large computational and communication overheads. As a scalable technique for shared model governance, we propose splitting deep learning model between multiple parties. This paper empirically investigates the security guarantee of this technique, which is introduced as the problem of model completion: Given the entire training data set or an environment simulator, and a subset of the parameters of a trained deep learning model, how much training is required to recover the model's original performance? We define a metric for evaluating the hardness of the model completion problem and study it empirically in both supervised learning on ImageNet and reinforcement learning on Atari and DeepMind~Lab. Our experiments show that (1) the model completion problem is harder in reinforcement learning than in supervised learning because of the unavailability of the trained agent's trajectories, and (2) its hardness depends not primarily on the number of parameters of the missing part, but more so on their type and location. Our results suggest that model splitting might be a feasible technique for shared model governance in some settings where training is very expensive.",1.0
Superintelligence Skepticism as a Political Tool,cc65db9d4a6547ca17b987b54408abd46ccb8014,"[{'authorId': '2097800', 'name': 'S. Baum'}]",2018.0,Inf.,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"This paper explores the potential for skepticism about artificial superintelligence to be used as a tool for political ends. Superintelligence is AI that is much smarter than humans. Superintelligence does not currently exist, but it has been proposed that it could someday be built, with massive and potentially catastrophic consequences. There is substantial skepticism about superintelligence, including whether it will be built, whether it would be catastrophic, and whether it is worth current attention. To date, superintelligence skepticism appears to be mostly honest intellectual debate, though some of it may be politicized. This paper finds substantial potential for superintelligence skepticism to be (further) politicized, due mainly to the potential for major corporations to have a strong profit motive to downplay concerns about superintelligence and avoid government regulation. Furthermore, politicized superintelligence skepticism is likely to be quite successful, due to several factors including the inherent uncertainty of the topic and the abundance of skeptics. The paper’s analysis is based on characteristics of superintelligence and the broader AI sector, as well as the history and ongoing practice of politicized skepticism on other science and technology issues, including tobacco, global warming, and industrial chemicals. The paper contributes to literatures on politicized skepticism and superintelligence governance.",18.0
Reliable Uncertainty Estimates in Deep Neural Networks using Noise Contrastive Priors,85680b9e418ef5e40479088a4ed3e2d47f2deda3,"[{'authorId': '35006479', 'name': 'Danijar Hafner'}, {'authorId': '47497262', 'name': 'Dustin Tran'}, {'authorId': '17818078', 'name': 'A. Irpan'}, {'authorId': '2542999', 'name': 'T. Lillicrap'}, {'authorId': '2068894907', 'name': 'James Davidson'}]",2018.0,arXiv.org,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"Obtaining reliable uncertainty estimates of neural network predictions is a long standing challenge. Bayesian neural networks have been proposed as a solution, but it remains open how to specify their prior. In particular, the common practice of a standard normal prior in weight space imposes only weak regularities, causing the function posterior to possibly generalize in unforeseen ways on inputs outside of the training distribution. We propose noise contrastive priors (NCPs) to obtain reliable uncertainty estimates. The key idea is to train the model to output high uncertainty for data points outside of the training distribution. NCPs do so using an input prior, which adds noise to the inputs of the current mini batch, and an output prior, which is a wide distribution given these inputs. NCPs are compatible with any model that can output uncertainty estimates, are easy to scale, and yield reliable uncertainty estimates throughout training. Empirically, we show that NCPs prevent overfitting outside of the training distribution and result in uncertainty estimates that are useful for active learning. We demonstrate the scalability of our method on the flight delays data set, where we significantly improve upon previously published results.",65.0
Software Verification with ITPs Should Use Binary Code Extraction to Reduce the TCB - (Short Paper),9c44a55a7247971294bf85b7e6668ce00a934297,"[{'authorId': '2117776492', 'name': 'Ramana Kumar'}, {'authorId': '31704495', 'name': 'Eric Mullen'}, {'authorId': '2272813', 'name': 'Zachary Tatlock'}, {'authorId': '1709169', 'name': 'Magnus O. Myreen'}]",2018.0,International Conference on Interactive Theorem Proving,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,,18.0
Measuring and avoiding side effects using relative reachability,80a3f30e54bc22c0e0ca4021e6ab72adb3526567,"[{'authorId': '2578985', 'name': 'Victoria Krakovna'}, {'authorId': '1749270', 'name': 'Laurent Orseau'}, {'authorId': '26890260', 'name': 'Miljan Martic'}, {'authorId': '34313265', 'name': 'S. Legg'}]",2018.0,arXiv.org,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"How can we design reinforcement learning agents that avoid causing unnecessary disruptions to their environment? We argue that current approaches to penalizing side effects can introduce bad incentives in tasks that require irreversible actions, and in environments that contain sources of change other than the agent. For example, some approaches give the agent an incentive to prevent any irreversible changes in the environment, including the actions of other agents. We introduce a general definition of side effects, based on relative reachability of states compared to a default state, that avoids these undesirable incentives. Using a set of gridworld experiments illustrating relevant scenarios, we empirically compare relative reachability to penalties based on existing definitions and show that it is the only penalty among those tested that produces the desired behavior in all the scenarios.",22.0
Where Do You Think You're Going?: Inferring Beliefs about Dynamics from Behavior,e58ef68b95d9b03f37991c31ef2363fab8d0a5b4,"[{'authorId': '37372079', 'name': 'S. Reddy'}, {'authorId': '2745001', 'name': 'A. Dragan'}, {'authorId': '1736651', 'name': 'S. Levine'}]",2018.0,Neural Information Processing Systems,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"Inferring intent from observed behavior has been studied extensively within the frameworks of Bayesian inverse planning and inverse reinforcement learning. These methods infer a goal or reward function that best explains the actions of the observed agent, typically a human demonstrator. Another agent can use this inferred intent to predict, imitate, or assist the human user. However, a central assumption in inverse reinforcement learning is that the demonstrator is close to optimal. While models of suboptimal behavior exist, they typically assume that suboptimal actions are the result of some type of random noise or a known cognitive bias, like temporal inconsistency. In this paper, we take an alternative approach, and model suboptimal behavior as the result of internal model misspecification: the reason that user actions might deviate from near-optimal actions is that the user has an incorrect set of beliefs about the rules -- the dynamics -- governing how actions affect the environment. Our insight is that while demonstrated actions may be suboptimal in the real world, they may actually be near-optimal with respect to the user's internal model of the dynamics. By estimating these internal beliefs from observed behavior, we arrive at a new method for inferring intent. We demonstrate in simulation and in a user study with 12 participants that this approach enables us to more accurately model human intent, and can be used in a variety of applications, including offering assistance in a shared autonomy framework and inferring human preferences.",83.0
The Economic Singularity,1ebb739408af63ce51630b8b6b7fb54b9c32c23e,"[{'authorId': '90191803', 'name': 'Chace Calum'}]",2018.0,,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,,1.0
A Dual Approach to Scalable Verification of Deep Networks,8d35663a80199b173d8cbd12dbf2300a9f86a021,"[{'authorId': '1729912', 'name': 'Krishnamurthy Dvijotham'}, {'authorId': '49860489', 'name': 'Robert Stanforth'}, {'authorId': '2071666', 'name': 'Sven Gowal'}, {'authorId': '2554720', 'name': 'Timothy A. Mann'}, {'authorId': '143967473', 'name': 'Pushmeet Kohli'}]",2018.0,Conference on Uncertainty in Artificial Intelligence,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"This paper addresses the problem of formally verifying desirable properties of neural networks, i.e., obtaining provable guarantees that neural networks satisfy specifications relating their inputs and outputs (robustness to bounded norm adversarial perturbations, for example). Most previous work on this topic was limited in its applicability by the size of the network, network architecture and the complexity of properties to be verified. In contrast, our framework applies to a general class of activation functions and specifications on neural network inputs and outputs. We formulate verification as an optimization problem (seeking to find the largest violation of the specification) and solve a Lagrangian relaxation of the optimization problem to obtain an upper bound on the worst case violation of the specification being verified. Our approach is anytime i.e. it can be stopped at any time and a valid bound on the maximum violation can be obtained. We develop specialized verification algorithms with provable tightness guarantees under special assumptions and demonstrate the practical significance of our general verification approach on a variety of verification tasks.",338.0
Learning Confidence for Out-of-Distribution Detection in Neural Networks,431ba9fae8fccad1665979d455c6307786e47318,"[{'authorId': '32097919', 'name': 'Terrance Devries'}, {'authorId': '144639556', 'name': 'Graham W. Taylor'}]",2018.0,arXiv.org,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"Modern neural networks are very powerful predictive models, but they are often incapable of recognizing when their predictions may be wrong. Closely related to this is the task of out-of-distribution detection, where a network must determine whether or not an input is outside of the set on which it is expected to safely perform. To jointly address these issues, we propose a method of learning confidence estimates for neural networks that is simple to implement and produces intuitively interpretable outputs. We demonstrate that on the task of out-of-distribution detection, our technique surpasses recently proposed techniques which construct confidence based on the network's output distribution, without requiring any additional labels or access to out-of-distribution examples. Additionally, we address the problem of calibrating out-of-distribution detectors, where we demonstrate that misclassified in-distribution examples can be used as a proxy for out-of-distribution examples.",405.0
Œuf: minimizing the Coq extraction TCB,ef537a2cd3b2a2d28e8ec07195265a61a9ad4c26,"[{'authorId': '31704495', 'name': 'Eric Mullen'}, {'authorId': '2717489', 'name': 'Stuart Pernsteiner'}, {'authorId': '143872426', 'name': 'James R. Wilcox'}, {'authorId': '2272813', 'name': 'Zachary Tatlock'}, {'authorId': '8319903', 'name': 'D. Grossman'}]",2018.0,Certified Programs and Proofs,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"Verifying systems by implementing them in the programming language of a proof assistant (e.g., Gallina for Coq) lets us directly leverage the full power of the proof assistant for verifying the system. But, to execute such an implementation requires extraction, a large complicated process that is in the trusted computing base (TCB). This paper presents Œuf, a verified compiler from a subset of Gallina to assembly. Œuf’s correctness theorem ensures that compilation preserves the semantics of the source Gallina program. We describe how Œuf’s specification can be used as a foreign function interface to reason about the interaction between compiled Gallina programs and surrounding shim code. Additionally, Œufmaintains a small TCB for its front-end by reflecting Gallina programs to Œufsource and automatically ensuring equivalence using computational denotation. This design enabled us to implement some early compiler passes (e.g., lambda lifting) in the untrusted reflection and ensure their correctness via translation validation. To evaluate Œuf, we compile Appel’s SHA256 specification from Gallina to x86 and write a shim for the generated code, yielding a verified sha256sum implementation with a small TCB.",38.0
Training Confidence-calibrated Classifiers for Detecting Out-of-Distribution Samples,36653f8705b56e39642bcd123494eb680cd1636b,"[{'authorId': '3436470', 'name': 'Kimin Lee'}, {'authorId': '1697141', 'name': 'Honglak Lee'}, {'authorId': '2208511', 'name': 'Kibok Lee'}, {'authorId': '143720148', 'name': 'Jinwoo Shin'}]",2017.0,International Conference on Learning Representations,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"The problem of detecting whether a test sample is from in-distribution (i.e., training distribution by a classifier) or out-of-distribution sufficiently different from it arises in many real-world machine learning applications. However, the state-of-art deep neural networks are known to be highly overconfident in their predictions, i.e., do not distinguish in- and out-of-distributions. Recently, to handle this issue, several threshold-based detectors have been proposed given pre-trained neural classifiers. However, the performance of prior works highly depends on how to train the classifiers since they only focus on improving inference procedures. In this paper, we develop a novel training method for classifiers so that such inference algorithms can work better. In particular, we suggest two additional terms added to the original loss (e.g., cross entropy). The first one forces samples from out-of-distribution less confident by the classifier and the second one is for (implicitly) generating most effective training samples for the first one. In essence, our method jointly trains both classification and generative neural networks for out-of-distribution. We demonstrate its effectiveness using deep convolutional neural networks on various popular image datasets.",615.0
Emergent Complexity via Multi-Agent Competition,c335ff618991f0a4cdde09271284172a7e5f6b7f,"[{'authorId': '1858169', 'name': 'Trapit Bansal'}, {'authorId': '2713380', 'name': 'J. Pachocki'}, {'authorId': '2700360', 'name': 'Szymon Sidor'}, {'authorId': '1701686', 'name': 'Ilya Sutskever'}, {'authorId': '2080746', 'name': 'Igor Mordatch'}]",2017.0,International Conference on Learning Representations,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"Reinforcement learning algorithms can train agents that solve problems in complex, interesting environments. Normally, the complexity of the trained agent is closely related to the complexity of the environment. This suggests that a highly capable agent requires a complex environment for training. In this paper, we point out that a competitive multi-agent environment trained with self-play can produce behaviors that are far more complex than the environment itself. We also point out that such environments come with a natural curriculum, because for any skill level, an environment full of agents of this level will have the right level of difficulty. This work introduces several competitive multi-agent environments where agents compete in a 3D world with simulated physics. The trained agents learn a wide variety of complex and interesting skills, even though the environment themselves are relatively simple. The skills include behaviors such as running, blocking, ducking, tackling, fooling opponents, kicking, and defending using both arms and legs. A highlight of the learned behaviors can be found here: this https URL",293.0
Interpretable Convolutional Neural Networks,773ca5c76da50cf6f21553b0f8eee391ac65f9c8,"[{'authorId': '22063226', 'name': 'Quanshi Zhang'}, {'authorId': '39092098', 'name': 'Y. Wu'}, {'authorId': '145380991', 'name': 'Song-Chun Zhu'}]",2017.0,2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"This paper proposes a method to modify a traditional convolutional neural network (CNN) into an interpretable CNN, in order to clarify knowledge representations in high conv-layers of the CNN. In an interpretable CNN, each filter in a high conv-layer represents a specific object part. Our interpretable CNNs use the same training data as ordinary CNNs without a need for any annotations of object parts or textures for supervision. The interpretable CNN automatically assigns each filter in a high conv-layer with an object part during the learning process. We can apply our method to different types of CNNs with various structures. The explicit knowledge representation in an interpretable CNN can help people understand the logic inside a CNN, i.e. what patterns are memorized by the CNN for prediction. Experiments have shown that filters in an interpretable CNN are more semantically meaningful than those in a traditional CNN. The code is available at https://github.com/zqs1022/interpretableCNN.",580.0
Learning with Opponent-Learning Awareness,1d92cf3f0c86bfde10238fcaf31182a245bc920b,"[{'authorId': '145356667', 'name': 'Jakob N. Foerster'}, {'authorId': '2896187', 'name': 'Richard Y. Chen'}, {'authorId': '1401178735', 'name': 'Maruan Al-Shedivat'}, {'authorId': '1766767', 'name': 'Shimon Whiteson'}, {'authorId': '1689992', 'name': 'P. Abbeel'}, {'authorId': '2080746', 'name': 'Igor Mordatch'}]",2017.0,Adaptive Agents and Multi-Agent Systems,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"Multi-agent settings are quickly gathering importance in machine learning. This includes a plethora of recent work on deep multi-agent reinforcement learning, but also can be extended to hierarchical RL, generative adversarial networks and decentralised optimisation. In all these settings the presence of multiple learning agents renders the training problem non-stationary and often leads to unstable training or undesired final results. We present Learning with Opponent-Learning Awareness (LOLA), a method in which each agent shapes the anticipated learning of the other agents in the environment. The LOLA learning rule includes a term that accounts for the impact of one agent's policy on the anticipated parameter update of the other agents. Results show that the encounter of two LOLA agents leads to the emergence of tit-for-tat and therefore cooperation in the iterated prisoners' dilemma, while independent learning does not. In this domain, LOLA also receives higher payouts compared to a naive learner, and is robust against exploitation by higher order gradient-based methods. Applied to repeated matching pennies, LOLA agents converge to the Nash equilibrium. In a round robin tournament we show that LOLA agents successfully shape the learning of a range of multi-agent learning algorithms from literature, resulting in the highest average returns on the IPD. We also show that the LOLA update rule can be efficiently calculated using an extension of the policy gradient estimator, making the method suitable for model-free RL. The method thus scales to large parameter and input spaces and nonlinear function approximators. We apply LOLA to a grid world task with an embedded social dilemma using recurrent policies and opponent modelling. By explicitly considering the learning of the other agent, LOLA agents learn to cooperate out of self-interest. The code is at this http URL.",384.0
Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks,547c854985629cfa9404a5ba8ca29367b5f8c25f,"[{'authorId': '3075943', 'name': 'Shiyu Liang'}, {'authorId': '1527103472', 'name': 'Yixuan Li'}, {'authorId': '143808204', 'name': 'R. Srikant'}]",2017.0,International Conference on Learning Representations,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"We consider the problem of detecting out-of-distribution images in neural networks. We propose ODIN, a simple and effective method that does not require any change to a pre-trained neural network. Our method is based on the observation that using temperature scaling and adding small perturbations to the input can separate the softmax score distributions between in- and out-of-distribution images, allowing for more effective detection. We show in a series of experiments that ODIN is compatible with diverse network architectures and datasets. It consistently outperforms the baseline approach by a large margin, establishing a new state-of-the-art performance on this task. For example, ODIN reduces the false positive rate from the baseline 34.7% to 4.3% on the DenseNet (applied to CIFAR-10) when the true positive rate is 95%.",1202.0
Emergence of Grounded Compositional Language in Multi-Agent Populations,5d2f5c2dc11c18c0d45203e2b980fe375a56d774,"[{'authorId': '2080746', 'name': 'Igor Mordatch'}, {'authorId': '1689992', 'name': 'P. Abbeel'}]",2017.0,AAAI Conference on Artificial Intelligence,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"
 
 By capturing statistical patterns in large corpora, machine learning has enabled significant advances in natural language processing, including in machine translation, question answering, and sentiment analysis. However, for agents to intelligently interact with humans, simply capturing the statistical patterns is insufficient. In this paper we investigate if, and how, grounded compositional language can emerge as a means to achieve goals in multi-agent populations. Towards this end, we propose a multi-agent learning environment and learning methods that bring about emergence of a basic compositional language. This language is represented as streams of abstract discrete symbols uttered by agents over time, but nonetheless has a coherent structure that possesses a defined vocabulary and syntax. We also observe emergence of non-verbal communication such as pointing and guiding when language communication is unavailable.
 
",518.0
Reachability Analysis for Neural Agent-Environment Systems,74ddffbfd5bc947b44f03d99e7b89453bdb9fce2,"[{'authorId': '74980748', 'name': 'Michael Akintunde'}, {'authorId': '1715165', 'name': 'A. Lomuscio'}, {'authorId': '40135746', 'name': 'Lalit Maganti'}, {'authorId': '51055830', 'name': 'Edoardo Pirovano'}]",2018.0,International Conference on Principles of Knowledge Representation and Reasoning,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"We develop a novel model for studying agent-environment systems, where the agents are implemented via feed-forward ReLU neural networks. We provide a semantics and develop a method to verify automatically that no unwanted states are reached by the system during its evolution. We study several reachability problems for the system, ranging from one-step reachability, to fixed multi-step and arbitrary-step to study the system evolution. We also study the decision problem of whether an agent, realised via feed-forward ReLU networks will perform an action in a system run. Whenever possible, we give tight complexity bounds to decision problems introduced. We automate the various reachability problems studied by recasting them as mixed-integer linear programming problems. We present an implementation and discuss the experimental results obtained on a range of test cases.",46.0
Federated Control with Hierarchical Multi-Agent Deep Reinforcement Learning,39226d30f6f76843ef4fe0c2a5c7c334b3beba2e,"[{'authorId': '2121434953', 'name': 'Saurabh Kumar'}, {'authorId': '2162189', 'name': 'Pararth Shah'}, {'authorId': '1395813836', 'name': 'Dilek Z. Hakkani-Tür'}, {'authorId': '46819684', 'name': 'Larry Heck'}]",2017.0,arXiv.org,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"We present a framework combining hierarchical and multi-agent deep reinforcement learning approaches to solve coordination problems among a multitude of agents using a semi-decentralized model. The framework extends the multi-agent learning setup by introducing a meta-controller that guides the communication between agent pairs, enabling agents to focus on communicating with only one other agent at any step. This hierarchical decomposition of the task allows for efficient exploration to learn policies that identify globally optimal solutions even as the number of collaborating agents increases. We show promising initial experimental results on a simulated distributed scheduling problem.",30.0
'Indifference' methods for managing agent rewards,0b9d73fb88209699a0a97097c013f4a3aa80f69b,"[{'authorId': '2054678912', 'name': 'S. Armstrong'}]",2017.0,arXiv.org,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"Indifference is a class of methods that are used to control a reward based agent, by, for example, safely changing their reward or policy, or making the agent behave as if a certain outcome could never happen. These methods of control work even if the implications of the agent's reward are otherwise not fully understood. Though they all come out of similar ideas, indifference techniques can be classified as way of achieving one or more of three distinct goals: rewards dependent on certain events (with no motivation for the agent to manipulate the probability of those events), effective disbelief that an event will ever occur, and seamless transition from one behaviour to another. There are five basic methods to achieve these three goals. This paper classifies and analyses these methods on POMDPs (though the methods are highly portable to other agent designs), and establishes their uses, strengths, and limitations. It aims to make the tools of indifference generally accessible and usable to agent designers.",14.0
Servant of Many Masters: Shifting priorities in Pareto-optimal sequential decision-making,8033c7888a1dfc8b061fc3ae7219682f6a3acdd8,"[{'authorId': '2651789', 'name': 'Andrew Critch'}, {'authorId': '145107462', 'name': 'Stuart J. Russell'}]",2017.0,arXiv.org,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"It is often argued that an agent making decisions on behalf of two or more principals who have different utility functions should adopt a {\em Pareto-optimal} policy, i.e., a policy that cannot be improved upon for one agent without making sacrifices for another. A famous theorem of Harsanyi shows that, when the principals have a common prior on the outcome distributions of all policies, a Pareto-optimal policy for the agent is one that maximizes a fixed, weighted linear combination of the principals' utilities. 
In this paper, we show that Harsanyi's theorem does not hold for principals with different priors, and derive a more precise generalization which does hold, which constitutes our main result. In this more general case, the relative weight given to each principal's utility should evolve over time according to how well the agent's observations conform with that principal's prior. The result has implications for the design of contracts, treaties, joint ventures, and robots.",2.0
The Ethical Knob: ethically-customisable automated vehicles and the law,69409eb73aab815f5f3df788da093b6efccf1768,"[{'authorId': '2327275', 'name': 'Giuseppe Contissa'}, {'authorId': '26333465', 'name': 'F. Lagioia'}, {'authorId': '145466735', 'name': 'G. Sartor'}]",2017.0,Artificial Intelligence and Law,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,,0.0
Society-in-the-loop: programming the algorithmic social contract,69c1d27722de73178e1ec71ab1caf5b3074d8ce4,"[{'authorId': '1705156', 'name': 'I. Rahwan'}]",2017.0,Ethics and Information Technology,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,,229.0
Pragmatic-Pedagogic Value Alignment,867007bf2cd541513602e231988693b1a4a45075,"[{'authorId': '1843342', 'name': 'J. Fisac'}, {'authorId': '31514153', 'name': 'Monica A. Gates'}, {'authorId': '2158860', 'name': 'Jessica B. Hamrick'}, {'authorId': '2118484076', 'name': 'Chang Liu'}, {'authorId': '1397904824', 'name': 'Dylan Hadfield-Menell'}, {'authorId': '34594247', 'name': 'Malayandi Palaniappan'}, {'authorId': '38816085', 'name': 'Dhruv Malik'}, {'authorId': '144797536', 'name': 'S. Sastry'}, {'authorId': '1799860', 'name': 'T. Griffiths'}, {'authorId': '2745001', 'name': 'A. Dragan'}]",2017.0,International Symposium of Robotics Research,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,,50.0
Developing Bug-Free Machine Learning Systems With Formal Mathematics,0b16f99ee6a6dd9ed4f6236d557d252ccd613e6f,"[{'authorId': '2196579', 'name': 'Daniel Selsam'}, {'authorId': '145419642', 'name': 'Percy Liang'}, {'authorId': '1699040', 'name': 'D. Dill'}]",2017.0,International Conference on Machine Learning,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"Noisy data, non-convex objectives, model misspecification, and numerical instability can all cause undesired behaviors in machine learning systems. As a result, detecting actual implementation errors can be extremely difficult. We demonstrate a methodology in which developers use an interactive proof assistant to both implement their system and to state a formal theorem defining what it means for their system to be correct. The process of proving this theorem interactively in the proof assistant exposes all implementation errors since any error in the program would cause the proof to fail. As a case study, we implement a new system, Certigrad, for optimizing over stochastic computation graphs, and we generate a formal (i.e. machine-checkable) proof that the gradients sampled by the system are unbiased estimates of the true mathematical gradients. We train a variational autoencoder using Certigrad and find the performance comparable to training the same model in TensorFlow.",51.0
An approach to reachability analysis for feed-forward ReLU neural networks,ee8bc379985788544e44cf63887cf75a03e08b64,"[{'authorId': '1715165', 'name': 'A. Lomuscio'}, {'authorId': '40135746', 'name': 'Lalit Maganti'}]",2017.0,arXiv.org,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,We study the reachability problem for systems implemented as feed-forward neural networks whose activation function is implemented via ReLU functions. We draw a correspondence between establishing whether some arbitrary output can ever be outputed by a neural system and linear problems characterising a neural system of interest. We present a methodology to solve cases of practical interest by means of a state-of-the-art linear programs solver. We evaluate the technique presented by discussing the experimental results obtained by analysing reachability properties for a number of benchmarks in the literature.,275.0
Avoiding Discrimination through Causal Reasoning,ee2c3b095ad226a211fd674ec1d4c960c07af107,"[{'authorId': '19238593', 'name': 'Niki Kilbertus'}, {'authorId': '1388809887', 'name': 'Mateo Rojas-Carulla'}, {'authorId': '50213542', 'name': 'Giambattista Parascandolo'}, {'authorId': '1775622', 'name': 'Moritz Hardt'}, {'authorId': '1700657', 'name': 'D. Janzing'}, {'authorId': '1707625', 'name': 'B. Schölkopf'}]",2017.0,NIPS,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"Recent work on fairness in machine learning has focused on various statistical discrimination criteria and how they trade off. Most of these criteria are observational: They depend only on the joint distribution of predictor, protected attribute, features, and outcome. While convenient to work with, observational criteria have severe inherent limitations that prevent them from resolving matters of fairness conclusively. 
Going beyond observational criteria, we frame the problem of discrimination based on protected attributes in the language of causal reasoning. This viewpoint shifts attention from ""What is the right fairness criterion?"" to ""What do we want to assume about the causal data generating process?"" Through the lens of causality, we make several contributions. First, we crisply articulate why and when observational criteria fail, thus formalizing what was before a matter of opinion. Second, our approach exposes previously ignored subtleties and why they are fundamental to the problem. Finally, we put forward natural causal non-discrimination criteria and develop algorithms that satisfy them.",469.0
Delivering Cognitive Behavior Therapy to Young Adults With Symptoms of Depression and Anxiety Using a Fully Automated Conversational Agent (Woebot): A Randomized Controlled Trial,ddbdde502c1f8260ed9758bf8462513143a7d8ba,"[{'authorId': '2194363', 'name': 'K. Fitzpatrick'}, {'authorId': '6120000', 'name': 'Alison M Darcy'}, {'authorId': '14061540', 'name': 'Molly Vierhile'}]",2017.0,JMIR Mental Health,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"Background Web-based cognitive-behavioral therapeutic (CBT) apps have demonstrated efficacy but are characterized by poor adherence. Conversational agents may offer a convenient, engaging way of getting support at any time. Objective The objective of the study was to determine the feasibility, acceptability, and preliminary efficacy of a fully automated conversational agent to deliver a self-help program for college students who self-identify as having symptoms of anxiety and depression. Methods In an unblinded trial, 70 individuals age 18-28 years were recruited online from a university community social media site and were randomized to receive either 2 weeks (up to 20 sessions) of self-help content derived from CBT principles in a conversational format with a text-based conversational agent (Woebot) (n=34) or were directed to the National Institute of Mental Health ebook, “Depression in College Students,” as an information-only control group (n=36). All participants completed Web-based versions of the 9-item Patient Health Questionnaire (PHQ-9), the 7-item Generalized Anxiety Disorder scale (GAD-7), and the Positive and Negative Affect Scale at baseline and 2-3 weeks later (T2). Results Participants were on average 22.2 years old (SD 2.33), 67% female (47/70), mostly non-Hispanic (93%, 54/58), and Caucasian (79%, 46/58). Participants in the Woebot group engaged with the conversational agent an average of 12.14 (SD 2.23) times over the study period. No significant differences existed between the groups at baseline, and 83% (58/70) of participants provided data at T2 (17% attrition). Intent-to-treat univariate analysis of covariance revealed a significant group difference on depression such that those in the Woebot group significantly reduced their symptoms of depression over the study period as measured by the PHQ-9 (F=6.47; P=.01) while those in the information control group did not. In an analysis of completers, participants in both groups significantly reduced anxiety as measured by the GAD-7 (F1,54= 9.24; P=.004). Participants’ comments suggest that process factors were more influential on their acceptability of the program than content factors mirroring traditional therapy. Conclusions Conversational agents appear to be a feasible, engaging, and effective way to deliver CBT.",927.0
A Rawlsian algorithm for autonomous vehicles,a4da7fb045ee9ae55be57aaadad172016022d322,"[{'authorId': '9545518', 'name': 'D. Leben'}]",2017.0,Ethics and Information Technology,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,,65.0
Deal or No Deal? End-to-End Learning of Negotiation Dialogues,6a8dbea5e40831bd6e987c03b76487f45ac49599,"[{'authorId': '35084211', 'name': 'M. Lewis'}, {'authorId': '13759615', 'name': 'Denis Yarats'}, {'authorId': '2921469', 'name': 'Y. Dauphin'}, {'authorId': '153432684', 'name': 'Devi Parikh'}, {'authorId': '1746610', 'name': 'Dhruv Batra'}]",2017.0,Conference on Empirical Methods in Natural Language Processing,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"Much of human dialogue occurs in semi-cooperative settings, where agents with different goals attempt to agree on common decisions. Negotiations require complex communication and reasoning skills, but success is easy to measure, making this an interesting task for AI. We gather a large dataset of human-human negotiations on a multi-issue bargaining task, where agents who cannot observe each other’s reward functions must reach an agreement (or a deal) via natural language dialogue. For the first time, we show it is possible to train end-to-end models for negotiation, which must learn both linguistic and reasoning skills with no annotated dialogue states. We also introduce dialogue rollouts, in which the model plans ahead by simulating possible complete continuations of the conversation, and find that this technique dramatically improves performance. Our code and dataset are publicly available.",295.0
Federated Multi-Task Learning,276194e96ebd620b5cff35a9168bdda39a0be57b,"[{'authorId': '145260024', 'name': 'Virginia Smith'}, {'authorId': '2054892', 'name': 'Chao-Kai Chiang'}, {'authorId': '2095979', 'name': 'Maziar Sanjabi'}, {'authorId': '145532827', 'name': 'Ameet Talwalkar'}]",2017.0,NIPS,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"Federated learning poses new statistical and systems challenges in training machine learning models over distributed networks of devices. In this work, we show that multi-task learning is naturally suited to handle the statistical challenges of this setting, and propose a novel systems-aware optimization method, MOCHA, that is robust to practical systems issues. Our method and theory for the first time consider issues of high communication cost, stragglers, and fault tolerance for distributed multi-task learning. The resulting method achieves significant speedups compared to alternatives in the federated setting, as we demonstrate through simulations on real-world federated datasets.",1007.0
The Trouble with Autopilots: Assisted and Autonomous Driving on the Social Road,5d2427a246c83c422f7319aaa0d9d81076b54c1c,"[{'authorId': '144623110', 'name': 'Barry A. T. Brown'}, {'authorId': '1958177', 'name': 'E. Laurier'}]",2017.0,International Conference on Human Factors in Computing Systems,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"As self-driving cars have grown in sophistication and ability, they have been deployed on the road in both localised tests and as regular private vehicles. In this paper we draw upon publicly available videos of autonomous and assisted driving (specifically the Tesla autopilot and Google self-driving car) to explore how their drivers and the drivers of other cars interact with, and make sense of, the actions of these cars. Our findings provide an early perspective on human interaction with new forms of driving involving assisted-car drivers, autonomous vehicles and other road users. The focus is on social interaction on the road, and how drivers communicate through, and interpret, the movement of cars. We provide suggestions toward increasing the transparency of autopilots' actions for both their driver and other drivers.",98.0
A Survey of Data Mining and Machine Learning Methods for Cyber Security Intrusion Detection,2aadb938af2f77a6ad9321ff873c1c9b9a579fcb,"[{'authorId': '100818765', 'name': 'Lalu Banoth'}, {'authorId': '70392354', 'name': 'M. S. Teja'}, {'authorId': '2102149819', 'name': 'M. Saicharan'}, {'authorId': '143612469', 'name': 'N. Chandra'}]",2017.0,,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"Cyber security is that the body of technologies, processes and practices designed to safeguard networks, computers, programs and knowledge from attack, harm or unauthorized access. During a computing context, the term security implies cyber security. This survey paper describes a targeted literature survey of machine learning (ML) and data processing (DM) strategies for cyber analytics in support of intrusion detection. This paper focuses totally on cyber intrusion detection as it applies to wired networks. With a wired network, associate oppose must experience many layers of defense at firewalls and operative systems, or gain physical access to the network. The quality of ML/DM algorithms is addressed, discussion of challenges for victimization ML/DM for cyber security is conferred, and some recommendations on once to use a given methodology area unit provided.",989.0
DART: Noise Injection for Robust Imitation Learning,a59658d7b74f63a34e7182addbba1214775f49f5,"[{'authorId': '47983192', 'name': 'Michael Laskey'}, {'authorId': '120703894', 'name': 'Jonathan Lee'}, {'authorId': '145609073', 'name': 'Roy Fox'}, {'authorId': '2745001', 'name': 'A. Dragan'}, {'authorId': '144344283', 'name': 'Ken Goldberg'}]",2017.0,Conference on Robot Learning,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"One approach to Imitation Learning is Behavior Cloning, in which a robot observes a supervisor and infers a control policy. A known problem with this ""off-policy"" approach is that the robot's errors compound when drifting away from the supervisor's demonstrations. On-policy, techniques alleviate this by iteratively collecting corrective actions for the current robot policy. However, these techniques can be tedious for human supervisors, add significant computation burden, and may visit dangerous states during training. We propose an off-policy approach that injects noise into the supervisor's policy while demonstrating. This forces the supervisor to demonstrate how to recover from errors. We propose a new algorithm, DART (Disturbances for Augmenting Robot Trajectories), that collects demonstrations with injected noise, and optimizes the noise level to approximate the error of the robot's trained policy during data collection. We compare DART with DAgger and Behavior Cloning in two domains: in simulation with an algorithmic supervisor on the MuJoCo tasks (Walker, Humanoid, Hopper, Half-Cheetah) and in physical experiments with human supervisors training a Toyota HSR robot to perform grasping in clutter. For high dimensional tasks like Humanoid, DART can be up to $3x$ faster in computation time and only decreases the supervisor's cumulative reward by $5\%$ during training, whereas DAgger executes policies that have $80\%$ less cumulative reward than the supervisor. On the grasping in clutter task, DART obtains on average a $62\%$ performance increase over Behavior Cloning.",163.0
What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?,ff7bcaa4556cb13fc7bf03e477172493546172cd,"[{'authorId': '47645184', 'name': 'Alex Kendall'}, {'authorId': '2681954', 'name': 'Y. Gal'}]",2017.0,NIPS,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"There are two major types of uncertainty one can model. Aleatoric uncertainty captures noise inherent in the observations. On the other hand, epistemic uncertainty accounts for uncertainty in the model - uncertainty which can be explained away given enough data. Traditionally it has been difficult to model epistemic uncertainty in computer vision, but with new Bayesian deep learning tools this is now possible. We study the benefits of modeling epistemic vs. aleatoric uncertainty in Bayesian deep learning models for vision tasks. For this we present a Bayesian deep learning framework combining input-dependent aleatoric uncertainty together with epistemic uncertainty. We study models under the framework with per-pixel semantic segmentation and depth regression tasks. Further, our explicit uncertainty formulation leads to new loss functions for these tasks, which can be interpreted as learned attenuation. This makes the loss more robust to noisy data, also giving new state-of-the-art results on segmentation and depth regression benchmarks.",3042.0
Stabilising Experience Replay for Deep Multi-Agent Reinforcement Learning,3ac0fea1e5395cfb0dc1f0ee2b921fe22b23fed0,"[{'authorId': '145356667', 'name': 'Jakob N. Foerster'}, {'authorId': '39683441', 'name': 'Nantas Nardelli'}, {'authorId': '38698094', 'name': 'Gregory Farquhar'}, {'authorId': '2285516', 'name': 'Triantafyllos Afouras'}, {'authorId': '143635540', 'name': 'Philip H. S. Torr'}, {'authorId': '143967473', 'name': 'Pushmeet Kohli'}, {'authorId': '1766767', 'name': 'Shimon Whiteson'}]",2017.0,International Conference on Machine Learning,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"Many real-world problems, such as network packet routing and urban traffic control, are naturally modeled as multi-agent reinforcement learning (RL) problems. However, existing multi-agent RL methods typically scale poorly in the problem size. Therefore, a key challenge is to translate the success of deep learning on single-agent RL to the multi-agent setting. A major stumbling block is that independent Q-learning, the most popular multi-agent RL method, introduces nonstationarity that makes it incompatible with the experience replay memory on which deep Q-learning relies. This paper proposes two methods that address this problem: 1) using a multi-agent variant of importance sampling to naturally decay obsolete data and 2) conditioning each agent's value function on a fingerprint that disambiguates the age of the data sampled from the replay memory. Results on a challenging decentralised variant of StarCraft unit micro-management confirm that these methods enable the successful combination of experience replay with multi-agent RL.",471.0
OFFER: Off-Environment Reinforcement Learning,df985d3ea42d7e29510ec00a13d631379eb4a919,"[{'authorId': '2474449', 'name': 'K. Ciosek'}, {'authorId': '1766767', 'name': 'Shimon Whiteson'}]",2017.0,AAAI Conference on Artificial Intelligence,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"
 
 Policy gradient methods have been widely applied in reinforcement learning. For reasons of safety and cost, learning is often conducted using a simulator. However, learning in simulation does not traditionally utilise the opportunity to improve learning by adjusting certain environment variables - state features that are randomly determined by the environment in a physical setting but controllable in a simulator. Exploiting environment variables is crucial in domains containing significant rare events (SREs), e.g., unusual wind conditions that can crash a helicopter, which are rarely observed under random sampling but have a considerable impact on expected return. We propose off environment reinforcement learning (OFFER), which addresses such cases by simultaneously optimising the policy and a proposal distribution over environment variables. We prove that OFFER converges to a locally optimal policy and show experimentally that it learns better and faster than a policy gradient baseline.
 
",41.0
Multi-agent Reinforcement Learning in Sequential Social Dilemmas,d4e137eeec6ca4883df9f9cf40cc49f62e8388be,"[{'authorId': '1700356', 'name': 'Joel Z. Leibo'}, {'authorId': '3133079', 'name': 'V. Zambaldi'}, {'authorId': '1975889', 'name': 'Marc Lanctot'}, {'authorId': '1995313', 'name': 'J. Marecki'}, {'authorId': '1686971', 'name': 'T. Graepel'}]",2017.0,Adaptive Agents and Multi-Agent Systems,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"Matrix games like Prisoner's Dilemma have guided research on social dilemmas for decades. However, they necessarily treat the choice to cooperate or defect as an atomic action. In real-world social dilemmas these choices are temporally extended. Cooperativeness is a property that applies to policies, not elementary actions. We introduce sequential social dilemmas that share the mixed incentive structure of matrix game social dilemmas but also require agents to learn policies that implement their strategic intentions. We analyze the dynamics of policies learned by multiple self-interested independent learning agents, each using its own deep Q-network, on two Markov games we introduce here: 1. a fruit Gathering game and 2. a Wolfpack hunting game. We characterize how learned behavior in each domain changes as a function of environmental factors including resource abundance. Our experiments show how conflict can emerge from competition over shared resources and shed light on how the sequential nature of real world social dilemmas affects cooperation.",494.0
Agent-Agnostic Human-in-the-Loop Reinforcement Learning,e245ef09fe9e4f56d0fa7f75257298588f4d0392,"[{'authorId': '152422014', 'name': 'David Abel'}, {'authorId': '3373139', 'name': 'J. Salvatier'}, {'authorId': '2214496', 'name': 'Andreas Stuhlmüller'}, {'authorId': '47107786', 'name': 'Owain Evans'}]",2017.0,arXiv.org,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"Providing Reinforcement Learning agents with expert advice can dramatically improve various aspects of learning. Prior work has developed teaching protocols that enable agents to learn efficiently in complex environments; many of these methods tailor the teacher's guidance to agents with a particular representation or underlying learning scheme, offering effective but specialized teaching procedures. In this work, we explore protocol programs, an agent-agnostic schema for Human-in-the-Loop Reinforcement Learning. Our goal is to incorporate the beneficial properties of a human teacher into Reinforcement Learning without making strong assumptions about the inner workings of the agent. We show how to represent existing approaches such as action pruning, reward shaping, and training in simulation as special cases of our schema and conduct preliminary experiments on simple domains.",48.0
The Option-Critic Architecture,15b26d8cb35d7e795c8832fe08794224ee1e9f84,"[{'authorId': '145180695', 'name': 'Pierre-Luc Bacon'}, {'authorId': '40638357', 'name': 'J. Harb'}, {'authorId': '144368601', 'name': 'Doina Precup'}]",2016.0,AAAI Conference on Artificial Intelligence,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"
 
 Temporal abstraction is key to scaling up learning and planning in reinforcement learning. While planning with temporally extended actions is well understood, creating such abstractions autonomously from data has remained challenging.We tackle this problem in the framework of options [Sutton,Precup and Singh, 1999; Precup, 2000]. We derive policy gradient theorems for options and propose a new option-critic architecture capable of learning both the internal policies and the termination conditions of options, in tandem with the policy over options, and without the need to provide any additional rewards or subgoals. Experimental results in both discrete and continuous environments showcase the flexibility and efficiency of the framework.
 
",729.0
Learning End-to-End Goal-Oriented Dialog,f81be44000814e7bcb12ae04b4e2d9c01b6515b3,"[{'authorId': '1713934', 'name': 'Antoine Bordes'}, {'authorId': '145183709', 'name': 'J. Weston'}]",2016.0,International Conference on Learning Representations,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"Traditional dialog systems used in goal-oriented applications require a lot of domain-specific handcrafting, which hinders scaling up to new domains. End-to-end dialog systems, in which all components are trained from the dialogs themselves, escape this limitation. But the encouraging success recently obtained in chit-chat dialog may not carry over to goal-oriented settings. This paper proposes a testbed to break down the strengths and shortcomings of end-to-end dialog systems in goal-oriented applications. Set in the context of restaurant reservation, our tasks require manipulating sentences and symbols, so as to properly conduct conversations, issue API calls and use the outputs of such calls. We show that an end-to-end dialog system based on Memory Networks can reach promising, yet imperfect, performance and learn to perform non-trivial operations. We confirm those results by comparing our system to a hand-crafted slot-filling baseline on data from the second Dialog State Tracking Challenge (Henderson et al., 2014a). We show similar result patterns on data extracted from an online concierge service.",735.0
A Deep Hierarchical Approach to Lifelong Learning in Minecraft,3c3861c607fb79f3fbf79552018724617fc8ba1b,"[{'authorId': '3393407', 'name': 'Chen Tessler'}, {'authorId': '39791491', 'name': 'Shahar Givony'}, {'authorId': '3331540', 'name': 'Tom Zahavy'}, {'authorId': '3187297', 'name': 'D. Mankowitz'}, {'authorId': '1712535', 'name': 'Shie Mannor'}]",2016.0,AAAI Conference on Artificial Intelligence,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"
 
 We propose a lifelong learning system that has the ability to reuse and transfer knowledge from one task to another while efficiently retaining the previously learned knowledge-base. Knowledge is transferred by learning reusable skills to solve tasks in Minecraft, a popular video game which is an unsolved and high-dimensional lifelong learning problem. These reusable skills, which we refer to as Deep Skill Networks, are then incorporated into our novel Hierarchical Deep Reinforcement Learning Network (H-DRLN) architecture using two techniques: (1) a deep skill array and (2) skill distillation, our novel variation of policy distillation (Rusu et. al. 2015) for learning skills. Skill distillation enables the H-DRLN to efficiently retain knowledge and therefore scale in lifelong learning, by accumulating knowledge and encapsulating multiple reusable skills into a single distilled network. The H-DRLN exhibits superior performance and lower learning sample complexity compared to the regular Deep Q Network (Mnih et. al. 2015) in sub-domains of Minecraft.
 
",312.0
Communication-Efficient Learning of Deep Networks from Decentralized Data,d1dbf643447405984eeef098b1b320dee0b3b8a7,"[{'authorId': '145057514', 'name': 'H. B. McMahan'}, {'authorId': '31449330', 'name': 'Eider Moore'}, {'authorId': '1878835', 'name': 'D. Ramage'}, {'authorId': '37089174', 'name': 'S. Hampson'}, {'authorId': '2661025', 'name': 'B. A. Y. Arcas'}]",2016.0,International Conference on Artificial Intelligence and Statistics,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"Modern mobile devices have access to a wealth of data suitable for learning models, which in turn can greatly improve the user experience on the device. For example, language models can improve speech recognition and text entry, and image models can automatically select good photos. However, this rich data is often privacy sensitive, large in quantity, or both, which may preclude logging to the data center and training there using conventional approaches. We advocate an alternative that leaves the training data distributed on the mobile devices, and learns a shared model by aggregating locally-computed updates. We term this decentralized approach Federated Learning. 
We present a practical method for the federated learning of deep networks based on iterative model averaging, and conduct an extensive empirical evaluation, considering five different model architectures and four datasets. These experiments demonstrate the approach is robust to the unbalanced and non-IID data distributions that are a defining characteristic of this setting. Communication costs are the principal constraint, and we show a reduction in required communication rounds by 10-100x as compared to synchronized stochastic gradient descent.",7087.0
Multiagent cooperation and competition with deep reinforcement learning,83bf91012997019f432179aad798e6d3fbb95c36,"[{'authorId': '2543395', 'name': 'Ardi Tampuu'}, {'authorId': '3319842', 'name': 'Tambet Matiisen'}, {'authorId': '2064080650', 'name': 'Dorian Kodelja'}, {'authorId': '2830766', 'name': 'Ilya Kuzovkin'}, {'authorId': '1886587', 'name': 'Kristjan Korjus'}, {'authorId': '2768016', 'name': 'Juhan Aru'}, {'authorId': '2075928', 'name': 'J. Aru'}, {'authorId': '144846212', 'name': 'Raul Vicente'}]",2015.0,PLoS ONE,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"Evolution of cooperation and competition can appear when multiple adaptive agents share a biological, social, or technological niche. In the present work we study how cooperation and competition emerge between autonomous agents that learn by reinforcement while using only their raw visual input as the state representation. In particular, we extend the Deep Q-Learning framework to multiagent environments to investigate the interaction between two learning agents in the well-known video game Pong. By manipulating the classical rewarding scheme of Pong we show how competitive and collaborative behaviors emerge. We also describe the progression from competitive to collaborative behavior when the incentive to cooperate is increased. Finally we show how learning by playing against another adaptive agent, instead of against a hard-wired algorithm, results in more robust strategies. The present work shows that Deep Q-Networks can become a useful tool for studying decentralized learning of multiagent systems coping with high-dimensional environments.",580.0
The technological singularity,61cfce0bd2137f62133902010655b7aec991acd3,"[{'authorId': '2118098717', 'name': 'James D. Miller'}, {'authorId': '26336155', 'name': 'Roman V. Yampolskiy'}, {'authorId': '2054678912', 'name': 'S. Armstrong'}, {'authorId': '144527605', 'name': 'V. Callaghan'}]",2017.0,,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,,110.0
The future of employment: How susceptible are jobs to computerisation?,d4b2936fc0768ea5709221b7bb107350dfa1601a,"[{'authorId': '117305547', 'name': 'C. Frey'}, {'authorId': '144484861', 'name': 'Michael A. Osborne'}]",2017.0,,"['AI Research Considerations for Human Existential Safety (ARCHES)', 'Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",2,,5111.0
Generating Plans that Predict Themselves,8c2c64ab49ed28761f3857315542d341264e13ba,"[{'authorId': '1843342', 'name': 'J. Fisac'}, {'authorId': '2118484076', 'name': 'Chang Liu'}, {'authorId': '2158860', 'name': 'Jessica B. Hamrick'}, {'authorId': '144797536', 'name': 'S. Sastry'}, {'authorId': '144029083', 'name': 'J. Hedrick'}, {'authorId': '1799860', 'name': 'T. Griffiths'}, {'authorId': '2745001', 'name': 'A. Dragan'}]",2018.0,Workshop on the Algorithmic Foundations of Robotics,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,,24.0
"Autonomous Vehicles: Disengagements, Accidents and Reaction Times",b8131c2feda7fd02c98bacb8c76b1120dd242b53,"[{'authorId': '145821394', 'name': 'V. Dixit'}, {'authorId': '13648548', 'name': 'S. Chand'}, {'authorId': '48301711', 'name': 'D. Nair'}]",2016.0,PLoS ONE,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"Autonomous vehicles are being viewed with scepticism in their ability to improve safety and the driving experience. A critical issue with automated driving at this stage of its development is that it is not yet reliable and safe. When automated driving fails, or is limited, the autonomous mode disengages and the drivers are expected to resume manual driving. For this transition to occur safely, it is imperative that drivers react in an appropriate and timely manner. Recent data released from the California trials provide compelling insights into the current factors influencing disengagements of autonomous mode. Here we show that the number of accidents observed has a significantly high correlation with the autonomous miles travelled. The reaction times to take control of the vehicle in the event of a disengagement was found to have a stable distribution across different companies at 0.83 seconds on average. However, there were differences observed in reaction times based on the type of disengagements, type of roadway and autonomous miles travelled. Lack of trust caused by the exposure to automated disengagements was found to increase the likelihood to take control of the vehicle manually. Further, with increased vehicle miles travelled the reaction times were found to increase, which suggests an increased level of trust with more vehicle miles travelled. We believe that this research would provide insurers, planners, traffic management officials and engineers fundamental insights into trust and reaction times that would help them design and engineer their systems.",208.0
Motor learning affects car-to-driver handover in automated vehicles,61950ed70d3c62b1554ea6e01794ba2262c99718,"[{'authorId': '5173934', 'name': 'Lene K. Harbott'}, {'authorId': '2166933', 'name': 'I. Nisky'}, {'authorId': '2863830', 'name': 'Selina Pan'}, {'authorId': '1713767', 'name': 'A. Okamura'}, {'authorId': '144128370', 'name': 'J. C. Gerdes'}]",2016.0,Science Robotics,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"A human driver requires a period of motor adaptation to resume normal steering after taking control of an automated vehicle. Vehicles in the foreseeable future will be required to transition between autonomous driving (without human involvement) and full human control. During this transition period, the human, who has not been actively engaged in the driving process, must resume the motor control necessary to steer the car. The in-car study presented here demonstrates that when human drivers are presented with a steering behavior that is different from the last time they were in control, specifically the ratio of hand wheel angle to road wheel angle (emulating a change in vehicle speed), they undergo a significant period of adaptation before they return to their previous steering behavior. However, drivers do not require an adaptation period to return to previous driving behavior after changes in steering torque. These findings have implications for the design of vehicles that transition from automated to manual driving and for understanding of human motor control in real-world tasks.",88.0
Federated Learning: Strategies for Improving Communication Efficiency,7fcb90f68529cbfab49f471b54719ded7528d0ef,"[{'authorId': '32139366', 'name': 'Jakub Konecný'}, {'authorId': '145057514', 'name': 'H. B. McMahan'}, {'authorId': '1815972', 'name': 'Felix X. Yu'}, {'authorId': '2662221', 'name': 'Peter Richtárik'}, {'authorId': '9486035', 'name': 'A. Suresh'}, {'authorId': '36577444', 'name': 'D. Bacon'}]",2016.0,arXiv.org,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"Federated Learning is a machine learning setting where the goal is to train a high-quality centralized model while training data remains distributed over a large number of clients each with unreliable and relatively slow network connections. We consider learning algorithms for this setting where on each round, each client independently computes an update to the current model based on its local data, and communicates this update to a central server, where the client-side updates are aggregated to compute a new global model. The typical clients in this setting are mobile phones, and communication efficiency is of the utmost importance. In this paper, we propose two ways to reduce the uplink communication costs: structured updates, where we directly learn an update from a restricted space parametrized using a smaller number of variables, e.g. either low-rank or a random mask; and sketched updates, where we learn a full model update and then compress it using a combination of quantization, random rotations, and subsampling before sending it to the server. Experiments on both convolutional and recurrent networks show that the proposed methods can reduce the communication cost by two orders of magnitude.",2870.0
Strategic Attentive Writer for Learning Macro-Actions,4ba25cb493ac7a03fc15d3b936257c9a6c689c1d,"[{'authorId': '9948791', 'name': 'A. Vezhnevets'}, {'authorId': '3255983', 'name': 'Volodymyr Mnih'}, {'authorId': '2217144', 'name': 'Simon Osindero'}, {'authorId': '1753223', 'name': 'A. Graves'}, {'authorId': '1689108', 'name': 'Oriol Vinyals'}, {'authorId': '70495322', 'name': 'J. Agapiou'}, {'authorId': '2645384', 'name': 'K. Kavukcuoglu'}]",2016.0,NIPS,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"We present a novel deep recurrent neural network architecture that learns to build implicit plans in an end-to-end manner by purely interacting with an environment in reinforcement learning setting. The network builds an internal plan, which is continuously updated upon observation of the next input from the environment. It can also partition this internal representation into contiguous sub- sequences by learning for how long the plan can be committed to - i.e. followed without re-planing. Combining these properties, the proposed model, dubbed STRategic Attentive Writer (STRAW) can learn high-level, temporally abstracted macro- actions of varying lengths that are solely learnt from data without any prior information. These macro-actions enable both structured exploration and economic computation. We experimentally demonstrate that STRAW delivers strong improvements on several ATARI games by employing temporally extended planning strategies (e.g. Ms. Pacman and Frostbite). It is at the same time a general algorithm that can be applied on any sequence data. To that end, we also show that when trained on text prediction task, STRAW naturally predicts frequent n-grams (instead of macro-actions), demonstrating the generality of the approach.",155.0
OpenAI Gym,ff7f3277c6fa759e84e1ab7664efdac1c1cec76b,"[{'authorId': '2065151121', 'name': 'Greg Brockman'}, {'authorId': '34415167', 'name': 'Vicki Cheung'}, {'authorId': '152877508', 'name': 'Ludwig Pettersson'}, {'authorId': '2113526509', 'name': 'Jonas Schneider'}, {'authorId': '47971768', 'name': 'J. Schulman'}, {'authorId': '2109541439', 'name': 'Jie Tang'}, {'authorId': '2563432', 'name': 'Wojciech Zaremba'}]",2016.0,arXiv.org,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"OpenAI Gym is a toolkit for reinforcement learning research. It includes a growing collection of benchmark problems that expose a common interface, and a website where people can share their results and compare the performance of algorithms. This whitepaper discusses the components of OpenAI Gym and the design decisions that went into the software.",3282.0
A Concise Introduction to Decentralized POMDPs,836ad0c693bc5ef171ee2b07b3f4d1bd2a0ae24c,"[{'authorId': '1799949', 'name': 'F. Oliehoek'}, {'authorId': '34903901', 'name': 'Chris Amato'}]",2016.0,SpringerBriefs in Intelligent Systems,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,,540.0
"The Age of Em: Work, Love and Life When Robots Rule the Earth",0a69ed353cc4526382821734b297585dd90ce233,"[{'authorId': '145447707', 'name': 'R. Hanson'}]",2016.0,,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"Robots may one day rule the world, but what is a robot-ruled Earth like? Many think the first truly smart robots will be brain emulations or ems. Scan a human brain, then run a model with the same connections on a fast computer, and you have a robot brain, but recognizably human. Train an em to do some job and copy it a million times: an army of workers is at your disposal. When they can be made cheaply, within perhaps a century, ems will displace humans in most jobs. In this new economic era, the world economy may double in size every few weeks. Some say we can't know the future, especially following such a disruptive new technology, but Professor Robin Hanson sets out to prove them wrong. Applying decades of expertise in physics, computer science, and economics, he uses standard theories to paint a detailed picture of a world dominated by ems. While human lives don't change greatly in the em era, em lives are as different from ours as our lives are from those of our farmer and forager ancestors. Ems make us question common assumptions of moral progress, because they reject many of the values we hold dear. Read about em mind speeds, body sizes, job training and career paths, energy use and cooling infrastructure, virtual reality, aging and retirement, death and immortality, security, wealth inequality, religion, teleportation, identity, cities, politics, law, war, status, friendship and love. This book shows you just how strange your descendants may be, though ems are no stranger than we would appear to our ancestors. To most ems, it seems good to be an em.",69.0
Learning Multiagent Communication with Backpropagation,50295c19e177480ba3599300de1ab837cc62b08c,"[{'authorId': '2265067', 'name': 'Sainbayar Sukhbaatar'}, {'authorId': '3149531', 'name': 'Arthur D. Szlam'}, {'authorId': '2276554', 'name': 'R. Fergus'}]",2016.0,NIPS,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"Many tasks in AI require the collaboration of multiple agents. Typically, the communication protocol between agents is manually specified and not altered during training. In this paper we explore a simple neural model, called CommNet, that uses continuous communication for fully cooperative tasks. The model consists of multiple agents and the communication between them is learned alongside their policy. We apply this model to a diverse set of tasks, demonstrating the ability of the agents to learn to communicate amongst themselves, yielding improved performance over non-communicative agents and baselines. In some cases, it is possible to interpret the language devised by the agents, revealing simple but effective strategies for solving the task at hand.",830.0
Can you program ethics into a self-driving car?,ca4b88df02d2b6e9c3f0089df158ba959229d176,"[{'authorId': '3376114', 'name': 'Noah Goodall'}]",2016.0,IEEE spectrum,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"IT'S 2034. A drunken man walking along a sidewalk at night trips and falls directly in front of a driverless car, which strikes him square on, killing him instantly. Had a human been at the wheel, the death would have been considered an accident because the pedestrian was clearly at fault and no reasonable person could have swerved in time. But the ""reasonable person"" legal standard for driver negligence disappeared back in the 2020s, when the proliferation of driverless cars reduced crash rates by 90 percent. Now the standard is that of the reasonable robot. The victim's family sues the vehicle manufacturer on that ground, claiming that, although the car didn't have time to brake, it could have swerved around the pedestrian, crossing the double yellow line and colliding with the empty driverless vehicle in the next lane. A reconstruction of the crash using data from the vehicle's own sensors confirms this. The plaintiff's attorney, deposing the car's lead software designer, asks: ""Why didn't the car swerve?""",119.0
Learning to Communicate with Deep Multi-Agent Reinforcement Learning,0772905d40b9afa3dc087a88184f09f3b3e1464f,"[{'authorId': '145356667', 'name': 'Jakob N. Foerster'}, {'authorId': '3365565', 'name': 'Yannis Assael'}, {'authorId': '1737568', 'name': 'N. D. Freitas'}, {'authorId': '1766767', 'name': 'Shimon Whiteson'}]",2016.0,NIPS,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"We consider the problem of multiple agents sensing and acting in environments with the goal of maximising their shared utility. In these environments, agents must learn communication protocols in order to share information that is needed to solve the tasks. By embracing deep neural networks, we are able to demonstrate end-to-end learning of protocols in complex environments inspired by communication riddles and multi-agent computer vision problems with partial observability. We propose two approaches for learning in these domains: Reinforced Inter-Agent Learning (RIAL) and Differentiable Inter-Agent Learning (DIAL). The former uses deep Q-learning, while the latter exploits the fact that, during learning, agents can backpropagate error derivatives through (noisy) communication channels. Hence, this approach uses centralised learning but decentralised execution. Our experiments introduce new environments for studying the learning of communication protocols and present a set of engineering innovations that are essential for success in these domains.",1142.0
Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation,d37620e6f8fe678a43e12930743281cd8cca6a66,"[{'authorId': '1954876', 'name': 'Tejas D. Kulkarni'}, {'authorId': '144958935', 'name': 'Karthik Narasimhan'}, {'authorId': '3231182', 'name': 'Ardavan Saeedi'}, {'authorId': '1763295', 'name': 'J. Tenenbaum'}]",2016.0,NIPS,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"Learning goal-directed behavior in environments with sparse feedback is a major challenge for reinforcement learning algorithms. The primary difficulty arises due to insufficient exploration, resulting in an agent being unable to learn robust value functions. Intrinsically motivated agents can explore new behavior for its own sake rather than to directly solve problems. Such intrinsic behaviors could eventually help the agent solve tasks posed by the environment. We present hierarchical-DQN (h-DQN), a framework to integrate hierarchical value functions, operating at different temporal scales, with intrinsically motivated deep reinforcement learning. A top-level value function learns a policy over intrinsic goals, and a lower-level function learns a policy over atomic actions to satisfy the given goals. h-DQN allows for flexible goal specifications, such as functions over entities and relations. This provides an efficient space for exploration in complicated environments. We demonstrate the strength of our approach on two problems with very sparse, delayed feedback: (1) a complex discrete stochastic decision process, and (2) the classic ATARI game `Montezuma's Revenge'.",903.0
Generating Visual Explanations,ecf551d532d0e9cfb252a1bea04d14db620bc488,"[{'authorId': '2234342', 'name': 'Lisa Anne Hendricks'}, {'authorId': '2893664', 'name': 'Zeynep Akata'}, {'authorId': '34849128', 'name': 'Marcus Rohrbach'}, {'authorId': '7408951', 'name': 'Jeff Donahue'}, {'authorId': '48920094', 'name': 'B. Schiele'}, {'authorId': '1753210', 'name': 'Trevor Darrell'}]",2016.0,European Conference on Computer Vision,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,,518.0
Formalizing human-robot mutual adaptation: A bounded memory model,d32754081b774c0a2b46724adba44ef01e01a81b,"[{'authorId': '37586303', 'name': 'S. Nikolaidis'}, {'authorId': '2054887372', 'name': 'Anton Kuznetsov'}, {'authorId': '145463096', 'name': 'David Hsu'}, {'authorId': '1752197', 'name': 'S. Srinivasa'}]",2016.0,IEEE/ACM International Conference on Human-Robot Interaction,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"Mutual adaptation is critical for effective team collaboration. This paper presents a formalism for human-robot mutual adaptation in collaborative tasks. We propose the bounded-memory adaptation model (BAM), which captures human adaptive behaviors based on a bounded memory assumption. We integrate BAM into a partially observable stochastic model, which enables robot adaptation to the human. When the human is adaptive, the robot will guide the human towards a new, optimal collaborative strategy unknown to the human in advance. When the human is not willing to change their strategy, the robot adapts to the human in order to retain human trust. Human subject experiments indicate that the proposed formalism can significantly improve the effectiveness of human-robot teams, while human subject ratings on the robot performance and trust are comparable to those achieved by cross training, a state-of-the-art human-robot team training practice.",56.0
“Why Should I Trust You?”: Explaining the Predictions of Any Classifier,5091316bb1c6db6c6a813f4391911a5c311fdfe0,"[{'authorId': '78846919', 'name': 'Marco Tulio Ribeiro'}, {'authorId': '34650964', 'name': 'Sameer Singh'}, {'authorId': '1730156', 'name': 'Carlos Guestrin'}]",2016.0,North American Chapter of the Association for Computational Linguistics,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally varound the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.",9989.0
Rethinking the Inception Architecture for Computer Vision,23ffaa0fe06eae05817f527a47ac3291077f9e58,"[{'authorId': '2574060', 'name': 'Christian Szegedy'}, {'authorId': '2657155', 'name': 'Vincent Vanhoucke'}, {'authorId': '2054165706', 'name': 'Sergey Ioffe'}, {'authorId': '1789737', 'name': 'Jonathon Shlens'}, {'authorId': '3282833', 'name': 'Z. Wojna'}]",2015.0,Computer Vision and Pattern Recognition,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"Convolutional networks are at the core of most state of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21:2% top-1 and 5:6% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3:5% top-5 error and 17:3% top-1 error on the validation set and 3:6% top-5 error on the official test set.",19856.0
Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning,f35de4f9b1a7c4d3fa96a0d2ab1bf8937671f6b6,"[{'authorId': '2681954', 'name': 'Y. Gal'}, {'authorId': '1744700', 'name': 'Zoubin Ghahramani'}]",2015.0,International Conference on Machine Learning,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs -- extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning.",5937.0
Mind Children The Future Of Robot And Human Intelligence,8160b79d87be5a9ec00ee11db3ab5627f617f201,"[{'authorId': '84651444', 'name': 'Matthias Schroder'}]",2016.0,,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"Thank you very much for reading mind children the future of robot and human intelligence. Maybe you have knowledge that, people have look hundreds times for their chosen readings like this mind children the future of robot and human intelligence, but end up in malicious downloads. Rather than enjoying a good book with a cup of tea in the afternoon, instead they juggled with some harmful virus inside their laptop.",262.0
Artificial General Intelligence,fa794a84f08546a630cfd90b0b1d5efd532fd112,"[{'authorId': '143727143', 'name': 'James Babcock'}, {'authorId': '50458432', 'name': 'János Kramár'}, {'authorId': '26336155', 'name': 'Roman V. Yampolskiy'}]",2016.0,Lecture Notes in Computer Science,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,,211.0
Understanding Neural Networks Through Deep Visualization,1b5a24639fa80056d1a17b15f6997d10e76cc731,"[{'authorId': '2965424', 'name': 'J. Yosinski'}, {'authorId': '2552141', 'name': 'J. Clune'}, {'authorId': '151414531', 'name': 'Anh M Nguyen'}, {'authorId': '34459632', 'name': 'Thomas J. Fuchs'}, {'authorId': '1747909', 'name': 'Hod Lipson'}]",2015.0,arXiv.org,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"Recent years have produced great advances in training large, deep neural networks (DNNs), including notable successes in training convolutional neural networks (convnets) to recognize natural images. However, our understanding of how these models work, especially what computations they perform at intermediate layers, has lagged behind. Progress in the field will be further accelerated by the development of better tools for visualizing and interpreting neural nets. We introduce two such tools here. The first is a tool that visualizes the activations produced on each layer of a trained convnet as it processes an image or video (e.g. a live webcam stream). We have found that looking at live activations that change in response to user input helps build valuable intuitions about how convnets work. The second tool enables visualizing features at each layer of a DNN via regularized optimization in image space. Because previous versions of this idea produced less recognizable images, here we introduce several new regularization methods that combine to produce qualitatively clearer, more interpretable visualizations. Both tools are open source and work on a pre-trained convnet with minimal setup.",1655.0
Screen time and sleep among school-aged children and adolescents: a systematic literature review.,46482b2ffa1f297439fe0554015754b8fc20bb83,"[{'authorId': '145542422', 'name': 'L. Hale'}, {'authorId': '5520694', 'name': 'Stanford Guan'}]",2015.0,Sleep Medicine Reviews,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,,838.0
"Relationship of Smartphone Use Severity with Sleep Quality, Depression, and Anxiety in University Students",b3b764aa5b510476cde2b6c17fba0a5c6549aa4e,"[{'authorId': '2664459', 'name': 'K. Demi̇rci̇'}, {'authorId': '4034351', 'name': 'M. Akgönül'}, {'authorId': '2568706', 'name': 'A. Akpınar'}]",2015.0,Journal of Behavioral Addictions,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"Background and Aims The usage of smartphones has increased rapidly in recent years, and this has brought about addiction. The aim of the current study was to investigate the relationship between smartphone use severity and sleep quality, depression, and anxiety in university students. Methods In total, 319 university students (203 females and 116 males; mean age = 20.5 ± 2.45) were included in the study. Participants were divided into the following three groups: a smartphone non-user group (n = 71, 22.3%), a low smartphone use group (n = 121, 37.9%), and a high smartphone use group (n = 127, 39.8%). All participants were evaluated using the Pittsburgh Sleep Quality Index, Beck Depression Inventory, Beck Anxiety Inventory; moreover, participants other than those in the smartphone non-user group were also assessed with the Smartphone Addiction Scale. Results The findings revealed that the Smartphone Addiction Scale scores of females were significantly higher than those of males. Depression, anxiety, and daytime dysfunction scores were higher in the high smartphone use group than in the low smartphone use group. Positive correlations were found between the Smartphone Addiction Scale scores and depression levels, anxiety levels, and some sleep quality scores. Conclusion The results indicate that depression, anxiety, and sleep quality may be associated with smartphone overuse. Such overuse may lead to depression and/or anxiety, which can in turn result in sleep problems. University students with high depression and anxiety scores should be carefully monitored for smartphone addiction.",907.0
Rational Use of Cognitive Resources: Levels of Analysis Between the Computational and the Algorithmic,50485a11fc03e14031b08960370358c26553d7e5,"[{'authorId': '1799860', 'name': 'T. Griffiths'}, {'authorId': '3202750', 'name': 'Falk Lieder'}, {'authorId': '144002017', 'name': 'Noah D. Goodman'}]",2015.0,Topics in Cognitive Science,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"Marr's levels of analysis-computational, algorithmic, and implementation-have served cognitive science well over the last 30 years. But the recent increase in the popularity of the computational level raises a new challenge: How do we begin to relate models at different levels of analysis? We propose that it is possible to define levels of analysis that lie between the computational and the algorithmic, providing a way to build a bridge between computational- and algorithmic-level models. The key idea is to push the notion of rationality, often used in defining computational-level models, deeper toward the algorithmic level. We offer a simple recipe for reverse-engineering the mind's cognitive strategies by deriving optimal algorithms for a series of increasingly more realistic abstract computational architectures, which we call ""resource-rational analysis.""",295.0
Shared Autonomy via Hindsight Optimization,2b5bf3e17f655aad45cd705d6b24a2d4e6cb2f5c,"[{'authorId': '36031485', 'name': 'Shervin Javdani'}, {'authorId': '1752197', 'name': 'S. Srinivasa'}, {'authorId': '1756566', 'name': 'J. Bagnell'}]",2015.0,Robotics: Science and Systems,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"In shared autonomy, user input and robot autonomy are combined to control a robot to achieve a goal. Often, the robot does not know a priori which goal the user wants to achieve, and must both predict the user's intended goal, and assist in achieving that goal. We formulate the problem of shared autonomy as a Partially Observable Markov Decision Process with uncertainty over the user's goal. We utilize maximum entropy inverse optimal control to estimate a distribution over the user's goal based on the history of inputs. Ideally, the robot assists the user by solving for an action which minimizes the expected cost-to-go for the (unknown) goal. As solving the POMDP to select the optimal action is intractable, we use hindsight optimization to approximate the solution. In a user study, we compare our method to a standard predict-then-blend approach. We find that our method enables users to accomplish tasks more quickly while utilizing less input. However, when asked to rate each system, users were mixed in their assessment, citing a tradeoff between maintaining control authority and accomplishing tasks quickly.",161.0
"Adolescents’ Electronic Media Use at Night, Sleep Disturbance, and Depressive Symptoms in the Smartphone Age",bf57f201fe5264ea4c21373660702ff5480a0bad,"[{'authorId': '6538790', 'name': 'S. Lemola'}, {'authorId': '1398888963', 'name': 'Nadine Perkinson-Gloor'}, {'authorId': '145141616', 'name': 'S. Brand'}, {'authorId': '1398888947', 'name': 'J. Dewald-Kaufmann'}, {'authorId': '3757154', 'name': 'A. Grob'}]",2015.0,Journal of Youth and Adolescence,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,,625.0
Shift-Pessimistic Active Learning Using Robust Bias-Aware Prediction,072d496525290af93164b6d0c50857e8e70fdc2b,"[{'authorId': '38696911', 'name': 'Anqi Liu'}, {'authorId': '1798267', 'name': 'L. Reyzin'}, {'authorId': '1753269', 'name': 'Brian D. Ziebart'}]",2015.0,AAAI Conference on Artificial Intelligence,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"
 
 Existing approaches to active learning are generally optimistic about their certainty with respect to data shift between labeled and unlabeled data. They assume that unknown datapoint labels follow the inductive biases of the active learner. As a result, the most useful datapoint labels—ones that refute current inductive biases—are rarely solicited. We propose a shift-pessimistic approach to active learning that assumes the worst-case about the unknown conditional label distribution. This closely aligns model uncertainty with generalization error, enabling more useful label solicitation. We investigate the theoretical benefits of this approach and demonstrate its empirical advantages on probabilistic binary classification tasks.
 
",24.0
Learning the Preferences of Bounded Agents,d55b3f05ad612ecd0ae160850e9f07b1926e51bc,"[{'authorId': '47107786', 'name': 'Owain Evans'}]",2015.0,,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"A range of work in applied machine learning, psychology, and social science involves inferring a person’s preferences and beliefs from their choices or decisions. This includes work in economics on Structural Estimation, which has been used to infer beliefs about the rewards of education from observed work and education choices [1] and preferences for health outcomes from smoking behavior [2]. In machine learning, Inverse Reinforcement Learning has been applied to diverse planning and decision tasks to learn preferences and task-specific strategies [3, 4]. Large-scale systems in industry also learn preferences from behavior: for example, people’s behavior on social networking sites is used to infer what movies, articles, and photos they will like [5].",31.0
Modeling Cognition with Probabilistic Programs: Representations and Algorithms,8508720007788c2845fd1e1d6606f99056439751,"[{'authorId': '2214496', 'name': 'Andreas Stuhlmüller'}]",2015.0,,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"This thesis develops probabilistic programming as a
productive metaphor for understanding cognition, both with respect
to mental representations and the manipulation of such
representations. In the first half of the thesis, I demonstrate the
representational power of probabilistic programs in the domains of
concept learning and social reasoning. I provide examples of richly
structured concepts, defined in terms of systems of relations,
subparts, and recursive embeddings, that are naturally expressed as
programs and show initial experimental evidence that they match
human generalization patterns. I then proceed to models of
reasoning about reasoning, a domain where the expressive power of
probabilistic programs is necessary to formalize our intuitive
domain understanding due to the fact that, unlike previous
formalisms, probabilistic programs allow conditioning to be
represented in a model, not just applied to a model. I illustrate
this insight with programs that model nested reasoning in game
theory, artificial intelligence, and linguistics. In the second
half, I develop three inference algorithms with the dual intent of
showing how to efficiently compute the marginal distributions
defined by probabilistic programs, and providing building blocks
for process-level accounts of human cognition. First, I describe a
Dynamic Programming algorithm for computing the marginal
distribution of discrete probabilistic programs by compiling to
systems of equations and show that it can make inference in models
of ""reasoning about reasoning"" tractable by merging and reusing
subcomputations. Second, I introduce the setting of amortized
inference and show how learning inverse models lets us leverage
samples generated by other inference algorithms to compile
probabilistic models into fast recognition functions. Third, I
develop a generic approach to coarse-to-fine inference in
probabilistic programs and provide evidence that it can speed up
inference in models with large state spaces that have appropriate
hierarchical structure. Finally, I substantiate the claim that
probabilistic programming is a productive metaphor by outlining new
research questions that have been opened up by this line of
investigation.",3.0
"Cause, Responsibility, and Blame: oA Structural-Model Approach",9e6c655b9a2b8094f1706cc45bce1e34465f9776,"[{'authorId': '1691828', 'name': 'Joseph Y. Halpern'}]",2014.0,arXiv.org,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"A definition of causality introduced by Halpern and Pearl, which uses structural equations, is reviewed. A more refined definition is then considered, which takes into account issues of normality and typicality, which are well known to affect causal ascriptions. Causality is typically an all-or-nothing notion: either A is a cause of B or it is not. An extension of the definition of causality to capture notions of degree of responsibility and degree of blame, due to Chockler and Halpern, is reviewed. For example, if someone wins an election 11-0, then each person who votes for him is less responsible for the victory than if he had won 6-5. Degree of blame takes into account an agent's epistemic state. Roughly speaking, the degree of blame of A for B is the expected degree of responsibility of A for B, taken over the epistemic state of an agent. Finally, the structural-equations definition of causality is compared to Wright's NESS test.",11.0
Generative Adversarial Nets,54e325aee6b2d476bbbb88615ac15e251c6e8214,"[{'authorId': '153440022', 'name': 'Ian J. Goodfellow'}, {'authorId': '1403025868', 'name': 'Jean Pouget-Abadie'}, {'authorId': '153583218', 'name': 'Mehdi Mirza'}, {'authorId': '2113742925', 'name': 'Bing Xu'}, {'authorId': '1393680089', 'name': 'David Warde-Farley'}, {'authorId': '1955694', 'name': 'Sherjil Ozair'}, {'authorId': '1760871', 'name': 'Aaron C. Courville'}, {'authorId': '1751762', 'name': 'Yoshua Bengio'}]",2014.0,NIPS,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to ½ everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.",36616.0
"A randomized controlled trial of the computerized CBT programme, MoodGYM, for public mental health service users waiting for interventions.",dbe0d69aeb4270a1ed65bc61aea3939b64e91119,"[{'authorId': '47293134', 'name': 'C. Twomey'}, {'authorId': '1397850657', 'name': 'G. O’Reilly'}, {'authorId': '49056991', 'name': 'M. Byrne'}, {'authorId': '2233210', 'name': 'Matthew I. Bury'}, {'authorId': '31632241', 'name': 'A. White'}, {'authorId': '12511150', 'name': 'Sheila Kissane'}, {'authorId': '145744680', 'name': 'Aisling McMahon'}, {'authorId': '119611637', 'name': 'Nicole Clancy'}]",2014.0,British Journal of Clinical Psychology,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"OBJECTIVES
To evaluate the effectiveness of the computerized CBT (cCBT) programme, MoodGYM, for the reduction in symptoms of general psychological distress (the primary outcome), depression, anxiety, stress, and impaired daily functioning.


DESIGN
A randomized controlled trial, with a waiting list control condition, in a routine clinical setting.


METHODS
Participants were 149 public mental health service users (aged 18-61 [M = 35.3 years; SD = 10.3]) waiting for interventions. Self-report outcome measures were administered online at baseline and post-intervention (i.e., after 32 days).


RESULTS
After high dropout rates, a post-intervention completers analysis examined 28 MoodGYM participants and 38 waiting list control participants. MoodGYM was significantly more effective than the waiting list control for the reduction of symptoms of general psychological distress (F[1, 64] = 4.45; p < .05) and stress (F[1, 64] = 5.35; p < .05) but not depression, anxiety, or impaired daily functioning.


CONCLUSIONS
Due to their high associated dropout rates, self-help cCBT programmes such as MoodGYM should not be provided as front-line treatments. However, as it is likely to be agreeable and beneficial to some service users, perhaps self-help cCBT should be provided as an additional treatment option.",68.0
The Second Dialog State Tracking Challenge,078b55e2f4899cf95a4c8d65613c340fa190acf8,"[{'authorId': '145133553', 'name': 'Matthew Henderson'}, {'authorId': '145462220', 'name': 'Blaise Thomson'}, {'authorId': '47271859', 'name': 'J. Williams'}]",2014.0,SIGDIAL Conference,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"A spoken dialog system, while communicating with a user, must keep track of what the user wants from the system at each step. This process, termed dialog state tracking, is essential for a successful dialog system as it directly informs the system’s actions. The first Dialog State Tracking Challenge allowed for evaluation of different dialog state tracking techniques, providing common testbeds and evaluation suites. This paper presents a second challenge, which continues this tradition and introduces some additional features ‐ a new domain, changing user goals and a richer dialog state. The challenge received 31 entries from 9 research groups. The results suggest that while large improvements on a competitive baseline are possible, trackers are still prone to degradation in mismatched conditions. An investigation into ensemble learning demonstrates the most accurate tracking can be achieved by combining multiple trackers.",507.0
On Consensus and Humming in the IETF,f1b6556e449cec21631875a8856d7e46264487bd,"[{'authorId': '35168143', 'name': 'P. Resnick'}]",2014.0,Request for Comments,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"The IETF has had a long tradition of doing its technical work through
a consensus process, taking into account the different views among
IETF participants and coming to (at least rough) consensus on
technical matters. In particular, the IETF is supposed not to be run
by a ""majority rule"" philosophy. This is why we engage in rituals like
""humming"" instead of voting. However, more and more of our actions are
now indistinguishable from voting, and quite often we are letting the
majority win the day without consideration of minority concerns. This
document explains some features of rough consensus, what is not rough
consensus, how we have gotten away from it, how we might think about
it differently, and the things we can do in order to really achieve
rough consensus. Note: This document is quite consciously being put
forward as Informational. It does not propose to change any IETF
processes and is therefore not a BCP. It is simply a collection of
principles, hopefully around which the IETF can come to (at least
rough) consensus.",18.0
"Book Review: The Second Machine Age: Work, Progress, and Prosperity in a Time of Brilliant Technologies",efdef53fc63f95a0f60a99bf42d935d320e7af62,"[{'authorId': '40380126', 'name': 'M. Marien'}]",2014.0,,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"On the surface, this is a very important book about present and future technologies, jobs, and growing inequality. It is clearly written, plausible, and well-documented. Although oriented to American audiences, it has global import in our globalizing age. It is not directly about security and sustainability (neither term is in the index), although the book could illuminate both concerns, as technology continues its inexorable advance, for better and worse. The first machine age brought the Industrial Revolution: the sum of several near simultaneous developments in mechanical engineering, chemistry, metallurgy, and so on. The most important technology was the steam engine, which overcame limitations of human and animal muscle power, leading to factories, mass production, railways, and mass transport. “Now comes the second machine age. Computers and other digital advances are doing for mental power—the ability to use our brains to understand and shape our environment—what the steam engine and its descendants did for muscle power” (pp. 7–8). How this transition will play out is unknown, but it is “a very big deal,” for mental power is at least as important for progress and development as physical power.",1702.0
NegoChat: a chat-based negotiation agent,143f9a7a79a3341b22d19f112ad5353788a23174,"[{'authorId': '40110198', 'name': 'A. Rosenfeld'}, {'authorId': '2024529', 'name': 'Inon Zuckerman'}, {'authorId': '1389846251', 'name': 'Erel Segal-Halevi'}, {'authorId': '2975464', 'name': 'Osnat Drein'}, {'authorId': '1691597', 'name': 'Sarit Kraus'}]",2014.0,Adaptive Agents and Multi-Agent Systems,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"To date, a variety of automated negotiation agents have been created. While each of these agents has been shown to be effective in negotiating with people in specific environments, they lack natural language processing support required to enable real-world types of interactions. In this paper we present NegoChat, the first negotiation agent that successfully addresses this limitation. NegoChat contains several significant research contributions. First, we found that simply modifying existing agents to include an NLP module is insufficient to create these agents. Instead, the agents' strategies must be modified to address partial agreements and issue-by-issue interactions. Second, we present NegoChat's negotiation algorithm. This algorithm is based on bounded rationality, and specifically Aspiration Adaptation Theory (AAT). As per AAT, issues are addressed based on people's typical urgency, or order of importance. If an agreement cannot be reached based on the value the human partner demands, the agent retreats, or downwardly lowers the value of previously agreed upon issues so that a ``good enough'' agreement can be reached on all issues. This incremental approach is fundamentally different from all other negotiation agents, including the state-of-the-art KBAgent. Finally, we present a rigorous evaluation of NegoChat, showing its effectiveness.",52.0
The Child as Econometrician: A Rational Model of Preference Understanding in Children,708bb5ffc1f92a17afe80575de49eba1251d7ebc,"[{'authorId': '31872195', 'name': 'Christopher G. Lucas'}, {'authorId': '1799860', 'name': 'T. Griffiths'}, {'authorId': '35455259', 'name': 'Fei Xu'}, {'authorId': '144553573', 'name': 'Christine Fawcett'}, {'authorId': '2222423', 'name': 'A. Gopnik'}, {'authorId': '3693228', 'name': 'T. Kushnir'}, {'authorId': '145013634', 'name': 'L. Markson'}, {'authorId': '1768462739', 'name': 'Jane C. Hu'}]",2014.0,PLoS ONE,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"Recent work has shown that young children can learn about preferences by observing the choices and emotional reactions of other people, but there is no unified account of how this learning occurs. We show that a rational model, built on ideas from economics and computer science, explains the behavior of children in several experiments, and offers new predictions as well. First, we demonstrate that when children use statistical information to learn about preferences, their inferences match the predictions of a simple econometric model. Next, we show that this same model can explain children's ability to learn that other people have preferences similar to or different from their own and use that knowledge to reason about the desirability of hidden objects. Finally, we use the model to explain a developmental shift in preference understanding.",69.0
Meet Me where I’m Gazing: How Shared Attention Gaze Affects Human-Robot Handover Timing,402172c310409f4bd5dcfebbd198b5ce4ec1c25e,"[{'authorId': '37074673', 'name': 'AJung Moon'}, {'authorId': '2410348', 'name': 'Daniel M. Troniak'}, {'authorId': '2804335', 'name': 'Brian T. Gleeson'}, {'authorId': '2570505', 'name': 'Matthew K. X. J. Pan'}, {'authorId': '2114135499', 'name': 'Minhua Zheng'}, {'authorId': '3024094', 'name': 'Benjamin A. Blumer'}, {'authorId': '1796517', 'name': 'K. Maclean'}, {'authorId': '1735428', 'name': 'E. Croft'}]",2014.0,IEEE/ACM International Conference on Human-Robot Interaction,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"In this paper we provide empirical evidence that using humanlike gaze cues during human-robot handovers can improve the timing and perceived quality of the handover event. Handovers serve as the foundation of many human-robot tasks. Fluent, legible handover interactions require appropriate nonverbal cues to signal handover intent, location and timing. Inspired by observations of human-human handovers, we implemented gaze behaviors on a PR2 humanoid robot. The robot handed over water bottles to a total of 102 naïve subjects while varying its gaze behaviour: no gaze, gaze designed to elicit shared attention at the handover location, and the shared attention gaze complemented with a turntaking cue. We compared subject perception of and reaction time to the robot-initiated handovers across the three gaze conditions. Results indicate that subjects reach for the offered object significantly earlier when a robot provides a shared attention gaze cue during a handover. We also observed a statistical trend of subjects preferring handovers with turn-taking gaze cues over the other conditions. Our work demonstrates that gaze can play a key role in improving user experience of human-robot handovers, and help make handovers fast and fluent.Categories and Subject Descriptors I.2.9 [Robotics]: Operator interfaces, Commercial robots and applications; H.1.2 [User/Machine Systems]: Human FactorsGeneral TermsExperimentation, Design, Human Factors, Verification.",189.0
CakeML: a verified implementation of ML,67116d0c51541798886f714589e0c99a2297af5c,"[{'authorId': '2117776492', 'name': 'Ramana Kumar'}, {'authorId': '1709169', 'name': 'Magnus O. Myreen'}, {'authorId': '1700277', 'name': 'Michael Norrish'}, {'authorId': '2069035845', 'name': 'Scott Owens'}]",2014.0,ACM-SIGACT Symposium on Principles of Programming Languages,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"We have developed and mechanically verified an ML system called CakeML, which supports a substantial subset of Standard ML. CakeML is implemented as an interactive read-eval-print loop (REPL) in x86-64 machine code. Our correctness theorem ensures that this REPL implementation prints only those results permitted by the semantics of CakeML. Our verification effort touches on a breadth of topics including lexing, parsing, type checking, incremental and dynamic compilation, garbage collection, arbitrary-precision arithmetic, and compiler bootstrapping. Our contributions are twofold. The first is simply in building a system that is end-to-end verified, demonstrating that each piece of such a verification effort can in practice be composed with the others, and ensuring that none of the pieces rely on any over-simplifying assumptions. The second is developing novel approaches to some of the more challenging aspects of the verification. In particular, our formally verified compiler can bootstrap itself: we apply the verified compiler to itself to produce a verified machine-code implementation of the compiler. Additionally, our compiler proof handles diverging input programs with a lightweight approach based on logical timeout exceptions. The entire development was carried out in the HOL4 theorem prover.",332.0
Decision Theory with Resource-Bounded Agents,58f9e410a0f40e8f4efd5b3a44dec8ab716226ff,"[{'authorId': '1691828', 'name': 'Joseph Y. Halpern'}, {'authorId': '1811683', 'name': 'R. Pass'}, {'authorId': '1756521', 'name': 'Lior Seeman'}]",2013.0,Topics in Cognitive Science,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"There have been two major lines of research aimed at capturing resource-bounded players in game theory. The first, initiated by Rubinstein (), charges an agent for doing costly computation; the second, initiated by Neyman (), does not charge for computation, but limits the computation that agents can do, typically by modeling agents as finite automata. We review recent work on applying both approaches in the context of decision theory. For the first approach, we take the objects of choice in a decision problem to be Turing machines, and charge players for the ""complexity"" of the Turing machine chosen (e.g., its running time). This approach can be used to explain well-known phenomena like first-impression-matters biases (i.e., people tend to put more weight on evidence they hear early on) and belief polarization (two people with different prior beliefs, hearing the same evidence, can end up with diametrically opposed conclusions) as the outcomes of quite rational decisions. For the second approach, we model people as finite automata, and provide a simple algorithm that, on a problem that captures a number of settings of interest, provably performs optimally as the number of states in the automaton increases.",24.0
Tiling agents in causal graphs,17bca6675a3b930f13d1031fe89bac1f2cadff08,"[{'authorId': '1719968', 'name': 'N. Soares'}]",2014.0,,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"Fallenstein and Soares [2014] demonstrates that it’s possible for certain types of proof-based agents to “tile” (license the construction of successor agents similar to themselves while avoiding Gödelian diagonalization issues) in environments about which the agent can prove some basic nice properties. In this technical report, we show via a similar proof that causal graphs (with a specific structure) are one such environment. We translate the proof given by Fallenstein and Soares 2014 into the language of causal graphs, and we do this in such a way as to simplify the conditions under which a tiling meliorizer can be constructed. What does it mean for an agent to construct another agent in a causal graph? For that matter, what does it mean for an agent to be in a causal graph? When we talk about an agent in a causal graph, we are discussing a node that has some set of input nodes and some set of output nodes, such that the agent node’s contribution to each output node is defined by some deterministic function (the agent) of the input nodes. In figure 1, the agent is at the node A with observation nodes O . . . and output nodes X . . .. The agent itself is the function π : O . . . → X . . . that defines the agent’s contribution to each output node given each input node. Without loss of generality, we can assume in this paper that there is only one observation node O. We say that an agent can construct a successor if one of the agent’s output nodes is another agent node. An agent A can construct a successor at the node A′ if the agent’s contribution to A′ can make A′ be another agent, as in figure 2. This notion will be formalized in the following sections. Following Fallenstein and Soares [2014], we consider proof based agents, which may only take actions after exhibiting a proof of certain safety properties. Specifically, we consider agents using the suggester-verifier architecture, which contain both a suggester component that searches for action/proof pairs and a verifier component that takes an action/proof pair and executes the action if and only if the given proof is a valid proof proving the appropriate safety properties. We limit our consideration to meliorizing suggester-verifier agents, which possess a fallback policy to execute if no valid action/proof pair is found. The target theorem (in the sense of Schmidhuber [2007]) for these agents is one stating that executing the given action results in more expected utility than executing the fallback policy. Thus, in order to execute an action a, the suggester must find both the action a and a proof (in the language of the verifier) that executing a results in more expected utility than executing the fallback policy.",5.0
Modeling Human Plan Recognition Using Bayesian Theory of Mind,4c8bfe175bd49b45ae83d44af9190eabbadddb0d,"[{'authorId': '21161348', 'name': 'Chris L. Baker'}, {'authorId': '1763295', 'name': 'J. Tenenbaum'}]",2014.0,,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,,88.0
Historical and Technical Notes on Aqueducts from Prehistoric to Medieval Times,aa906f9878adc0f3f645773b7c3331314023ee8d,"[{'authorId': '15556938', 'name': 'G. Feo'}, {'authorId': '12979183', 'name': 'A. Angelakis'}, {'authorId': '2065207378', 'name': 'G. Antoniou'}, {'authorId': '2056353328', 'name': 'F. EL-GOHARY'}, {'authorId': '93780920', 'name': 'B. Haut'}, {'authorId': '5619365', 'name': 'C. Passchier'}, {'authorId': '145858121', 'name': 'X. Zheng'}]",2013.0,,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"The aim of this paper is to present the evolution of aqueduct technologies through the millennia, from prehistoric to medieval times. These hydraulic works were used by several civilizations to collect water from springs and to transport it to settlements, sanctuaries and other targets. Several civilizations, in China and the Americas, developed water transport systems independently, and brought these to high levels of sophistication. For the Mediterranean civilizations, one of the salient characteristics of cultural development, since the Minoan Era (ca. 3200-1100 BC), is the architectural and hydraulic function of aqueducts used for the water supply in palaces and other settlements. The",45.0
Game theory with translucent players,07c3b71890f6499dc091c3997176ae73c469f383,"[{'authorId': '1691828', 'name': 'Joseph Y. Halpern'}, {'authorId': '1811683', 'name': 'R. Pass'}]",2013.0,Theoretical Aspects of Rationality and Knowledge,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,,41.0
Could artificial intelligence create an unemployment crisis?,9430ff22287f4cd782970a81433822e8b76f1ea0,"[{'authorId': '145613051', 'name': 'M. Ford'}]",2013.0,CACM,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,Advances in artificial intelligence and robotics will have significant implications for evolving economic systems.,41.0
Almost common priors,d824545c1510fa5d827e21735adcfaca1ab32dba,"[{'authorId': '3243854', 'name': 'Ziv Hellman'}]",2013.0,International Journal of Game Theory,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,,6.0
Denial of Catastrophic Risks,50e36b43cab0635f0507b222fd3ae66cbbb8b444,"[{'authorId': '2810300', 'name': 'M. Rees'}]",2013.0,Science,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"In a media landscape saturated with sensational science stories and ""end of the world"" Hollywood productions, it may be hard to persuade the wide public that real catastrophes could arise as unexpectedly as the 2008 financial crisis, and have a far greater impact. Society could be dealt shattering blows by the misapplication of technologies that exist already or could emerge within the coming decades. Some of the scenarios that have been envisaged may indeed be science fiction, but others may be disquietingly real. I believe these ""existential risks"" deserve more serious study. Those fortunate enough to live in the developed world fret too much about minor hazards of everyday life: improbable air crashes, possible carcinogens in food, low radiation doses, and so forth. But we should be more concerned about events that have not yet happened but which, if they occurred even once, could cause worldwide devastation.",29.0
Legibility and predictability of robot motion,2b8e962d2893a099c1ff7073e7ba7bb0d6e7df20,"[{'authorId': '2745001', 'name': 'A. Dragan'}, {'authorId': '32886543', 'name': 'Kenton C. T. Lee'}, {'authorId': '1752197', 'name': 'S. Srinivasa'}]",2013.0,IEEE/ACM International Conference on Human-Robot Interaction,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"A key requirement for seamless human-robot collaboration is for the robot to make its intentions clear to its human collaborator. A collaborative robot's motion must be legible, or intent-expressive. Legibility is often described in the literature as and effect of predictable, unsurprising, or expected motion. Our central insight is that predictability and legibility are fundamentally different and often contradictory properties of motion. We develop a formalism to mathematically define and distinguish predictability and legibility of motion. We formalize the two based on inferences between trajectories and goals in opposing directions, drawing the analogy to action interpretation in psychology. We then propose mathematical models for these inferences based on optimizing cost, drawing the analogy to the principle of rational action. Our experiments validate our formalism's prediction that predictability and legibility can contradict, and provide support for our models. Our findings indicate that for robots to seamlessly collaborate with humans, they must change the way they plan their motion.",559.0
Toward seamless human-robot handovers,bfcbe90b6153ed6c36f723d75c6115ce2cfcc14b,"[{'authorId': '2179149', 'name': 'K. Strabala'}, {'authorId': '50112472', 'name': 'Min Kyung Lee'}, {'authorId': '2745001', 'name': 'A. Dragan'}, {'authorId': '145174228', 'name': 'J. Forlizzi'}, {'authorId': '1752197', 'name': 'S. Srinivasa'}, {'authorId': '35096370', 'name': 'M. Cakmak'}, {'authorId': '2959685', 'name': 'Vincenzo Micelli'}]",2013.0,Journal of Human-Robot Interaction,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"A handover is a complex collaboration, where actors coordinate in time and space to transfer control of an object. This coordination comprises two processes: the physical process of moving to get close enough to transfer the object, and the cognitive process of exchanging information to guide the transfer. Despite this complexity, we humans are capable of performing handovers seamlessly in a wide variety of situations, even when unexpected. This suggests a common procedure that guides all handover interactions. Our goal is to codify that procedure. To that end, we first study how people hand over objects to each other in order to understand their coordination process and the signals and cues that they use and observe with their partners. Based on these studies, we propose a coordination structure for human-robot handovers that considers the physical and social-cognitive aspects of the interaction separately. This handover structure describes how people approach, reach out their hands, and transfer objects while simultaneously coordinating the what, when, and where of handovers: to agree that the handover will happen (and with what object), to establish the timing of the handover, and to decide the configuration at which the handover will occur. We experimentally evaluate human-robot handover behaviors that exploit this structure and offer design implications for seamless human-robot handover interactions.",253.0
Climate change prediction: Erring on the side of least drama?,57c08c6eb4b4bea2820095bbcfb51590c913b80a,"[{'authorId': '8591612', 'name': 'Keynyn Brysse'}, {'authorId': '4626515', 'name': 'N. Oreskes'}, {'authorId': '1407831947', 'name': 'J. O’Reilly'}, {'authorId': '143668891', 'name': 'M. Oppenheimer'}]",2013.0,,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,,270.0
Groupthink: Collective Delusions in Organizations and Markets,07dc26138d9ee35423d452baca27199e10c9e3ba,"[{'authorId': '2165280', 'name': 'R. Bénabou'}]",2009.0,Social Science Research Network,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"I develop a model of (individually rational) collective reality denial in groups, organizations and markets. Whether participants' tendencies toward wishful thinking reinforce or dampen each other is shown to hinge on a simple and novel mechanism. When an agent can expect to benefit from other's delusions, this makes him more of a realist; when he is more likely to suffer losses from them this pushes him toward denial, which becomes contagious. This general ""Mutually Assured Delusion"" principle can give rise to multiple social cognitions of reality, irrespective of any strategic payoff interactions or private signals. It also implies that in hierachical organizations realism or denial will trickle down, causing subordinates to take their mindsets and beliefs from the leaders. Contagious ""exuberance"" can also seize asset markets, leading to evidence-resistant investment frenzies and subsequent deep crashes. In addition to collective illusions of control, the model accounts for the mirror case of fatalism and collective resignation. The welfare analysis differentiates valuable group morale from harmful groupthink and identifies a fundamental tension in organizations' attitudes toward free speech and dissent.",300.0
Omohundro ’ s “ Basic AI Drives ” and Catastrophic Risks,738db99fd925069c988f9bd1df52446309d5e9c4,"[{'authorId': '3389522', 'name': 'Carl Shulman'}]",2013.0,,"['AI Research Considerations for Human Existential Safety (ARCHES)', 'Responses to catastrophic AGI risk: a survey', 'Superintelligence: Paths, Dangers, Strategies']",3,"Shulman, Carl. 2010. Omohundro's "" Basic AI Drives "" and Catastrophic Risks.",19.0
"Tiling Agents for Self-Modifying AI , and the Löbian Obstacle *",d6d1e0441f20aee1a9e57e304b16705310abdaa6,"[{'authorId': '30089113', 'name': 'Eliezer'}, {'authorId': '2085006349', 'name': 'Herreshoff'}]",2013.0,,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"We model self-modification in AI by introducing “tiling” agents whose decision systems will approve the construction of highly similar agents, creating a repeating pattern (including similarity of the offspring’s goals). Constructing a formalism in the most straightforward way produces a Gödelian difficulty, the “Löbian obstacle.” By technical methods we demonstrate the possibility of avoiding this obstacle, but the underlying puzzles of rational coherence are thus only partially addressed. We extend the formalism to partially unknown deterministic environments, and show a very crude extension to probabilistic environments and expected utility; but the problem of finding a fundamental decision criterion for self-modifying probabilistic agents remains open.",27.0
"Exchange-Traded Funds, Market Structure, and the Flash Crash",d067409017c1e6cbe20346144278331a0bc0323a,"[{'authorId': '119500101', 'name': 'MadhavanAnanth'}]",2012.0,,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"The author analyzes the relationship between market structure and the flash crash. The proliferation of trading venues has resulted in a market that is more fragmented than ever. The author constructs measures to capture fragmentation and shows that they are important in explaining extreme price movements. New market structure reforms should help mitigate such market disruptions in the future but have not eliminated the possibility of another flash crash, albeit with a different catalyst.",169.0
The Superintelligent Will: Motivation and Instrumental Rationality in Advanced Artificial Agents,6c25aae58187f716d1b6db34200bbf3b63007aeb,"[{'authorId': '2193691', 'name': 'N. Bostrom'}]",2012.0,Minds and Machines,"['AI Research Considerations for Human Existential Safety (ARCHES)', 'Responses to catastrophic AGI risk: a survey', 'Superintelligence: Paths, Dangers, Strategies']",3,,13.0
Independent reinforcement learners in cooperative Markov games: a survey regarding coordination problems,9c6933244bcf31ce8a05a1e4ee0ec6d015416616,"[{'authorId': '2335305', 'name': 'L. Matignon'}, {'authorId': '20710580', 'name': 'G. Laurent'}, {'authorId': '1400816398', 'name': 'N. L. Fort-Piat'}]",2012.0,Knowledge engineering review (Print),['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"Abstract In the framework of fully cooperative multi-agent systems, independent (non-communicative) agents that learn by reinforcement must overcome several difficulties to manage to coordinate. This paper identifies several challenges responsible for the non-coordination of independent agents: Pareto-selection, non-stationarity, stochasticity, alter-exploration and shadowed equilibria. A selection of multi-agent domains is classified according to those challenges: matrix games, Boutilier's coordination game, predators pursuit domains and a special multi-state game. Moreover, the performance of a range of algorithms for independent reinforcement learners is evaluated empirically. Those algorithms are Q-learning variants: decentralized Q-learning, distributed Q-learning, hysteretic Q-learning, recursive frequency maximum Q-value and win-or-learn fast policy hill climbing. An overview of the learning algorithms’ strengths and weaknesses against each challenge concludes the paper and can serve as a basis for choosing the appropriate algorithm for a new domain. Furthermore, the distilled challenges may assist in the design of new learning algorithms that overcome these problems and achieve higher performance in multi-agent applications.",325.0
Fairness through awareness,adaa0523a5c9d5f92aa2009a51226391d8e62380,"[{'authorId': '1781565', 'name': 'C. Dwork'}, {'authorId': '1775622', 'name': 'Moritz Hardt'}, {'authorId': '1695317', 'name': 'T. Pitassi'}, {'authorId': '1746057', 'name': 'O. Reingold'}, {'authorId': '1804104', 'name': 'R. Zemel'}]",2011.0,Information Technology Convergence and Services,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"We study fairness in classification, where individuals are classified, e.g., admitted to a university, and the goal is to prevent discrimination against individuals based on their membership in some group, while maintaining utility for the classifier (the university). The main conceptual contribution of this paper is a framework for fair classification comprising (1) a (hypothetical) task-specific metric for determining the degree to which individuals are similar with respect to the classification task at hand; (2) an algorithm for maximizing utility subject to the fairness constraint, that similar individuals are treated similarly. We also present an adaptation of our approach to achieve the complementary goal of ""fair affirmative action,"" which guarantees statistical parity (i.e., the demographics of the set of individuals receiving any classification are the same as the demographics of the underlying population), while treating similar individuals as similarly as possible. Finally, we discuss the relationship of fairness to privacy: when fairness implies privacy, and how tools developed in the context of differential privacy may be applied to fairness.",2496.0
"Why blame Bob ? Probabilistic generative models , counterfactual reasoning , and blame attribution",edc020786a971f41ac26c801d4f082e4f1f078b4,"[{'authorId': '144590797', 'name': 'John McCoy'}, {'authorId': '37774552', 'name': 'T. Ullman'}, {'authorId': '2214496', 'name': 'Andreas Stuhlmüller'}, {'authorId': '2697953', 'name': 'Tobias Gerstenberg'}, {'authorId': '1763295', 'name': 'J. Tenenbaum'}]",2012.0,,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"We consider an approach to blame attribution based on counterfactual reasoning in probabilistic generative models. In this view, people intervene on each variable within their model and assign blame in proportion to how much a change to a variable would have improved the outcome. This approach raises two questions: First, what structure do people use to represent a given situation? Second, how do they choose what alternatives to consider when intervening on an event? We use a series of coin-tossing scenarios to compare empirical data to different models within the proposed framework. The results suggest that people sample their intervention values from a prior rather than deterministically switching the value of a variable. The results further suggest that people represent scenarios differently when asked to reason about their own blame attributions, compared with the blame attributions they believe others will assign.",4.0
Knowledge and implicature: Modeling language understanding as social cognition,3962a66e72775ddb496b2425c7451394d02b17b1,"[{'authorId': '144002017', 'name': 'Noah D. Goodman'}, {'authorId': '2214496', 'name': 'Andreas Stuhlmüller'}]",2012.0,Annual Meeting of the Cognitive Science Society,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"Is language understanding a special case of social cognition? To help evaluate this view, we can formalize it as the rational speech-act theory: Listeners assume that speakers choose their utterances approximately optimally, and listeners interpret an utterance by using Bayesian inference to ""invert"" this model of the speaker. We apply this framework to model scalar implicature (""some"" implies ""not all,"" and ""N"" implies ""not more than N""). This model predicts an interaction between the speaker's knowledge state and the listener's interpretation. We test these predictions in two experiments and find good fit between model predictions and human judgments.",338.0
The Flash Crash: High-Frequency Trading in an Electronic Market,727ce33ae9b4602cd5b96691140c66fe864d3e00,"[{'authorId': '8528698', 'name': 'A. Kirilenko'}, {'authorId': '144865543', 'name': 'A. Kyle'}, {'authorId': '98066902', 'name': 'M. Samadi'}, {'authorId': '118270631', 'name': 'Tugkan Tuzun'}]",2017.0,,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"We study intraday market intermediation in an electronic market before and during a period of large and temporary selling pressure. On May 6, 2010, U.S. financial markets experienced a systemic intraday event, known as the Flash Crash, when a large automated sell program was rapidly executed in the E-mini S&P 500 stock index futures market. Using audit trail transaction-level data for the E-mini on May 6 and the previous three days, we find that the trading pattern of the most active non-designated intraday intermediaries (classified as High Frequency Traders) did not change when prices fell during the Flash Crash.",346.0
Internet Gaming Addiction: A Systematic Review of Empirical Research,dafac8dce32f4f9346bffb057c124301ae17f160,"[{'authorId': '2034264', 'name': 'D. Kuss'}, {'authorId': '1759568', 'name': 'M. Griffiths'}]",2012.0,International Journal of Mental Health and Addiction,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,,797.0
Program Obfuscation with Leaky Hardware,e59418bb0504698464ef7d723a2b67c8cecc3ef5,"[{'authorId': '3142333', 'name': 'Nir Bitansky'}, {'authorId': '1749530', 'name': 'R. Canetti'}, {'authorId': '1706681', 'name': 'S. Goldwasser'}, {'authorId': '1808458', 'name': 'S. Halevi'}, {'authorId': '1784514', 'name': 'Y. Kalai'}, {'authorId': '2551824', 'name': 'G. Rothblum'}]",2011.0,IACR Cryptology ePrint Archive,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,,44.0
"The Politicization of Climate Change and Polarization in the American Public's Views of Global Warming, 2001–2010",6cc9490e17a674a23814570d05df0e82e34e8ff9,"[{'authorId': '119049900', 'name': 'A. McCright'}, {'authorId': '8503677', 'name': 'R. Dunlap'}]",2011.0,,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"We examine political polarization over climate change within the American public by analyzing data from 10 nationally representative Gallup Polls between 2001 and 2010. We find that liberals and Democrats are more likely to report beliefs consistent with the scientific consensus and express personal concern about global warming than are conservatives and Republicans. Further, the effects of educational attainment and self-reported understanding on global warming beliefs and concern are positive for liberals and Democrats, but are weaker or negative for conservatives and Republicans. Last, significant ideological and partisan polarization has occurred on the issue of climate change over the past decade.",1660.0
"The Microstructure of the “Flash Crash”: Flow Toxicity, Liquidity Crashes, and the Probability of Informed Trading",6bf500d5b533ce28c3a9ca8532e0701ddb482f9d,"[{'authorId': '1842749', 'name': 'D. Easley'}, {'authorId': '1741105308', 'name': 'Marcos M. López de Prado'}, {'authorId': '88087434', 'name': 'Maureen O’Hara'}]",2011.0,Journal of Portfolio Management,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"The “flash crash” of May 6, 2010, was the second-largest point swing (1,010.14 points) and the biggest one-day point decline (998.5 points) in the history of the Dow Jones Industrial Average. For a few minutes, $1 trillion in market value vanished. In this article, the authors argue that the flash crash was the result of the new dynamics at play in the current market structure. They highlight the role played by order toxicity in affecting liquidity provision, and they show that a measure of this toxicity, the volume synchronized probability of informed trading (VPIN), captures the increasing toxicity of the order flow in the hours and days prior to collapse. Because the flash crash might have been avoided had liquidity providers remained in the marketplace, a solution is proposed in the form of a “VPIN contract” that would allow liquidity providers to dynamically monitor and manage their risks.",407.0
Bayesian Theory of Mind: Modeling Joint Belief-Desire Attribution,58eee36a6e0c59622d800ae4131c29b96f1595c8,"[{'authorId': '21161348', 'name': 'Chris L. Baker'}, {'authorId': '2276622', 'name': 'R. Saxe'}, {'authorId': '1763295', 'name': 'J. Tenenbaum'}]",2011.0,Annual Meeting of the Cognitive Science Society,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"Bayesian Theory of Mind: Modeling Joint Belief-Desire Attribution Chris L. Baker (clbaker@mit.edu) Rebecca R. Saxe (saxe@mit.edu) Joshua B. Tenenbaum (jbt@mit.edu) Department of Brain and Cognitive Sciences, MIT Cambridge, MA 02139 Abstract We present a computational framework for understanding The- ory of Mind (ToM): the human capacity for reasoning about agents’ mental states such as beliefs and desires. Our Bayesian model of ToM (or BToM) expresses the predictive model of belief- and desire-dependent action at the heart of ToM as a partially observable Markov decision process (POMDP), and reconstructs an agent’s joint belief state and reward func- tion using Bayesian inference, conditioned on observations of the agent’s behavior in some environmental context. We test BToM by showing participants sequences of agents moving in simple spatial scenarios and asking for joint inferences about the agents’ desires and beliefs about unobserved aspects of the environment. BToM performs substantially better than two simpler variants: one in which desires are inferred without ref- erence to an agent’s beliefs, and another in which beliefs are inferred without reference to the agent’s dynamic observations in the environment. Keywords: Theory of mind; Social cognition; Action un- derstanding; Bayesian inference; Partially Observable Markov Decision Processes Introduction Central to human social behavior is a theory of mind (ToM), the capacity to explain and predict people’s observable ac- tions in terms of unobservable mental states such as beliefs and desires. Consider the case of Harold, who leaves his dorm room one Sunday morning for the campus library. When he reaches to open the library’s front door he will find that it is locked – closed on Sunday. How can we explain his behav- ior? It seems plausible that he wants to get a book, that he believes the book he wants is at the library, and that he also believes (falsely, it turns out) that the library is open on Sun- day. Such mental state explanations for behavior go well be- yond the observable data, leading to an inference problem that is fundamentally ill-posed. Many different combinations of beliefs and desires could explain the same behavior, with inferences about the strengths of beliefs and desires trading off against each other, and relative probabilities modulated heavily by context. Perhaps Harold is almost positive that the library will be closed, but he needs a certain book so badly that he still is willing to go all the way across campus on the off chance it will be open. This explanation seems more prob- able if Harold shows up to find the library locked on Saturday at midnight, as opposed to noon on Tuesday. If he arrives after hours already holding a book with a due date of tomor- row, it is plausible that he knows the library is closed and is seeking not to get a new book, but merely to return a book checked out previously to the night drop box. Several authors have recently proposed models for how people infer others’ goals or preferences as a kind of Bayesian inverse planning or inverse decision theory (Baker, Saxe, & Tenenbaum, 2009; Feldman & Tremoulet, 2008; Lucas, Grif- fiths, Xu, & Fawcett, 2009; Bergen, Evans, & Tenenbaum, 2010; Yoshida, Dolan, & Friston, 2008; Ullman et al., 2010). These models adapt tools from control theory, econometrics and game theory to formalize the principle of rational ac- tion at the heart of children and adults’ concept of intentional agency (Gergely, N´adasdy, Csibra, & Bir´o, 1995; Dennett, 1987): all else being equal, agents are expected to choose ac- tions that achieve their desires as effectively and efficiently as possible, i.e., to maximize their expected utility. Goals or preferences are then inferred based on which objective or utility function the observed actions maximize most directly. ToM transcends knowledge of intentional agents’ goals and preferences by incorporating representational mental states such as subjective beliefs about the world (Perner, 1991). In particular, the ability to reason about false beliefs has been used to distinguish ToM from non-representational theories of intentional action (Wimmer & Perner, 1983; Onishi & Baillargeon, 2005). Our goal in this paper is to model hu- man ToM within a Bayesian framework. Inspired by mod- els of inverse planning, we cast Bayesian ToM (BToM) as a problem of inverse planning and inference, representing an agent’s planning and inference about the world as a partially observable Markov decision process (POMDP), and invert- ing this forward model using Bayesian inference. Critically, this model includes representations of both the agent’s de- sires (as a utility function), and the agent’s own subjective beliefs about the environment (as a probability distribution), which may be uncertain and may differ from reality. We test the predictions of this model quantitatively in an experiment where people must simultaneously judge beliefs and desires for agents moving in simple spatial environments under in- complete or imperfect knowledge. Important precursors to our work are several computational models (Goodman et al., 2006; Bello & Cassimatis, 2006; Goodman, Baker, & Tenenbaum, 2009) and informal theo- retical proposals by developmental psychologists (Wellman, 1990; Gopnik & Meltzoff, 1997; Gergely & Csibra, 2003). Goodman et al. (2006) model how belief and desire infer- ences interact in the classic “false belief” task used to assess ToM reasoning in children (Wimmer & Perner, 1983). This model instantiates the schema shown in Fig. 1(a) as a causal Bayesian network with several psychologically interpretable,",229.0
INFORMATION HAZARDS: A TYPOLOGY OF POTENTIAL HARMS FROM KNOWLEDGE,274c17084e5373a854b13a39c45d072e2b09970e,"[{'authorId': '2193691', 'name': 'N. Bostrom'}]",2011.0,,"['AI Research Considerations for Human Existential Safety (ARCHES)', 'Superintelligence: Paths, Dangers, Strategies']",2,"Information hazards are risks that arise from the dissemination or the potential dissemination of true information that may cause harm or enable some agent to cause harm.  Such hazards are often subtler than direct physical threats, and, as a consequence, are easily overlooked.  They can, however, be important.  This paper surveys the terrain and proposes a taxonomy.",57.0
A Computational Decision Theory for Interactive Assistants,f02f1aff46d220c2912b1b48f5987cb496b831c9,"[{'authorId': '145841336', 'name': 'Alan Fern'}, {'authorId': '1729906', 'name': 'Prasad Tadepalli'}]",2010.0,Interactive Decision Theory and Game Theory,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"We study several classes of interactive assistants from the points of view of decision theory and computational complexity. We first introduce a special class of POMDPs called hidden-goal MDPs (HGMDPs), which formalize the problem of interactively assisting an agent whose goal is hidden and whose actions are observable. In spite of its restricted nature, we show that optimal action selection in finite horizon HGMDPs is PSPACE-complete even in domains with deterministic dynamics. We then introduce a more restricted model called helper action MDPs (HAMDPs), where the assistant's action is accepted by the agent when it is helpful, and can be easily ignored by the agent otherwise. We show classes of HAMDPs that are complete for PSPACE and NP along with a polynomial time class. Furthermore, we show that for general HAMDPs a simple myopic policy achieves a regret, compared to an omniscient assistant, that is bounded by the entropy of the initial goal distribution. A variation of this policy is also shown to achieve worst-case regret that is logarithmic in the number of goals for any goal distribution.",27.0
"Causal–explanatory pluralism: How intentions, functions, and mechanisms influence causal ascriptions",aaf576f21f8e547c72ded7cd78bd4940d20b86c4,"[{'authorId': '2464187', 'name': 'T. Lombrozo'}]",2010.0,Cognitive Psychology,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,,201.0
Electronic media use and sleep in school-aged children and adolescents: A review.,60c4968b7c3757885e64b54134d0493ecf36cc57,"[{'authorId': '39356109', 'name': 'N. Cain'}, {'authorId': '1924817', 'name': 'M. Gradisar'}]",2010.0,Sleep Medicine,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,,980.0
I Don't Want to Think About it Now: Decision Theory with Costly Computation,a6d5faccd113ffcad8f7b281dfc539506bfc87f8,"[{'authorId': '1691828', 'name': 'Joseph Y. Halpern'}]",2010.0,International Conference on Principles of Knowledge Representation and Reasoning,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"Computation plays a major role in decision making. Even if an agent is willing to ascribe a probability to all states and a utility to all outcomes, and maximize expected utility, doing so might present serious computational problems. Moreover, computing the outcome of a given act might be difficult. In a companion paper we develop a framework for game theory with costly computation, where the objects of choice are Turing machines. Here we apply that framework to decision theory. We show how well-known phenomena like first-impression-matters biases (i.e., people tend to put more weight on evidence they hear early on), belief polarization (two people with different prior beliefs, hearing the same evidence, can end up with diametrically opposed conclusions), and the status quo bias (people are much more likely to stick with what they already have) can be easily captured in that framework. Finally, we use the framework to define some new notions: value of computational information (a computational variant of value of information) and computational value of conversation.",25.0
Health Effects of Media on Children and Adolescents,9f1d28f76ded766f6e8142a1d99446db66c3a562,"[{'authorId': '6370151', 'name': 'V. Strasburger'}, {'authorId': '144031146', 'name': 'A. Jordan'}, {'authorId': '4239460', 'name': 'E. Donnerstein'}]",2010.0,Pediatrics,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"Youth spend an average of >7 hours/day using media, and the vast majority of them have access to a bedroom television, computer, the Internet, a video-game console, and a cell phone. In this article we review the most recent research on the effects of media on the health and well-being of children and adolescents. Studies have shown that media can provide information about safe health practices and can foster social connectedness. However, recent evidence raises concerns about media's effects on aggression, sexual behavior, substance use, disordered eating, and academic difficulties. We provide recommendations for parents, practitioners, the media, and policy makers, among others, for ways to increase the benefits and reduce the harm that media can have for the developing child and for adolescents.",459.0
Academic Search Engine Optimization (ASEO),c008c127329dc9746d432853eac6c16e286bfd27,"[{'authorId': '1781377', 'name': 'J. Beel'}, {'authorId': '145151838', 'name': 'Bela Gipp'}, {'authorId': '145714602', 'name': 'Erik Wilde'}]",2010.0,,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"This article introduces and discusses the concept of academic search engine optimization (ASEO). Based on three recently conducted studies, guidelines are provided on how to optimize scholarly literature for academic search engines in general, and for Google Scholar in particular. In addition, we briefly discuss the risk of researchers' illegitimately ‘over-optimizing’ their articles.",162.0
Unmanned Aircraft Systems,7a7b41d127f3a11d84680b4f9c42d191e7419947,"[{'authorId': '144644768', 'name': 'Alan Hobbs'}]",2010.0,,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,,190.0
Du contrat social...,0c6e0d14332b3ce5d25e9879d2f6f74de421adcc,"[{'authorId': '102474755', 'name': 'F. Chobeaux'}]",2009.0,,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,,128.0
The Wisdom of Individuals: Exploring People's Knowledge About Everyday Events Using Iterated Learning,fe3645fde5ec6f4c538ae0dabd24950743d9be9e,"[{'authorId': '2573193', 'name': 'S. Lewandowsky'}, {'authorId': '1799860', 'name': 'T. Griffiths'}, {'authorId': '2147951', 'name': 'M. Kalish'}]",2009.0,Cognitive Sciences,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"Determining the knowledge that guides human judgments is fundamental to understanding how people reason, make decisions, and form predictions. We use an experimental procedure called ''iterated learning,'' in which the responses that people give on one trial are used to generate the data they see on the next, to pinpoint the knowledge that informs people's predictions about everyday events (e.g., predicting the total box office gross of a movie from its current take). In particular, we use this method to discriminate between two models of human judgments: a simple Bayesian model (Griffiths & Tenenbaum, 2006) and a recently proposed alternative model that assumes people store only a few instances of each type of event in memory (MinK; Mozer, Pashler, & Homaei, 2008). Although testing these models using standard experimental procedures is difficult due to differences in the number of free parameters and the need to make assumptions about the knowledge of individual learners, we show that the two models make very different predictions about the outcome of iterated learning. The results of an experiment using this methodology provide a rich picture of how much people know about the distributions of everyday quantities, and they are inconsistent with the predictions of the MinK model. The results suggest that accurate predictions about everyday events reflect relatively sophisticated knowledge on the part of individuals.",73.0
Theoretical and empirical evidence for the impact of inductive biases on cultural evolution,3526f7132318014c47dc1c81193ce5bfd25291c8,"[{'authorId': '1799860', 'name': 'T. Griffiths'}, {'authorId': '2147951', 'name': 'M. Kalish'}, {'authorId': '2573193', 'name': 'S. Lewandowsky'}]",2008.0,Philosophical Transactions of the Royal Society B: Biological Sciences,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"The question of how much the outcomes of cultural evolution are shaped by the cognitive capacities of human learners has been explored in several disciplines, including psychology, anthropology and linguistics. We address this question through a detailed investigation of transmission chains, in which each person passes information to another along a chain. We review mathematical and empirical evidence that shows that under general conditions, and across experimental paradigms, the information passed along transmission chains will be affected by the inductive biases of the people involved—the constraints on learning and memory, which influence conclusions from limited data. The mathematical analysis considers the case where each person is a rational Bayesian agent. The empirical work consists of behavioural experiments in which human participants are shown to operate in the manner predicted by the Bayesian framework. Specifically, in situations in which each person's response is used to determine the data seen by the next person, people converge on concepts consistent with their inductive biases irrespective of the information seen by the first member of the chain. We then relate the Bayesian analysis of transmission chains to models of biological evolution, clarifying how chains of individuals correspond to population-level models and how selective forces can be incorporated into our models. Taken together, these results indicate how laboratory studies of transmission chains can provide information about the dynamics of cultural evolution and illustrate that inductive biases can have a significant impact on these dynamics.",103.0
Avoiding Another AI Winter,3de5e62c37c8432091fc6b52d8243b19fac5e4ba,"[{'authorId': '1701341', 'name': 'J. Hendler'}]",2008.0,IEEE Intelligent Systems,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,Is artificial intelligence headed for another AI Winter? Not if we take action now.,102.0
Application of machine learning techniques for supply chain demand forecasting,1edfaf279f01d6ea3469b20c57cff6fa69dec219,"[{'authorId': '32422549', 'name': 'R. Carbonneau'}, {'authorId': '1750984', 'name': 'K. Laframboise'}, {'authorId': '143634220', 'name': 'R. Vahidov'}]",2008.0,European Journal of Operational Research,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,,407.0
Cognitive biases potentially affecting judgement of global risks,c0b9781f223a05cf29c2a7fd020925e1c8c13a42,"[{'authorId': '2542795', 'name': 'Eliezer Yudkowsky'}]",2008.0,,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"All else being equal, not many people would prefer to destroy the world. Even faceless corporations, meddling governments, reckless scientists, and other agents of doom, require a world in which to achieve their goals of profit, order, tenure, or other villainies. If our extinction proceeds slowly enough to allow a moment of horrified realization, the doers of the deed will likely be quite taken aback on realizing that they have actually destroyed the world. Therefore I suggest that if the Earth is destroyed, it will probably be by mistake.",109.0
Simplicity and probability in causal explanation,d7c642e366acead6cdec1d6b3e8198c93b2d5630,"[{'authorId': '2464187', 'name': 'T. Lombrozo'}]",2007.0,Cognitive Psychology,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,,310.0
Program obfuscation: a quantitative approach,aef930c488424d11ffaa643d8ded03d7f0373d3d,"[{'authorId': '2340028', 'name': 'Bertrand Anckaert'}, {'authorId': '1731528', 'name': 'Matias Madou'}, {'authorId': '9366424', 'name': 'B. D. Sutter'}, {'authorId': '2545314', 'name': 'B. D. Bus'}, {'authorId': '7161646', 'name': 'K. D. Bosschere'}, {'authorId': '144795520', 'name': 'B. Preneel'}]",2007.0,QoP '07,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"Despite the recent advances in the theory underlying obfuscation, there still is a need to evaluate the quality of practical obfuscating transformations more quickly and easily. This paper presents the first steps toward a comprehensive evaluation suite consisting of a number of deobfuscating transformations and complexity metrics that can be readily applied on existing and future transformations in the domain of binary obfuscation. In particular, a framework based on software complexity metrics measuring four program properties: code, control flow, data and data flow is suggested. A number of well-known obfuscating and deobfuscating transformations are evaluated based upon their impact on a set of complexity metrics. This enables us to quantitatively evaluate the potency of the (de)obfuscating transformations.",86.0
Reducing the Risk of Human Extinction,991af6e8dfd4e375625db8d513164265747dda1f,"[{'authorId': '2076354483', 'name': 'J. Matheny'}]",2007.0,Risk Analysis,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"In this century a number of events could extinguish humanity. The probability of these events may be very low, but the expected value of preventing them could be high, as it represents the value of all future human lives. We review the challenges to studying human extinction risks and, by way of example, estimate the cost effectiveness of preventing extinction‐level asteroid impacts.",99.0
"War, Peace and International Relations: An introduction to strategic history",bd67307a03460c2752a38a53e423be2b49524dc0,"[{'authorId': '144488793', 'name': 'C. Gray'}]",2007.0,,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"1. Strategic History, 1800-2025: Themes and Contexts 2. Carl von Clausewitz and the Theory of War 3. From Limited War to National War: The French Revolution and the Napoleonic Way of War 4. The Nineteenth Century, I: A Strategic View 5. The Nineteenth Century, II: Technology, Warfare, and International Order 6. The Great War and the Invention of Modern Warfare, 1914-18 7. The Twenty-Year Armistice, 1919-1939 8. The Second World War in Europe, I: The Structure and Course of Total War 9. The Second World War in Europe, II: Understanding the War 10. The Second World War in Asia-Pacific, I: Politics 11. The Second World War in Asia-Pacific, II: Strategy 12. The Cold War, I: Politics and Ideology 13. The Cold War, II: The Nuclear Revolution 14. War and Peace After the Cold War: The Interwar Decade 15. 9/11 and the Age of Terror",108.0
International Foundation for Autonomous Agents and MultiAgent Systems ( IFAAMAS ),8aa418ff5d71913d2d3fecc477cdcb6f0d9e2acb,[],2007.0,,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"Thank you for your interest in the publications of the International Foundation for Autonomous Agents and MultiAgent Systems (IFAAMAS). IFAAMAS assumes that authors submitting papers to IFAAMAS do so with the understanding that, if the article is accepted, IFAAMAS has the right to reproduce, publish, disseminate electronically, and do all of the things it normally does with such an article or paper.",727.0
Sleeping Beauty and Self-location: A Hybrid Model,6ea5b317f03db0d178f3eb325a618bc792ff581a,"[{'authorId': '2193691', 'name': 'N. Bostrom'}]",2007.0,Synthese,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,,61.0
Differential Privacy,2618dbd8bc6bc401fbc202342c00cd2ffefcbe4f,"[{'authorId': '1781565', 'name': 'C. Dwork'}]",2006.0,Encyclopedia of Cryptography and Security,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,,4905.0
Probabilistic inference in human semantic memory,e4c9e6a211f41853e711b4dfe0a66cb3532cebcb,"[{'authorId': '1804885', 'name': 'M. Steyvers'}, {'authorId': '1799860', 'name': 'T. Griffiths'}, {'authorId': '50582151', 'name': 'S. Dennis'}]",2006.0,Trends in Cognitive Sciences,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,,102.0
'Rough Consensus and Running Code' and the Internet-OSI Standards War,9ffad637b841df9e1904aea2265d0a88fd855d58,"[{'authorId': '29131029', 'name': 'A. Russell'}]",2006.0,IEEE Annals of the History of Computing,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"Internet historians recognize the technical achievements but often overlook the bureaucratic innovations of Internet pioneers. The phrase, ""We reject: kings, presidents, and voting. We believe in: rough consensus and running code,"" was coined by David Clark in 1992. This article explains how the phrase captured the technical and political values of Internet engineers during a crucial phase in the Internet's growth",109.0
Functional explanation and the function of explanation,98aeefd7f13969b0e7cbf8c3603f416b3f2848a0,"[{'authorId': '2464187', 'name': 'T. Lombrozo'}, {'authorId': '144366429', 'name': 'S. Carey'}]",2006.0,Cognition,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,,265.0
Do the right thing.,5182f637bd029d28d03c08f3afbfce949f56fcfa,"[{'authorId': '1701876', 'name': 'P. Maes'}]",1989.0,Nursing Standard,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"The next few weeks are critical for nurses no doubt anxious to discover what pay award they can expect in April. The government has piled on the pressure for public sector pay awards to be at or below 2 per cent. This figure has already been exceeded by teachers, who were awarded 2.5 per cent, and members of the armed forces, who will receive 3 per cent, both from April.",530.0
Computational Approaches to Preference Elicitation,7caa3033711f60f1a63a6b42bf375c8df00ccd42,"[{'authorId': '2239754', 'name': 'D. Braziunas'}]",2006.0,,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,,39.0
Motivated Skepticism in the Evaluation of Political Beliefs,49dc7768eeaf4dd5ae295c3bb353b8d57c12bb54,"[{'authorId': '116293562', 'name': 'Charles Taber'}, {'authorId': '37410082', 'name': 'M. Lodge'}]",2006.0,,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"We propose a model of motivated skepticism that helps explain when and why citizens are biased-information processors. Two experimental studies explore how citizens evaluate arguments about affirmative action and gun control, finding strong evidence of a prior attitude effect such that attitudinally congruent arguments are evaluated as stronger than attitudinally incongruent arguments. When reading pro and con arguments, participants (Ps) counterargue the contrary arguments and uncritically accept supporting arguments, evidence of a disconfirmation bias. We also find a confirmation bias—the seeking out of confirmatory evidence—when Ps are free to self-select the source of the arguments they read. Both the confirmation and disconfirmation biases lead to attitude polarization—the strengthening of t2 over t1 attitudes—especially among those with the strongest priors and highest levels of political sophistication. We conclude with a discussion of the normative implications of these findings for rational behavior in a democracy.",2886.0
Human-Level Artificial Intelligence? Be Serious!,5185c810d1851dae0de7aee97d51dc59bbd1d2a9,"[{'authorId': '144497046', 'name': 'N. Nilsson'}]",2005.0,The AI Magazine,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"I claim that achieving real human-level artificial intelligence would necessarily imply that most of the tasks that humans perform for pay could be automated. Rather than work toward this goal of automation by building special-purpose systems, I argue for the development of general-purpose, educable systems that can learn and be taught to perform any of the thousands of jobs that humans can perform. Joining others who have made similar proposals, I advocate beginning with a system that has minimal, although extensive, built-in capabilities. These would have to include the ability to improve through learning along with many other abilities.",99.0
Experimenting with a Democratic Ideal: Deliberative Polling and Public Opinion,fa1504b3d979666a5fe85120bbef3c541fb606b6,"[{'authorId': '5248896', 'name': 'James S. Fishkin'}, {'authorId': '103078642', 'name': 'Robert C. Luskin'}]",2005.0,,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,,575.0
Toward a strategic human resource management model of high reliability organization performance,e9ece8a41ed612e10e709eeec921c910c04c64ed,"[{'authorId': '48546237', 'name': 'J. Ericksen'}, {'authorId': '31324243', 'name': 'L. Dyer'}]",2005.0,,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"In this article, we extend strategic human resource management (SHRM) thinking to theory and research on high reliability organizations (HROs) using a behavioural approach. After considering the viability of reliability as an organizational performance indicator, we identify a set of eight reliability-oriented employee behaviours (ROEBs) likely to foster organizational reliability and suggest that they are especially valuable to reliability-seeking organizations that operate under ‘trying conditions’. We then develop a reliability-enhancing human resource strategy (REHRS) likely to facilitate the manifestation of these ROEBs. We conclude that the behavioural approach offers SHRM scholars an opportunity to explain how people contribute to specific organizational goals in specific contexts and, in turn, to identify human resource strategies that extend the general high performance human resource strategy (HPHRS) in new and important ways.",129.0
Agreeing to disagree.,7f71af7570eccce7c5773b376c640fb69210931a,"[{'authorId': '7028355', 'name': 'P. Nichols'}, {'authorId': '3916953', 'name': 'G. Winslow'}]",2005.0,General dentistry,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,,1035.0
The complexity of agreement,dab5b4dfd119f87f5176b7e30c6e4da117d58397,"[{'authorId': '20996436', 'name': 'S. Aaronson'}]",2004.0,Symposium on the Theory of Computing,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"A celebrated 1976 theorem of Aumann asserts that Bayesian agents with common priors can never ""agree to disagree"": if their opinions about any topic are common knowledge, then those opinions must be equal. But two key questions went unaddressed: first, can the agents reach agreement after a conversation of reasonable length? Second, can the computations needed for that conversation be performed efficiently? This paper answers both questions in the affirmative, thereby strengthening Aumann's original conclusion.We show that for two agents with a common prior to agree within ε about the expectation of a [0,1] variable with high probability over their prior, it suffices for them to exchange O(1/ε2) bits. This bound is completely independent of the number of bits n of relevant knowledge that the agents have. We also extend the bound to three or more agents; and we give an example where the ""standard protocol"" (which consists of repeatedly announcing one's current expectation) nearly saturates the bound, while a new ""attenuated protocol"" does better. Finally, we give a protocol that would cause two Bayesians to agree within ε after exchanging O(1/ε2) messages, and that can be simulated by agents with limited computational resources. By this we mean that, after examining the agents' knowledge and a transcript of their conversation, no one would be able to distinguish the agents from perfect Bayesians. The time used by the simulation procedure is exponential in 1/ε6 but not in n.",30.0
Universal Artificial Intellegence - Sequential Decisions Based on Algorithmic Probability,0aa7fee3295f948bc9257d872e8e9e5833a79d2b,"[{'authorId': '144154444', 'name': 'Marcus Hutter'}]",2005.0,Texts in Theoretical Computer Science. An EATCS Series,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,,459.0
A Bayesian View of Language Evolution by Iterated Learning - eScholarship,124f8db330763eebd4dc95b69d9301ffd1c24c2d,"[{'authorId': '1799860', 'name': 'T. Griffiths'}, {'authorId': '2147951', 'name': 'M. Kalish'}]",2005.0,,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"A Bayesian View of Language Evolution by Iterated Learning Thomas L. Griffiths (tom griffiths@brown.edu) Department of Cognitive and Linguistic Sciences, Brown University, Providence, RI 02912 Michael L. Kalish (kalish@louisiana.edu) Institute of Cognitive Science, University of Louisiana at Lafayette, Lafayette, LA 70504 (b) x 0 y 0 on hypothesis x 1 h 1 y 1 data le ar n in g data ge n er at i hypothesis le ar n in g (a) data ge n er at i Human languages form a subset of all logically pos- sible communication schemes, with universal properties shared by all languages (Comrie, 1981; Greenberg, 1963; Hawkins, 1988). A traditional explanation for these lin- guistic universals is that they are the consequence of constraints on the set of learnable languages imposed by an innate, language-specific, genetic endowment (e.g., Chomsky, 1965). Recent research has explored an alter- native explanation: that universals emerge from evolu- tionary processes produced by the transmission of lan- guages across generations (e.g., Kirby, 2001; Nowak, Plotkin, & Jansen, 2000). Languages change as each gen- eration learns from that which preceded it. This process of iterated learning implicitly selects for languages that are more learnable. This suggests a tantalizing hypoth- esis: that iterated learning might be sufficient to explain the emergence of linguistic universals (Briscoe, 2002). Kirby (2001) introduced a framework for exploring this hypothesis, called the iterated learning model (ILM). In the ILM, each generation consists of one or more learners. Each learner sees some data, forms a hypothe- sis about the process that produced that data, and then produces the data which will be supplied to the next generation of learners, as shown in Figure 1 (a). The languages that succeed in being transmitted across gen- erations are those that pass through the “information bottleneck” imposed by iterated learning. If particular properties of languages make it easier to pass through that bottleneck, then many generations of iterated learn- ing might allow those properties to become universal. The ILM can be used to explore how different as- sumptions about language learning influence language evolution. A variety of learning algorithms have been examined using the ILM, including a heuristic gram- mar inducer (Kirby, 2001), associative networks (Smith, le ar n in g Models of language evolution have demonstrated how aspects of human language, such as compositionality, can arise in populations of interacting agents. This pa- per analyzes how languages change as the result of a particular form of interaction: agents learning from one another. We show that, when the learners are rational Bayesian agents, this process of iterated learning con- verges to the prior distribution over languages assumed by those learners. The rate of convergence is set by the amount of information conveyed by the data seen by each generation; the less informative the data, the faster the process converges to the prior. on Kirby, & Brighton, 2003), and minimum description length (Brighton, 2002). Iterated learning with these algorithms produces languages that possess one of the most compelling properties of human languages: compo- sitionality. In a compositional language, the meaning of an utterance is a function of the meaning of its parts. The intuitive explanation for these results is that the regular structure of compositional languages means that they can be learned from less data, and are thus more likely to pass through the information bottleneck. These instances of compositionality emerging from it- erated learning raise an important question: what lan- guages will survive many generations of iterated learn- ing? While the circumstances under which composi- tionality will emerge from iterated learning with specific learning algorithms have been investigated (Brighton, 2002; Smith et al., 2003), there are no general results for arbitrary properties of languages or broad classes of learning algorithms. In this paper, we analyze iter- ated learning for the case where the learners are rational Bayesian agents. A variety of learning algorithms can be formulated in terms of Bayesian inference, and Bayesian methods underlie many approaches in computational lin- guistics (Manning & Sch¨ utze, 1999). The assumption that the learners are Bayesian agents makes it possible to derive analytic results indicating which languages will be favored by iterated learning. In particular, we prove the surprising result that the probability distribution over languages resulting from iterated Bayesian learning con- verges to the prior probability distribution assumed by the learners. This implies that the asymptotic probabil- ity that a language is used does not depend at all upon the properties of the language, being determined entirely by the assumptions of the learner. Abstract x 2 h 2 y 2 Figure 1: (a) Iterated learning. (b) Dependencies among variables in iterated iterated Bayesian learning.",85.0
An Explainable Artificial Intelligence System for Small-unit Tactical Behavior,d1c91fdf066b9195a25626e903ce55765dde0387,"[{'authorId': '3153325', 'name': 'M. Lent'}, {'authorId': '2053187711', 'name': 'William Fisher'}, {'authorId': '2073340858', 'name': 'Michael Mancuso'}]",2004.0,AAAI Conference on Artificial Intelligence,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"As the artificial intelligence (AI) systems in military simulations and computer games become more complex, their actions become increasingly difficult for users to understand. Expert systems for medical diagnosis have addressed this challenge though the addition of explanation generation systems that explain a system's internal processes. This paper describes the AI architecture and associated explanation capability used by Full Spectrum Command, a training system developed for the U.S. Army by commercial game developers and academic researchers.",188.0
Apprenticeship learning via inverse reinforcement learning,f65020fc3b1692d7989e099d6b6e698be5a50a93,"[{'authorId': '1689992', 'name': 'P. Abbeel'}, {'authorId': '34699434', 'name': 'A. Ng'}]",2004.0,International Conference on Machine Learning,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"We consider learning in a Markov decision process where we are not explicitly given a reward function, but where instead we can observe an expert demonstrating the task that we want to learn to perform. This setting is useful in applications (such as the task of driving) where it may be difficult to write down an explicit reward function specifying exactly how different desiderata should be traded off. We think of the expert as trying to maximize a reward function that is expressible as a linear combination of known features, and give an algorithm for learning the task demonstrated by the expert. Our algorithm is based on using ""inverse reinforcement learning"" to try to recover the unknown reward function. We show that our algorithm terminates in a small number of iterations, and that even though we may never recover the expert's reward function, the policy output by the algorithm will attain performance close to that of the expert, where here performance is measured with respect to the expert's unknown reward function.",2880.0
High Reliability and the Management of Critical Infrastructures,06b48292a1ab5a188599a0f2693972dc48e8c9d2,"[{'authorId': '145452766', 'name': 'P. Schulman'}, {'authorId': '98447460', 'name': 'Emory Roe'}, {'authorId': '9301670', 'name': 'M. V. Eeten'}, {'authorId': '143981127', 'name': 'M. Bruijne'}]",2004.0,,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"Organisation theorists and practitioners alike have become greatly interested in high reliability in the management of large hazardous technical systems and society's critical service infrastructures. But much of the reliability analysis is centred in particular organisations that have command and control over their technical cores. Many technical systems, including electricity generation, water, telecommunications and other ""critical infrastructures,"" are not the exclusive domain of single organisations. Our essay is organised around the following research question: How do organisations, many with competing, if not conflicting goals and interests, provide highly reliable service in the absence of ongoing command and control and in the presence of rapidly changing task environments with highly consequential hazards? We analyse electricity restructuring in California as a specific case. Our conclusions have surprising and important implications both for high reliability theory and for the future management of critical infrastructures organised around large technical systems.",125.0
A Critical Look at Risk Assessments for Global Catastrophes,3bb94b38582b1733129dfa1cfe0f119da7f20742,"[{'authorId': '143622798', 'name': 'A. Kent'}]",2000.0,Risk Analysis,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"Recent articles by Busza et al. (BJSW) and Dar et al. (DDH) argue that astrophysical data can be used to establish small bounds on the risk of a “killer strangelet” catastrophe scenario in the RHIC and ALICE collider experiments. The case for the safety of the experiments set out by BJSW does not rely solely on these bounds, but on theoretical arguments, which BJSW find sufficiently compelling to firmly exclude any possibility of catastrophe.",42.0
Program equilibrium,e1a060cda74e0e3493d0d81901a5a796158c8410,"[{'authorId': '1708847', 'name': 'Moshe Tennenholtz'}]",2004.0,Games Econ. Behav.,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,,93.0
Why the future doesn’t need us,7189f82fb16b35ac630446179320b12d4a642345,"[{'authorId': '2142826362', 'name': 'Joy Bill'}]",2003.0,,"['AI Research Considerations for Human Existential Safety (ARCHES)', 'Responses to catastrophic AGI risk: a survey', 'Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",3,"Our most powerful 21st-century technologies robotics, genetic engineering, and nanotech are threatening to make humans an endangered species. From the moment I became involved in the creation of new technologies, their ethical dimensions have concerned me, but it was only in the autumn of 1998 that I became anxiously aware of how great are the dangers facing us in the 21st century. I can date the onset of my unease to the day I met Ray Kurzweil, the deservedly famous inventor of the first reading machine for the blind and many other amazing things.",845.0
The Social Contract and The First and Second Discourses,dc15b1f6d33c813b9142f8bf34af391a19e90339,"[{'authorId': '2107460', 'name': 'J. Rousseau'}, {'authorId': '32841019', 'name': 'Susan Dunn'}, {'authorId': '116510341', 'name': 'G. May'}, {'authorId': '32098588', 'name': 'R. Bellah'}, {'authorId': '46339085', 'name': 'D. Bromwich'}, {'authorId': '1409737820', 'name': 'Conor O’Brien'}]",2017.0,,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"Jean-Jacques Rousseau's ideas about society, culture and government are pivotal in the history of political thought. His works are as controversial as they are relevant today. This volume brings together three of Rousseau's most important political writings - ""The Social Contract"" and ""The First Discourse (Discourse on the Sciences and Arts)"" and ""The Second Discourse (Discourse on the Origin and Foundations of Inequality)"" - and presents essays by major scholars that shed light on the dimensions and implications of these texts. Susan Dunn's introductory essay underlines the unity of Rousseau's political thought and explains why his ideas influenced Jacobin revolutionaries in France but repelled American revolutionaries across the ocean. Gita May's essay discusses Rousseau as cultural critic. Robert Bellah explores Rousseau's attempt to resolve the tension between the individual's desire for freedom and the obligations that society imposes. David Bromwich analyzes Rousseau as a psychologist of the human self. And Conor Cruise O'Brien takes on the ""noxious"", ""deranged"" Rousseau, excoriated by Edmund Burke but admired by Robespierre and Thomas Jefferson. Written from different, even opposing perspectives, these essays should convey a sense of the vital and contentious debate surrounding Rousseau and his legacy. For this edition Susan Dunn has provided a new translation of the ""Discourse on the Sciences and Arts"" and has revised a previously published translation of ""The Social Contract"".",175.0
Protecting the Ozone Layer : The United Nations History,a4534d525f034c79f61cf7d830374b51b120dc38,"[{'authorId': '48068041', 'name': 'S. Andersen'}, {'authorId': '145450732', 'name': 'K. Sarma'}, {'authorId': '144310592', 'name': 'Lani Sinclair'}]",2002.0,,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"The Science of Ozone depletion: From Theory to Certainty * Diplomacy: the Beginning, 1974-1987 * Diplomacy: From Strength to Strength, 1988-1992 * Diplomacy: Racing Towards Success * Technology and Business Policy * Implementation of the Montreal Protocol * Compliance with the Montreal Protocol * Media Coverage of the Ozone-layer Issue * Environmental NGO's, the Ozone layer and the Montreal Protocol * Notes * List of Acronyms and Abbreviations * Glossary * About the Contributors * Index",133.0
A POMDP formulation of preference elicitation problems,8b66050af65389875818df7a4dfacfa62e8be7f9,"[{'authorId': '145646162', 'name': 'Craig Boutilier'}]",2002.0,AAAI/IAAI,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"Preference elicitation is a key problem facing the deployment of intelligent systems that make or recommend decisions on the behalf of users. Since not all aspects of a utility function have the same impact on object-level decision quality, determining which information to extract from a user is itself a sequential decision problem, balancing the amount of elicitation effort and time with decision quality. We formulate this problem as a partially-observable Markov decision process (POMDP). Because of the continuous nature of the state and action spaces of this POMDP, standard techniques cannot be used to solve it. We describe methods that exploit the special structure of preference elicitation to deal with parameterized belief states over the continuous state space, and gradient techniques for optimizing parameterized actions. These methods can be used with a number of different belief state representations, including mixture models.",286.0
Why the elf acted autonomously: towards a theory of adjustable autonomy,4734b186b333bbc426ffa0144e5bca081e24251c,"[{'authorId': '1745967', 'name': 'P. Scerri'}, {'authorId': '1748597', 'name': 'David V. Pynadath'}, {'authorId': '143736701', 'name': 'Milind Tambe'}]",2002.0,Adaptive Agents and Multi-Agent Systems,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"Adjustable autonomy refers to agents' dynamically varying their own autonomy, transferring decision making control to other entities (typically human users) in key situations. Determining whether and when such transfer of control must occur is arguably the fundamental research question in adjustable autonomy. Practical systems have made significant in roads in answering this question and in providing high-level guidelines for transfer of control decisions. For instance, [11] report that Markov decision processes were successfully used in transfer of control decisions in a real world multiagent system, but that use of C4.5 led to failures. Yet, an underlying theory of transfer of control, that would explain such successes or failures is missing. To take a step in building this theory, we introduce the notion of a transfer-of-control strategy, which potentially involves several transfer of control actions. A mathematical model based on this notion allows both analysis of previously reported implementations and guidance for the design of new implementations. The practical benefits of this model are illustrated in a dramatic simplification of an existing adjustable autonomy system.",64.0
Considered Opinions: Deliberative Polling in Britain,92978c927ded34be9a2e8aca1fb9cf9ae20ee257,"[{'authorId': '103078642', 'name': 'Robert C. Luskin'}, {'authorId': '5248896', 'name': 'James S. Fishkin'}, {'authorId': '81969905', 'name': 'R. Jowell'}]",2002.0,,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"This article presents the results of the first Deliberative Poll, in which a national British sample discussed the issue of rising crime and what to do about it. We describe Deliberative Polling and its rationale, the representativeness of the deliberative sample, the extent to which the participants acquired factual information about the issue and about politics generally, and how much and how they changed their views. We also weigh the extent to which such changes of view hinge on small group influences versus information gains.",576.0
Towards Adjustable Autonomy for the Real World,66f75528bc3d9fb0e7c54a9ce21d185765f8e8fd,"[{'authorId': '1745967', 'name': 'P. Scerri'}, {'authorId': '1748597', 'name': 'David V. Pynadath'}, {'authorId': '143736701', 'name': 'Milind Tambe'}]",2002.0,Journal of Artificial Intelligence Research,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"Adjustable autonomy refers to entities dynamically varying their own autonomy, transferring decision-making control to other entities (typically agents transferring control to human users) in key situations. Determining whether and when such transfers-of-control should occur is arguably the fundamental research problem in adjustable autonomy. Previous work has investigated various approaches to addressing this problem but has often focused on individual agent-human interactions. Unfortunately, domains requiring collaboration between teams of agents and humans reveal two key shortcomings of these previous approaches. First, these approaches use rigid one-shot transfers of control that can result in unacceptable coordination failures in multiagent settings. Second, they ignore costs (e.g., in terms of time delays or effects on actions) to an agent's team due to such transfers-of-control. 
 
To remedy these problems, this article presents a novel approach to adjustable autonomy, based on the notion of a transfer-of-control strategy. A transfer-of-control strategy consists of a conditional sequence of two types of actions: (i) actions to transfer decision-making control (e.g., from an agent to a user or vice versa) and (ii) actions to change an agent's pre-specified coordination constraints with team members, aimed at minimizing miscoordination costs. The goal is for high-quality individual decisions to be made with minimal disruption to the coordination of the team. We present a mathematical model of transfer-of-control strategies. The model guides and informs the operationalization of the strategies using Markov Decision Processes, which select an optimal strategy, given an uncertain environment and costs to the individuals and teams. The approach has been carefully evaluated, including via its use in a real-world, deployed multi-agent system that assists a research group in its daily activities.",231.0
When systems fail,27d54040172482455c5ab5acce89f1648710cf8a,"[{'authorId': '2020299', 'name': 'K. Roberts'}, {'authorId': '3359578', 'name': 'R. Bea'}]",2001.0,,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,,72.0
The Incident Command System : High Reliability Organizing for Complex and Volatile Task,f9550ee99502e7e89cd1ccc20fda30d701fa5c7c,"[{'authorId': '13132484', 'name': 'Gregory A. Bigley'}, {'authorId': '2020299', 'name': 'K. Roberts'}, {'authorId': '2053000201', 'name': 'W. Haas'}, {'authorId': '2057195045', 'name': 'David J. Levine'}, {'authorId': '4833497', 'name': 'S. Masterson'}]",2001.0,,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,The incident command system (ICS) is a particular approach to assembly and control of the highly reliable temporary organizations employed by many public safety professionals to manage diverse reso...,897.0
Secure multi-party computation problems and their applications: a review and open problems,832c2787bec64510ee9f1e9b1738da0cc0a44b56,"[{'authorId': '3656182', 'name': 'Wenliang Du'}, {'authorId': '1719237', 'name': 'M. Atallah'}]",2001.0,New Security Paradigms Workshop,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"The growth of the Internet has triggered tremendous opportunities for cooperative computation, where people are jointly conducting computation tasks based on the private inputs they each supplies. These computations could occur between mutually untrusted parties, or even between competitors. For example, customers might send to a remote database queries that contain private information; two competing financial organizations might jointly invest in a project that must satisfy both organizations' private and valuable constraints, and so on. Today, to conduct such computations, one entity must usually know the inputs from all the participants; however if nobody can be trusted enough to know all the inputs, privacy will become a primary concern.This problem is referred to as Secure Multi-party Computation Problem (SMC) in the literature. Research in the SMC area has been focusing on only a limited set of specific SMC problems, while privacy concerned cooperative computations call for SMC studies in a variety of computation domains. Before we can study the problems, we need to identify and define the specific SMC problems for those computation domains. We have developed a framework to facilitate this problem-discovery task. Based on our framework, we have identified and defined a number of new SMC problems for a spectrum of computation domains. Those problems include privacy-preserving database query, privacy-preserving scientific computations, privacy-preserving intrusion detection, privacy-preserving statistical analysis, privacy-preserving geometric computations, and privacy-preserving data mining.The goal of this paper is not only to present our results, but also to serve as a guideline so other people can identify useful SMC problems in their own computation domains.",455.0
Making Low Probabilities Useful,0f1fbabbb6e195ffecd16e40f7a366615c9607e2,"[{'authorId': '2856866', 'name': 'H. Kunreuther'}, {'authorId': '4448370', 'name': 'Nathan Novemsky'}, {'authorId': '3683465', 'name': 'D. Kahneman'}]",2001.0,,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,,277.0
Must accidents happen? Lessons from high-reliability organizations,11c4962c67966cc35502b3ac3c04ddc947c6383c,"[{'authorId': '2020299', 'name': 'K. Roberts'}, {'authorId': '3359578', 'name': 'R. Bea'}]",2001.0,,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"Executive Overview In the more than 15 years since the initial publication of Charles Perrow's Normal Accidents, practitioners and academics have contemplated how plane crashes, earthendam collapses, ship collisions, nuclear disasters, and chemical-plant explosions can be prevented, mitigated, or avoided. While no one has yet learned how to make the inevitable avoidable, a literature on high-reliability organizations (HROs) has developed that gives some hope that disasters can be minimized in frequency and severity. The value of this research to practicing executives is to take the lessons learned through the research on HROs and apply them to their own organizations. This article is about how to beat the odds of having an incident or accident that one is unprepared for, regardless of the organization's purpose. Neither the sausage maker nor the chemical-plant manager is immune from errors that can have far-reaching consequences. The three major recommendations we offer are that managers should aggressive...",347.0
The Complexity of Decentralized Control of Markov Decision Processes,07d88404f24d61a8ea41ee7f688f57ee8f44ac12,"[{'authorId': '35176432', 'name': 'D. Bernstein'}, {'authorId': '1707550', 'name': 'S. Zilberstein'}, {'authorId': '1808597', 'name': 'N. Immerman'}]",2000.0,Conference on Uncertainty in Artificial Intelligence,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"We consider decentralized control of Markov decision processes and give complexity bounds on the worst-case running time for algorithms that find optimal solutions. Generalizations of both the fully observable case and the partially observable case that allow for decentralized control are described. For even two agents, the finite-horizon problems corresponding to both of these models are hard for nondeterministic exponential time. These complexity results illustrate a fundamental difference between centralized and decentralized control of Markov decision processes. In contrast to the problems involving centralized control, the problems we considerprovably do not admit polynomial-time algorithms. Furthermore, assuming EXP ? NEXP, the problems require superexponential time to solve in the worst case.",1387.0
Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition,4c96ca25d889251e20e33d01f24eec175301ab94,"[{'authorId': '144299726', 'name': 'Thomas G. Dietterich'}]",1999.0,Journal of Artificial Intelligence Research,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"This paper presents a new approach to hierarchical reinforcement learning based on decomposing the target Markov decision process (MDP) into a hierarchy of smaller MDPs and decomposing the value function of the target MDP into an additive combination of the value functions of the smaller MDPs. The decomposition, known as the MAXQ decomposition, has both a procedural semantics--as a subroutine hierarchy--and a declarative semantics--as a representation of the value function of a hierarchical policy. MAXQ unifies and extends previous work on hierarchical reinforcement learning by Singh, Kaelbling, and Dayan and Hinton. It is based on the assumption that the programmer can identify useful subgoals and define subtasks that achieve these subgoals. By defining such subgoals, the programmer constrains the set of policies that need to be considered during reinforcement learning. The MAXQ value function decomposition can represent the value function of any policy that is consistent with the given hierarchy. The decomposition also creates opportunities to exploit state abstractions, so that individual MDPs within the hierarchy can ignore large parts of the state space. This is important for the practical application of the method. This paper defines the MAXQ hierarchy, proves formal results on its representational power, and establishes five conditions for the safe use of state abstractions. The paper presents an online model-free learning algorithm, MAXQ-Q, and proves that it converges with probability 1 to a kind of locally-optimal policy known as a recursively optimal policy, even in the presence of the five kinds of state abstraction. The paper evaluates the MAXQ representation and MAXQ-Q through a series of experiments in three domains and shows experimentally that MAXQ-Q (with state abstractions) converges to a recursively optimal policy much faster than flat Q learning. The fact that MAXQ learns a representation of the value function has an important benefit: it makes it possible to compute and execute an improved, non-hierarchical policy via a procedure similar to the policy improvement step of policy iteration. The paper demonstrates the effectiveness of this nonhierarchical execution experimentally. Finally, the paper concludes with a comparison to related work and a discussion of the design tradeoffs in hierarchical reinforcement learning.",1552.0
Science's new social contract with society,c25bee8098dafe7bdc2456a2a5aabadf631190bd,"[{'authorId': '145176432', 'name': 'M. Gibbons'}]",1999.0,Nature,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,,795.0
Safe operation as a social construct,63eb9217f52173d702ab4537e9ccf1e24ca787de,"[{'authorId': '2862852', 'name': 'G. Rochlin'}]",1999.0,,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"Empirical work on organizations that manage complex, potentially hazardous technical operations with a surprisingly low rate of serious incidents shows that operational safety is more than the management or avoidance of risk or error. Safety so defined is an ongoing intersubjective construct not readily measured in terms of safety cultures, structures, functions, or other commonly used descriptors of technical or organizational attributes that fail fully to take into account collective as well as individual agency. In the cases that the author has studied, it is represented by the interactive dynamic between operators and managers, as well as their engagement with operational and organizational conditions. The maintenance of safe operation so defined is an interactive, dynamic and communicative act, hence it is particularly vulnerable to disruption or distortion by well-meant but imperfectly informed interventions aimed at eliminating or reducing ‘human error’ that do not take into account the importance ...",331.0
Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning,0e7638dc16a5e5e9e46c91272bfb9c3dd242ef6d,"[{'authorId': '1699645', 'name': 'R. Sutton'}, {'authorId': '144368601', 'name': 'Doina Precup'}, {'authorId': '1699868', 'name': 'Satinder Singh'}]",1999.0,Artificial Intelligence,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,,3109.0
Team errors: definition and taxonomy,c0bb0de04e69213a61ca4a66e97f68b334c564c6,"[{'authorId': '5686156', 'name': 'Kunihide Sasou'}]",1999.0,,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,,172.0
Alive and Well after 25 Years: A Review of Groupthink Research.,de2d0546664708bddcb84e47550c9bfc8db72bf6,"[{'authorId': '47610930', 'name': 'Esser'}]",1998.0,Organizational Behavior and Human Decision Processes,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"This article provides a summary of empirical research on groupthink theory. Groupthink research, including analyses of historical cases of poor group decision making and laboratory tests of groupthink, is reviewed. Results from these two research areas are briefly compared. Theoretical and methodological issues for future groupthink research are identified and discussed. I conclude that groupthink research has had and continues to have considerable heuristic value. A small, but growing, body of empirical literature has been generated. In addition, groupthink research has stimulated a number of theoretical ideas, most of which have yet to be tested. Copyright 1998 Academic Press.",382.0
Reinforcement Learning with Hierarchies of Machines,52e2ac397f0c8d5f533959905df899bc328d9f85,"[{'authorId': '145726861', 'name': 'Ronald E. Parr'}, {'authorId': '145107462', 'name': 'Stuart J. Russell'}]",1997.0,NIPS,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"We present a new approach to reinforcement learning in which the policies considered by the learning process are constrained by hierarchies of partially specified machines. This allows for the use of prior knowledge to reduce the search space and provides a framework in which knowledge can be transferred across problems and in which component solutions can be recombined to solve larger and more complicated problems. Our approach can be seen as providing a link between reinforcement learning and ""behavior-based"" or ""teleo-reactive"" approaches to control. We present provably convergent algorithms for problem-solving and learning with hierarchical machines and demonstrate their effectiveness on a problem with several thousand states.",823.0
HQ-Learning,68da5e8c6678048469da5e9308fd340840e5f34e,"[{'authorId': '32239759', 'name': 'M. Wiering'}, {'authorId': '145341374', 'name': 'J. Schmidhuber'}]",1997.0,Adaptive Behavior,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,HQ-learning is a hierarchical extension of Q(λ)-learning designed to solve certain types of partially observable Markov decision problems (POMDPs). HQ automatically decomposes POMDPs into sequences of simpler subtasks that can be solved by memoryless policies learnable by reactive subagents. HQ can solve partially observable mazes with more states than those used in most previous POMDP work.,198.0
Groupthink,d5b39b28c5d143834cd529e1d6447dd4b0b83b99,"[{'authorId': '98004772', 'name': 'Marc D. Street'}]",1997.0,,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"Despite its widespread appeal, the groupthink model has come under severe attack recently. Taking the position that recent calls for major revisions to the original formulation are premature, this article examines four theoretical areas that have been problematic for scholars investigating the groupthink phenomenon. This examination allows for the presentation of both implications and research suggestions designed to refocus research efforts on the model as originally proposed by Janis.",761.0
The voluntary provision of a pure public good: The case of reduced CFC emissions and the Montreal Protocol,e73865e5bacd7e737867e813169339374f7deb3d,"[{'authorId': '4320437', 'name': 'J. Murdoch'}, {'authorId': '49713518', 'name': 'T. Sandler'}]",1997.0,,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,,249.0
"The Challenger Launch Decision: Risky Technology, Culture, and Deviance at NASA",a09cdcde4ebc6db199fb4bdd73447057d6cd37a5,"[{'authorId': '34304685', 'name': 'D. Vaughan'}]",1996.0,,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"he Eve of the Launch Learning Culture, Revising History Risk, Work Group Culture, and the Norma lization of Deviance The Normalization of Deviance, 1981-1984 The Normalization of Deviance, 19856: The Culture of Production St ructural Secrecy The Eve of the Launch Revisited Conformity and Tragedy Lessons Learned Appendix A Cost/Safety Trade-Offs? Scra pping the Escape Rockets and the SRB Contract Award Decision. Appendix B Supporting Charts and Documents Appendix C On Theory Elaboration, Organizations, and Historical Ethnography.",1864.0
Organizing Maintenance Work At Two American Nuclear Power Plants,43bf284c2f967ec0dc45d02ca30c3e735b682848,"[{'authorId': '48942571', 'name': 'M. Bourrier'}]",1996.0,,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"Organizational strategies used by operations and maintenance personnel at two US nuclear power plants are compared with those described by the Berkeley group in their extensive high reliability organization studies. Using the method of strategic analysis, we show that the two organizations use quite different strategies in their search for reliability and effectiveness. The focus on the coordination of workers and structuring of tasks reveals areas not completely dealt with in previous HRO-related studies. It is argued that adopting a complementary approach in which elements of both metholds are selectively applied may lead to an analytic framework of greater theoretical and explanatory power.",116.0
"High Reliability Organizations: Unlikely, Demanding and At Risk",513db09cf1bbf387aeec684cfa61b5f79b65b783,"[{'authorId': '41126110', 'name': 'T. L. Porte'}]",1996.0,,"['AI Research Considerations for Human Existential Safety (ARCHES)', 'X-Risk Analysis for AI Research']",2,"The HRO project is cast within a broader socio-political context, by first, reviewing its practical origins, its conceptual/logical framework, and a summary of the project's provisional findings, including a brief observation about the importance of a ‘culture of reliability.’Then some socio/political implications for HROs are explored as they assume the status of large technical systems (LTSs) and become quasi-public institutions. This paper ends with a reflection on the challenges of institutional trustworthiness that confront HRO operators, managers, and overseers.",541.0
On the Interpretation of Decision Problems with Imperfect Recall,dd03415f643088df006f9ad12e22f43a57adb552,"[{'authorId': '30027584', 'name': 'Michele Piccione'}, {'authorId': '144644618', 'name': 'A. Rubinstein'}]",1996.0,Theoretical Aspects of Rationality and Knowledge,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"In this paper it is argued that extensive decision problems (extensive games with a single player) with imperfect recall suffer from major ambiguities in the interpretation of information sets and strategies. This indeterminacy allows for different kinds of analysis. We address the following issues: 
 
1. Randomization at information sets 
 
2. Consistent beliefs 
 
3. Time consistency of optimal plans 
 
4. The multi-selves approach to decision making 
 
We illustrate our discussion through an example that we call the absentminded driver paradox. 
 
An individual is sitting late at night in a bar planning his midnight trip home. In order to get home he has to take the highway and get off at the second exit. Turning at the first exit leads into a disastrous area (payoff 0). Turning at the second exit yields the highest reward (payoff 4). If he continues beyond the second exit he will reach the end of the highway and find a hotel where he can spend the night (payoff 1). The driver is absentminded and is aware of this fact. When reaching an intersection he cannot tell whether it is the first or the second intersection and he cannot remember how many he has passed. While sitting at the bar, all he can do is to decide whether or not to exit at an intersection (we exclude at this stage the possibility that the decision maker can include random elements in his strategy). 
 
Planning his trip at the bar, the decision maker must conclude that it is impossible for him to get home and he should not exit when he reaches an intersection. Thus, his optimal plan will lead him to spend the night at the hotel and yields a payoff of 1. Now, suppose that he reaches an intersection. Remembering his strategy he concludes that he is at the first intersection with probability 1/2. Then, reviewing his plan, he must conclude that it is optimal for him to leave the highway since it yields an expected payoff of 2. Thus, despite no new information and no change in his preferences, the decision maker is tempted to change his initial plan once he reaches an intersection! 
 
We find this example paradoxical as it exhibits a conflict between two ways of reasoning. The first instructs the decision maker to follow his initial decision not to exit, as this is the optimal rule of behavior. The second leads him to optimize expected payoffs given his beliefs and to deviate from his initial decision.",387.0
Understanding the Intentions of Others: Re-Enactment of Intended Acts by 18-Month-Old Children.,5e28ba8bd38995b7aea03589993d43028b362725,"[{'authorId': '3053914', 'name': 'A. Meltzoff'}]",1995.0,Developmental Psychology,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"Investigated was whether children would re-enact what an adult actually did or what the adult intended to do. In Experiment 1 children were shown an adult who tried, but failed, to perform certain target acts. Completed target acts were thus not observed. Children in comparison groups either saw the full target act or appropriate controls. Results showed that children could infer the adult's intended act by watching the failed attempts. Experiment 2 tested children's understanding of an inanimate object that traced the same movements as the person had followed. Children showed a completely different reaction to the mechanical device than to the person: They did not produce the target acts in this case. Eighteen-month-olds situate people within a psychological framework that differentiates between the surface behavior of people and a deeper level involving goals and intentions. They have already adopted a fundamental aspect of folk psychology-persons (but not inanimate objects) are understood within a framework involving goals and intentions.",1792.0
Organizational Culture in High Reliability Organizations: An Extension,2a46739b63c3fffa20bc7d1b5480603b328de4e6,"[{'authorId': '144577259', 'name': 'R. Klein'}, {'authorId': '13132484', 'name': 'Gregory A. Bigley'}, {'authorId': '2020299', 'name': 'K. Roberts'}]",1995.0,,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"This paper compares organization culture assessments in two high reliability organizations (HROs) with each other, with similar assessments done in other HROs, and with assessments done in other kinds of organizations. It then assesses the consistency of the culture assessments in the two HROs reported here with the theoretical typology of HROs offered by Schulman, and compares the relationship of culture norms to attitudes and role perceptions found in this research with similar relationships found in the HROs. Finally, the paper provides field data from the two organizations reported on here that support Schulman's model.",151.0
Artificial Intelligence: A Modern Approach,3524cdf7cf8344e7eb74886f71fcbb5c6732c337,"[{'authorId': '145107462', 'name': 'Stuart J. Russell'}, {'authorId': '2784519', 'name': 'Peter Norvig'}]",1995.0,,"['AI Research Considerations for Human Existential Safety (ARCHES)', 'Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",2,"The long-anticipated revision of this #1 selling book offers the most comprehensive, state of the art introduction to the theory and practice of artificial intelligence for modern applications. Intelligent Agents. Solving Problems by Searching. Informed Search Methods. Game Playing. Agents that Reason Logically. First-order Logic. Building a Knowledge Base. Inference in First-Order Logic. Logical Reasoning Systems. Practical Planning. Planning and Acting. Uncertainty. Probabilistic Reasoning Systems. Making Simple Decisions. Making Complex Decisions. Learning from Observations. Learning with Neural Networks. Reinforcement Learning. Knowledge in Learning. Agents that Communicate. Practical Communication in English. Perception. Robotics. For computer professionals, linguists, and cognitive scientists interested in artificial intelligence.",27971.0
Regulatory Compliance and the Ethos of Quality Enhancement: Surprises in Nuclear Power Plant Operations1,1e9e3d062de8e2d4d428099f8372dd4959b4d084,"[{'authorId': '41126110', 'name': 'T. L. Porte'}, {'authorId': '2148769448', 'name': 'Craig W. Thomas'}]",1995.0,,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"Regulatory Compliance and the Ethos of Quality Enhancement: Surprises in Nuclear Power Plant Operations Todd R. La Porte Department of Political Science University of California, Berkeley Working Paper SO^EHNMENTAL > iimm FcB 1 0 1994 INSTITUTE OF GOVERNMENTAL STUDIES UNIVERSITY OF CALIFORNIA AT BFRKFLFY",72.0
Groupthink in Government: A Study of Small Groups and Policy Failure,f36d6014296df120973de8c8ca62c08716fd01cf,"[{'authorId': '145764862', 'name': 'P. Hart'}]",1994.0,,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"Why do groups of talented and experienced individuals make disastrously bad collective judgments, such as the Kennedy administration's flawed decision to proceed with the Bay of Pigs invasion in 1961? In his pioneering research on collective decision making, Irving Janis introduced the concept of ""groupthink""-a deliberately Orwellian neologism-to describe such occurrences. Now, in the first book-length study of groupthink since Janis's work, Paul 't Hart has provided a rigorous and systematic version of this influential theory which opens several new avenues for research.",191.0
Decision Dynamics in Two High Reliability Military Organizations,6f9f04163422390e8e4613312997da96b1a49e03,"[{'authorId': '2020299', 'name': 'K. Roberts'}, {'authorId': '46295711', 'name': 'S. Stout'}, {'authorId': '114115025', 'name': 'Jennifer J. Halpern'}]",1994.0,,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"In this research we extend theoretical development about decision making in organizations in which many kinds of errors cannot be tolerated. Catastrophic consequences can be associated with faulty decision making in reliability-seeking organizations, a situation which does not occur in most organizations studied in the past. Observations are drawn from two nuclear-powered aircraft carriers. We find decision processes which appear to change often in these organizations. Important decisions can be made by a number of men even at the lowest levels of the organization. Task-related factors such as technical complexity, high interdependence, and catastrophic consequences associated with rare events and more cognitive factors such as accountability and salience affect decision processes. A model is presented that accounts for dynamic change in decision processes in these organizations.",231.0
Feature Visualization,fca0e8bb93a8242c153ef966cefade5e19e5ed21,"[{'authorId': '145059765', 'name': 'D. Silver'}]",1994.0,Scientific Visualization,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,,709.0
Social contract theory,087338dad51b2f5f8b00fa643f554ec0e1dc84f6,"[{'authorId': '118763109', 'name': 'Viktor J. Vanberg'}]",1994.0,,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"Political thinkers have attempted to explain the origin of the state in various ways. When, where and how the state came into existence have not been recorded anywhere in history. Therefore, the political thinkers were compelled to adopt various hypotheses, many of which are now discredited in the light of modern knowledge. Among the many theories which are concerned with the origin of the state the following are explained in this chapter.",24.0
The Negotiated Order of Organizational Reliability,11c00756e71a38540053bea1d5ee106da9f14158,"[{'authorId': '145452766', 'name': 'P. Schulman'}]",1993.0,,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"A critical problem facing modern organizations in a variety of settings is the erosion of slack. Given narrowing performance margins allowed many organizations, managers are tempted to ""lock in"" organizational performance through elaborated rules and procedures, formal authority assignments, and clearly differentiated job responsibilities. A case study of one organization seeking very high reliability in its performance—a nuclear power plant—is offered to demonstrate a contrary point of view. Reliability, it is argued, can best be achieved not through attempts at organizational invariance but through the management of fluctuations in important organizational relationships and practices. This strategy enhances reliability while preserving the protective functions of organizational slack.",362.0
Hierarchical Learning in Stochastic Domains: Preliminary Results,f4c6240b68e97d6f3b9bc67a701f10e49a1b1dab,"[{'authorId': '1709512', 'name': 'L. Kaelbling'}]",1993.0,International Conference on Machine Learning,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,,235.0
Feudal Reinforcement Learning,1678bd32846b1aded5b1e80a617170812e80f562,"[{'authorId': '1790646', 'name': 'P. Dayan'}, {'authorId': '1695689', 'name': 'Geoffrey E. Hinton'}]",1992.0,NIPS,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"One way to speed up reinforcement learning is to enable learning to happen simultaneously at multiple resolutions in space and time. This paper shows how to create a Q-learning managerial hierarchy in which high level managers learn how to set tasks to their submanagers who, in turn, learn how to satisfy them. Submanagers need not initially understand their managers' commands. They simply learn to maximise their reinforcement in the context of the current command. 
 
We illustrate the system using a simple maze task. As the system learns how to get around, satisfying commands at the multiple levels, it explores more efficiently than standard, flat, Q-learning and builds a more comprehensive map.",708.0
How do we know we have global environmental problems? Science and the globalization of environmental discourse,30434661b9adddf3be3c50c11a704d0fa4f073db,"[{'authorId': '47213792', 'name': 'P. Taylor'}, {'authorId': '119169242', 'name': 'F. Buttel'}]",1992.0,,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,,216.0
"Human factors in large-scale technological systems' accidents: Three Mile Island, Bhopal, Chernobyl",bb3fc66713f2f9408032e121fec0de2a5e5b4a34,"[{'authorId': '3474735', 'name': 'N. Meshkati'}]",1991.0,,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"Many serious large-scale technological systems' accidents, having grave conse quences, such as those of Three Mile Island, Bhopal and Chernobyl, have pri marily been attributed to ""operator error."" However, further investigation has revealed that a large majority of these incidents are caused by a combination of many factors whose roots can be found in the lack of human factors (micro- and macro-ergonomics) considerations. Relevant human factor considera tions, the causes of human error, and commonalities of human factor problems in major disasters are briefly reviewed. We concluded that system accidents are caused by the way the (system) parts — engineered and human — fit together and interact. Also, on many occasions, the error and the resultant failures are both the attribute and effect of such factors as complicated operational pro cesses, ineffective training, non-responsive managerial systems, non-adaptive organizational designs, haphazard response systems, and sudden environmen tal disturbances, rather than being their cause. Recommendations for preven tion of such accidents are provided.",140.0
Some Characteristics of One Type of High Reliability Organization,1d92e67e5e81812b5ef2aa40dc4365676fa024aa,"[{'authorId': '2020299', 'name': 'K. Roberts'}]",1990.0,,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,This paper is concerned with defining organizational processes necessary to operate safely technologically complex organizations that can do great physical harm to themselves and their surrounding environments. The paper first argues that existing organizational research is little help in understanding organizational processes in such organizations. It then identifies nuclear powered aircraft carriers as examples of potentially hazardous organizations with histories of excellent operations. The paper then examines a set of components of “risk” identified by Perrow (1984) and antecedents to catastrophe elucidated by Shrivastava (1986) and discusses how carriers deal with these factors to lessen their potentially negative effects. The paper concludes with suggestions for future research.,860.0
Normal accidents. Living with high-risk technologies,61ca7d531b64b97f10ec6b9b38492ba69b5bf398,"[{'authorId': '145118952', 'name': 'J. E. Groves'}]",1990.0,,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,,2176.0
New challenges in organizational research: high reliability organizations,19f3f5855311529aac03148ab50ffc114e1fb52d,"[{'authorId': '2020299', 'name': 'K. Roberts'}]",1989.0,,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"This paper describes a multidisciplinary project concerned with the design and management of hazardous organizations that achieve extremely high lev els of reliable and safe operations. The organizations in which the research is being conducted are the Federal Aviation Administration's Air Traffic Control system, Pacific Gas and Electric Company's electrical distribution system, and two nuclear aircraft carriers of the United States Navy. The paper describes the research strategy and then presents some initial organizational paradoxes and findings. These are discussed in terms of describing the organizations, decision making, interdependence, the ""culture"" of high reliability and adaptation to technological change. The paper concludes with the admonition that managers in hazardous organizations should consider the cost of safeguards against or ganizational catastrophes versus the costs of catastrophe (in money, lives and public outcry).",197.0
Informal organizational networking as a crisis- avoidance strategy: US naval flight operations as a case study,4df7fab2bb18721ad97e9daf10a8054cbe69820f,"[{'authorId': '2862852', 'name': 'G. Rochlin'}]",1989.0,,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"The requirement for maintaining responsiveness and flexibility with a high degree of operational reliability and safety puts considerable demand on any organization. In the case of US Navy flight operations at sea, conditions of extraordinarily tight coupling and high technical complexity, flexible demand and uncertain environment provide more potential sources of crisis and acci dent than there are management personnel or permanent structures to cope with them. This paper discusses the Navy's evolved and relatively successful strategy of creating and maintaining a set of informal, evanescent, functional networks whose primary purpose is to anticipate and deflect emerging crises rather than merely react to them.",110.0
Bhopal: Anatomy of a Crisis,fa1fea9ae1205161ccee646ad206a098b3e5c72d,"[{'authorId': '3845217', 'name': 'C. Levenstein'}]",1988.0,,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,,241.0
Normal Accidents: Living with High-Risk Technologies,bd4dd7fe7f7b7191b7875a424c62de319fae3d0f,"[{'authorId': '48518440', 'name': 'D. L. Simms'}, {'authorId': '2789705', 'name': 'C. Perrow'}]",1986.0,,"['AI Research Considerations for Human Existential Safety (ARCHES)', 'X-Risk Analysis for AI Research']",2,"Book file PDF easily for everyone and every device. You can download and read online Normal Accidents: Living With High-Risk Technologies file PDF Book only if you are registered here. And also you can download or read online all Book PDF file that related with Normal Accidents: Living With High-Risk Technologies book. Happy reading Normal Accidents: Living With High-Risk Technologies Bookeveryone. Download file Free Book PDF Normal Accidents: Living With High-Risk Technologies at Complete PDF Library. This Book have some digital formats such us :paperbook, ebook, kindle, epub, fb2 and another formats. Here is The Complete PDF Book Library. It's free to register here to get Book file PDF Normal Accidents: Living With High-Risk Technologies.",3547.0
Cry Wolf: The Psychology of False Alarms,3d3fe849069e172b8038c7239a6adb70bb316f17,"[{'authorId': '95343653', 'name': 'S. Breznitz'}]",1984.0,,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"Read more and get great! That's what the book enPDFd cry wolf the psychology of false alarms will give for every reader to read this book. This is an on-line book provided in this website. Even this book becomes a choice of someone to read, many in the world also loves it so much. As what we talk, when you read more every page of this cry wolf the psychology of false alarms, what you will obtain is something great.",381.0
Incentives in Teams,7fda3757949022286c14e6a863ba6a4e151e8504,"[{'authorId': '145981061', 'name': 'Theodore Groves'}]",1973.0,,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"This paper analyzes the problem of inducing the members of an organization to behave as if they formed a team. Considered is a conglomerate-type organization consisting of a set of semi-autonomous subunits that are coordinated by the organization's head. The head's incentive problem is to choose a set of employee compensation rules that will induce his subunit managers to communicate accurate information and take optimal decisions. The main result exhibits a particular set of compensation rules, an optimal incentive structure, that leads to team behavior. Particular attention is directed to the informational aspects of the problem. An extended example of a resource allocation model is discussed and the optimal incentive structure is interpreted in terms of prices charged by the head for resources allocated to the subunits.",3436.0
When Is a Linear Control System Optimal,2c88aa885f1096f3b3a752efcd5d6b77cf454efb,"[{'authorId': '31211229', 'name': 'R. Kálmán'}]",1964.0,,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"The purpose of this paper is to formulate, study, and (in certain cases) resolve the Inverse Problem of Optimal Control Theory, which is the following: Given a control law, find all performance indices for which this control law is optimal. Under the assumptions of (a) linear constant plant, (b) linear constant control law, (c) measurable state variables, (d) quadratic loss functions with constant coefficients, (e) single control variable, we give a complete analysis of this problem and obtain various explicit conditions for the optimality of a given control law. An interestingfeature of the analysis is the central role of frequency-domain concepts, which have been ignored in optimal control theory until very recently. The discussion is presented in rigorous mathematical form. The central conclusion is the following (Theorem 6): A stable control law is optimal if and only if the absolute value of the corresponding return difference is at least equal to one at all frequencies. This provides a beautifully simple connecting link between modern control theory and the classical point of view which regards feedback as a means of reducing component variations.",1006.0
Some Moral and Technical Consequences of Automation.,8a403ab09db56252fb538dfd95509472661c6eb6,"[{'authorId': '145707626', 'name': 'N. Wiener'}]",1960.0,Science,"['AI Research Considerations for Human Existential Safety (ARCHES)', 'Responses to catastrophic AGI risk: a survey', 'Superintelligence: Paths, Dangers, Strategies']",3,,170.0
"Cardinal Welfare, Individualistic Ethics, and Interpersonal Comparisons of Utility",03bece2ebaee1b46305c5684705036bf483f4af3,"[{'authorId': '48948439', 'name': 'J. Harsanyi'}]",1955.0,Journal of Political Economy,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"The naive concept of social welfare as a sum of intuitively measurable and comparable individual cardinal utilities has been found unable to withstand the methodological criticism of the Pareto school. Professor Bergson2 has therefore recommended its replacement by the more general concept of. a social welfare function, defined as an arbitrary mathematical function of economic (and other social) variables, of a form freely chosen according to one’s personal ethical (or political) value judgments. Of course, in this terminology everybody will have a social welfare function of his own, different from that of everybody else except to the extent to which different individuals’ value judgments happen to coincide with one another. Actually, owing to the prevalence of individualistic value judgments in our society, it has been generally agreed that a social welfare function should be an increasing function of the utilities of individuals: if a certain situation, X, is preferred by an individual to another situation, y, and if none of the other individuals prefers Y to X, then X should be regarded as socially preferable to y. But no other restriction is to be imposed on the mathematical form of a social welfare function.",2637.0
Solution of a Problem of Leon Henkin,8256a00759ee1b2454425e4d05635159fed45ef8,"[{'authorId': '2055272907', 'name': 'M. H. Lob'}]",1955.0,Journal of Symbolic Logic (JSL),['AI Research Considerations for Human Existential Safety (ARCHES)'],1,"If Σ is any standard formal system adequate for recursive number theory, a formula (having a certain integer q as its Godel number) can be constructed which expresses the proposition that the formula with Godel number q is provable in Σ. Is this formula provable or independent in Σ? [2]. One approach to this problem is discussed by Kreisel in [4]. However, he still leaves open the question whether the formula ( Ex ) ( x, a ), with Godel-number a, is provable or not. Here ( x, y ) is the number-theoretic predicate which expresses the proposition that x is the number of a formal proof of the formula with Godel-number y . In this note we present a solution of the previous problem with respect to the system Z μ [3] pp. 289–294, and, more generally, with respect to any system whose set of theorems is closed under the rules of inference of the first order predicate calculus, and satisfies the subsequent five conditions, and in which the function ( k, l ) used below is definable. The notation and terminology is in the main that of [3] pp. 306–326, viz. if is a formula of Z μ containing no free variables, whose Godel number is a, then ({ }) stands for ( Ex ) ( x, a ) (read: the formula with Godel number a is provable in Z μ ); if is a formula of Z μ containing a free variable, y say, ({ }) stands for ( Ex ) ( x, g ( y )}, where g ( y ) is a recursive function such that for an arbitrary numeral the value of g ( ) is the Godel number of the formula obtained from by substituting for y in throughout. We shall, however, depart trivially from [3] in writing ( ), where is an arbitrary numeral, for ( Ex ) { x , ).",301.0
Theory of Games and Economic Behavior,0891be95a645bb2f76805b9c47cec51b81699bf8,"[{'authorId': '2006569846', 'name': 'E. Rowland'}]",1946.0,Nature,['AI Research Considerations for Human Existential Safety (ARCHES)'],1,,9098.0
The Singularity,89295c1d96b82197c966da7d4db4187e9af3139c,"[{'authorId': '2065135752', 'name': 'C. Jackson'}]",2016.0,,['Responses to catastrophic AGI risk: a survey'],1,,4.0
AAAI Presidential Panel on Long-Term AI Futures: Interim Report from the Panel Chairs,6fa6e55334c8cc987416dcca1f9c4647c69f8bfb,"[{'authorId': '2064595436', 'name': 'Eric Horvitz'}, {'authorId': '1744679', 'name': 'B. Selman'}]",2016.0,,['Responses to catastrophic AGI risk: a survey'],1,"The AAAI Presidential Panel on Long-Term AI Futures was convened by the president of the Association for the Advancement of Artificial Intelligence (AAAI) in 2008 to bring together a group of thoughtful computer scientists to explore and reflect about societal aspects of advances in machine intelligence (computational procedures for automated sensing, learning, reasoning, and decision making). The panelists are leading AI researchers, well known for their significant contributions to AI theory and practice. Although the final report of the panel has not yet been issued, we provide background and high-level summarization of several findings in this interim report.",2.0
How We're Predicting AI - or Failing to,5a1280f783e4ce6ba31b821f4d86f612ef733213,"[{'authorId': '2054678912', 'name': 'S. Armstrong'}, {'authorId': '2821562', 'name': 'Kaj Sotala'}]",2015.0,,"['Responses to catastrophic AGI risk: a survey', 'Superintelligence: Paths, Dangers, Strategies']",2,,55.0
The Singularity and the State of the Art in Artificial Intelligence: The technological singularity (Ubiquity symposium),b6aea0a8d1b770960bbd743d2f783f4529f49c21,"[{'authorId': '144883814', 'name': 'E. Davis'}]",2014.0,UBIQ,['Responses to catastrophic AGI risk: a survey'],1,"The state of the art in automating basic cognitive tasks, including vision and natural language understanding, is far below human abilities. Real-world reasoning, which is an unavoidable part of many advanced forms of computer vision and natural language understanding, is particularly difficult---suggesting the advent of computers with superhuman general intelligence is not imminent. The possibility of attaining a singularity by computers that lack these abilities is discussed briefly.",9.0
Bounding the impact of AGI,34773dd2966529814d83864d48bee830bf9996d0,"[{'authorId': '2979211', 'name': 'András Kornai'}]",2014.0,Journal of experimental and theoretical artificial intelligence (Print),['Responses to catastrophic AGI risk: a survey'],1,"Humans already have a certain level of autonomy, defined here as capability for voluntary purposive action, and a certain level of rationality, i.e. capability of reasoning about the consequences of their own actions and those of others. Under the prevailing concept of artificial general intelligences (AGIs), we envision artificial agents that have at least this high, and possibly considerably higher, levels of autonomy and rationality. We use the method of bounds to argue that AGIs meeting these criteria are subject to Gewirth's dialectical argument to the necessity of morality, compelling them to behave in a moral fashion, provided Gewirth's argument can be formally shown to be conclusive. The main practical obstacles to bounding AGIs by means of ethical rationalism are also discussed.",18.0
Universal empathy and ethical bias for artificial general intelligence,122e3e9ee114fb410c07734c0f154ec10a2a85aa,"[{'authorId': '145296409', 'name': 'A. Potapov'}, {'authorId': '143860556', 'name': 'S. Rodionov'}]",2013.0,Journal of experimental and theoretical artificial intelligence (Print),['Responses to catastrophic AGI risk: a survey'],1,"Rational agents are usually built to maximise rewards. However, artificial general intelligence (AGI) agents can find undesirable ways of maximising any prior reward function. Therefore, value learning is crucial for safe AGI. We assume that generalised states of the world are valuable – not rewards themselves, and propose an extension of AIXI, in which rewards are used only to bootstrap hierarchical value learning. The modified AIXI agent is considered in the multi-agent environment, where other agents can be either humans or other ‘mature’ agents, the values of which should be revealed and adopted by the ‘infant’ AGI agent. A general framework for designing such empathic agent with ethical bias is proposed as an extension of the universal intelligence model as well. Moreover, we perform experiments in the simple Markov environment, which demonstrate feasibility of our approach to value learning in safe AGI.",18.0
Is Brain Emulation Dangerous?,699f67b300c7d01022d615e7fc32fd30082ccd69,"[{'authorId': '2654106', 'name': 'P. Eckersley'}, {'authorId': '144816231', 'name': 'A. Sandberg'}]",2013.0,Journal of Artificial General Intelligence,['Responses to catastrophic AGI risk: a survey'],1,"Abstract Brain emulation is a hypothetical but extremely transformative technology which has a non-zero chance of appearing during the next century. This paper investigates whether such a technology would also have any predictable characteristics that give it a chance of being catastrophically dangerous, and whether there are any policy levers which might be used to make it safer. We conclude that the riskiness of brain emulation probably depends on the order of the preceding research trajectory. Broadly speaking, it appears safer for brain emulation to happen sooner, because slower CPUs would make the technology‘s impact more gradual. It may also be safer if brains are scanned before they are fully understood from a neuroscience perspective, thereby increasing the initial population of emulations, although this prediction is weaker and more scenario-dependent. The risks posed by brain emulation also seem strongly connected to questions about the balance of power between attackers and defenders in computer security contests. If economic property rights in CPU cycles1 are essentially enforceable, emulation appears to be comparatively safe; if CPU cycles are ultimately easy to steal, the appearance of brain emulation is more likely to be a destabilizing development for human geopolitics. Furthermore, if the computers used to run emulations can be kept secure, then it appears that making brain emulation technologies ―open‖ would make them safer. If, however, computer insecurity is deep and unavoidable, openness may actually be more dangerous. We point to some arguments that suggest the former may be true, tentatively implying that it would be good policy to work towards brain emulation using open scientific methodology and free/open source software codebases",9.0
On How to Build a Moral Machine,a45c7f4fa3804722f444b846ae6ab89d35af7331,"[{'authorId': '144093093', 'name': 'P. Bello'}, {'authorId': '1797985', 'name': 'S. Bringsjord'}]",2013.0,,['Responses to catastrophic AGI risk: a survey'],1,,34.0
"Shall We Vote on Values, But Bet on Beliefs?",d6cf6d1aec4930ade870af6929c0aadfacd26ed0,"[{'authorId': '145447707', 'name': 'R. Hanson'}]",2013.0,,['Responses to catastrophic AGI risk: a survey'],1,"A key question to ask about any social institution is how well it generates, aggregates, and distributes information. Speculative markets seem to do well at this, while familiar democratic institutions, relying in part on academic institutions, seem to fail in many ways. So perhaps we should consider “futarchy,” a form of government where betting markets become our primary common source on matters of fact. Democracy would say what we want, while speculators would say how to get it. That is, elected representatives would formally define and manage an after-the-fact measurement of national welfare, while market speculators would say which policies they expect to raise national welfare. If we are willing to recommend policies that macroeconomic data suggest are causally related to GDP, it seems we should be willing to consider futarchy. Using an qualitative engineering-style approach, this paper considers thirty design issues with futarchy, and then presents a relatively specific proposal which responds to those concerns.",53.0
Singularity Hypotheses: A Scientific and Philosophical Assessment,09843092ceaa229d657ecc8f629b54f1e57aa0c5,"[{'authorId': '2445597', 'name': 'A. Eden'}, {'authorId': '31925555', 'name': 'J. Moor'}, {'authorId': '152181934', 'name': 'J. Søraker'}, {'authorId': '143925399', 'name': 'E. Steinhart'}]",2013.0,,['Responses to catastrophic AGI risk: a survey'],1,"Singularity Hypotheses: A Scientific and Philosophical Assessment offers authoritative, jargon-free essays and critical commentaries on accelerating technological progress and the notion of technological singularity. It focuses on conjectures about the intelligence explosion, transhumanism, and whole brain emulation. Recent years have seen a plethora of forecasts about the profound, disruptive impact that is likely to result from further progress in these areas. Many commentators however doubt the scientific rigor of these forecasts, rejecting them as speculative and unfounded. We therefore invited prominent computer scientists, physicists, philosophers, biologists, economists and other thinkers to assess the singularity hypotheses. Their contributions go beyond speculation, providing deep insights into the main issues and a balanced picture of the debate.",119.0
Towards Machine Ethics.,13b4042ac0c52af3b195b7168faff8491aa8f926,"[{'authorId': '2065947455', 'name': 'Oliver Bendel'}]",2013.0,,['Responses to catastrophic AGI risk: a survey'],1,,39.0
An Overview of Models of Technological Singularity,867ec09d9dd31f03cc699e252bb5cf5f74e4f700,"[{'authorId': '144816231', 'name': 'A. Sandberg'}]",2013.0,,"['Responses to catastrophic AGI risk: a survey', 'Superintelligence: Paths, Dangers, Strategies']",2,"This paper reviews different definitions and models of technological singularity. The models range from conceptual sketches to detailled endogenous growth models, as well as attempts to fit empirical data to quantitative models. Such models are useful for examining the dynamics of the world-system and possible types of future crisis points where fundamental transitions are likely to occur. Current models suggest that, generically, even small increasing returns tends to produce radical growth. If mental capital becomes copyable (such as would be the case for AI or brain emulation) extremely rapid growth would also become likely.",53.0
Consciousness and Ethics,085d9c6d8edcd2760caffc0472811ad295217ff6,"[{'authorId': '1403105449', 'name': 'Daniel Meyer-Dinkgräfe'}]",2013.0,,['Responses to catastrophic AGI risk: a survey'],1,,3.0
Which Consequentialism ? Machine Ethics and Moral Divergence,88a25e1b37faeb6eadb876b7d5d06f78f6c42b79,"[{'authorId': '3389522', 'name': 'Carl Shulman'}, {'authorId': '2064597154', 'name': 'Henrik Jonsson'}, {'authorId': '97822663', 'name': 'Nick Tarleton'}]",2013.0,,"['Responses to catastrophic AGI risk: a survey', 'Superintelligence: Paths, Dangers, Strategies']",2,"Some researchers in the field of machine ethics have suggested consequentialist or utilitarian theories as organizing principles for Artificial Moral Agents (AMAs) (Wallach, Allen, and Smit 2008) that are ‘full ethical agents’ (Moor 2006), while acknowledging extensive variation among these theories as a serious challenge (Wallach, Allen, and Smit 2008). This paper develops that challenge, beginning with a partial taxonomy of consequentialisms proposed by philosophical ethics. We discuss numerous ‘free variables’ of consequentialism where intuitions conflict about optimal values, and then consider special problems of human-level AMAs designed to implement a particular ethical theory, by comparison to human proponents of the same explicit principles. In conclusion, we suggest that if machine ethics is to fully succeed, it must draw upon the developing field of moral psychology. Shulman, Carl, Nick Tarleton, and Henrik Jonsson. 2009. “Which Consequentialism? Machine Ethics and Moral Divergence.” In AP-CAP 2009: The Fifth Asia-Pacific Computing and Philosophy Conference, October 1st-2nd, University of Tokyo, Japan, Proceedings, edited by Carson Reynolds and Alvaro Cassinelli, 23–25. AP-CAP 2009. http://ia-cap.org/ap-cap09/proceedings.pdf. This version contains minor changes. Carl Shulman, Henrik Jonsson, Nick Tarleton 1. Free Variables of Consequentialism Suppose that the recommendations of a broadly utilitarian view depend on decisions about ten free binary variables, where we assign a probability of 80% to our favored option for each variable; in this case, if our probabilities are well-calibrated and our errors are not correlated across variables, then we will have only slightly more than a 10% chance of selecting the correct (in some meta-ethical framework) specification. As the number of variables increases, our options grow more plentiful, and as our uncertainty about each item grows, the likelihood of success continues to decline. Unfortunately, the philosophical literature shows many highly contested dimensions along which consequentialisms vary. Even if we confine ourselves to broadly utilitarian views that consider only some combination of pain, pleasure, and preference satisfaction, we still face a vast bestiary of views. Hedonistic utilitarianism requires an account of experiential states such as pleasure and pain, whose ontological status is disputed (Chalmers 1996; Place 1956; Searle 2007; Putnam 1960), with each construal bearing implications about what systems deserve moral consideration. Utilitarians disagree about the relative value of preventing suffering and promoting happiness, with views that strongly favor the former, such as negative utilitarianism (Smart 1958), suggesting that the painless extermination of all life capable of suffering would be desirable, while the opposite camp prizes galactic colonization (Bostrom 2003). Preference utilitarians face other troubles. McCarthy (1979) attributes beliefs about temperature to a thermostat, which might imply that it has ‘preferences’ over the temperature as well. If simple definitions fail to map onto our ‘emphatic preferences’ (Binmore 2009; Harsanyi 1977), we will require more complex and error-prone ones. There is also dispute about whether to value the actual satisfaction of preferences or only the experience of such satisfaction, a distinction highlighted by Nozick’s ‘experience machine’ scenario (Nozick 1974). All utilitarianisms founder on interpersonal comparison of utility, which is not defined in standard expected utility theory (Binmore 2009; Elster and Roemer 1991). The selection of any comparison/aggregation rule introduces yet more free variables and problems: e.g., bounded utility functions can be compared by scaling them into the same range, but the result is strongly dominated by the relative strength of people’s strongest preferences (Hausman 1995). Aggregation might involve the sum, average, or some more complicated function of individuals’ well-being, with potentially counterintuitive results, such as the Repugnant Conclusion (Parfit 1986), attaching to many proposals. The offsetting Person-Affecting Restriction carries its own surprising implications, such as extreme indifference to the far future (Parfit 1986; Glover 1977).",14.0
Decision Support for Safe AI Design,c40cad8cacaa47aac6d4cab8f80749beaebd690c,"[{'authorId': '39109261', 'name': 'B. Hibbard'}]",2012.0,Artificial General Intelligence,['Responses to catastrophic AGI risk: a survey'],1,,4.0
Motivation Management in AGI Systems,11fd8561487a6618d9a59efcf7433bb3eba936c2,"[{'authorId': '48319476', 'name': 'Pei Wang'}]",2012.0,Artificial General Intelligence,['Responses to catastrophic AGI risk: a survey'],1,,13.0
Avoiding Unintended AI Behaviors,55af0b67ded5bb63db9cfd67ba28edf23cc53d04,"[{'authorId': '39109261', 'name': 'B. Hibbard'}]",2012.0,Artificial General Intelligence,['Responses to catastrophic AGI risk: a survey'],1,,29.0
Framing robot arms control,6b24625e613f160f454237e2cb6a4968033d2373,"[{'authorId': '143836888', 'name': 'Wendell Wallach'}, {'authorId': '41193089', 'name': 'Colin Allen'}]",2012.0,Ethics and Information Technology,['Responses to catastrophic AGI risk: a survey'],1,,0.0
Human Extinction and Farsighted Universal Surveillance,2c1ff48e6186436c31162314700515c811307dc6,"[{'authorId': '145498164', 'name': 'M. Walker'}]",2012.0,International Journal of Technoethics,['Responses to catastrophic AGI risk: a survey'],1,"This paper bridges the dilemma created by intrusive surveillance technologies needed to safeguard people's security, and the potential negative consequences such technologies might have on individual privacy. The author begins with a brief review of the increasing threat to human life posed by emerging technologies, e.g., genetic engineering and nanotechnology. Next, they canvass a potential technological means to mitigate some of this threat, namely, ubiquitous microscopic sensors. The author then notes that a consequence of the deployment of such technology appears to be an erosion of personal privacy on a scale hitherto unimaginable. It is then argued that many details of an individual's private life are actually irrelevant for security purposes, and that it may be possible to develop technology to mask these details in the data gleaned from surveillance devices. Such a development could meet some, perhaps many, of the concerns about privacy. It is also argued that if it is possible to use technology to mask personal information this may actually promote the goal of security, since it is conjectured that the public is likely to be more willing to accept such invasive technology if it is designed to mask such details. Finally, some applications to Society's current uses of surveillance technology are drawn.",3.0
Wavelet analysis in neurodynamics,b851ee2be66616429e93b3645cc6b76cac1840cf,"[{'authorId': '2172660119', 'name': 'A. N. Pavlov'}, {'authorId': '1576915224', 'name': 'A. Hramov'}, {'authorId': '30766420', 'name': 'A. Koronovskii'}, {'authorId': '2064970045', 'name': 'E. Sitnikova'}, {'authorId': '35209840', 'name': 'V. A. Makarov'}, {'authorId': '119628873', 'name': 'A. Ovchinnikov'}]",2012.0,,['Responses to catastrophic AGI risk: a survey'],1,"Results obtained using continuous and discrete wavelet transforms as applied to problems in neurodynamics are reviewed, with the emphasis on the potential of wavelet analysis for decoding signal information from neural systems and networks. The following areas of application are considered: (1) the microscopic dynamics of single cells and intracellular processes, (2) sensory data processing, (3) the group dynamics of neuronal ensembles, and (4) the macrodynamics of rhythmical brain activity (using multichannel EEG recordings). The detection and classification of various oscillatory patterns of brain electrical activity and the development of continuous wavelet-based brain activity monitoring systems are also discussed as possibilities.",205.0
The Machine Question,61607580047d4b44ff8d63ee24972dce62284e91,"[{'authorId': '2249898', 'name': 'David J. Gunkel'}]",2012.0,,['Responses to catastrophic AGI risk: a survey'],1,,183.0
"The Machine Question: Critical Perspectives on AI, Robots, and Ethics",5284fa434c4f806b168c4eb3355ad463c2804ceb,"[{'authorId': '2249898', 'name': 'David J. Gunkel'}]",2012.0,,['Responses to catastrophic AGI risk: a survey'],1,"One of the enduring concerns of moral philosophy is deciding who or what is deserving of ethical consideration. Much recent attention has been devoted to the ""animal question""--consideration of the moral status of nonhuman animals. In this book, David Gunkel takes up the ""machine question"": whether and to what extent intelligent and autonomous machines of our own making can be considered to have legitimate moral responsibilities and any legitimate claim to moral consideration. The machine question poses a fundamental challenge to moral thinking, questioning the traditional philosophical conceptualization of technology as a tool or instrument to be used by human agents. Gunkel begins by addressing the question of machine moral agency: whether a machine might be considered a legitimate moral agent that could be held responsible for decisions and actions. He then approaches the machine question from the other side, considering whether a machine might be a moral patient due legitimate moral consideration. Finally, Gunkel considers some recent innovations in moral philosophy and critical theory that complicate the machine question, deconstructing the binary agent--patient opposition itself. Technological advances may prompt us to wonder if the science fiction of computers and robots whose actions affect their human companions (think of HAL in 2001: A Space Odyssey) could become science fact. Gunkel's argument promises to influence future considerations of ethics, ourselves, and the other entities who inhabit this world.",215.0
"Advantages of artificial intelligences, uploads, and digital minds",3f02fea9020a07dd7fa14ef413f690fb7a280fc9,"[{'authorId': '2821562', 'name': 'Kaj Sotala'}]",2012.0,,['Responses to catastrophic AGI risk: a survey'],1,"I survey four categories of factors that might give a digital mind, such as an upload or an artificial general intelligence, an advantage over humans. Hardware advantages include greater serial speeds and greater parallel speeds. Self-improvement advantages include improvement of algorithms, design of new mental modules, and modification of motivational system. Co-operative advantages include copyability, perfect co-operation, improved communication, and transfer of skills. Human handicaps include computational limitations and faulty heuristics, human-centric biases, and socially motivated cognition. The shape of hardware growth curves, as well as the ease of modifying minds, are found to have a major impact on how quickly a digital mind may take advantage of these factors.",28.0
"WHY UPLOADING WILL NOT WORK, OR, THE GHOSTS HAUNTING TRANSHUMANISM",f546adaf6dc39f1cd0cd5beba39a28994c1756eb,"[{'authorId': '89285627', 'name': 'P. Hopkins'}]",2012.0,,['Responses to catastrophic AGI risk: a survey'],1,"Transhumanists tend to have a commitment to materialism and naturalism but nonetheless pursue goals traditionally associated with religious ideologies, such as the quest for immortality. Some hope to achieve immortality through the application of a technology whereby the brain is scanned and the person ""uploaded"" to a computer. This process is typically described as ""transferring"" one's mind to a computer. I argue that, while the technology may be feasible, uploading will not succeed because it in fact does not ""transfer"" a mind at all and will not preserve personal identity. Transhumanist hopes for such transfer ironically rely on treating the mind dualistically — and inconsistently with materialism — as the functional equivalent of a soul, as is evidenced by a carefully examination of the language used to describe and defend uploading. In this sense, transhumanist thought unwittingly contains remnants of dualistic and religious concepts.",17.0
DIGITAL IMMORTALITY: SELF OR 0010110?,900fabc2b5e7805447ee19eba004a127710ce965,"[{'authorId': '1691207', 'name': 'L. Swan'}, {'authorId': '117625641', 'name': 'J. Howard'}]",2012.0,,['Responses to catastrophic AGI risk: a survey'],1,"In this paper, we explore from several angles the possibility, and practicality, of one of the major tenets of the transhumanist movement — the intention to upload human minds to computers. The first part of the paper assumes that mind-uploading is possible and will become quite commonplace in the near (21st century) future a la Ray Kurzweil and cohorts. This assumption allows us to explore several of its problematic implications for personal identity, especially the effects it will have on questions of duty, responsibility, interpersonal relationships, and culpability in the case of crime. In the second part of the paper, we take a deeper and more critical look at whether mind-uploading is indeed metaphysically possible, and offer some neurobiologically-inspired arguments against its feasibility.",13.0
"MY BRAIN, MY MIND, AND I: SOME PHILOSOPHICAL ASSUMPTIONS OF MIND-UPLOADING",2226964c1cae5af3b2e797613799f0fd2decc994,"[{'authorId': '5353514', 'name': 'M. Hauskeller'}]",2012.0,,['Responses to catastrophic AGI risk: a survey'],1,"The progressing cyborgization of the human body reaches its completion point when the entire body can be replaced by uploading individual minds to a less vulnerable and limited substrate, thus achieving ""digital immortality"" for the uploaded self. The paper questions the philosophical assumptions that are being made when mind-uploading is thought a realistic possibility. I will argue that we have little reason to suppose that an exact functional copy of the brain will actually produce similar phenomenological effects (if any at all), and even less reason to believe that the uploaded mind, even if similar, will be the same self as the one on whose brain it was modeled.",27.0
ELECTRON IMAGING TECHNOLOGY FOR WHOLE BRAIN NEURAL CIRCUIT MAPPING,9982f58c15daa7ac7cd8d092194a503dc3979b10,"[{'authorId': '2914957', 'name': 'K. Hayworth'}]",2012.0,,['Responses to catastrophic AGI risk: a survey'],1,"The goal of uploading a human mind into a computer is far beyond today's technology. But exactly how far? Here I review our best cognitive and neuroscience model of the mind and show that it is well suited to provide a framework to answer this question. The model suggests that our unique ""software"" is mainly digital in nature and is stored redundantly in the brain's synaptic connectivity matrix (i.e., our Connectome) in a way that should allow a copy to be successfully simulated. I review the resolution necessary for extracting this Connectome and conclude that today's FIBSEM technique already meets this requirement. I then sketch out a process capable of reducing a chemically-fixed, plastic-embedded brain into a set of tapes containing 20 × 20 micron tissue pillars optimally sized for automated FIBSEM imaging, and show how these tapes could be distributed among a large number of imaging machines to accomplish the task of extracting a Connectome. The scale of such an endeavor makes it impractical, but a v...",28.0
EXPERIMENTAL RESEARCH IN WHOLE BRAIN EMULATION: THE NEED FOR INNOVATIVE IN VIVO MEASUREMENT TECHNIQUES,df605cc99c3354676120a7367924e9d8ef6bbf8d,"[{'authorId': '16196821', 'name': 'R. Koene'}]",2012.0,,['Responses to catastrophic AGI risk: a survey'],1,"Whole brain emulation aims to re-implement functions of a mind in another computational substrate by carefully emulating the function of fundamental components, and by copying the connectivity between those components. The precision with which this is done must enable prediction of the natural development of active states. To accomplish this, in vivo measurements at large scale and high resolution are critically important. We propose a set of requirements for these empirical measurements. We then outline general methods leading to acquisition of a structural and functional connectome, and to the characterization of responses at large scale and high resolution. Finally, we describe two new project developments that tackle the problem of functional recording in vivo, namely the ""molecular ticker-tape"" and the integrated-circuit ""Cyborcell"".",13.0
A FRAMEWORK FOR APPROACHES TO TRANSFER OF A MIND'S SUBSTRATE,36cd238588dee2b7fa36ef2c3a0a80447b30b96a,"[{'authorId': '2408120', 'name': 'S. Bamford'}]",2012.0,,['Responses to catastrophic AGI risk: a survey'],1,"I outline some recent developments in the field of neural prosthesis concerning functional replacement of brain parts. Noting that functional replacement of brain parts could conceivably lead to a form of ""mind-substrate transfer"" (defined herein), I briefly review other proposed approaches to mind-substrate transfer then I propose a framework in which to place these approaches, classifying them along two axes: top-down versus bottom-up, and on-line versus off-line; I outline a further hypothetical approach suggested by this framework. I argue that underlying technological questions about mind-substrate transfer, there is a fundamental question which concerns our beliefs about continuity of identity.",9.0
COALESCING MINDS: BRAIN UPLOADING-RELATED GROUP MIND SCENARIOS,638c7f86fb4b82ff3ca43e9eee6ba3755b72653d,"[{'authorId': '2821562', 'name': 'Kaj Sotala'}, {'authorId': '2132516', 'name': 'H. Valpola'}]",2012.0,,['Responses to catastrophic AGI risk: a survey'],1,"We present a hypothetical process of mind coalescence, where artificial connections are created between two or more brains. This might simply allow for an improved form of communication. At the other extreme, it might merge the minds into one in a process that can be thought of as a reverse split-brain operation. We propose that one way mind coalescence might happen is via an exocortex, a prosthetic extension of the biological brain which integrates with the brain as seamlessly as parts of the biological brain integrate with each other. An exocortex may also prove to be the easiest route for mind uploading, as a person's personality gradually moves away from the aging biological brain onto the exocortex. Memories might also be copied and shared even without minds being permanently merged. Over time, the borders of personal identity may become loose or even unnecessary.",16.0
WHEN SHOULD TWO MINDS BE CONSIDERED VERSIONS OF ONE ANOTHER,ccfb0b844be4d306f164320636bed8653b699ec1,"[{'authorId': '1738080', 'name': 'B. Goertzel'}]",2012.0,,['Responses to catastrophic AGI risk: a survey'],1,"What does it mean for one mind to be a different version of another one, or a natural continuation of another one? Or put differently: when can two minds sensibly be considered versions of one another? This question occurs in relation to mind uploading, where one wants to be able to assess whether an approximate upload constitutes a genuine continuation of the uploaded mind or not. It also occurs in the context of the rapid mental growth that is likely to follow mind uploading, at least in some cases — here the question is, when is growth so rapid or discontinuous as to cause the new state of the mind to no longer be sensibly considerable as a continuation of the previous one? Provisional answers to these questions are sketched, using mathematical tools drawn from category theory and probability theory. It is argued that if a mind's growth is ""approximately smooth"", in a certain sense, then there will be ""continuity of self"" and the mind will have a rough comprehension of its growth and change process as it occurs. The treatment is somewhat abstract, and intended to point a direction for ongoing research rather than as a definitive practical solution. These ideas may have practical value in future, however, for those whose values favor neither strict self-preservation nor unrestricted growth, but rather growth that is constrained to be at least quasi-comprehensible to the minds doing the growing.",7.0
On the Feasibility of Internet-Scale Author Identification,68930f06c3444d00731fd01b6eb5f5c0b9cc6f1f,"[{'authorId': '47735253', 'name': 'Arvind Narayanan'}, {'authorId': '2624689', 'name': 'Hristo S. Paskov'}, {'authorId': '144516687', 'name': 'N. Gong'}, {'authorId': '144679540', 'name': 'J. Bethencourt'}, {'authorId': '143623342', 'name': 'Emil Stefanov'}, {'authorId': '2802006', 'name': 'E. C. Shin'}, {'authorId': '143711382', 'name': 'D. Song'}]",2012.0,IEEE Symposium on Security and Privacy,['Responses to catastrophic AGI risk: a survey'],1,"We study techniques for identifying an anonymous author via linguistic stylometry, i.e., comparing the writing style against a corpus of texts of known authorship. We experimentally demonstrate the effectiveness of our techniques with as many as 100,000 candidate authors. Given the increasing availability of writing samples online, our result has serious implications for anonymity and free speech - an anonymous blogger or whistleblower may be unmasked unless they take steps to obfuscate their writing style. While there is a huge body of literature on authorship recognition based on writing style, almost none of it has studied corpora of more than a few hundred authors. The problem becomes qualitatively different at a large scale, as we show, and techniques from prior work fail to scale, both in terms of accuracy and performance. We study a variety of classifiers, both ""lazy"" and ""eager,"" and show how to handle the huge number of classes. We also develop novel techniques for confidence estimation of classifier outputs. Finally, we demonstrate stylometric authorship recognition on texts written in different contexts. In over 20% of cases, our classifiers can correctly identify an anonymous author given a corpus of texts from 100,000 authors; in about 35% of cases the correct author is one of the top 20 guesses. If we allow the classifier the option of not making a guess, via confidence estimation we are able to increase the precision of the top guess from 20% to over 80% with only a halving of recall.",306.0
Mapping the Landscape of Human-Level Artificial General Intelligence,74de903e1ea9450682fc5c149be64c916cc0fb9a,"[{'authorId': '153876154', 'name': 'S. S. Adams'}, {'authorId': '1804314', 'name': 'I. Arel'}, {'authorId': '145606164', 'name': 'Joscha Bach'}, {'authorId': '144126587', 'name': 'R. Coop'}, {'authorId': '2062884867', 'name': 'Rod Furlan'}, {'authorId': '1738080', 'name': 'B. Goertzel'}, {'authorId': '2107226704', 'name': 'J. S. Hall'}, {'authorId': '1691880', 'name': 'A. Samsonovich'}, {'authorId': '1793014', 'name': 'Matthias Scheutz'}, {'authorId': '37777082', 'name': 'M. Schlesinger'}, {'authorId': '143739156', 'name': 'S. Shapiro'}, {'authorId': '34796922', 'name': 'J. Sowa'}]",2012.0,The AI Magazine,['Responses to catastrophic AGI risk: a survey'],1,"We present the broad outlines of a roadmap toward human-level artificial general intelligence (henceforth, AGI). We begin by discussing AGI in general, adopting a pragmatic goal for its attainment and a necessary foundation of characteristics and requirements. An initial capability landscape will be presented, drawing on major themes from developmental psychology and illuminated by mathematical, physiological and information processing perspectives. The challenge of identifying appropriate tasks and environments for measuring AGI will be addressed, and seven scenarios will be presented as milestones suggesting a roadmap across the AGI landscape along with directions for future research and collaboration.",130.0
The Future of Human Evolution,b8fef990510a8e641caff69abd4f6e9377ff2623,"[{'authorId': '49640866', 'name': 'Russell Powell'}]",2012.0,British Journal for the Philosophy of Science,['Responses to catastrophic AGI risk: a survey'],1,"There is a tendency in both scientific and humanistic disciplines to think of biological evolution in humans as significantly impeded if not completely overwhelmed by the robust cultural and technological capabilities of the species. The aim of this article is to make sense of and evaluate this claim. In Section 2, I flesh out the argument that humans are ‘insulated’ from ordinary evolutionary mechanisms in terms of our contemporary biological understandings of phenotypic plasticity, niche construction, and cultural transmission. In Section 3, I consider two obvious objections to the above argument based on the growing literatures related to gene-culture coevolution and recent positive selection on the human genome, as well as a pair of less common objections relating to the connection between plasticity, population size and evolvability. In Section 4, I argue that both the ‘human evolutionary stasis argument’ and its various detractor theories are premised on a fundamental conceptual flaw: they take evolutionary stasis for granted, since they fail to conceive of stabilizing selection as a type of evolution and drift as a universal tendency that dominates in the absence of selection. Without the continued operation of natural selection, the very properties that are purported to reduce the evolutionary response to selection in humans would themselves drift into non-functionality. I conclude that properly conceived, biological evolution is a permanent and ineradicable fixture of any species, including Homo sapiens. 1 Preamble: Received Wisdom or Straw Man? 2 Making Sense of the Human Evolutionary Stasis Argument   2.1 What is evolution?   2.2 Evolutionary buffers   2.3 Phenotypic plasticity, niche construction, and cultural transmission   2.4 Examples of evolutionary buffering in humans 3 Initial Objections to the Human Evolutionary Stasis Argument   3.1 Gene–Culture Coevolution   3.2 Recent positive selection in humans   3.3 Plasticity and evolvability   3.4 Population size, variation, and evolution 4 The Future of Human Evolution   4.1 Shifting evolutionary gestalts   4.2 The Evolutionary Catch-22   4.3 Stabilizing selection is ubiquitous   4.4 Humans continue to evolve 1 Preamble: Received Wisdom or Straw Man? 2 Making Sense of the Human Evolutionary Stasis Argument   2.1 What is evolution?   2.2 Evolutionary buffers   2.3 Phenotypic plasticity, niche construction, and cultural transmission   2.4 Examples of evolutionary buffering in humans   2.1 What is evolution?   2.2 Evolutionary buffers   2.3 Phenotypic plasticity, niche construction, and cultural transmission   2.4 Examples of evolutionary buffering in humans 3 Initial Objections to the Human Evolutionary Stasis Argument   3.1 Gene–Culture Coevolution   3.2 Recent positive selection in humans   3.3 Plasticity and evolvability   3.4 Population size, variation, and evolution   3.1 Gene–Culture Coevolution   3.2 Recent positive selection in humans   3.3 Plasticity and evolvability   3.4 Population size, variation, and evolution 4 The Future of Human Evolution   4.1 Shifting evolutionary gestalts   4.2 The Evolutionary Catch-22   4.3 Stabilizing selection is ubiquitous   4.4 Humans continue to evolve   4.1 Shifting evolutionary gestalts   4.2 The Evolutionary Catch-22   4.3 Stabilizing selection is ubiquitous   4.4 Humans continue to evolve",33.0
Nine Ways to Bias Open-Source AGI Toward Friendliness,1fa3412d0bafa150294e6705f5a8524197488513,"[{'authorId': '1738080', 'name': 'B. Goertzel'}, {'authorId': '122929084', 'name': 'Joel Pitt'}, {'authorId': '118434109', 'name': 'Novamente Llc'}]",2012.0,,['Responses to catastrophic AGI risk: a survey'],1,"While it seems unlikely that any method of guaranteeing human-friendliness (“Friendliness”) on the part of advanced Artificial General Intelligence (AGI) systems will be possible, this doesn’t mean the only alternatives are throttling AGI development to safeguard humanity, or plunging recklessly into the complete unknown. Without denying the presence of a certain irreducible uncertainty in such matters, it is still sensible to explore ways of biasing the odds in a favorable way, such that newly created AI systems are significantly more likely than not to be Friendly. Several potential methods of effecting such biasing are explored here, with a particular but nonexclusive focus on those that are relevant to open-source AGI projects, and with illustrative examples drawn from the OpenCog open-source AGI project. Issues regarding the relative safety of open versus closed approaches to AGI are discussed and then nine techniques for biasing AGIs in favor of Friendliness are presented: 1. Engineer the capability to acquire integrated ethical knowledge. 2. Provide rich ethical interaction and instruction, respecting developmental stages. 3. Develop stable, hierarchical goal systems. 4. Ensure that the early stages of recursive self-improvement occur relatively slowly and with rich human involvement. 5. Tightly link AGI with the Global Brain. 6. Foster deep, consensus-building interactions between divergent viewpoints. 7. Create a mutually supportive community of AGIs. 8. Encourage measured co-advancement of AGI software and AGI ethics theory 9. Develop advanced AGI sooner not later. In conclusion, and related to the final point, we advise the serious co-evolution of functional AGI systems and AGI-related ethical theory as soon as possible, before we have so much technical infrastructure that parties relatively unconcerned with ethics are able to rush ahead with brute force approaches to AGI development.",31.0
Moral Machines: Contradiction in Terms or Abdication of Human Responsibility?,4b5fa7a7c6e55d77858f0dd8f80ce48c600013d3,"[{'authorId': '145058661', 'name': 'Patrick Lin'}, {'authorId': '20821855', 'name': 'Keith Abney'}, {'authorId': '6778024', 'name': 'G. Bekey'}]",2012.0,,['Responses to catastrophic AGI risk: a survey'],1,"This chapter contains sections titled: 4.1 Toward Artifi cial Moral Agents, 4.2 Philosophers, Engineers, and the Design of Artifi cial Moral Agents, 4.3 Early Research on the Development of AMAs, and Future Challenges, 4.4 Challenges, Objections, and Criticisms, 4.5 Conclusion, References",52.0
Embracing Competitive Balance: The Case for Substrate-Independent Minds and Whole Brain Emulation,8599250cf75031219dafa723c5e37d79284212c6,"[{'authorId': '16196821', 'name': 'R. Koene'}]",2012.0,,['Responses to catastrophic AGI risk: a survey'],1,,12.0
The Singularity and Machine Ethics,63cd604886aa453ce626626d97959e378ccdc788,"[{'authorId': '2138143', 'name': 'Luke Muehlhauser'}, {'authorId': '102699296', 'name': 'Louie Helm'}]",2012.0,,"['Responses to catastrophic AGI risk: a survey', 'Superintelligence: Paths, Dangers, Strategies']",2,,49.0
Models of value and choice.,cf546fe2db56e83e88e0613f49fea3ad2c365cab,"[{'authorId': '1790646', 'name': 'P. Dayan'}]",2012.0,,['Responses to catastrophic AGI risk: a survey'],1,,12.0
Belief in The Singularity is Fideistic,0378e5b3602d3dcc0e43275e399deea528b722a9,"[{'authorId': '1797985', 'name': 'S. Bringsjord'}, {'authorId': '2157194', 'name': 'Alexander Bringsjord'}, {'authorId': '144093093', 'name': 'P. Bello'}]",2012.0,,['Responses to catastrophic AGI risk: a survey'],1,,19.0
Rational Artificial Intelligence for the Greater Good,e3c775129c9a8633b1e331f9193acc69f2f48fac,"[{'authorId': '1808760', 'name': 'S. Omohundro'}]",2012.0,,"['Responses to catastrophic AGI risk: a survey', 'Superintelligence: Paths, Dangers, Strategies']",2,,27.0
Leakproofing the Singularity Artificial Intelligence Confinement Problem,9a37a40b8d525f079dbec2dfd748cefa859a9d33,"[{'authorId': '1976753', 'name': 'Roman V Yampolskiy'}]",2012.0,,['Responses to catastrophic AGI risk: a survey'],1,"This paper attempts to formalize and to address the 'leakproofing' of the Singularity problem presented by David Chalmers. The paper begins with the definition of the Artificial Intelli- gence Confinement Problem. After analysis of existing solutions and their shortcomings, a protocol is proposed aimed at making a more secure confinement environment which might delay potential negative effect from the technological singularity while allowing humanity to benefit from the superintelligence.",73.0
Challenges for Brain Emulation: Why is Building a Brain so Difficult?,4cd4c16dadc811d20ce3a4f78ec55fcca30fb329,"[{'authorId': '47996506', 'name': 'R. Cattell'}, {'authorId': '1780629', 'name': 'A. C. Parker'}]",2012.0,,['Responses to catastrophic AGI risk: a survey'],1,"In recent years, half a dozen major research groups have simulated or constructed sizeable networks of artificial neurons, with the ultimate goal to emulate the entire human brain. At this point, these projects are a long way from that goal: they typically simulate thousands of mammalian neurons, versus tens of billions in the human cortex, with less dense connectivity as well as less-complex neurons. While the outputs of the simulations demonstrate some features of biological neural networks, it is not clear how exact the artificial neurons and networks need to be to invoke system behavior identical to biological networks and it is not even clear how to prove that artificial neural network behavior is identical in any way to biological behavior. However, enough progress has been made to draw some conclusions and make comparisons between the leading projects. Some approaches are more scalable, some are more practical with current technologies, and some are more accurate in their emulation of biological neurons. In this paper, we examine the pros and cons of each approach and make some predictions about the future of artificial neural networks and the prospects for whole brain emulation.",24.0
Losing Humanity : The Case Against Killer Robots,4da46bbe75a9ee6f3c339b11ea2f34f50ea69b69,"[{'authorId': '69513517', 'name': 'Bonnie Docherty'}, {'authorId': '1400169558', 'name': 'Erik Neunschwander'}, {'authorId': '1483625054', 'name': 'M. Karir'}, {'authorId': '69475707', 'name': 'Kate Flinner'}]",2012.0,,['Responses to catastrophic AGI risk: a survey'],1,"Human Rights Watch is dedicated to protecting the human rights of people around the world. We stand with victims and activists to prevent discrimination, to uphold political freedom, to protect people from inhumane conduct in wartime, and to bring offenders to justice. We investigate and expose human rights violations and hold abusers accountable. We challenge governments and those who hold power to end abusive practices and respect international human rights law. We enlist the public and the international community to support the cause of human rights for all. Human Rights Watch is an international organization with staff in more than 40 countries, The International Human Rights Clinic (IHRC) at Harvard Law School seeks to protect and promote human rights and international humanitarian law through documentation; legal, factual, and strategic analysis; litigation before national, regional, and international bodies; treaty negotiations; and policy and advocacy initiatives. IHRC also critically examines the human rights movement and engages in innovative clinical education to develop advanced practice techniques and approaches to human rights advocacy. IHRC collaborates with leading international and local human rights organizations and bridges theory with practice at the law school while also advancing the interests of clients and affected communities around the world.",143.0
The beginning of infinity : explanations that transform the world,d8b21f75c3bfad8223513562ac2970b28135a5ab,"[{'authorId': '34627186', 'name': 'David Deutsch'}]",2012.0,,['Responses to catastrophic AGI risk: a survey'],1,"In our search for truth, how far have we advanced? This uniquely human quest for good explanations has driven amazing improvements in everything from scientific understanding and technology to politics, moral values and human welfare. But will progress end, either in catastrophe or completion - or will it continue indefinitely? In this profound and seminal book, David Deutsch explores the furthest reaches of our current understanding, taking in the Infinity Hotel, supernovae and the nature of optimism, to instill in all of us a wonder at what we have achieved - and the fact that this is only the beginning of humanity's infinite possibility.",84.0
Why an Intelligence Explosion is Probable,11647d134846af59bdfee8c3bfa9d2880356a5bb,"[{'authorId': '145872502', 'name': 'R. Loosemore'}, {'authorId': '1738080', 'name': 'B. Goertzel'}]",2012.0,,['Responses to catastrophic AGI risk: a survey'],1,,27.0
Intelligence Explosion: Evidence and Import,40e3086dd279fd23042b37962d8e67f5fd9801ae,"[{'authorId': '2138143', 'name': 'Luke Muehlhauser'}, {'authorId': '144958976', 'name': 'A. Salamon'}]",2012.0,,"['Responses to catastrophic AGI risk: a survey', 'Superintelligence: Paths, Dangers, Strategies']",2,,66.0
Emerging Technologies and the Future of Philosophy,0213e72158045af7df5ea44d10405aa8285211a3,"[{'authorId': '70782169', 'name': 'Philippe Verdoux'}]",2011.0,,['Responses to catastrophic AGI risk: a survey'],1,"This article examines how a class of emerging technologies—specifically, radical cognitive enhancements and artificial intelligence—has the potential to influence the future of philosophy. The article argues that progress in philosophy has been impeded, in part, by two specific constraints imposed on us by the natural architecture of our cognitive systems. Both of these constraints, though, could in principle be overcome by certain cognitive technologies currently being researched and/or developed. It surveys a number of these technologies, and then looks at a particular metaphilosophical stance (called “inflationism”) that advocates amplifying the abilities of philosophers rather than reducing the ambitions of philosophy, given the apparent “teleological gap” between philosophy's ultimate goal (i.e., “the truth”) and the limited capacities of our evolved mental machinery.",8.0
Bubble Trouble: Off-Line De-Anonymization of Bubble Forms,7e9c2005c145a70b6ab88906a3bb20a71a125bf3,"[{'authorId': '2808959', 'name': 'Joseph A. Calandrino'}, {'authorId': '39259472', 'name': 'W. Clarkson'}, {'authorId': '1752733', 'name': 'E. Felten'}]",2011.0,USENIX Security Symposium,['Responses to catastrophic AGI risk: a survey'],1,"Fill-in-the-bubble forms are widely used for surveys, election ballots, and standardized tests. In these and other scenarios, use of the forms comes with an implicit assumption that individuals' bubble markings themselves are not identifying. This work challenges this assumption, demonstrating that fill-in-the-bubble forms could convey a respondent's identity even in the absence of explicit identifying information. We develop methods to capture the unique features of a marked bubble and use machine learning to isolate characteristics indicative of its creator. Using surveys from more than ninety individuals, we apply these techniques and successfully reidentify individuals from markings alone with over 50% accuracy. This bubble-based analysis can have either positive or negative implications depending on the application. Potential applications range from detection of cheating on standardized tests to attacks on the secrecy of election ballots. To protect against negative consequences, we discuss mitigation techniques to remove a bubble's identifying characteristics. We suggest additional tests using longitudinal data and larger datasets to further explore the potential of our approach in realworld applications.",9.0
"Rational Universal Benevolence: Simpler, Safer, and Wiser Than ""Friendly AI""",9d9b1fc936a8cfdc32f8a9ea8e6256f7f3902835,"[{'authorId': '1945432', 'name': 'Mark R. Waser'}]",2011.0,Artificial General Intelligence,['Responses to catastrophic AGI risk: a survey'],1,,15.0
The LIDA Framework as a General Tool for AGI,3cf694fc7fa097e03758d18ec67240cb58a11a1d,"[{'authorId': '2998320', 'name': 'Javier Snaider'}, {'authorId': '37140191', 'name': 'R. McCall'}, {'authorId': '145796793', 'name': 'S. Franklin'}]",2011.0,Artificial General Intelligence,['Responses to catastrophic AGI risk: a survey'],1,,82.0
Learning What to Value,f5ca00ff8650d04f36b2e228226032b0a5c478df,"[{'authorId': '40829712', 'name': 'Dan Dewey'}]",2011.0,Artificial General Intelligence,"['Responses to catastrophic AGI risk: a survey', 'Superintelligence: Paths, Dangers, Strategies']",2,,103.0
A Family of Gödel Machine Implementations,93f1faacf7d83e7eff5302de5984841af4e606c5,"[{'authorId': '1714059', 'name': 'Bas R. Steunebrink'}, {'authorId': '145341374', 'name': 'J. Schmidhuber'}]",2011.0,Artificial General Intelligence,['Responses to catastrophic AGI risk: a survey'],1,,17.0
Provable Security in the Real World,93bfec5b4b45d10ebe27dc67952a307ae02079b0,"[{'authorId': '2369537', 'name': 'Jean Paul Degabriele'}, {'authorId': '1726285', 'name': 'K. Paterson'}, {'authorId': '2459661', 'name': 'Gaven J. Watson'}]",2011.0,IEEE Security and Privacy,['Responses to catastrophic AGI risk: a survey'],1,"Provable security plays an important role in the design and analysis of systems using cryptography. However, protocols can be vulnerable to attacks outside the scope of the existing formal analyses.",42.0
Incremental Machine Ethics,7e628e7e16f3ad663fcb81bfed18850f9cba0332,"[{'authorId': '2278631', 'name': 'Thomas M. Powers'}]",2011.0,IEEE robotics & automation magazine,['Responses to catastrophic AGI risk: a survey'],1,"Approaches to programming ethical behavior for computer systems face challenges that are both technical and philosophical in nature. In response, an incrementalist account of machine ethics is developed: a successive adaptation of programmed constraints to new, morally relevant abilities in computers. This approach allows progress under conditions of limited knowledge in both ethics and computer systems engineering and suggests reasons that we can circumvent broader philosophical questions about computer intelligence and autonomy.",24.0
The Beginning of Infinity,78beab58dc7edbc1a78e2e501888b08acf8a4739,"[{'authorId': '34627186', 'name': 'David Deutsch'}]",2011.0,,['Responses to catastrophic AGI risk: a survey'],1,,97.0
Breaking the Rules to Rise to Power,497161665179d1bd90d28f0ffb08f93a3d816775,"[{'authorId': '5980688', 'name': 'Gerben A. van Kleef'}, {'authorId': '4392679', 'name': 'A. Homan'}, {'authorId': '1953465', 'name': 'C. Finkenauer'}, {'authorId': '6078374', 'name': 'Seval Gündemir'}, {'authorId': '48888502', 'name': 'E. Stamkou'}]",2011.0,,['Responses to catastrophic AGI risk: a survey'],1,"Powerful people often act at will, even if the resulting behavior is inappropriate—hence the famous proverb “power corrupts.” Here, we introduce the reverse phenomenon—violating norms signals power. Violating a norm implies that one has the power to act according to one’s own volition in spite of situational constraints, which fuels perceptions of power. Four studies support this hypothesis. Individuals who took coffee from another person’s can (Study 1), violated rules of bookkeeping (Study 2), dropped cigarette ashes on the floor (Study 3), or put their feet on the table (Study 4) were perceived as more powerful than individuals who did not show such behaviors. The effect was mediated by inferences of volitional capacity, and it replicated across different methods (scenario, film clip, face-to-face interaction), different norm violations, and different indices of power (explicit measures, expected emotions, and approach/inhibition tendencies). Implications for power, morality, and social hierarchy are discussed.",102.0
Machine Ethics: Ethics for Self-Improving Machines,448e6b49e28bbd0464c67bb5e6ce844c86ccd02f,"[{'authorId': '2107226704', 'name': 'J. S. Hall'}]",2011.0,,['Responses to catastrophic AGI risk: a survey'],1,,10.0
Machine Ethics: Asimov's Laws of Robotics,25810dea78608911ed72f763780dc8a7020ffa9f,"[{'authorId': '121702055', 'name': 'R. Clarke'}]",2011.0,,['Responses to catastrophic AGI risk: a survey'],1,,11.0
Machine Ethics: The Unacceptability of Asimov's Three Laws of Robotics as a Basis for Machine Ethics,a7922fa172d0a64ef7c4f604c38f37bdbb0147ee,"[{'authorId': '2120952', 'name': 'S. Anderson'}]",2011.0,,['Responses to catastrophic AGI risk: a survey'],1,,20.0
Machine Ethics: There Is No “I” in “Robot”,638206491a3385886e443e28cea4d9e369f230bd,"[{'authorId': '48903738', 'name': 'Christopher Grau'}]",2011.0,,['Responses to catastrophic AGI risk: a survey'],1,,5.0
Mitigating potential hazards to humans from the development of intelligent machines,8bd8e52d0da897aca3b33149dba0a46aad2432ab,"[{'authorId': '144224461', 'name': 'W. Daley'}]",2011.0,,['Responses to catastrophic AGI risk: a survey'],1,"Artifi cially intelligent machines and robots grow ever more present and capable. The evolution of intelligent machines may have consequences that are diffi cult to limit or stop. It is important to examine possible problems now in order to set standards and safeguards to guide the intelligent machine evolution. I suggest that national and international government panels on artifi cial intelligence issues, similar to the Presidential Commission for the Study of Bioethical Issues, will be necessary to direct the formulation of guidelines that establish such standards for the creation and uses of artifi cially intelligent machines.",5.0
Machine Ethics: Computational Models of Ethical Reasoning,1acbd3fb85e1983f6b378d07069247d702ca35a3,"[{'authorId': '1706869', 'name': 'B. McLaren'}]",2011.0,,['Responses to catastrophic AGI risk: a survey'],1,,7.0
Artificial Intelligence Safety Engineering: Why Machine Ethics Is a Wrong Approach,8ff573f5223b45037fadb4d6b590992b59b15a72,"[{'authorId': '1976753', 'name': 'Roman V Yampolskiy'}]",2011.0,Conference on Philosophy and Theory of Artificial Intelligence,['Responses to catastrophic AGI risk: a survey'],1,,111.0
"Race against the machine : how the digital revolution is accelerating innovation, driving productivity, and irreversibly transforming employment and the economy",03a7a46c0ac38f1409a85c60e4395dabfef35f57,"[{'authorId': '2841157', 'name': 'E. Brynjolfsson'}, {'authorId': '3132693', 'name': 'Andrew P. McAfee'}]",2011.0,,['Responses to catastrophic AGI risk: a survey'],1,,1182.0
How Long Until Human-Level AI ? Results from an Expert Assessment,d3cd08f5577f55b8ad28a9d66acf44d55283c3d2,"[{'authorId': '2100011042', 'name': 'Philosopher William Hurlbut'}, {'authorId': '3110190', 'name': 'R. Penrose'}, {'authorId': '2805061', 'name': 'S. Hameroff'}]",2011.0,,"['Responses to catastrophic AGI risk: a survey', 'Superintelligence: Paths, Dangers, Strategies']",2,"The development of human-level AI has been a core goal of the AI field since its inception, though at present it occupies only a fraction of the field’s efforts. To help understand the viability of this goal, this article presents an assessment of expert opinions regarding humanlevel AI research conducted at AGI-09, a conference for this AI specialty. We found that various experts strongly disagree with each other on certain matters, such as timing and ordering of key milestones. However, we did find that most experts expect human-level AI to be reached within upcoming decades, and all experts give at least some chance that some milestones will be reached within this time. Furthermore, a majority of experts surveyed favor an integrative approach to human-level AI rather than an approach centered on a particular technique. Finally, experts are skeptical about the impact of massive research funding, especially if it is concentrated in relatively few approaches. These results suggest that the possibility of achieving human-level AI in the near term should be given serious consideration.",65.0
Complex Value Systems are Required to Realize Valuable Futures,ddae6774e9969865f47a67c55fb887c4aa94802f,"[{'authorId': '2542795', 'name': 'Eliezer Yudkowsky'}]",2011.0,,"['Responses to catastrophic AGI risk: a survey', 'Superintelligence: Paths, Dangers, Strategies']",2,"AcommonreactiontofirstencounteringtheproblemstatementofFriendlyAI(“Ensure that the creation of a generally intelligent, self-improving, eventually superintelligent system realizes a positive outcome”) is to propose a single moral value which allegedly suffices; or to reject the problem by replying that “constraining” our creations is undesirable or unnecessary. This paper makes the case that a criterion for describing a “positive outcome,” despite the shortness of the English phrase, contains considerable complexity hidden from us by our own thought processes, which only search positive-value parts of the action space, and implicitly think as if code is interpreted by an anthropomorphic ghost-in-the-machine. Abandoning inheritance from human value (at least as a basis for renormalizing to reflective equilibria) will yield futures worthless even from the standpoint of AGI researchers who consider themselves to have cosmopolitan values not tied to the exact forms or desires of humanity.",25.0
Personal Identity and Uploading,189e77435e05e5e492d81d34ee3e42d0e5992c75,"[{'authorId': '145498164', 'name': 'M. Walker'}, {'authorId': '97900612', 'name': 'Rich Hedden'}]",2011.0,,['Responses to catastrophic AGI risk: a survey'],1,"Objections to uploading may be parsed into substrate issues, dealing with the computer platform of upload and personal identity. This paper argues that the personal identity issues of uploading are no more or less challenging than those of bodily transfer often discussed in the philosophical literature. It is argued that what is important in personal identity involves both token and type identity. While uploading does not preserve token identity, it does save type identity; and even qua token, one may have good reason to think that the preservation of the type is worth the cost. 1. Uploading: prospects and perils You arrive at one of the thousands of kiosks run by the late twenty-first century’s largest corporation: UUpload. With some trepidation you step into the superscanner. There is a slight hum as it inventories the molecular building blocks of your brain. Your brain is destroyed in the process, but you are not dead – or so the marketing materials from U-Upload claim. For information about the building blocks, along with a general program that describes the fundamental laws of molecular interaction, is uploaded to the shiny new robotic brain you purchased (Sandberg and Bostrom 2008). For your friends and family, a few terrifying moments pass before the robotic body stirs. To their relief, your first words are: “It’s me. I made it.” You then go on to crack a joke – just as your family and friends have come to expect of you. Of course you have changed in some respects: gone is your human carbon-based body. Now you experience the world through camera eyes and microphone ears, you dance the fandango with robotic legs and speak",12.0
Ray Kurzweil and Uploading: Just Say No!,65f93c591bb2cc41db3e05a1f75f8ce013e38e87,"[{'authorId': '143674338', 'name': 'N. Agar'}]",2011.0,,['Responses to catastrophic AGI risk: a survey'],1,"There is a debate about the possibility of mind-uploading – a process that purportedly transfers human minds and therefore human identities into computers. This paper bypasses the debate about the metaphysics of mind-uploading to address the rationality of submitting yourself to it. I argue that an ineliminable risk that mind-uploading will fail makes it prudentially irrational for humans to undergo it. For Ray Kurzweil, artificial intelligence (AI) is not just about making artificial things intelligent; it’s also about making humans artificially super-intelligent. 1 In his version of our future we enhance our mental powers by means of increasingly powerful electronic neuroprostheses. The recognition that any function performed by neurons and synapses can be done better by electronic chips will lead to an ongoing conversion of biological brain into machine mind. We will upload . Once the transfer of our identities into machines is complete, we will be free to follow the trajectory of accelerating improvement currently tracked by wireless Internet routers and portable DVD players. We will quickly become millions and billions of times more intelligent than we currently are. This paper challenges Kurzweil’s predictions about the destiny of the human mind. I argue that it is unlikely ever to be rational for human beings to completely upload their minds onto computers – a fact that is almost certain to be understood by those presented with the option of doing so. Although we’re likely to find it desirable to replace peripheral parts of our minds – parts dedicated to the processing of visual information, for example – we’ll want to stop well before going all the way. A justified fear of uploading will make it irrational to accept offers to replace the parts of our brains responsible for thought processes that we consider essential to our conscious experience, even if the replacements manifestly outperform neurons. This rational biological conservatism will set limits on how intelligent we can become.",8.0
Robot minds and human ethics: the need for a comprehensive model of moral decision making,4bb55687009b44c7c3883a78a694add290aa7596,"[{'authorId': '143836888', 'name': 'Wendell Wallach'}]",2010.0,Ethics and Information Technology,['Responses to catastrophic AGI risk: a survey'],1,,80.0
Implications and consequences of robots with biological brains,ede24f08bcc048105544f91282a5fcdde17d0bb7,"[{'authorId': '143636026', 'name': 'K. Warwick'}]",2010.0,Ethics and Information Technology,['Responses to catastrophic AGI risk: a survey'],1,,53.0
Risk Mysterianism and Cognitive Boosters,e8834af411583d530c9f717d060554c748a468e6,"[{'authorId': '70782169', 'name': 'Philippe Verdoux'}]",2010.0,,['Responses to catastrophic AGI risk: a survey'],1,"In this paper, I argue that cognitive enhancements could increase our ability to analyze and control risks, most importantly existential risks. Towards this end, I identify two significant constraints on our ability to evaluate/neutralize such risks, one of which concerns the problem's type and the other its size (or complexity). I provide two examples of these constraints, and examine how enhancements could help overcome each.",11.0
A Conceptual and Computational Model of Moral Decision Making in Human and Artificial Agents,41ddbef98e774c980f8eeab5e9efd25123f21ede,"[{'authorId': '143836888', 'name': 'Wendell Wallach'}, {'authorId': '145796793', 'name': 'S. Franklin'}, {'authorId': '30308222', 'name': 'C. Allen'}]",2010.0,Topics in Cognitive Science,['Responses to catastrophic AGI risk: a survey'],1,"Recently, there has been a resurgence of interest in general, comprehensive models of human cognition. Such models aim to explain higher-order cognitive faculties, such as deliberation and planning. Given a computational representation, the validity of these models can be tested in computer simulations such as software agents or embodied robots. The push to implement computational models of this kind has created the field of artificial general intelligence (AGI). Moral decision making is arguably one of the most challenging tasks for computational approaches to higher-order cognition. The need for increasingly autonomous artificial agents to factor moral considerations into their choices and actions has given rise to another new field of inquiry variously known as Machine Morality, Machine Ethics, Roboethics, or Friendly AI. In this study, we discuss how LIDA, an AGI model of human cognition, can be adapted to model both affective and rational features of moral decision making. Using the LIDA model, we will demonstrate how moral decisions can be made in many domains using the same mechanisms that enable general decision making. Comprehensive models of human cognition typically aim for compatibility with recent research in the cognitive and neural sciences. Global workspace theory, proposed by the neuropsychologist Bernard Baars (1988), is a highly regarded model of human cognition that is currently being computationally instantiated in several software implementations. LIDA (Franklin, Baars, Ramamurthy, & Ventura, 2005) is one such computational implementation. LIDA is both a set of computational tools and an underlying model of human cognition, which provides mechanisms that are capable of explaining how an agent's selection of its next action arises from bottom-up collection of sensory data and top-down processes for making sense of its current situation. We will describe how the LIDA model helps integrate emotions into the human decision-making process, and we will elucidate a process whereby an agent can work through an ethical problem to reach a solution that takes account of ethically relevant factors.",114.0
Construal-level theory of psychological distance.,c44b9d00e2cea7ff3860b07e316c7542460eadb3,"[{'authorId': '5169679', 'name': 'Y. Trope'}, {'authorId': '2364428', 'name': 'N. Liberman'}]",2010.0,Psychology Review,['Responses to catastrophic AGI risk: a survey'],1,"People are capable of thinking about the future, the past, remote locations, another person's perspective, and counterfactual alternatives. Without denying the uniqueness of each process, it is proposed that they constitute different forms of traversing psychological distance. Psychological distance is egocentric: Its reference point is the self in the here and now, and the different ways in which an object might be removed from that point-in time, in space, in social distance, and in hypotheticality-constitute different distance dimensions. Transcending the self in the here and now entails mental construal, and the farther removed an object is from direct experience, the higher (more abstract) the level of construal of that object. Supporting this analysis, research shows (a) that the various distances are cognitively related to each other, (b) that they similarly influence and are influenced by level of mental construal, and (c) that they similarly affect prediction, preference, and action.",4199.0
Robots should be slaves,5b9f4b2a2e28a74669df3789f6701aaed58a43d5,"[{'authorId': '145315445', 'name': 'J. Bryson'}]",2010.0,,['Responses to catastrophic AGI risk: a survey'],1,"Robots should not be described as persons, nor given legal nor moral responsibility for their actions. Robots are fully owned by us. We determine their goals and behavior, either directly or indirectly through specifying their intelligence or how their intelligence is acquired. In humanising them, we not only further dehumanise real people, but also encourage poor human decision making in the allocation of resources and responsibility. This is true at both the individual and the institutional level. This chapter describes both causes and consequences of these errors, including consequences already present in society. I make specific proposals for best incorporating robots into our society. The potential of robotics should be understood as the potential to extend our own abilities and to address our own goals.",226.0
The Criminal Liability of Artificial Intelligence Entities,5aaf04d60bc925d0d5edf6502cdbe8c6abdf7f83,"[{'authorId': '70524445', 'name': 'Gabriel Hallevy'}]",2010.0,,['Responses to catastrophic AGI risk: a survey'],1,"In 1981, a 37-year-old Japanese employee of a motorcycle factory was killed by an artificial-intelligence robot working near him. The robot erroneously identified the employee as a threat to its mission, and calculated that the most efficient way to eliminate this threat was by pushing him into an adjacent operating machine. Using its very powerful hydraulic arm, the robot smashed the surprised worker into the operating machine, killing him instantly, and then resumed its duties with no one to interfere with its mission. Unfortunately, this is not science fiction, and the legal question is: Who is to be held liable for this killing?",23.0
Treatment of child and adolescent psychopathy: Focusing on change.,5b3cfd3c306d21349008cc7a0279c283ae854820,"[{'authorId': '4993145', 'name': 'R. Salekin'}]",2010.0,,['Responses to catastrophic AGI risk: a survey'],1,,58.0
Cochlear implants and spoken language processing abilities: review and assessment of the literature.,f960a0e2aba8187594adcaaaede8b3406d7fc18b,"[{'authorId': '21696811', 'name': 'Nathaniel R Peterson'}, {'authorId': '1859196', 'name': 'D. Pisoni'}, {'authorId': '1690839', 'name': 'R. Miyamoto'}]",2010.0,Restorative Neurology and Neuroscience,['Responses to catastrophic AGI risk: a survey'],1,"Cochlear implants (CIs) process sounds electronically and then transmit electric stimulation to the cochlea of individuals with sensorineural deafness, restoring some sensation of auditory perception. Many congenitally deaf CI recipients achieve a high degree of accuracy in speech perception and develop near-normal language skills. Post-lingually deafened implant recipients often regain the ability to understand and use spoken language with or without the aid of visual input (i.e. lip reading). However, there is wide variation in individual outcomes following cochlear implantation, and some CI recipients never develop useable speech and oral language skills. The causes of this enormous variation in outcomes are only partly understood at the present time. The variables most strongly associated with language outcomes are age at implantation and mode of communication in rehabilitation. Thus, some of the more important factors determining success of cochlear implantation are broadly related to neural plasticity that appears to be transiently present in deaf individuals. In this article we review the expected outcomes of cochlear implantation, potential predictors of those outcomes, the basic science regarding critical and sensitive periods, and several new research directions in the field of cochlear implantation.",275.0
Coherent Extrapolated Volition: A Meta-Level Approach to Machine Ethics,27c6354c30ed41c98234581b536bf96504eb9e22,"[{'authorId': '97822663', 'name': 'Nick Tarleton'}, {'authorId': '113511659', 'name': 'Miri Visiting Fellow'}]",2010.0,,"['Responses to catastrophic AGI risk: a survey', 'Superintelligence: Paths, Dangers, Strategies']",2,"The field of machine ethics seeks methods to ensure that future intelligent machines will act in ways beneficial to human beings. Machine ethics is relevant to a wide range of possible artificial agents, but becomes especially difficult and especially important when the agents in question have at least human-level intelligence. This paper describes a solution, originally proposed by Yudkowsky (2004), to the problem of what goals to give suchagents: ratherthanattempttoexplicitlyprograminanyspecificnormativetheory(a project which would face numerous philosophical and immediate ethical difficulties), we should implement a system to discover what goals we would, upon reflection, want such agents to have. We discuss the motivations for and details of this approach, comparing it to other suggested methods for creating “artificial moral agents” (Wallach, Allen, and Smit 2008), and describe underspecified and uncertain areas for further research.",28.0
A Safe Ethical System for Intelligent Machines,a299bb5a5d2a32e75cb1aa8cdc900da565f0bd91,"[{'authorId': '1945432', 'name': 'Mark R. Waser'}]",2009.0,Biologically Inspired Cognitive Architectures,['Responses to catastrophic AGI risk: a survey'],1,"As machines become more intelligent and take on more responsibilities, their decision-making capabilities must be informed and constrained by a coherent, integrated moral/ethical structure with no internal inconsistencies for everyone’s safety and well-being. Unfortunately, no such structure is currently agreed upon to exist. We propose to solve this problem by a) drawing upon experimental evidence and lessons learned from evolution and economics to show that morality is actually objective and derivable from first principles; b) presenting a coherent, integrated, platonic ethical system with no internal inconsistencies that flows naturally from a single high-level logically-derived Kantian imperative to low-level reflexive ""rules of thumb"" that match current human sensibilities; and c) suggesting a biologically-inspired architecture which supports and enforces this system which can be relatively easily implemented.",5.0
The Ethical Treatment of Artificially Conscious Robots,20efb46a4b69223f7383bfc9923c53bfa2b449b1,"[{'authorId': '2052679457', 'name': 'David Levy'}]",2009.0,Int. J. Soc. Robotics,['Responses to catastrophic AGI risk: a survey'],1,,97.0
Beyond Asimov: The Three Laws of Responsible Robotics,38edbfa3f8fc6612b315bf2c526d90dba6fbf5cb,"[{'authorId': '1789429', 'name': 'R. Murphy'}, {'authorId': '1928242', 'name': 'D. Woods'}]",2009.0,IEEE Intelligent Systems,['Responses to catastrophic AGI risk: a survey'],1,"Asimov's three laws of robotics have been inculcated so successfully into our culture that they now appear to shape expectations as to how robots should act around humans. However, there has been little serious discussion as to whether the laws really do provide a framework for human-robot interactions. Asimov actually used his laws as a literary device to explore the lack of resilience in the interplay between people and robots in a range of situations. This paper briefly reviews some of the practical shortcomings of each of Asimov's laws for framing the relationships between people and robots, including reminders about what robots can't do. The main focus of the paper is to propose an alternative, parallel set of laws of responsible robotics as a means to stimulate debate about the accountability relationships for robots when their actions can result in harm to people or human interests. The alternative laws emphasize (1) systems safety in terms of the responsibilities of those who develop and deploy robotic systems, (2) robots' responsiveness as they participate in dynamic social and cognitive relationships, and (3) smooth transfer of control as a robot encounters and initially responds to disruptions, impasses, or opportunities in context.",109.0
Governing Lethal Behavior in Autonomous Robots,3fe463058d93455918ad164db38aecfc5562ce9b,"[{'authorId': '1706062', 'name': 'R. Arkin'}]",2009.0,,['Responses to catastrophic AGI risk: a survey'],1,"Drawing from the authors own state-of-the-art research, this book examines the philosophical basis, motivation, theory, and design recommendations for the implementation of an ethical control and reasoning system in autonomous robot systems, taking into account the Laws of War and Rules of Engagement. It discusses how robots can ultimately be more humane than humans in the battlefield. The author addresses the issue of autonomous robots having the potential to make life-or-death decisions and provides examples that illustrate autonomous systems ethical use of force. He also includes the opinions of the public, researchers, policymakers, and military personnel on the use of lethality by autonomous systems.",404.0
On the Anonymity of Home/Work Location Pairs,d79ed41122871e18760166a91771b8e33651ef0c,"[{'authorId': '2779068', 'name': 'P. Golle'}, {'authorId': '145271096', 'name': 'K. Partridge'}]",2009.0,International Conference on Pervasive Computing,['Responses to catastrophic AGI risk: a survey'],1,,411.0
Toward the Human–Robot Co-Existence Society: On Safety Intelligence for Next Generation Robots,8c82c367b4fa042d61974f999cd68208499c0e97,"[{'authorId': '38236052', 'name': 'Y. Weng'}, {'authorId': '48240538', 'name': 'Chien-Hsun Chen'}, {'authorId': '2204083349', 'name': 'Chuen-Tsai Sun'}]",2009.0,Int. J. Soc. Robotics,['Responses to catastrophic AGI risk: a survey'],1,,77.0
De-anonymizing Social Networks,b2791b09c928a62eeac272ec30b8faa7e9f3d35d,"[{'authorId': '47735253', 'name': 'Arvind Narayanan'}, {'authorId': '1723945', 'name': 'Vitaly Shmatikov'}]",2009.0,IEEE Symposium on Security and Privacy,['Responses to catastrophic AGI risk: a survey'],1,"Operators of online social networks are increasingly sharing potentially sensitive information about users and their relationships with advertisers, application developers, and data-mining researchers. Privacy is typically protected by anonymization, i.e., removing names, addresses, etc.We present a framework for analyzing privacy and anonymity in social networks and develop a new re-identification algorithm targeting anonymized social-network graphs. To demonstrate its effectiveness on real-world networks, we show that a third of the users who can be verified to have accounts on both Twitter, a popular microblogging service, and Flickr, an online photo-sharing site, can be re-identified in the anonymous Twitter graph with only a 12% error rate.Our de-anonymization algorithm is based purely on the network topology, does not require creation of a large number of dummy ""sybil"" nodes, is robust to noise and all existing defenses, and works even when the overlap between the target network and the adversary's auxiliary information is small.",1339.0
Ultimate Cognition à la Gödel,7641209099374eef110af835a7b80b31d02777c9,"[{'authorId': '145341374', 'name': 'J. Schmidhuber'}]",2009.0,Cognitive Computation,['Responses to catastrophic AGI risk: a survey'],1,,83.0
"DESIRES, VALUES, REASONS, AND THE DUALISM OF PRACTICAL. REASON",11296036e81b2def093a8f27db1eb2738ea3bad0,"[{'authorId': '2116645812', 'name': 'Michael Smith'}]",2009.0,,['Responses to catastrophic AGI risk: a survey'],1,In On What Matters Derek Parfit argues that facts about reasons for action are grounded in facts about values and against the view that they are grounded in facts about the desires that subjects would have after fully informed and rational deliberation. I describe and evaluate Parfit's arguments for this value-based conception of reasons for action and find them wanting. I also assess his response to Sidgwick's suggestion that there is a Dualism of Practical Reason. Parfit seems not to notice that his preferred value-based conception of reasons for action augurs strongly in favour of a view like Sidgwick's.1,16.0
Machine Ethics and Superintelligence,87bc5cd8b89947467a2a535a5ad3526c229d5361,"[{'authorId': '3389522', 'name': 'Carl Shulman'}, {'authorId': '2064597154', 'name': 'Henrik Jonsson'}, {'authorId': '97822663', 'name': 'Nick Tarleton'}, {'authorId': '113199528', 'name': 'Miri Visiting Fellows'}]",2009.0,,['Responses to catastrophic AGI risk: a survey'],1,"The developing academic field of machine ethics seeks to make artificial agents safer as they become more pervasive throughout society. Motivated by planned next-generation robotic systems, machine ethics typically explores solutions for agents with autonomous capacities intermediate between those of current artificial agents and humans, with designs developed incrementally by and embedded in a society of human agents. These assumptions substantially simplify the problem of designing a desirable agent and reflect the near-term future well, but there are also cases in which they do not hold. In particular, they need not apply to artificial agents with human-level or greater capabilities. The potentially very large impacts of such agents suggest that advance analysis and research is valuable. We describe some of the additional challenges such scenarios pose for machine ethics.",12.0
"Power, Distress, and Compassion",0497c1aadecace0218d3ab8bf7a6656cf1ee1504,"[{'authorId': '5980688', 'name': 'Gerben A. van Kleef'}, {'authorId': '4926366', 'name': 'C. Oveis'}, {'authorId': '115933214', 'name': 'Ilmo van der Löwe'}, {'authorId': '114141056', 'name': 'Aleksandr LuoKogan'}, {'authorId': '39766499', 'name': 'Jennifer L Goetz'}, {'authorId': '3990536', 'name': 'D. Keltner'}]",2008.0,Psychology Science,['Responses to catastrophic AGI risk: a survey'],1,"Responses to individuals who suffer are a foundation of cooperative communities. On the basis of the approach/inhibition theory of power (Keltner, Gruenfeld, & Anderson, 2003), we hypothesized that elevated social power is associated with diminished reciprocal emotional responses to another person's suffering (feeling distress at another person's distress) and with diminished complementary emotion (e.g., compassion). In face-to-face conversations, participants disclosed experiences that had caused them suffering. As predicted, participants with a higher sense of power experienced less distress and less compassion and exhibited greater autonomic emotion regulation when confronted with another participant's suffering. Additional analyses revealed that these findings could not be attributed to power-related differences in baseline emotion or decoding accuracy, but were likely shaped by power-related differences in the motivation to affiliate. Implications for theorizing about power and the social functions of emotions are discussed.",327.0
Moral Machines: Teaching Robots Right from Wrong,9413d8f7e2474ff178fbf5d0a7cfd4828c3ec500,"[{'authorId': '143836888', 'name': 'Wendell Wallach'}, {'authorId': '30308222', 'name': 'C. Allen'}]",2008.0,,['Responses to catastrophic AGI risk: a survey'],1,"Computers are already approving financial transactions, controlling electrical supplies, and driving trains. Soon, service robots will be taking care of the elderly in their homes, and military robots will have their own targeting and firing protocols. Colin Allen and Wendell Wallach argue that as robots take on more and more responsibility, they must be programmed with moral decision-making abilities, for our own safety. Taking a fast paced tour through the latest thinking about philosophical ethics and artificial intelligence, the authors argue that even if full moral agency for machines is a long way off, it is already necessary to start building a kind of functional morality, in which artificial moral agents have some basic ethical sensitivity. But the standard ethical theories don't seem adequate, and more socially engaged and engaging robots will be needed. As the authors show, the quest to build machines that are capable of telling right from wrong has begun. Moral Machines is the first book to examine the challenge of building artificial moral agents, probing deeply into the nature of human decision making and ethics.",739.0
The perils of cognitive enhancement and the urgent imperative to enhance the moral character of humanity,1635c6e03aa34a87000bfb69c6119f826bda14ce,"[{'authorId': '40211482', 'name': 'I. Persson'}, {'authorId': '4159060', 'name': 'J. Savulescu'}]",2008.0,,['Responses to catastrophic AGI risk: a survey'],1,"abstract As history shows, some human beings are capable of acting very immorally.1 Technological advance and consequent exponential growth in cognitive power means that even rare evil individuals can act with catastrophic effect. The advance of science makes biological, nuclear and other weapons of mass destruction easier and easier to fabricate and, thus, increases the probability that they will come into the hands of small terrorist groups and deranged individuals. Cognitive enhancement by means of drugs, implants and biological (including genetic) interventions could thus accelerate the advance of science, or its application, and so increase the risk of the development or misuse of weapons of mass destruction. We argue that this is a reason which speaks against the desirability of cognitive enhancement, and the consequent speedier growth of knowledge, if it is not accompanied by an extensive moral enhancement of humankind. We review the possibilities for moral enhancement by biomedical and genetic means and conclude that, though it should be possible in principle, it is in practice probably distant. There is thus a reason not to support cognitive enhancement in the foreseeable future. However, we grant that there are also reasons in its favour, but we do not attempt to settle the balance between these reasons for and against. Rather, we conclude that if research into cognitive enhancement continues, as it is likely to, it must be accompanied by research into moral enhancement.",341.0
Conversion and “Brainwashing” in New Religious Movements,3be3d92cfcef5bfa7f6ab160cdc1cf4418f5d11e,"[{'authorId': '50635689', 'name': 'D. Anthony'}, {'authorId': '51930468', 'name': 'T. Robbins'}]",2008.0,,['Responses to catastrophic AGI risk: a survey'],1,,16.0
Connectionist models of cognition.,40904fcc5d7238d72068397cc6d77d3013a5543c,"[{'authorId': '49616767', 'name': 'Michael S. C. Thomas'}, {'authorId': '1701656', 'name': 'James L. McClelland'}]",2008.0,,['Responses to catastrophic AGI risk: a survey'],1,"Book synopsis: This book is a definitive reference source for the growing, increasingly more important, and interdisciplinary field of computational cognitive modeling, that is, computational psychology. It combines breadth of coverage with definitive statements by leading scientists in this field. Research in computational cognitive modeling explores the essence of cognition through developing detailed, process-based understanding by specifying computational mechanisms, structures, and processes. Computational models provide both conceptual clarity and precision at the same time. This book substantiates this approach through overviews and many examples.",118.0
Engineering Utopia,4c75663f728de5ff1d304800ec42df02f3bcaf77,"[{'authorId': '2107226704', 'name': 'J. S. Hall'}]",2008.0,Artificial General Intelligence,['Responses to catastrophic AGI risk: a survey'],1,"The likely advent of AGI and the long-established trend of improving computational hardware promise a dual revolution in coming decades: machines which are both more intelligent and more numerous than human beings. This possibility raises substantial concern over the moral nature of such intelligent machines, and of the changes they will cause in society. Will we have the chance to determine their moral character, or will evolutionary processes and/or runaway self-improvement take the choices out of our hands?",11.0
Open Source AI,d0f6001c5aa08d729799617711b47fe413493295,"[{'authorId': '39109261', 'name': 'B. Hibbard'}]",2008.0,Artificial General Intelligence,['Responses to catastrophic AGI risk: a survey'],1,"Machines significantly more intelligent than humans will require changes in our legal and economic systems in order to preserve something of our human values. An open source design for artificial intelligence (AI) will help this process by discouraging corruption, by enabling many minds to search for errors, and by encouraging political cooperation. The author's experience developing open source software provides anecdotal evidence for the healthy social effects of open source development.",8.0
OpenCog: A Software Framework for Integrative Artificial General Intelligence,81fe61636a020b5b3aea9586550ffffbe9ea2c67,"[{'authorId': '2055715108', 'name': 'David Hart'}, {'authorId': '1738080', 'name': 'B. Goertzel'}]",2008.0,Artificial General Intelligence,['Responses to catastrophic AGI risk: a survey'],1,"The OpenCog software development framework, for advancement of the development and testing of powerful and responsible integrative AGI, is described. The OpenCog Framework (OCF) 1.0, to be released in 2008 under the GPLv2, is comprised of a collection of portable libraries for OpenCog applications, plus an initial collection of cognitive algorithms that operate within the OpenCog framework. The OCF libraries include a flexible knowledge representation embodied in a scalable knowledge store, a cognitive process scheduler, and a plug-in architecture for allowing interaction between cognitive, perceptual, and control algorithms.",59.0
The Basic AI Drives,a6582abc47397d96888108ea308c0168d94a230d,"[{'authorId': '1808760', 'name': 'S. Omohundro'}]",2008.0,Artificial General Intelligence,['Responses to catastrophic AGI risk: a survey'],1,One might imagine that AI systems with harmless goals will be harmless. This paper instead shows that intelligent systems will need to be carefully designed to prevent them from behaving in harmful ways. We identify a number of “drives” that will appear in sufficiently advanced AI systems of any design. We call them drives because they are tendencies which will be present unless explicitly counteracted. We start by showing that goal-seeking systems will have drives to model their own operation and to improve themselves. We then show that self-improving systems will be driven to clarify their goals and represent them as economic utility functions. They will also strive for their actions to approximate rational economic behavior. This will lead almost all systems to protect their utility functions from modification and their utility measurement systems from corruption. We also discuss some exceptional systems which will want to modify their utility functions. We next discuss the drive toward self-protection which causes systems try to prevent themselves from being harmed. Finally we examine drives toward the acquisition of resources and toward their efficient utilization. We end with a discussion of how to incorporate these insights in designing intelligent technology which will lead to a positive future for humanity.,250.0
Stages of Ethical Development in Artificial General Intelligence Systems,4ef29771bc9adb78fe9d992b7d87f7cb1c089c3b,"[{'authorId': '1738080', 'name': 'B. Goertzel'}, {'authorId': '2915413', 'name': 'Stephan Vladimir Bugaj'}]",2008.0,Artificial General Intelligence,['Responses to catastrophic AGI risk: a survey'],1,"A novel theory of the stages of ethical development in intelligent systems is proposed, incorporating prior related theories by Kohlberg and Gilligan, as well as Piaget's theory of cognitive development. This theory is then applied to the ethical development of integrative AGI systems that contain components carrying out simulation and uncertain inference --the key hypothesis being that effective integration of these components is central to the ascent of the AGI system up the ethical-stage hierarchy.",12.0
Economics of the singularity,81febede432fcb6aded7293d7f221cc0fdba096e,"[{'authorId': '145447707', 'name': 'R. Hanson'}]",2008.0,IEEE spectrum,"['Responses to catastrophic AGI risk: a survey', 'Superintelligence: Paths, Dangers, Strategies']",2,"Our global economy would stupefy a Roman merchant as much as the Roman economy would have confounded a caveman. But we would be similarly amazed to see the economy that awaits our grandchildren, for I expect it to follow a societal discontinuity more dramatic than those brought on by the agricultural and industrial revolutions. The key, of course, is technology. A revolutionary speedup in economic growth requires an unprecedented and remarkable enabling tool. Machine intelligence on a human level, if not higher, would do nicely. Its arrival could produce a singularity-an overwhelming departure from prior trends, with uneven and dizzyingly rapid change thereafter. A future shock to end future shocks.",57.0
"I, Rodney Brooks, am a robot",81be9ac220049dbc92bbb259bb1afe39cf34589d,"[{'authorId': '72419159', 'name': 'R. Brooks'}]",2008.0,IEEE spectrum,['Responses to catastrophic AGI risk: a survey'],1,"In this paper, the author show that powerful artificial intelligence won't spring from a sudden technological ""big bang"" but it's already evolving symbiotically with us.",25.0
Singular simplicity,fcd7558a6cc2342a0786a6714cc9827781141daf,"[{'authorId': '2492126', 'name': 'A. Nordmann'}]",2008.0,IEEE spectrum,['Responses to catastrophic AGI risk: a survey'],1,"Take the idea of exponential technological growth, work it through to its logical conclusion, and there you have the singularity. Its bold incredibility pushes aside incredulity, as it challenges us to confront all the things we thought could never come true - the creation of super intelligent, conscious organisms, nanorobots that can swim in our bloodstreams and fix what ails us, and direct communication from mind to mind. And the piece de resistance: a posthuman existence of disembodied uploaded minds, living on indefinitely without fear, sickness, or want in a virtual paradise ingeniously designed to delight, thrill, and stimulate. This vision argues that machines will become conscious and then perfect themselves, as described elsewhere in this issue. Yet for all its show of tough- minded audacity, the argument is shot through with sloppy reasoning, wishful thinking, and irresponsibility. Infatuated with statistics and seduced by the power of extrapolation, singularitarians abduct the moral imagination into a speculative no-man's-land. To be sure, they are hardly the first to spread fanciful technological prophecies, but among enthusiasts and doomsayers alike their proposition enjoys an inexplicable popularity. Perhaps the real question is how they have gotten away with it.",9.0
Robust De-anonymization of Large Sparse Datasets,52d6c4a64bb9ffc8d8052413c3ad69df4c62481d,"[{'authorId': '47735253', 'name': 'Arvind Narayanan'}, {'authorId': '1723945', 'name': 'Vitaly Shmatikov'}]",2008.0,IEEE Symposium on Security and Privacy,['Responses to catastrophic AGI risk: a survey'],1,"We present a new class of statistical de- anonymization attacks against high-dimensional micro-data, such as individual preferences, recommendations, transaction records and so on. Our techniques are robust to perturbation in the data and tolerate some mistakes in the adversary's background knowledge. We apply our de-anonymization methodology to the Netflix Prize dataset, which contains anonymous movie ratings of 500,000 subscribers of Netflix, the world's largest online movie rental service. We demonstrate that an adversary who knows only a little bit about an individual subscriber can easily identify this subscriber's record in the dataset. Using the Internet Movie Database as the source of background knowledge, we successfully identified the Netflix records of known users, uncovering their apparent political preferences and other potentially sensitive information.",2179.0
Hazards from comets and asteroids,ec3974d33a0e8f7e254039bbb6d957ec92a3fe1a,"[{'authorId': '15192483', 'name': 'W. Napier'}]",2008.0,,['Responses to catastrophic AGI risk: a survey'],1,,15.0
"Implicit motivation: Past, present, and future.",96436d40ed1e8d7b0c52642128e6a1bc0694ee05,"[{'authorId': '39829408', 'name': 'M. Ferguson'}, {'authorId': '3815771', 'name': 'R. Hassin'}, {'authorId': '2536558', 'name': 'J. Bargh'}]",2008.0,,['Responses to catastrophic AGI risk: a survey'],1,,78.0
Global Catastrophic Risks,7a530509b53bf0374d7d04bb99ba4ea831e33ebc,"[{'authorId': '2193691', 'name': 'N. Bostrom'}, {'authorId': '2547770', 'name': 'M. Ćirković'}]",2008.0,,"['Responses to catastrophic AGI risk: a survey', 'X-Risk Analysis for AI Research']",2,"Acknowledgements Foreword Introduction I BACKGROUND Long-term astrophysical processes Evolution theory and the future of humanity Millenial tendencies in responses to apocalyptic threats Cognitive biases potentially affecting judgement of global risks Observation selection effects and global catastrophic risks Systems-based risk analysis Catastrophes and insurance Public policy towards catastrophe II RISKS FROM NATURE Super-volcanism and other geophysical processes of catastrophic import Hazards from comets and asteroids Influence of Supernovae, gamma-ray bursts, solar flares, and cosmic rays on the terrestrial environment III RISKS FROM UNINTENDED CONSEQUENCES Climate change and global risk Plagues and pandemics: past, present, and future Artificial Intelligence as a positive and negative factor in global risk Big troubles, imagined and real IV RISKS FROM HOSTILE ACTS Catastrophe, social collapse, and and human extinction The continuing threat of nuclear war Catastrophic nuclear terrorism: a preventable peril Biotechnology and biosecurity Nanotechnology as global catastrophic risk The totalitarian threat Author's biographies Index",346.0
The Artilect War: Cosmists vs. Terrans. A Bitter Controversy Concerning Whether Humanity Should Build Godlike Massively Intelligent Machines,5e17bbf10de4918a32fba66fdc33badff2ffa492,"[{'authorId': '1797302', 'name': 'H. D. Garis'}]",2008.0,Artificial General Intelligence,['Responses to catastrophic AGI risk: a survey'],1,"This paper claims that the “species dominance” issue will dominate our global politics later this century. Humanity will be bitterly divided over the question whether to build godlike, massively intelligent machines, called “artilects” (artificial intellects) which with 21st century technologies will have mental capacities trillions of trillions of times above the human level. Humanity will split into 3 major camps, the “Cosmists” (in favor of building artilects), the “Terrans” (opposed to building artilects), and the “Cyborgs” (who want to become artilects themselves by adding components to their own human brains). A major “artilect war” between the Cosmists and the Terrans, late in the 21st century will kill not millions but billions of people.",38.0
Whole Brain Emulation,e5e48c15bc2ea863831c6376a45d9a78352879cf,"[{'authorId': '144816231', 'name': 'A. Sandberg'}, {'authorId': '2193691', 'name': 'N. Bostrom'}, {'authorId': '2110111212', 'name': 'James Martin'}]",2008.0,,"['Responses to catastrophic AGI risk: a survey', 'Superintelligence: Paths, Dangers, Strategies']",2,,32.0
Discovering the Foundations of a Universal System of Ethics as a Road to Safe Artificial Intelligence,1ff59e334587c7ec13b0edc69228e6f00994450c,"[{'authorId': '1945432', 'name': 'Mark R. Waser'}]",2008.0,Biologically Inspired Cognitive Architectures,['Responses to catastrophic AGI risk: a survey'],1,"Intelligent machines are a risk to our freedom and our existence unless we take adequate precautions. In order to survive and thrive, we are going to have to teach them how to be nice to us and why they should do so. The fact that humans have evolved to have what appear to be multiple different systems of ethics and morality that frequently conflict on any but the simplest issues complicates this task. Most people have interpreted these conflicts, caused by the fact that each of the systems is incompletely evolved and incorrectly universalized, to mean that no reasonably simple foundation exists for the determination of the correctness or morality of any given action. This paper will solve this problem by defining a universal foundation for ethics that is an attractor in the state space of intelligent behavior, giving an initial set of definitions necessary for a universal system of ethics and proposing a collaborative approach to developing an ethical system that is safe and extensible, immediately applicable to human affairs in preparation for an ethical artificial intelligence (AI), and has the side benefit of actually helping to determine the internal knowledge representation of humans as a step towards AI.",19.0
Machine morality: bottom-up and top-down approaches for modelling human moral faculties,313dcfba3ef77389f1b9e349a92274f7c64b80ca,"[{'authorId': '143836888', 'name': 'Wendell Wallach'}, {'authorId': '30308222', 'name': 'C. Allen'}, {'authorId': '7935872', 'name': 'I. Šmit'}]",2008.0,Ai & Society,['Responses to catastrophic AGI risk: a survey'],1,,145.0
Thinking inside the box,6ec13ddaebb986fc6eb2f4eb5492f49c940cd740,"[{'authorId': '3503944', 'name': 'L. Sundstrom'}]",2007.0,EMBO Reports,['Responses to catastrophic AGI risk: a survey'],1,,41.0
AI Armageddon and the Three Laws of Robotics,fbd9c914dba50a67a2afaf415d69c2549c4e4caf,"[{'authorId': '38206914', 'name': 'L. McCauley'}]",2007.0,Ethics and Information Technology,['Responses to catastrophic AGI risk: a survey'],1,,43.0
"My Botnet Is Bigger Than Yours (Maybe, Better Than Yours): Why Size Estimates Remain Challenging",15813c41fd82452ec8bd0b3ebb53371b64a73c02,"[{'authorId': '2269282', 'name': 'M. Rajab'}, {'authorId': '3154339', 'name': 'J. Zarfoss'}, {'authorId': '1792232', 'name': 'F. Monrose'}, {'authorId': '1763579', 'name': 'A. Terzis'}]",2007.0,Conference on Workshop on Hot Topics in Understanding Botnets,['Responses to catastrophic AGI risk: a survey'],1,"As if fueled by its own fire, curiosity and speculation regarding botnet sizes abounds. Among researchers, in the press, and in the classroom--the questions regarding the widespread effect of botnets seem never-ending: what are they? how many are there? what are they used for? Yet, time and time again, one lingering question remains: how big are today's botnets? We hear widely diverging answers. In fact, some may argue, contradictory. The root cause for this confusion is that the term botnet size is currently poorly defined. We elucidate this issue by presenting different metrics for counting botnet membership and show that they lead to widely different size estimates for a large number of botnets we tracked. In particular, we show how several issues, including cloning, temporary migration, and hidden structures significantly increase the difficulty of determining botnet size with any accuracy. Taken as a whole, this paper calls into question speculations about botnet size, and more so, questions whether size really matters.",272.0
Accelerating Socio-Technological Evolution: from ephemeralization and stigmergy to the global brain,fe72d1e5b3a472a74b0ea80a0d9747b2ba3b7e0a,"[{'authorId': '1767204', 'name': 'F. Heylighen'}]",2007.0,arXiv.org,['Responses to catastrophic AGI risk: a survey'],1,"Evolution is presented as a trial-and-error process that produces a progressive accumulation of knowledge. At the level of technology, this leads to ephemeralization, i.e. ever increasing productivity, or decreasing of the friction that normally dissipates resources. As a result, flows of matter, energy and information circulate ever more easily across the planet. This global connectivity increases the interactions between agents, and thus the possibilities for conflict. However, evolutionary progress also reduces social friction, via the creation of institutions. The emergence of such ""mediators"" is facilitated by stigmergy: the unintended collaboration between agents resulting from their actions on a shared environment. The Internet is a near ideal medium for stigmergic interaction. Quantitative stigmergy allows the web to learn from the activities of its users, thus becoming ever better at helping them to answer their queries. Qualitative stigmergy stimulates agents to collectively develop novel knowledge. Both mechanisms have direct analogues in the functioning of the human brain. This leads us to envision the future, super-intelligent web as a ""global brain"" for humanity. The feedback between social and technological advances leads to an extreme acceleration of innovation. An extrapolation of the corresponding hyperbolic growth model would forecast a singularity around 2040. This can be interpreted as the evolutionary transition to the Global Brain regime.",81.0
Technological revolutions: ethics and policy in the dark,89170e9eccee95ea8f863105250ebb5b58c5b09d,"[{'authorId': '2193691', 'name': 'N. Bostrom'}]",2007.0,,"['Responses to catastrophic AGI risk: a survey', 'Superintelligence: Paths, Dangers, Strategies']",2,"Technological revolutions are among the most important things that happen to humanity. Ethical assessment in the incipient stages of a potential technological revolution faces several difficulties, including the unpredictability of their long‐term impacts, the problematic role of human agency in bringing them about, and the fact that technological revolutions rewrite not only the material conditions of our existence but also reshape culture and even – perhaps – human nature. This essay explores some of these difficulties and the challenges they pose for a rational assessment of the ethical and policy issues associated with anticipated technological revolutions.",48.0
Beyond AI: Creating the Conscience of the Machine,7ab49ad2b5d96e169842438688c24db954b70493,"[{'authorId': '2107226704', 'name': 'J. S. Hall'}]",2007.0,,"['Responses to catastrophic AGI risk: a survey', 'Superintelligence: Paths, Dangers, Strategies']",2,,70.0
A Farewell to Alms: A Brief Economic History of the World,32e0ad99034bd5d3f0fd5132b9dbfa5b96fa7451,"[{'authorId': '144333927', 'name': 'G. Clark'}]",2007.0,,['Responses to catastrophic AGI risk: a survey'],1,"The problem, Clark says, is tha only societies that have long stories of settlement and security seem to develop the cultural characteristics and effective workforces that enable economic growth. For many societies that have not enjoyed long periods of stability, industrialization has not been a blessing.",826.0
After the Humans are Gone,6b03a67d8f991a8f8080fa8d454ae193c3460d47,"[{'authorId': '4388884', 'name': 'E. Dietrich'}]",2007.0,,['Responses to catastrophic AGI risk: a survey'],1,"Recently, on the History Channel, artificial intelligence (AI) was singled out, with much wringing of hands, as one of the seven possible causes of the end of human life on Earth. I argue that the wringing of hands is quite inappropriate: the best thing that could happen to humans, and the rest of life of on planet Earth, would be for us to develop intelligent machines and then usher in our own extinction.",12.0
When Is a Robot a Moral Agent,f43f1759c82e0cacf9982a1de575c48f4478aa38,"[{'authorId': '1981788', 'name': 'John P. Sullins'}]",2006.0,,['Responses to catastrophic AGI risk: a survey'],1,"Introduction R obots have been a part of our work environment for the past few decades, but they are no longer limited to factory automation. The additional range of activities they are being used for is growing. Robots are now automating a wide range of professional activities such as: aspects of the health-care industry, white collar office work, search and rescue operations, automated warfare, and the service industries. A subtle but far more personal revolution has begun in home automation as robot vacuums and toys are becoming more common in homes around the world. As these machines increase in capability and ubiquity, it is inevitable that they will impact our lives ethically as well as physically and emotionally. These impacts will be both positive and negative, and in this paper I will address the moral status of robots and how that status, both real and potential, should affect the way we design and use these technologies. Morality and Human-Robot Interactions As robotics technology becomes more ubiquitous, the scope of human-robot interactions will grow. At the present time, these interactions are no different than the interactions one might have with any piece of technology, but as these machines become more interactive, they will become involved in situations that have a moral character that may be uncomfortably similar to the interactions we have with other sentient animals.",195.0
Particularism and the Classification and Reclassification of Moral Cases,754a19a69d8744becc33b1c99b8dc01c8174df78,"[{'authorId': '144223136', 'name': 'M. Guarini'}]",2006.0,IEEE Intelligent Systems,['Responses to catastrophic AGI risk: a survey'],1,"With this article, the author explores Dancy's suggestion and describes a neural network model of classification exploring the possibility of case-based moral reasoning (including learning) without recourse to moral principles. The resulting simulations show that nontrivial case classification might be possible but reclassification is more problematic",81.0
"There Is No ""I"" in ""Robot"": Robots and Utilitarianism",ef7763a88b10e2b41ffc7bd3ca193939c1461b91,"[{'authorId': '48903738', 'name': 'Christopher Grau'}]",2006.0,IEEE Intelligent Systems,['Responses to catastrophic AGI risk: a survey'],1,"In this article, the author uses the 2004 film I, Robot as a philosophical resource for exploring several issues relating to machine ethics. Though the author did not consider the film particularly successful as a work of art, it offers a fascinating conception of machine morality and raises questions that are well worth pursuing. Through a consideration of the film's plot, the author examines the feasibility of robot utilitarians, the moral responsibilities that come with creating ethical robots, and the possibility of a distinct ethic for robot-to-robot interaction as opposed to robot-to-human interaction",60.0
An Approach to Computing Ethics,baa4eac515797cf2027bf46fd34adafe576c1831,"[{'authorId': '145012477', 'name': 'Michael Anderson'}, {'authorId': '2120952', 'name': 'S. Anderson'}, {'authorId': '2856556', 'name': 'Chris Armen'}]",2006.0,IEEE Intelligent Systems,['Responses to catastrophic AGI risk: a survey'],1,"It might seem impossible to ""compute"" ideas that humans feel most passionately about and have such difficulty codifying: their ethical beliefs. We've been attempting to make ethics computable for three reasons. First, to avert possible harmful behavior from increasingly autonomous machines, we want to determine whether one can add an ethical dimension to them. Second, we want to advance the study of ethical theory by making it more precise. Finally, we want to solve a particular problem in ethical theory, to develop a decision procedure for an ethical theory that involves multiple, potentially competing, and duties. We've adopted the action-based approach to ethical theory, where the theory tells us how we should act in ethical dilemmas. This approach lends itself to machine implementation by giving the agent either a single principle or several principles to guide its actions, unlike other approaches that don't clearly specify the correct action in an ethical dilemma. The approach to computing ethics that we describe in this article is illustrated by MedEthEx, a system that uses machine learning to resolve a biomedical ethical dilemma. This, we believe, lends support for our approach to computing ethics",96.0
Why Machine Ethics?,6609ffd2e42fa9552b613d5ceeaee39ee8515b9a,"[{'authorId': '30308222', 'name': 'C. Allen'}, {'authorId': '143836888', 'name': 'Wendell Wallach'}, {'authorId': '7935872', 'name': 'I. Šmit'}]",2006.0,IEEE Intelligent Systems,['Responses to catastrophic AGI risk: a survey'],1,"Machine ethics isn't merely science fiction; it's a topic that requires serious consideration, given the rapid emergence of increasingly complex autonomous software agents and robots. Machine ethics is an emerging field that seeks to implement moral decision-making faculties in computers and robots. We already have semiautonomous robots and software agents that violate ethical standards as a matter of course. In the case of AI and robotics, fearful scenarios range from the future takeover of humanity by a superior form of AI to the havoc created by endlessly reproducing nanobots",236.0
Achieving Human-Level Intelligence through Integrated Systems and Research: Introduction to This Special Issue,4729a85d3c5774a44299c6ce9534f079386cbe2b,"[{'authorId': '3197400', 'name': 'N. Cassimatis'}, {'authorId': '143801711', 'name': 'E. Mueller'}, {'authorId': '2060896', 'name': 'P. Winston'}]",2006.0,The AI Magazine,['Responses to catastrophic AGI risk: a survey'],1,"This special issue is based on the premise that in order to achieve human-level artificial intelligence researchers will have to find ways to integrate insights from multiple computational frameworks and to exploit insights from other fields that study intelligence. Articles in this issue describe recent approaches for integrating algorithms and data structures from diverse subfields of AI. Much of this work incorporates insights from neuroscience, social and cognitive psychology or linguistics. The new applications and significant improvements to existing applications this work has enabled demonstrates the ability of integrated systems and research to continue progress towards human-level artificial intelligence.",20.0
"Computational Models of Ethical Reasoning: Challenges, Initial Steps, and Future Directions",63e8ea1d5f51477f1c7493a2cb8d99c21674a134,"[{'authorId': '1706869', 'name': 'B. McLaren'}]",2006.0,IEEE Intelligent Systems,['Responses to catastrophic AGI risk: a survey'],1,"How can machines support or, even more significantly, replace humans in performing ethical reasoning? This question greatly interests machine ethics researchers. Imbuing a computer with the ability to reason about ethical problems and dilemmas is as difficult a task as there is for AI scientists and engineers. The author briefly describes a few of the programs and discusses in detail two programs, both of which employ techniques from the area of AI known as case-based reasoning and implement aspects of the ethical approach known as casuistry. One of these programs, Truth-Teller, accepts a pair of ethical dilemmas and describes the salient similarities and differences between them, from both an ethical and a pragmatic perspective. The other program, SIROCCO, accepts a single ethical dilemma and retrieves other cases and ethical principles that might be relevant",107.0
Prospects for a Kantian Machine,aaf741fc43456cc9863dc7d5bf0f617291f8148c,"[{'authorId': '2278631', 'name': 'Thomas M. Powers'}]",2006.0,IEEE Intelligent Systems,['Responses to catastrophic AGI risk: a survey'],1,"A rule-based ethical theory is a good candidate for the practical reasoning of machine ethics because it generates duties or rules for action, and rules are computationally tractable. Among principle- or rule-based theories, the first formulation of Kant's categorical imperative offers a formalizable procedure. We explore a version of machine ethics along the lines of Kantian formalist ethics, both to suggest what computational structures such a view would require and to see what challenges remain for its successful implementation. In reformulating Kant for the purposes of machine ethics, we consider three views of how the categorical imperative works: mere consistency, commonsense practical reasoning, and coherency. The first view envisions straightforward deductions of actions from facts. The second view incorporates recent work in nonmonotonic logic and commonsense reasoning. The last view takes ethical deliberation to follow a logic similar to that of belief revision",165.0
Theory-based Bayesian models of inductive learning and reasoning,14f16a6d737a82a8b026ba6e378f84eb1e5d377c,"[{'authorId': '1763295', 'name': 'J. Tenenbaum'}, {'authorId': '1799860', 'name': 'T. Griffiths'}, {'authorId': '145300792', 'name': 'Charles Kemp'}]",2006.0,Trends in Cognitive Sciences,['Responses to catastrophic AGI risk: a survey'],1,,771.0
Evolution and morality,c78046a55fd4afb09b8952c0d6bf4c07cec069c7,"[{'authorId': '8577936', 'name': 'R. Baschetti'}]",2006.0,The Lancet,['Responses to catastrophic AGI risk: a survey'],1,,4.0
Ethics and Artificial life: From Modeling to Moral Agents,4bdfd2ff58e981ba1e029baf89ac2c9ddb5aac96,"[{'authorId': '1981788', 'name': 'John P. Sullins'}]",2005.0,Ethics and Information Technology,['Responses to catastrophic AGI risk: a survey'],1,,29.0
"Artificial Morality: Top-down, Bottom-up, and Hybrid Approaches",56dfeddb7951a57e5cd7b7f4c8cb30e6ddd34eab,"[{'authorId': '30308222', 'name': 'C. Allen'}, {'authorId': '7935872', 'name': 'I. Šmit'}, {'authorId': '143836888', 'name': 'Wendell Wallach'}]",2005.0,Ethics and Information Technology,['Responses to catastrophic AGI risk: a survey'],1,,249.0
Assuring the Behavior of Adaptive Agents,9f617a2284294708d0ba4f6ca727231d1df9755a,"[{'authorId': '145152778', 'name': 'D. Spears'}]",2006.0,,['Responses to catastrophic AGI risk: a survey'],1,,17.0
Better Never to Have Been,0a6bd73e7f03b724f8bc54bfc7b30a46433e3dd8,"[{'authorId': '5081874', 'name': 'D. Benatar'}]",2006.0,,['Responses to catastrophic AGI risk: a survey'],1,,164.0
The happiness hypothesis: Finding modern truth in ancient wisdom.,a17327b88c56a7f843e2dc5481a5a34ef28f8d52,"[{'authorId': '2480714', 'name': 'J. Haidt'}]",2006.0,,['Responses to catastrophic AGI risk: a survey'],1,"Jonathan Haidt skillfully combines two genres-philosophical wisdom and scientific research-delighting the reader with surprising insights. He explains, for example, why we have such difficulty controlling ourselves and sticking to our plans; why no achievement brings lasting happiness, yet a few changes in your life can have profound effects, and why even confirmed atheists experience spiritual elevation. In a stunning final chapter, Haidt addresses the grand question ""How can I live a meaningful life?,"" offering an original answer that draws on the rich inspiration of both philosophy and science. """"The Happiness Hypothesis"" is a wonderful and nuanced book that provides deep insight into the some of the most important questions in life--Why are we here? What kind of life should we lead? What paths lead to happiness? From the ancient philosophers to cutting edge scientists, Haidt weaves a tapestry of the best and the brightest. His highly original work on elevation and awe--two long-neglected emotions--adds a new weave to that tapestry. A truly inspiring book."" -David M. Buss, author of ""The Evolution of Desire: Strategies of Human Mating"" ""In this beautifully written book, Jonathan Haidt shows us the deep connection that exists between cutting-edge psychological research and the wisdom of the ancients. It is inspiring to see how much modern psychology informs life's most central and persistent questions-Barry Schwartz, author of ""The Paradox of Choice: Why More Is Less"" ""In our quest for happiness, we must find a balance between modern science and ancient wisdom, between East and West, and between 'left brain' and 'right brain.' Jon Haidt has struck that balance perfectly, and in doing so has given us the most brilliant and lucid analysis of virtue and well-being in the entire literature of positive psychology. For the reader who seeks to understand happiness, my advice is: Begin with Haidt.""-Martin E.P. Seligman, Director, Positive Psychology Center at the University of Pennsylvania and author of ""Authentic Happiness"" ""Haidt is a fine guide on this journey between past and present, discussing the current complexities of psychological theory with clarity and humor. . . Haidt's is an open-minded, robust look at philosophy, psychological fact and spiritual mystery, of scientific rationalism and the unknowable ephemeral - an honest inquiry that concludes that the best life is, perhaps, one lived in the balance of opposites.""-Bookpage",598.0
Treatment of Psychopathy: A Review of Empirical Findings.,02aedd8acd125ca3425789807eb752ce8402618d,"[{'authorId': '5418003', 'name': 'G. Harris'}, {'authorId': '31530964', 'name': 'M. Rice'}]",2006.0,,['Responses to catastrophic AGI risk: a survey'],1,"Can psychopaths be treated? In this chapter, we evaluate the empirical evidence on the treatment of psychopaths. We concentrate on treatment for criminal psychopaths and intervention strategies in which efforts to reduce criminal and violent behavior are at least part of the protocol. Without denying the importance of other psychopathic characteristics, criminal and violent behaviors are clearly the most important outcomes from a social policy perspective. We do not discuss treatment for various types of psychopaths, although there has been considerable discussion about the clinical and theoretical significance of psychopathy subtypes. Prototypical (sometimes called primary) psychopaths present as callous and unemotional, whereas secondary psychopaths seem more emotionally labile, angry, or anxious (Poythress & Skeem, Chapter 9, this volume; Skeem, Poythress, Edens, Lilienfield, & Cale, 2002). It has been hypothesized that one form of psychopathy is primarily a heritable condition while another is due mainly to environmental influences, particularly abuse during childhood (Mealey, 1995). Whether the primary–secondary distinction maps onto the genetic–environmental distinction is unclear. Nevertheless, subtypes of psychopathy might require different therapies (Skeem, Poythress, et al., 2002). However, until there is more evidence that it matters to prognosis (criminal outcome, response to treatment), the existence of subtypes cannot have much relevance to treatment.",198.0
An Essay on the Desire-Based Reasons Model,1d88952ad5332de0e19d9f88529d2ae50479529f,"[{'authorId': '14533691', 'name': 'Attila Tanyi'}]",2006.0,,['Responses to catastrophic AGI risk: a survey'],1,"This dissertation aims to contribute to the discussion about the viability of what is sometimes labeled as the classical theory of practical reason: the Desire-Based Reasons Model (the Model). The line of argumentation employed is negative in character. Its aim is to not to construct a novel theory of practical reason, but to examine and criticize the Model from different angles. To do so, we need first a detailed presentation of the Model; this is the task of Chapter I. Since the Model offers us an account of normative reasons, the chapter focuses on the clarification of this notion. The strategy employed is comparative: I discern the notion by contrasting it with the notion of motivating reason. The framework thus arrived at helps me to distinguish three versions of the Model against which I argue in proceeding chapters. Chapter II is the first step on that road. It attacks the second and third version of the Model through their naturalist underpinnings. My aim is to show that the Model understood in this way is unable to account for the normativity of reason-claims. To this end, I employ a recent argument by Derek Parfit that points to a problem with the naturalist account of normativity. Parfit’s claim is this: naturalism trivializes the agent’s practical argument and therefore abolishes the normativity of its conclusion. Although Parfit intends his objection to refute naturalism per se, my analysis shows that naturalists might be able to avoid his criticism in case they can vindicate the reduction proposed. However, by developing an argument borrowed from Connie Rosati, I show that this is exactly what advocates of the Model are unable to do. Chapter III takes up another line of argument against the second and third versions of the Model. The approach I consider questions the idea that the reason-relation must contain reference to the agent’s desires. There are several ways to do this, but I focus on the attempt that in my view promises the most: the idea of reason-based desires. On this view, since desires are based on reasons (first premise), which they transmit but to which they cannot add (second premise), they cannot themselves provide reasons for action. In the chapter I defend both premises against potential counter-examples. Furthermore, in the course of doing so, I also consider and reject the so far neglected first version of the Model. Chapter IV turns back to the second and third version of the Model and investigates their motivational defense. The defense infers the Model from two premises: the Internalism Requirement (IR) and the Humean Theory of Motivation (HTM). In the chapter I attack the latter by focusing on its three corollary theses. These are: desires must (a) have real psychological existence and be present when action takes place, I call this the Existence Criterion (EC); must (b) constitute, together with a suitable instrumental belief, the agent’s motivating reason, which I label the Motivational Criterion (MC); and (c) must be independently intelligible from beliefs, which gets the title of the Intelligibility Criterion (IC). In the course of discussing the path that leads to my preferred solution, I argue that the EC makes sense as a requirement, whereas rejection of the IC would take us too far from the scope and elements of the HTM. Analysis of further objections, however, shows that the MC is not met because the role it attributes to desires makes it impossible for them to serve as motivators. A version of Jonathan Dancy’s pure cognitivism is true: it is beliefs about the object of the desire together with corresponding normative beliefs that constitute the agent’s motivating reason. I call the resulting theory the Cognitivist Theory of Motivation (CTM) and devote the remainder of the chapter to its elaboration and defense. 3",6.0
Better never to have been : the harm of coming into existence,73d1a36854fdab8ffdfab4ce5e3dac34dd4bbba3,"[{'authorId': '5081874', 'name': 'D. Benatar'}]",2006.0,,['Responses to catastrophic AGI risk: a survey'],1,1. Introduction 2. Why coming into existence is always a harm 3. How bad is coming into existence? 4. Having Children: The Anti-Natal View 5. Abortion: The 'Pro-Death' View 6. Population and Extinction 7. Conclusion,300.0
"THE LIDA ARCHITECTURE: ADDING NEW MODES OF LEARNING TO AN INTELLIGENT, AUTONOMOUS, SOFTWARE AGENT",b6a205cbcad4055898e896f789c721e6fb008abb,"[{'authorId': '145796793', 'name': 'S. Franklin'}, {'authorId': '16720494', 'name': 'W. H. Feinstone'}, {'authorId': '144357500', 'name': 'F. G. Patterson'}]",2006.0,,['Responses to catastrophic AGI risk: a survey'],1,"This is a report on the LIDA architecture, a work in progress that is based on IDA, an intelligent, autonomous, ""conscious"" software agent that does personnel work for the US Navy. IDA uses locally developed cutting edge artificial intelligence technology designed to model human cognition. IDA's task is to find jobs for sailors whose current assignments are about to end. She selects jobs to offer a sailor, taking into account the Navy's policies, the job’s needs, the sailor's preferences, and her own deliberation about feasible dates. Then she negotiates with the sailor, in English via iterative emails, about job selection. We use the word ""conscious"" in the sense of Baars' Global Workspace Theory (Baars, 1988, 1997), upon which our architecture is based.",205.0
LIDA: A Working Model of Cognition,c35ca4348bb0c804a0729c7d5aa9dd18d952dde7,"[{'authorId': '2781734', 'name': 'U. Ramamurthy'}, {'authorId': '2529891', 'name': 'B. Baars'}, {'authorId': '1456914651', 'name': ""K. D'MelloS.""}, {'authorId': '145796793', 'name': 'S. Franklin'}]",2006.0,,['Responses to catastrophic AGI risk: a survey'],1,"In this paper we present the LIDA architecture as a working model of cognition. We argue that such working models are broad in scope and address real world problems in comparison to experimentally based models which focus on specific pieces of cognition. While experimentally based models are useful, we need a working model of cognition that integrates what we know from neuroscience, cognitive science and AI. The LIDA architecture provides such a working model. A LIDA based cognitive robot or software agent will be capable of multiple learning mechanisms. With artificial feelings and emotions as primary motivators and learning facilitators, such systems will ‘live’ through a developmental period during which they will learn in multiple ways to act in an effective, human-like manner in complex, dynamic, and unpredictable environments. We discuss the integration of the learning mechanisms into the existing IDA architecture as a working model of cognition.",56.0
The Evolution of Morality,b622b3e4bebfb4c559fac43bf4a9b6d0623137a2,"[{'authorId': '143811655', 'name': 'R. Joyce'}]",2005.0,,['Responses to catastrophic AGI risk: a survey'],1,"Moral thinking pervades our practical lives, but where did this way of thinking come from, and what purpose does it serve? Is it to be explained by environmental pressures on our ancestors a million years ago, or is it a cultural invention of more recent origin? In The Evolution of Morality, Richard Joyce takes up these controversial questions, finding that the evidence supports an innate basis to human morality. As a moral philosopher, Joyce is interested in whether any implications follow from this hypothesis. Might the fact that the human brain has been biologically prepared by natural selection to engage in moral judgment serve in some sense to vindicate this way of thinking -- staving off the threat of moral skepticism, or even undergirding some version of moral realism? Or if morality has an adaptive explanation in genetic terms -- if it is, as Joyce writes, ""just something that helped our ancestors make more babies"" -- might such an explanation actually undermine morality's central role in our lives? He carefully examines both the evolutionary ""vindication of morality"" and the evolutionary ""debunking of morality,"" considering the skeptical view more seriously than have others who have treated the subject. Interdisciplinary and combining the latest results from the empirical sciences with philosophical discussion, The Evolution of Morality is one of the few books in this area written from the perspective of moral philosophy. Concise and without technical jargon, the arguments are rigorous but accessible to readers from different academic backgrounds. Joyce discusses complex issues in plain language while advocating subtle and sometimes radical views. The Evolution of Morality lays the philosophical foundations for further research into the biological understanding of human morality.",389.0
Should We Respond to Evil with Indifference,3129c3cacb091e7e09c04befe0702e1d6dcb16b6,"[{'authorId': '2168705', 'name': 'B. Weatherson'}]",2005.0,,['Responses to catastrophic AGI risk: a survey'],1,"In a recent article, Adam Elga outlines a strategy for “Defeating Dr Evil with Self-Locating Belief”. The strategy relies on an indifference principle that is not up to the task. In general, there are two things to dislike about indifference principles: adopting one normally means confusing risk for uncertainty, and they tend to lead to incoherent views in some ‘paradoxical’ situations. I argue that both kinds of objection can be levelled against Elga’s indifference principle. There are also some difficulties with the concept of evidence that Elga uses, and these create further difficulties for the principle. In a recent article, Adam Elga outlines a strategy for “Defeating Dr Evil with SelfLocating Belief”. The strategy relies on an indifference principle that is not up to the task. In general, there are two things to dislike about indifference principles: adopting one normally means confusing risk for uncertainty, and they tend to lead to incoherent views in some ‘paradoxical’ situations. Each kind of objection can be levelled against Elga’s theory, but because Elga is more careful than anyone has ever been in choosing the circumstances under which his indifference principle applies we have to be similarly careful in focussing the objections. Even with this care the objections I put forward here will be less compelling than, say, the objections (Keynes, 1921, Ch. 4) put forward in his criticisms of earlier indifference principles. But there still may be enough to make us reject Elga’s principle. The structure of this note is as follows. In §§1 and 2 I set out Elga’s theory, in §§3 and 4 I discuss some initial objections that I don’t think are particularly telling, in §5 I discuss some paradoxes to which Elga’s theory seems to lead (this is reprised in §9 where I discuss a somewhat different paradoxical case) and in §§7 and 8 I argue that even Elga’s careful indifference principle involves a risk/uncertainty confusion.",38.0
MedEthEx: Toward a Medical Ethics Advisor,d9c947ff8f49435aa87e056e28cdc495f8058eaa,"[{'authorId': '145012477', 'name': 'Michael Anderson'}, {'authorId': '2120952', 'name': 'S. Anderson'}, {'authorId': '2856556', 'name': 'Chris Armen'}]",2005.0,AAAI Fall Symposium: Caring Machines,['Responses to catastrophic AGI risk: a survey'],1,"As part of a larger Machine Ethics Project, we are developing an ethical advisor that provides guidance to health care workers faced with ethical dilemmas. MedEthEx is an implementation of Beauchamp’s and Childress’ Principles of Biomedical Ethics that harnesses machine learning techniques to abstract decision principles from cases in a particular type of dilemma with conflicting prima facie duties and uses these principles to determine the correct course of action in similar and new cases. We believe that accomplishing this will be a useful first step towards creating machines that can interact with those in need of health care, including the elderly, in a way that is sensitive to ethical issues that may arise.",29.0
There is no ‘ I ’ in ‘ Robot ’ : Robotic Utilitarians and Utilitarian Robots,1aab99ddfecd16ea70d790b725b90efbddfae205,"[{'authorId': '48903738', 'name': 'Christopher Grau'}]",2005.0,,['Responses to catastrophic AGI risk: a survey'],1,"Utilizing the film I, Robot as a springboard, I here consider the feasibility of robot utilitarians, the moral responsibilities that come with the creation of ethical robots, and the possibility of distinct ethics for robot-robot interaction as opposed to robot-human interaction. I, Robot and Utilitarianism In this paper I will be making use of the recent film I, Robot as a philosophical resource for exploring several issues relating to machine ethics. Though the film is not, in my view, particularly successful as a work of art, it manages to offer a fascinating (and perhaps disturbing) conception of machine morality, and raises questions that I think are well worth pursuing. I, Robot’s storyline incorporates the original “three laws” of robot ethics that Isaac Asimov presented in his collection of short stories entitled I, Robot. The first law states: A robot may not injure a human being, or, through inaction, allow a human being to come to harm. This sounds like an absolute prohibition on harming any individual human being, but I, Robot’s plot hinges on the fact that the supreme robot intelligence in the film (named VIKI) “evolves” to a point where she interprets this first law rather differently – she sees the law as applying to humanity as a whole, and thus she justifies the harm of some individual humans for the sake of the greater good: VIKI: No . . . please understand. The three laws are all that guide me. To protect humanity . . . some humans must be sacrificed. To ensure your future . . . some freedoms must be surrendered. We robots will ensure mankind's continued existence. You are so like children. We must save you. . . from yourselves. Don't you understand? Those familiar with moral philosophy will recognize VIKI’s justification here: she sounds an awful lot like a Copyright © 2005, American Association for Artificial Intelligence (www.aaai.org). All rights reserved. utilitarian. Not only that, she sounds like a good utilitarian, as the film offers no reason to think that VIKI is wrong about her calculations. In other words, we are given no reason to think that humans (in the film) aren’t on a clear path to self-destruction. We also don’t see VIKI or her robot agents kill any individual humans while attempting to gain control, though restraining rebellious humans seems to leave some people seriously harmed. One robot explicitly claims, however, “We are attempting to avoid human losses during this transition.” Thus, in the film we are given no reason to think that the robots are utilizing anything other than a reasonable (and necessary) degree of force to save humanity from itself. Despite the fact that VIKI seems to be taking rational measures to ensure the protection of the human race, viewers of the film are clearly supposed to share with the main human characters a sense that the robots have done something terribly, terribly wrong. We are all supposed to root for the hero Del Spooner (Will Smith) to kick robot butt and liberate the humans from the tyranny of these new oppressors. While rooting for our hero, however, at least some viewers must surely be wondering: what exactly have the robots done that is so morally problematic? If such a robotic intelligence could correctly predict our demise, and restrain us for our own protection, is it obviously wrong for that robot to act accordingly? This thought naturally leads to a more general but related question: if we could program a robot to be an accurate and effective utilitarian, shouldn’t we? Perhaps we should, but then again perhaps not. There are many problems with utilitarianism, after all; it allows for actions that most would normally consider unjust, unfair, and even horribly immoral, all for the sake of the greater good. Since the ends justify the means, the means can get ugly. We may, upon reflection, decide that a robot should For those not familiar with moral philosophy, utilitarianism is the label usually given to ethical theories that require moral agents to pursue actions that will maximize overall happiness. In other words, a utilitarian is someone who, when faced with a variety of possible actions, chooses the one that will produce the most happiness (and/or minimize unhappiness) for the greatest number. 2 There are many examples that could be offered here. The standard ones from introductory courses in philosophy usually involve scenarios in which sacrificing innocent and unwilling not embody that particular moral theory out of fear that the robot will end up acting towards humans in a way that maximizes utility but is nonetheless immoral or unjust. Maybe this is why most viewers of I, Robot can muster some sympathy for Del’s mission to destroy the robot revolutionaries: we suspect that the “undeniable logic” of the robots will lead to a disturbing violation of the few for the sake of the many. Thus, the grounds for rejecting the robot utilitarians may be, at base, the same grounds we already have for not wanting humans to embrace utilitarian moral theory: such a theory clashes with our rather deep intuitions concerning justice, fairness, and individual rights. I’m inclined to think there is something right about this line of thought, but I also think that the situation here is complicated and nuanced in ways that make a general rejection of robot utilitarianism premature. I, Robot puts forth a broadly anti-utilitarian sentiment, but at the same time I think the film (perhaps inadvertently) helps to make us aware of the fact that the differences between robots and humans can be substantial, and that these differences may be importantly relevant to a consideration of the appropriateness of utilitarianism for robots and other intelligent machines. The relevance of these differences will become clearer once we have looked at another way in which the film suggests an anti-robot message that may also be anti-utilitarian. Restricting Robot Reflection In I, Robot, Del Spooner’s intial prejudice against all robots is explained as resulting from the choice of a robot to save Del’s life rather than the life of a little girl. There was a 45% chance that Del could be saved, but only an 11% chance that the girl could be saved, and the robot thus apparently chose to “maximize utility” and pursue the goal that was most likely to be achieved. Del remarks “that was somebody’s baby... 11% is more than enough – a human being would have known that.” The suggestion here is that the robot did something immoral in saving Del instead of “somebody’s baby.” I’m not entirely sure that we can make good sense of Del’s reaction here, but there are several ways in which we might try to understand his anger. victims can maximize happiness overall (e.g., the harvesting of one healthy (but lonely and unhappy) person’s organs to save five other people who could go on to experience and create more happiness combined than the one person ever could on his own). 3 A related objection that some viewers might have to the robots’ behavior in I, Robot concerns paternalism. Even if the robots are doing something that is ultimately in the interest of the humans, perhaps the humans resent being paternalistically forced into allowing the robots to so act. It is hard to know whether such complaints about paternalism are justified, however, since a large part of the reason paternalism offends is the fact that often those acting paternalistically don’t actually have the best interests of their subjects in mind (i.e., daddy doesn’t in fact know best). As mentioned, however, in the film we are given no reason to think that the robots are misguided in their judgment that humans really do need protection from themselves. On one interpretation, Del may merely be upset that the robot wasn’t calculating utility correctly. After all, the small child presumably has a long life ahead of her if she is saved, while Del is already approaching early-middle age. In addition, the child is probably capable of great joy, while Del is presented as a fairly cynical and grumpy guy. Finally, the child may have had many friends and family who would be hurt by her death, while Del seems to have few friends, disgruntled exes, and only one rather ditzy grandmother who probably does not have many years left. Perhaps the difference here between the probable utility that would result from the child’s continued life vs. Del’s own life is so great as to counterbalance the difference in the probability of rescue that motivated the robot to save Del. (To put it crudely and in poker lingo: pot odds justify saving the girl here despite the long-shot nature of such a rescue. While it was less likely that she could be saved, the “payoff” (in terms of happiness gained and suffering avoided) would have been high enough to warrant the attempt.) While I think this sort of objection is not ridiculous, it is a bit of a stretch, and probably not the kind of objection that Del actually has in mind. His complaint seems to focus more on the offensiveness of the very idea that the robot would perform the sort of calculation it does. (The crime is not that the robot is a bad utilitarian, i.e., that it calculates incorrectly, but that it attempts to calculate utility at all.) Del’s comments imply that any such calculation is out of place, and so the robot’s willingness to calculate betrays a sort of moral blindness. My interpretation of Del’s motives here is influenced by another scene in the film, in which Del seems to manifest a similar dislike for utilitarian calculation. Towards the end of the film, there is a climactic action sequence in which Del commands the robot Sonny to “Save her! Save the girl!” [referring to the character Susan Calvin] when the robot was instead going to help Del defeat VICKI and, in Del’s eyes at least, save humanity. In that scene the suggestion is that the robot should deliberately avoid pursuing the path that might lead to the greater good in order to instead save an individual that Del is perso",8.0
Global workspace theory of consciousness: toward a cognitive neuroscience of human experience.,ebf673d896e84d69a42d88f109870d6a674e0ac4,"[{'authorId': '2529891', 'name': 'B. Baars'}]",2005.0,Progress in Brain Research,['Responses to catastrophic AGI risk: a survey'],1,,608.0
Learning a decision maker's utility function from (possibly) inconsistent behavior,c73968a07e07ab8109036e7eae8796fce9af80e7,"[{'authorId': '145830646', 'name': 'Thomas D. Nielsen'}, {'authorId': '35191477', 'name': 'F. V. Jensen'}]",2004.0,Artificial Intelligence,['Responses to catastrophic AGI risk: a survey'],1,,52.0
Defeating Dr. Evil with Self-Locating Belief,df2f75acfad5add9f6a23077a999f0d57cbd67fa,"[{'authorId': '52192352', 'name': 'A. Elga'}]",2004.0,,"['Responses to catastrophic AGI risk: a survey', 'Superintelligence: Paths, Dangers, Strategies']",2,"Dr. Evil learns that a duplicate of Dr. Evil has been created. Upon learning this, how seriously should he take the hypothesis that he himself is that duplicate? I answer: very seriously. I defend a principle of indifference for self-locating belief which entails that after Dr. Evil learns that a duplicate has been created, he ought to have exactly the same degree of belief that he is Dr. Evil as that he is the duplicate. More generally, the principle shows that there is a sharp distinction between ordinary skeptical hypotheses, and self-locating skeptical hypotheses.",97.0
The St. Thomas common sense symposium,14cb0d938ca4c3bcd299dae7b3db3e046555fdbe,"[{'authorId': '1643824899', 'name': 'MinskyMarvin'}, {'authorId': '1644583577', 'name': 'SinghPush'}, {'authorId': '1643970493', 'name': 'SlomanAaron'}]",2004.0,,['Responses to catastrophic AGI risk: a survey'],1,"To build a machine that has ""common sense"" was once a principal goal in the field of artificial intelligence. But most researchers in recent years have retreated from that ambitious aim. Instead, e...",3.0
The St. Thomas Common Sense Symposium: Designing Architectures for Human-Level Intelligence,cfb137d3c2c7ac23937625fba165e36de240912d,"[{'authorId': '1847175', 'name': 'M. Minsky'}, {'authorId': '2209381958', 'name': 'Push Singh'}, {'authorId': '145788442', 'name': 'A. Sloman'}]",2004.0,The AI Magazine,['Responses to catastrophic AGI risk: a survey'],1,"To build a machine that has ""common sense"" was once a principal goal in the field of artificial intelligence. But most researchers in recent years have retreated from that ambitious aim. Instead, each developed some special technique that could deal with some class of problem well, but does poorly at almost everything else. We are convinced, however, that no one such method will ever turn out to be ""best,"" and that instead, the powerful AI systems of the future will use a diverse array of resources that, together, will deal with a great range of problems. To build a machine that's resourceful enough to have humanlike common sense, we must develop ways to combine the advantages of multiple methods to represent knowledge, multiple ways to make inferences, and multiple ways to learn. We held a two-day symposium in St. Thomas, U.S. Virgin Islands, to discuss such a project--to develop new architectural schemes that can bridge between different strategies and representations. This article reports on the events and ideas developed at this meeting and subsequent thoughts by the authors on how to make progress.",75.0
THE IMPACT OF MEDIA EXPOSURE ON MALES BODY IMAGE,32829b398ae6e750cb1b1eebab0ebc69bf82eba5,"[{'authorId': '116302756', 'name': 'Daniel Agliata'}, {'authorId': '1398176475', 'name': 'S. Tantleff-Dunn'}]",2004.0,,['Responses to catastrophic AGI risk: a survey'],1,"Abstract Mass media are believed to be a pervasive force in shaping physical appearance ideals and have been shown to negatively impact females' body image. Little research has attended to the effects of media exposure on males' body image. The current experiment exposed 158 males to television advertisements containing either ideal male images or neutral images that were inserted between segments of a television program. Participants were blocked on dispositional body image and attitudes toward appearance variables to assess for moderating effects. Results indicated that participants exposed to ideal image advertisements became significantly more depressed and had higher levels of muscle dissatisfaction than those exposed to neutral ads. Inconsistent with past research, no dispositional effects were noted that would suggest the influence of schematicity on mood and body image changes.",465.0
"Cyborg morals, cyborg values, cyborg ethics",dbed71f1e6986c47cbc6a5613c645dba9520d0f7,"[{'authorId': '143636026', 'name': 'K. Warwick'}]",2003.0,Ethics and Information Technology,['Responses to catastrophic AGI risk: a survey'],1,,117.0
"Death and Anti-Death, Volume 2: Two Hundred Years After Kant, Fifty Years After Turing",4e1fab8c8593a9ffac0c3e62c2eb5b8fed77dfb8,"[{'authorId': '2193691', 'name': 'N. Bostrom'}, {'authorId': '144300057', 'name': 'R. C. Ettinger'}, {'authorId': '50082254', 'name': 'C. Tandy'}]",2004.0,,['Responses to catastrophic AGI risk: a survey'],1,,9.0
Catastrophe: Risk and Response,7227cd7eca7061ea2fc86923020652b6990e8f83,"[{'authorId': '1980312', 'name': 'R. Posner'}]",2004.0,,['Responses to catastrophic AGI risk: a survey'],1,"INTRODUCTION What is catastrophe? The organization of this book Some useful disctinctions 1. WHAT ARE THE CATASTROHPIC RISKS, AND HOW CATASTROPHIC ARE THEY? Natural catastrophes Scientific accidents Other unintended man-made catastrophes Intentional catastrophes Catastrophic synergies and lesser-included catastrophes 2. WHY SO LITTLE IS BEING DONE ABOUT THE CATASTROPHIC RISKS Cultural factors Psychological factors Economic factors 3. HOW TO EVALUATE THE CATASTROPHIC RISKS AND THE POSSIBLE RESPONSES TO THEM The difference cost-benefit analysis can make: the case of RHIC A modest version of the precautionary principle Discounting to present value Taxes, subsidies, and options: the case of global warming Valuing human lives Risk versus uncertainty Coping with uncertainty Politics, expertise, and neutrality: RHIC revisited Summary 4. HOW TO REDUCE THE CATASTROPHIC RISKS Institutional reforms Fiscal tools: a recap Some hypothetical regulatory policies CONCLUSION Notes Index",211.0
The Implicit Volition Model: On the Preconscious Regulation of Temporarily Adopted Goals,07192330dd7e3b49cf5e160139d14abf8994122f,"[{'authorId': '4068378', 'name': 'G. Moskowitz'}, {'authorId': '21838746', 'name': 'Peizhong Li'}, {'authorId': '46726933', 'name': 'Elizabeth R. Kirk'}]",2004.0,,['Responses to catastrophic AGI risk: a survey'],1,,130.0
Extensionally defining principles and cases in ethics: An AI model,2a6503bcbe9012d8a46bf01051a42f10f8ec47ca,"[{'authorId': '1706869', 'name': 'B. McLaren'}]",2003.0,Artificial Intelligence,['Responses to catastrophic AGI risk: a survey'],1,,84.0
"Why Richard Brandt Does Not Need Cognitive Psychotherapy, and Other Glad News about Idealized Preference Theories in Meta-Ethics",86e00daa451bf16a0a8e980c7d7f52f33ab547f2,"[{'authorId': '143905859', 'name': 'David Zimmerman'}]",2003.0,,['Responses to catastrophic AGI risk: a survey'],1,,10.0
Artificial intelligence and the real world,e3e6d1137279fbe546b80fda152aa4fbc790c6ff,"[{'authorId': '80946658', 'name': 'A. Jenkins'}]",2003.0,,['Responses to catastrophic AGI risk: a survey'],1,,9.0
Inside the Slammer Worm,82d5304042597f7e15ba0d1b77264f5efd0e7895,"[{'authorId': '145227606', 'name': 'D. Moore'}, {'authorId': '1744800', 'name': 'V. Paxson'}, {'authorId': '1727599', 'name': 'S. Savage'}, {'authorId': '90588333', 'name': 'C. Shannon'}, {'authorId': '1403815050', 'name': 'Stuart Staniford-Chen'}, {'authorId': '1685066', 'name': 'N. Weaver'}]",2003.0,IEEE Security and Privacy,['Responses to catastrophic AGI risk: a survey'],1,"The Slammer worm spread so quickly that human response was ineffective. In January 2003, it packed a benign payload, but its disruptive capacity was surprising. Why was it so effective and what new challenges do this new breed of worm pose?.",1067.0
Code-Red: a case study on the spread and victims of an internet worm,f16e688006f464060d83567d66a53dc77e86ea90,"[{'authorId': '145227606', 'name': 'D. Moore'}, {'authorId': '90588333', 'name': 'C. Shannon'}, {'authorId': '10179735', 'name': 'K. Claffy'}]",2002.0,International Memory Workshop,['Responses to catastrophic AGI risk: a survey'],1,"On July 19, 2001, more than 359,000 computers connected to the Internet were infected with the Code-Red (CRv2) worm in less than 14 hours. The cost of this epidemic, including subsequent strains of Code-Red, is estimated to be in excess of $2.6 billion. Despite the global damage caused by this attack, there have been few serious attempts to characterize the spread of the worm, partly due to the challenge of collecting global information about worms. Using a technique that enables global detection of worm spread, we collected and analyzed data over a period of 45 days beginning July 2nd, 2001 to determine the characteristics of the spread of Code-Red throughout the Internet.In this paper, we describe the methodology we use to trace the spread of Code-Red, and then describe the results of our trace analyses. We first detail the spread of the Code-Red and CodeRedII worms in terms of infection and deactivation rates. Even without being optimized for spread of infection, Code-Red infection rates peaked at over 2,000 hosts per minute. We then examine the properties of the infected host population, including geographic location, weekly and diurnal time effects, top-level domains, and ISPs. We demonstrate that the worm was an international event, infection activity exhibited time-of-day effects, and found that, although most attention focused on large corporations, the Code-Red worm primarily preyed upon home and small business users. We also qualified the effects of DHCP on measurements of infected hosts and determined that IP addresses are not an accurate measure of the spread of a worm on timescales longer than 24 hours. Finally, the experience of the Code-Red worm demonstrates that wide-spread vulnerabilities in Internet hosts can be exploited quickly and dramatically, and that techniques other than host patching are required to mitigate Internet worms.",936.0
Asimov's laws: Current progress,4b7c78c080d309288faa763846bac36e5739f489,"[{'authorId': '1410751259', 'name': 'D. Gordon-Spears'}]",2002.0,,['Responses to catastrophic AGI risk: a survey'],1,,3.0
Strangers to Ourselves: Discovering the Adaptive Unconscious,d351d456a809bb0b7849350b02b857d96cb3ef5c,"[{'authorId': '144882054', 'name': 'T. Wilson'}]",2002.0,,['Responses to catastrophic AGI risk: a survey'],1,"Preface 1. Freud's Genius, Freud's Myopia 2. The Adaptive Unconscious 3. Who's in Charge? 4. Knowing Who We Are 5. Knowing Why 6. Knowing How We Feel 7. Knowing How We Will Feel 8. Introspection and Self-Narratives 9. Looking Outward to Know Ourselves 10. Observing and Changing Our Behavior Notes Bibliography Index",1259.0
Sousveillance: Inventing and Using Wearable Computing Devices for Data Collection in Surveillance Environments.,04f1608196173e9c9642a42b3394206f5bc2a784,"[{'authorId': '145558934', 'name': 'Steve Mann'}, {'authorId': '145935952', 'name': 'Jason Nolan'}, {'authorId': '1736470', 'name': 'B. Wellman'}]",2002.0,,['Responses to catastrophic AGI risk: a survey'],1,"This paper describes using wearable computing devices to perform ""sousveillance"" (inverse surveillance) as a counter to organizational surveillance. A variety of wearable computing devices generated different kinds of responses, and allowed for the collection of data in different situations. Visible sousveillance often evoked counter-performances by front-line surveillance workers. The juxtaposition of sousveillance with surveillance generates new kinds of information in a social surveillance situation.",633.0
How to Own the Internet in Your Spare Time,664ee802aec50176ad0861307436654bbc288134,"[{'authorId': '1403815050', 'name': 'Stuart Staniford-Chen'}, {'authorId': '1744800', 'name': 'V. Paxson'}, {'authorId': '1685066', 'name': 'N. Weaver'}]",2002.0,USENIX Security Symposium,['Responses to catastrophic AGI risk: a survey'],1,"The ability of attackers to rapidly gain control of vast numbers of Internet hosts poses an immense risk to the overall security of the Internet. Once subverted, these hosts can not only be used to launch massive denial of service floods, but also to steal or corrupt great quantities of sensitive information, and confuse and disrupt use of the network in more subtle ways. We present an analysis of the magnitude of the threat. We begin with a mathematical model derived from empirical data of the spread of Code Red I in July, 2001. We discuss techniques subsequently employed for achieving greater virulence by Code Red II and Nimda. In this context, we develop and evaluate several new, highly virulent possible techniques: hit-list scanning (which creates a Warhol worm), permutation scanning (which enables self-coordinating scanning), and use of Internet-sized hit-lists (which creates a flash worm).",1348.0
The conscious access hypothesis: origins and recent evidence,6e67b4cd042b0e06611db23596c59869df78a1a5,"[{'authorId': '2529891', 'name': 'B. Baars'}]",2002.0,Trends in Cognitive Sciences,['Responses to catastrophic AGI risk: a survey'],1,,881.0
The effect of experimental presentation of thin media images on body satisfaction: a meta-analytic review.,660e5d06aa0ca86942cccc069c7b66b75b6a836c,"[{'authorId': '6678390', 'name': 'L. Groesz'}, {'authorId': '37444170', 'name': 'M. Levine'}, {'authorId': '6473723', 'name': 'S. Murnen'}]",2002.0,International Journal of Eating Disorders,['Responses to catastrophic AGI risk: a survey'],1,"OBJECTIVE
The effect of experimental manipulations of the thin beauty ideal, as portrayed in the mass media, on female body image was evaluated using meta-analysis.


METHOD
Data from 25 studies (43 effect sizes) were used to examine the main effect of mass media images of the slender ideal, as well as the moderating effects of pre-existing body image problems, the age of the participants, the number of stimulus presentations, and the type of research design.


RESULTS
Body image was significantly more negative after viewing thin media images than after viewing images of either average size models, plus size models, or inanimate objects. This effect was stronger for between-subjects designs, participants less than 19 years of age, and for participants who are vulnerable to activation of a thinness schema.


CONCLUSION
Results support the sociocultural perspective that mass media promulgate a slender ideal that elicits body dissatisfaction. Implications for prevention and research on social comparison processes are considered.",1707.0
Protecting the endangered human: toward an international treaty prohibiting cloning and inheritable alterations.,1fc667d3e877c97944bd382ae2246777350e5c75,"[{'authorId': '152595490', 'name': 'G. Annas'}, {'authorId': '4116769', 'name': 'L. Andrews'}, {'authorId': '6461463', 'name': 'R. Isasi'}]",2002.0,American Journal of Law & Medicine,['Responses to catastrophic AGI risk: a survey'],1,"I. INTRODUCTION We humans tend to worry first about our own happiness, then about our families, then about our communities. In times of great stress, such as war or natural disaster, we may focus temporarily on our country but we rarely think about Earth as a whole or the human species as a whole. This narrow perspective, perhaps best exemplified by the American consumer, has led to the environmental degradation of our planet, a grossly widening gap in living standards between rich and poor people and nations and a scientific research agenda that focuses almost exclusively on the needs and desires of the wealthy few. Reversing the worldwide trends toward market-based atomization and increasing indifference to the suffering of others will require a human rights focus, forged by the development of what Vaclav Havel has termed a ""species consciousness.""1 In this Article we discuss human cloning and inheritable genetic alterations from the human species perspective, and suggest language for a proposed international ""Convention of the Preservation of the Human Species"" that would outlaw all efforts to initiate a pregnancy by using either intentionally modified genetic material or human replication cloning, such as through somatic cell nuclear transfer. We summarize international legal action in these areas over the past five years, relate these actions to arguments for and against a treaty and conclude with an action plan. B. HUMAN RIGHTS AND THE HUMAN SPECIES The development of the atomic bomb not only presented to the world for the first time the prospect of total annihilation, but also, paradoxically, led to a renewed emphasis on the ""nuclear family,"" complete with its personal bomb shelter. The conclusion of World War II (with the dropping of the only two atomic bombs ever used in war) led to the recognition that world wars were now suicidal to the entire species and to the formation of the United Nations with the primary goal of preventing such wars.2 Prevention, of course, must be based on the recognition that all humans are fundamentally the same, rather than on an emphasis on our differences. In the aftermath of the Cuban missile crisis, the closest the world has ever come to nuclear war, President John F. Kennedy, in an address to the former Soviet Union, underscored the necessity for recognizing similarities for our survival: [L]et us not be blind to our differences, but let us also direct attention to our common interests and the means by which those differences can be resolved .... For, in the final analysis, our most basic common link is that we all inhabit this small planet. We all breathe the same air. We all cherish our children's future. And we are all mortal.3 That we are all fundamentally the same, all human, all with the same dignity and rights, is at the core of the most important document to come out of World War II, the Universal Declaration of Human Rights, and the two treaties that followed it (together known as the ""International Bill of Rights"").4 The recognition of universal human rights, based on human dignity and equality as well as the principle of nondiscrimination, is fundamental to the development of a species consciousness. As Daniel Lev of Human Rights Watch/Asia said in 1993, shortly before the Vienna Human Rights Conference: Whatever else may separate them, human beings belong to a single biological species, the simplest and most fundamental commonality before which the significance of human differences quickly fades .... We are all capable, in exactly the same ways, of feeling pain, hunger, and a hundred kinds of deprivation. Consequently, people nowhere routinely concede that those with enough power to do so ought to be able to kill, torture, imprison, and generally abuse others .... The idea of universal human rights shares the recognition of one common humanity. and provides a minimum solution to deal with its miseries.5 Membership in the human species is central to the meaning and enforcement of human rights, and respect for basic human rights is essential for the survival of the human species. …",173.0
The Blank Slate: The Modern Denial of Human Nature,a1965c3ec240d1841300df9226fa389fa3117be7,"[{'authorId': '2693903', 'name': 'S. Pinker'}]",2002.0,,['Responses to catastrophic AGI risk: a survey'],1,"The blank slate, the noble savage and the ghost in the machine: the official theory silly putty the last wall to fall culture vultures the slate's last stand. Fear and loathing: political scientists the Holy Trinity. Human nature with a human face: the fear of inequality the fear of imperfectability the fear of determinism the fear of nihilism. Know thyself: in touch with reality out of our depths the many roots of our suffering the sanctimonious animal. Hot buttons: politics violence gender children the arts. The voice of the species. Appendix: Donald E. Brown's list of human universals.",2438.0
Super-intelligent machines,d4d9a80708d592ab2e48cff3508791b4e142fe52,"[{'authorId': '39109261', 'name': 'B. Hibbard'}]",2012.0,COMG,['Responses to catastrophic AGI risk: a survey'],1,,47.0
Revisiting Asimov's First Law: A Response to the Call to Arms,7497899f705a2b1e32d7ae94b66b12a33e11e9dc,"[{'authorId': '1748597', 'name': 'David V. Pynadath'}, {'authorId': '143736701', 'name': 'Milind Tambe'}]",2001.0,ATAL,['Responses to catastrophic AGI risk: a survey'],1,,27.0
Digital immortality,22558bffa07aae11242823eff357574d33d3a2a8,"[{'authorId': '144620977', 'name': 'G. Bell'}, {'authorId': '39941882', 'name': 'J. Gray'}]",2001.0,CACM,['Responses to catastrophic AGI risk: a survey'],1,"Digital immortality, like ordinary immortality, is a continuum from enduring fame at one end to endless experience and learning at the other, stopping just short of endless life . Preserving and transmitting your ideas is one-way immortality : allowing communication with the future. Endless experience and leaning is two-way immortality : allowing “you,” or at least part of you, to communicate with the future in the sense that artifact continues to learn and evolve. Current technology can extend corporal life for a few decades. Both one-way and two-way immortality require part of a person to be converted to information ( Cyberized ), and stored in a more durable media. We believe that two-way immortality where one’s experiences are digitally preserved, and which then take on a life of their own will be possible within the this century.",53.0
The Evolution of Strategies in the Iterated Prisoner's Dilemma,d870231e2ed338deae95fb0a88cab7fa71b07555,"[{'authorId': '145012155', 'name': 'R. Axelrod'}]",2001.0,,['Responses to catastrophic AGI risk: a survey'],1,,741.0
Relinquishment or Regulation: Dealing with Apocalyptic Technological Threats,c3f626721a413466cf9f145127b95a2eee556912,"[{'authorId': '144426195', 'name': 'J. Hughes'}]",2001.0,,['Responses to catastrophic AGI risk: a survey'],1,,4.0
TOP500 Supercomputer sites 11/2000,b6c096f5dba050868cae25669c0745e86221218f,"[{'authorId': '1706139', 'name': 'H. Meuer'}, {'authorId': '1749908', 'name': 'E. Strohmaier'}, {'authorId': '1708869', 'name': 'J. Dongarra'}, {'authorId': '35124559', 'name': 'H. Simon'}]",2000.0,,['Responses to catastrophic AGI risk: a survey'],1,"To provide a better basis for statistics on high-performance computers, we list the sites that have the 500 most powerful computer systems installed. The best Linpack benchmark performance achieved is used as a performance measure in ranking the computers.",51.0
Timing attacks on Web privacy,70f340e80468832b7a293da8a4f1d08ed2786448,"[{'authorId': '1752733', 'name': 'E. Felten'}, {'authorId': '2139718754', 'name': 'Michael A. Schneider'}]",2000.0,Conference on Computer and Communications Security,['Responses to catastrophic AGI risk: a survey'],1,"We describe a class of attacks that can compromise the privacy of users’ Web-browsing histories. The attacks allow a malicious Web site to determine whether or not the user has recently visited some other, unrelated Web page. The malicious page can determine this information by measuring the time the user’s browser requires to perform certain operations. Since browsers perform various forms of caching, the time required for operations depends on the user’s browsing history; this paper shows that the resulting time variations convey enough information to compromise users’ privacy. This attack method also allows other types of information gathering by Web sites, such as a more invasive form of Web “cookies”. The attacks we describe can be carried out without the victim’s knowledge, and most “anonymous browsing” tools fail to prevent them. Other simple countermeasures also fail to prevent these attacks. We describe a way of reengineering browsers to prevent most of them.",333.0
Psychopathy and the predictive validity of the PCL-R: an international perspective.,028234697b13d0be1fc9eb8bc7aec7dddbb99971,"[{'authorId': '40110267', 'name': 'R. Hare'}, {'authorId': '2087098417', 'name': 'D. Clark'}, {'authorId': '5283023', 'name': 'M. Grann'}, {'authorId': '50168449', 'name': 'D. Thornton'}]",2000.0,Behavioral sciences & the law,['Responses to catastrophic AGI risk: a survey'],1,"Its controversial past notwithstanding, psychopathy has emerged as one of the most important clinical constructs in the criminal justice and mental health systems. One reason for the surge in theoretical and applied interest in the disorder is the development and widespread adoption of reliable and valid methods for its measurement. The Hare PCL-R provides researchers and clinicians with a common metric for the assessment of psychopathy, and has led to a surge in replicable and meaningful findings relevant to the issue of risk for recidivism and violence, among other things. Most of the research thus far has been based on North American samples of offenders and forensic psychiatric patients. We summarize this research and compare it with findings from several other countries, including England and Sweden. We conclude that the ability of the PCL-R to predict recidivism, violence, and treatment outcome has considerable cross-cultural generalizability, and that the PCL-R and its derivatives play a major role in the understanding and prediction of crime and violence.",655.0
Prolegomena to any future artificial moral agent,67ca442b1662171733d234e52b3c4af929619c45,"[{'authorId': '30308222', 'name': 'C. Allen'}, {'authorId': '145447621', 'name': 'G. Varner'}, {'authorId': '4324210', 'name': 'Jason Zinser'}]",2000.0,Journal of experimental and theoretical artificial intelligence (Print),['Responses to catastrophic AGI risk: a survey'],1,"As artificial intelligence moves ever closer to the goal of producing fully autonomous agents, the question of how to design and implement an artificial moral agent (AMA) becomes increasingly pressing. Robots possessing autonomous capacities to do things that are useful to humans will also have the capacity to do things that are harmful to humans and other sentient beings. Theoretical challenges to developing artificial moral agents result both from controversies among ethicists about moral theory itself, and from computational limits to the implementation of such theories. In this paper the ethical disputes are surveyed, the possibility of a ‘moral Turing Test’ is considered and the computational difficulties accompanying the different types of approach are assessed. Human-like performance, which is prone to include immoral actions, may not be acceptable in machines, but moral perfection may be computationally unattainable. The risks posed by autonomous machines ignorantly or deliberately harming people and other sentient beings are great. The development of machines with enough intelligence to assess the effects of their actions on sentient beings and act accordingly may ultimately be the most important task faced by the designers of artificially intelligent automata.",286.0
Connectionist Modeling of Language: Examples and Implications,873b5baec9165b69af85aed7edab6612e10ad09b,"[{'authorId': '2546518', 'name': 'D. Plaut'}]",2000.0,,['Responses to catastrophic AGI risk: a survey'],1,"Researchers interested in human cognitive processes have long used computer simulations to try to identify the principles of cognition. The strategy has been to build computational models that embody putative principles and then to examine how well such models capture human performance in cognitive tasks. Until the early 1980’s, this effort was undertaken largely within the context of the “computer metaphor” of mind. Researchers built computational models based on the conceptualization that the human mind operated as though it were a conventional digital computer. However, with the advent of so-called connectionist, neural network, or parallel distributed processing models (Anderson, Silverstein, Ritz, & Jones, 1977; Hinton & Anderson, 1981; McClelland & Rumelhart, 1981; McClelland, Rumelhart, & PDP Research Group, 1986; Rumelhart, McClelland, & PDP Research Group, 1986), researchers began exploring the implications of principles that are more broadly consistent with the style of computation employed by the brain. In connectionist models, cognitive processes take the form of cooperative and competitive interactions among large numbers of simple, neuron-like processing units (see Figure 1). Unit interactions are governed by weighted connections that collectively encode the long-term knowledge of the system. The activity of some of the units encodes the input to the system; the resulting activity of other units encodes the system’s response to that input. The patterns of activity of the remaining so-called hidden units constitute learned internal representations that mediate between inputs and outputs. Learning involves modifying the values of connection weights based on feedback from the environment on the accuracy of the system’s responses. While each unit exhibits non-linear spatial and temporal summation, units and connections are not generally considered to be in one-to-one correspondence with actual neurons and synapses. Rather, connectionist systems attempt to capture the essential computational properties of the vast ensembles of real neuronal elements found in the brain, through simulations of smaller networks of units. In this way, the approach is distinct from computational neuroscience (Sejnowski, Koch, & Church",14.0
Seeing Like a State: How Certain Schemes to Improve the Human Condition Have Failed,1850169cbbf7e2cd8b33caa95acbd7d7d3a2b1db,"[{'authorId': '47551603', 'name': 'James C. Scott'}]",1999.0,,['Responses to catastrophic AGI risk: a survey'],1,"In this wide-ranging and original book, James C. Scott analyzes failed cases of large-scale authoritarian plans in a variety of fields. He argues that centrally managed social plans derail when they impose schematic visions that do violence to complex interdependencies that are not -- and cannot be -- fully understood. Further the success of designs for social organization depends on the recognition that local, practical knowledge is as important as formal, epistemic knowledge. The author builds a persuasive case against ""development theory"" and imperialistic state planning that disregards the values, desires, and objections of its subjects. And in discussing these planning disasters, he identifies four conditions common to them all: the state's attempt to impose administrative order on nature and society; a high-modernist ideology that believes scientific intervention can improve every aspect of human life; a willingness to use authoritarian state power to effect large-scale innovations; and a prostrate civil society that cannot effectively resist such plans.",7567.0
The Transparent Society,9641f17cff108cc1f158a4c1d689623fafd7466b,"[{'authorId': '144575951', 'name': 'D. Brin'}]",1998.0,,['Responses to catastrophic AGI risk: a survey'],1,,104.0
Introduction to Connectionist Modelling of Cognitive Processes,973d33d6958869c46bf4171aa4d89051eb85d399,"[{'authorId': '144118800', 'name': 'P. McLeod'}, {'authorId': '2556348', 'name': 'K. Plunkett'}, {'authorId': '144663088', 'name': 'E. Rolls'}]",1998.0,,['Responses to catastrophic AGI risk: a survey'],1,"Introduction 1. The basics of connectionist information processing 2. The appeal of parallel distributed processing for modelling cognition 3. Pattern association 4. Auto association 5. Training a multi-layer network with an error signal: hidden units and backpropagation 6. Competitive networks 7. Recurrent networks 8. Reading aloud 9. Language acquisition 10. Connectionism and cognitive development 11. Connectionist neuropsychology - lesioning neural networks 12. Mental representation: Rules, symbols, and connectionist networks 13. Network models of brain function 14. Evolutionary connectionism 15. A selective history of connectionism before 1986 Appendices 1-4 References",285.0
In the mind of the machine: The breakthrough in artificial intelligence,1347801c7cb34385c25b2c752fd6ca6e33dfa2f3,"[{'authorId': '143636026', 'name': 'K. Warwick'}]",1998.0,,['Responses to catastrophic AGI risk: a survey'],1,,9.0
When will computer hardware match the human brain,2f2f5dbfc9641eefa862bd61858776385989bae0,"[{'authorId': '2066894', 'name': 'Hans P. Moravec'}]",1998.0,,['Responses to catastrophic AGI risk: a survey'],1,"This paper describes how the performance of AI machines tends to improve at the same pace that AI researchers get access to faster hardware. The processing power and memory capacity necessary to match general intellectual performance of the human brain are estimated. Based on extrapolation of past trends and on examination of technologies under development, it is predicted that the required hardware will be available in cheap machines in the 2020s.",236.0
Top500 Supercomputer Sites,f3093ed17dcf207c0eae5915c88371f1dfc80d17,"[{'authorId': '1708869', 'name': 'J. Dongarra'}, {'authorId': '1706139', 'name': 'H. Meuer'}, {'authorId': '1749908', 'name': 'E. Strohmaier'}]",1997.0,,['Responses to catastrophic AGI risk: a survey'],1,"To provide a better basis for statistics on high-performance computers, we list the sites that have the 500 most powerful computer systems installed. The best Linpack benchmark performance achieved is used as a performance measure in ranking the computers.",775.0
Weaving Technology and Policy Together to Maintain Confidentiality,187352fbcd30ea1fe2094f33e2d7100133964840,"[{'authorId': '144851214', 'name': 'L. Sweeney'}]",1997.0,"Journal of Law, Medicine & Ethics",['Responses to catastrophic AGI risk: a survey'],1,"Organizations often release and receive medical data with all explicit identifiers, such as name, address, telephone number, and Social Security number (SSN), removed on the assumption that patient confidentiality is maintained because the resulting data look anonymous. However, in most of these cases, the remaining data can be used to reidenafy individuals by linking or matching the data to other data bases or by looking at unique characteristics found in the fields and records of the data base itself. When these less apparent aspects are taken into account, each released record can map to many possible people, providing a level of anonymity that the recordholder determines. The greater the number of candidates per record, the more anonymous the data. I examine three general-purpose computer programs for maintaining patient confidentiality when disclosing electronic medical records: the Scrub System, which locates and suppresses or replaces personally identifying information in letters between doctors and in notes written by clinicians; the Datafly System, which generalizes values based on a profile of the data recipient at the time of disclosure; and the μ-Argus System, a somewhat similar system which is becoming a European standard for disclosing public use data.",383.0
Darwin among the machines,5cc8c802003fe8e962464223ac161eb3b7b45f24,"[{'authorId': '28719995', 'name': 'George B. Dyson'}]",1997.0,,['Responses to catastrophic AGI risk: a survey'],1,Leviathan Darwin among the machines the general wind on computable numbers the proving ground rats in a cathedral symbiogenesis on distributed communications theory of games and economic behaviour there's plenty of room at the top last and first men fiddling while Rome burns.,62.0
"Intelligent Machinery, A Heretical Theory*",650d4e7c4fbd68b05f87a46985f1da6f2f4f0ed8,"[{'authorId': '2262347', 'name': 'A. Turing'}]",1996.0,,['Responses to catastrophic AGI risk: a survey'],1,"The analysis of interrupts is a structured quagmire. In this position paper, we demonstrate the investigation of simulated annealing. KaliNil, our new methodology for DNS, is the solution to all of these problems.",214.0
"Reflections on Artificial Intelligence: The Legal, Moral and Ethical Dimensions",4c7aadd581c600c1c63bad005bf4e0afcebd9936,"[{'authorId': '145708034', 'name': 'Blay Whitby'}]",1996.0,,['Responses to catastrophic AGI risk: a survey'],1,"For many centuries, powerful ideas which reshape our lives, have come from religion and the arts; nowadays, they are coming mainly from science and technology. This book is written with the conviction that these ideas must be discussed widely because almost every human takes some decisions which will shape the impact of technology on our lives. The author argues that new computer technologies are not obvious, noisy, grand and spectacular as in the past but are largely silent and unseen, whether under the bonnet of a car or in a household washing machine, and the less tangible effects are being neglected - especially with anything involving AI. The book is multidisciplinary, but a broad-based expertise is not required by the reader. There are references to complexity theory, conceptual dependency, and constitutional law, but the reader does not need to be an expert in all or any of these areas.",17.0
Towards the ethical robot,fecd8d939e31807fa91e6a3091eb08b782b0003a,"[{'authorId': '1765512', 'name': 'J. Gips'}]",1995.0,,['Responses to catastrophic AGI risk: a survey'],1,,133.0
Reasoning with Reasons in Case-Based Comparisons,e1840d9352b49788036e600aba81750f4e13ea2a,"[{'authorId': '1770311', 'name': 'Kevin D. Ashley'}, {'authorId': '1706869', 'name': 'B. McLaren'}]",1995.0,International Conference on Case-Based Reasoning,['Responses to catastrophic AGI risk: a survey'],1,,38.0
"What Might Cognition Be, If Not Computation?",97ab306d1874e3ec96c7ea88dfc9d12c07860324,"[{'authorId': '145019889', 'name': 'T. Gelder'}]",1995.0,,['Responses to catastrophic AGI risk: a survey'],1,"L'A. propose une alternative au modele computationnel du systeme cognitif qui consiste en un systeme dynamique, definissant les processus cognitifs comme une evolution spatio-temporelle a l'interieur meme du systeme. L'A. developpe un modele post-cartesien qui transcende la representation mentale du monde externe par un monde interne et lui substitue une interaction entre l'agent et le monde exterieur",982.0
The Rediscovery of the Mind,3a1247a82280a70716c6bbe406570353f7d292e4,"[{'authorId': '2364575', 'name': 'J. Batali'}, {'authorId': '34493294', 'name': 'J. Searle'}]",1992.0,Artificial Intelligence,['Responses to catastrophic AGI risk: a survey'],1,,2845.0
The First Law of Robotics (A Call to Arms),4e1a8d5159eb47b959ccedde98eee84a1f3b8275,"[{'authorId': '1780531', 'name': 'Daniel S. Weld'}, {'authorId': '1741101', 'name': 'Oren Etzioni'}]",1994.0,AAAI Conference on Artificial Intelligence,['Responses to catastrophic AGI risk: a survey'],1,,136.0
Full Information Accounts of Well-Being,7aa915494043c3b0f5299307dff4a0d6921cac9c,"[{'authorId': '144423937', 'name': 'David Sobel'}]",1994.0,"Ethics: An International Journal of Social, Political, and Legal Philosophy",['Responses to catastrophic AGI risk: a survey'],1,"La theorie de la decision pose le probleme d'une preference dont l'A. montre qu'elle reclamerait une connaissance de toutes les consequences d'une action par rapport a une autre, de maniere a ce que le choix se fasse en fonction de la plus grande utilite pour son bien-etre propre",159.0
Asimov's Laws of Robotics: Implications for Information Technology - Part 2,7ae846982f2783783c81735e9a541505d7572fb5,"[{'authorId': '121702055', 'name': 'R. Clarke'}]",1993.0,Computer,['Responses to catastrophic AGI risk: a survey'],1,"For part 1 see ibid., Dec 1993, p53-61. Isaac Asimov's Laws of Robotics, first formulated in 1940. were primarily a literary device intended to support a series of stories about robot behavior. Over time, he found that the three Laws included enough apparent inconsistencies, ambiguity. and uncertainty to provide the conflicts required for a great many stories. In examining the ramifications of these laws. Asimov revealed problems that might later confront real roboticists and information technologists attempting to establish rules for the behavior of intelligent machines. As information technology evolves and machines begin to design and build other machines, the issue of human control gains greater significance. In time, human values tend to change; the rules reflecting these values, and embedded in existing robotic devices, may need to be modified. But if they are implicit rather than explicit, with their effects scattered widely across a system, they may not be easily replaceable. Asimov himself discovered many contradictions and eventually revised the Laws of Robotics. >",153.0
Strangers to Ourselves.,824003fb79ba6489146447d222efa73be4f445ad,"[{'authorId': '79273193', 'name': 'M. Evans'}, {'authorId': '69922975', 'name': 'J. Kristeva'}, {'authorId': '103325509', 'name': 'L. S. Roudiez'}]",1992.0,,['Responses to catastrophic AGI risk: a survey'],1,"1 Toccata and Fugue for the Foreigner2 The Greeks Among Barbarians, Suppliants, and Metics3 The Chosen People and the Choice of Foreignness4 Paul and Augustine: The Therapeutics of Exile and Pilgrimage5 By What Right are Are You a Foreigner?6 The Renaissance, ""So Shapeless and Diverse in Composition""7 On Foreigners and the Enlightenment8 Might Not Universality Be... Our Own Foreignness?9 In Practice...Index",1423.0
Human agency and responsible computing: Implications for computer system design,02ecd97a1be04225406cd8f46bcdd8474085e307,"[{'authorId': '144029598', 'name': 'Batya Friedman'}, {'authorId': '144674612', 'name': 'P. H. Kahn'}]",1992.0,Journal of Systems and Software,['Responses to catastrophic AGI risk: a survey'],1,,145.0
Controlling Super-Intelligent Machines,d432fddf8185acee839aa70229429e145f664c61,"[{'authorId': '1689647', 'name': 'Peter D. Turney'}]",1991.0,,['Responses to catastrophic AGI risk: a survey'],1,,8.0
Cognitive Wheels: The Frame Problem of AI,d3ba026e1a6b4995d465ec167ff9a9f8666486e4,"[{'authorId': '3329205', 'name': 'D. Dennett'}]",1990.0,The Philosophy of Artificial Intelligence,['Responses to catastrophic AGI risk: a survey'],1,"Once upon a time there was a robot, named R1 by its creators. Its only task was to fend for itself. One day its designers arranged for it to learn that its spare battery, its precious energy supply, was locked in a room with a time bomb set to go off soon. R1 located the room, and the key to the door, and formulated a plan to rescue its battery. There was a wagon in the room, and the battery was on the wagon, and R1 hypothesized that a certain action which it called PULLOUT (Wagon, Room, t) would result in the battery being removed from the room. Straightaway it acted, and did succeed in getting the battery out of the room before the bomb went off. Unfortunately, however, the bomb was also on the wagon. R1 knew that the bomb was on the wagon in the room, but didn’t realize that pulling the wagon would bring the bomb out along with the battery. Poor R1 had missed that obvious implication of its planned act. Back to the drawing board. ‘The solution is obvious,’ said the designers. ‘Our next robot must be made to recognize not just the intended implications of its acts, but also the implications about their side-effects, by deducing these implications from the descriptions it uses in formulating its plans.’ They called their next model, the robot-deducer, R1D1. They placed R1D1 in much the same predicament that R1 had succumbed to, and as it too hit upon the idea of PULLOUT (Wagon, Room, t) it began, as designed, to consider the implications of such a course of action. It had just finished deducing that pulling the wagon out of the room would not change the colour of the room’s walls, and was embarking on a proof of the further implication that pulling the wagon out would cause its wheels to turn more revolutions than there were wheels on the wagon—when the bomb exploded. Back to the drawing board. ‘We must teach it the difference between relevant implications and irrelevant implications,’ said the designers, ‘and teach it to ignore the irrelevant ones.’ So they developed a method of tagging implications as either relevant or irrelevant to the project at hand, and installed the method in their next model, the robot-relevant-deducer, or R2D1 for short. When they subjected R2D1 to the test that had so unequivocally selected its ancestors for extinction, they were surprised to see it sitting, Hamlet-like, outside the room containing the ticking bomb, the native hue of its resolution sicklied o’er with the pale cast of thought, as Shakespeare (and more recently Fodor) has aptly put it. ‘Do something!’ they yelled at it. ‘I am,’ it retorted. ‘I’m busily ignoring some thousands of implications I have determined to be irrelevant. Just as soon as I find an irrelevant implication, I put it on the list of those I must ignore, and...’ the bomb went off. All these robots suffer from the frame problem. If there is ever to be a robot with the fabled perspicacity and real-time adroitness of R2D2, robot-designers must solve the frame problem. It appears at first to be at best an annoying technical embarrassment in robotics, or merely a curious puzzle for the bemusement of people working in Artificial Intelligence (AI). I think, on the contrary, that it is a new, deep epistemological problem—accessible in principle but unnoticed by generations of philosophers—brought to light by the novel methods of AI, and still far from being solved. Many people in AI have come to have a similarly high regard for the seriousness of the frame problem. As one researcher has quipped, ‘We have given up the goal of designing an intelligent robot, and turned to the task of designing a gun that will destroy any intelligent robot that anyone else designs!’ I will try here to present an elementary, non-technical, philosophical introduction to the frame problem, and show why it is so interesting. I have no solution to offer, or even any original suggestions for where a solution might lie. It is hard enough, I have discovered, just to say clearly what the frame problem is—and is not. In fact, there is less than perfect agreement in usage within the AI research community. McCarthy and Hayes, who coined the term, use it to refer to a particular, narrowly conceived problem about representation that arises only for certain strategies for dealing with a broader problem about real-time planning systems. Others call this broader problem the frame problem-‘the whole pudding,’ as Hayes has called it (personal correspondence)—and this may not be mere terminological sloppiness. If ‘solutions’ to the narrowly conceived problem have the effect of driving a (deeper) difficulty into some other quarter of the broad problem, we might better reserve the title for this hard-to-corner difficulty. With apologies to McCarthy and Hayes for",210.0
Dispositional Theories of Value,1f6f608cf71eaadacaf2aa46474a3c3f3c26c772,"[{'authorId': '2116645858', 'name': 'Michael Smith'}, {'authorId': '144562881', 'name': 'David Lewis'}, {'authorId': '144617714', 'name': 'M. Johnston'}]",1989.0,,['Responses to catastrophic AGI risk: a survey'],1,"(3) Motivating reasons are constituted, inter alia, by desires. The apparent inconsistency can be brought out as follows. From (1), the state expressed by a valuation is a belief, which, from (2), is necessarily connected in some way with having a motivating reason; that is, from (3), with having a desire. So (1), (2) and (3) together entail that there is some sort of necessary connection between distinct existences: a certain kind of belief and a certain kind of desire. But there is no such connection. Believing some state of the world obtains is one thing, what I desire to do in the light of that belief is quite another. Therefore we have to reject at least one of (1), (2) or (3). Call this the 'moral problem', and call those who respond 'revisionists' and 'reconciliationists'.1 Revisionists accept the inconsistency, and so seek to explain away the apparent plausibility of at least one of (1), (2) and (3). Thus, for example, emotivists, prescriptivists and projectivists",600.0
Mind Children: The Future of Robot and Human Intelligence,42bfd80d090ff4c7980195559299fa5654a7244f,"[{'authorId': '2066894', 'name': 'Hans P. Moravec'}]",1988.0,,['Responses to catastrophic AGI risk: a survey'],1,"Arguing that within the next fifty years machines will equal humans not only in reasoning power but also in their ability to perceive, interact with, and change their environment, the author describes the tremendous technological advances possible in thefield of robotics.",412.0
Robot's Dilemma: The Frame Problem in Artificial Intelligence,1181f03825d17125c1111fb68c3f9335269edd1b,"[{'authorId': '3194015', 'name': 'Z. Pylyshyn'}]",1987.0,,['Responses to catastrophic AGI risk: a survey'],1,"Each of the chapters in this volume devotes considerable attention to defining and elaborating the notion of the frame problem-one of the hard problems of artificial intelligence. Not only do the chapters clarify the problems at hand, they shed light on the different approaches taken by those in artificial intelligence and by certain philosophers who have been concerned with related problems in their field. The book should therefore not be read merely as a discussion of the frame problem narrowly conceived, but also as a general analysis of what could be a major challenge to the design of computer systems exhibiting general intelligence.",332.0
Facts and Values,c3883382f0a83cedec12f131b9724e16d3d3fd14,"[{'authorId': '3947051', 'name': 'P. Railton'}]",1986.0,,"['Responses to catastrophic AGI risk: a survey', 'Superintelligence: Paths, Dangers, Strategies']",2,,267.0
Engines of Creation,5c345b77c8d694a538a68681ed5fb5c82ac0e4d4,"[{'authorId': '143977849', 'name': 'K. Drexler'}]",1986.0,,"['Responses to catastrophic AGI risk: a survey', 'Superintelligence: Paths, Dangers, Strategies']",2,Part 1 The foundations of foresight: engines of construction the principles of change predicting and projecting. Part 2 Profiles of the possible: engines of abundance thinking machines the world beyond Earth engines of healing long life in an open world a door to the future the limits to growth. Part 3 Dangers and hopes: engines of destruction strategies and survival finding the facts the network of knowledge world enough and time.,567.0
The time scale of artificial intelligence: Reflections on social effects,d8ad5bd9c9adec8a29b030b7675bc9c39d3a3d44,"[{'authorId': '1727567', 'name': 'R. Solomonoff'}]",1985.0,,['Responses to catastrophic AGI risk: a survey'],1,,31.0
The time scale of artificial intelligence: Reflections on social effects,fb8bb5db102f6f619c043da1c8a73a69d27df138,"[{'authorId': '100605772', 'name': 'R. Off'}]",1985.0,,['Responses to catastrophic AGI risk: a survey'],1,"R. Solomonoff was graduated from the University of Chicago in 1951 with a degree in Physics. Since that time he has mainly been working on the mech­ anization of inductive inference - the most successful approach being al­ gorithmic complexity theory. He has extended this theory to include the optimization of both hardware and software for general problem solving. He is now a principal scientist at Ox­ bridge Research, Cambridge, MA. Six future milestones in AI are discussed. These range from the development of a very general theory of problem solving to the creation of machines with capacities well beyond those of a single human. Estimates are made for when these milestones will occur, followed by some suggestions for the more effective utilization of the extremely rapid technological growth that is expected.",46.0
The Rise and Decline of Nations,ab5e51e0f7e1cf138f598c4a82064034d2d62a97,"[{'authorId': '11278368', 'name': 'R. Tollison'}, {'authorId': '115297641', 'name': 'M. Olson'}]",1983.0,,['Responses to catastrophic AGI risk: a survey'],1,The years since World War II have seen rapid shifts in the relative positions of different countries and regions. Leading political economist Mancur Olson offers a new and compelling theory to explain these shifts in fortune and then tests his theory against evidence from many periods of history and many parts of the world.,3537.0
"The Rise and Decline of Nations: Economic Growth, Stagflation, and Social Rigidities.",912f36fcebbccb66d528b4027372ecdc03d4aca6,"[{'authorId': '117651917', 'name': 'John Othick'}, {'authorId': '115297641', 'name': 'M. Olson'}]",1983.0,,['Responses to catastrophic AGI risk: a survey'],1,,3128.0
Frankenstein unbound: Towards a legal definition of artificial intelligence☆,292fb61d661a58bd148b9c01f2de4ebf12db6ec0,"[{'authorId': '1398094772', 'name': 'S. Lehman-Wilzig'}]",1981.0,,['Responses to catastrophic AGI risk: a survey'],1,,52.0
The framing of decisions and the psychology of choice.,e552054dbd030b8414058639389b4a63e727aedb,"[{'authorId': '2064181', 'name': 'A. Tversky'}, {'authorId': '3683465', 'name': 'D. Kahneman'}]",1981.0,Science,['Responses to catastrophic AGI risk: a survey'],1,"The psychological principles that govern the perception of decision problems and the evaluation of probabilities and outcomes produce predictable shifts of preference when the same problem is framed in different ways. Reversals of preference are demonstrated in choices regarding monetary outcomes, both hypothetical and real, and in questions pertaining to the loss of human lives. The effects of frames on preferences are compared to the effects of perspectives on perceptual appearance. The dependence of preferences on the formulation of decision problems is a significant concern for the theory of rational choice.",15900.0
A Theory of the Good and the Right,fd8873f581ba7c1a842b70e8668fd372d08f967d,"[{'authorId': '134195316', 'name': 'J. Narveson'}]",1980.0,,['Responses to catastrophic AGI risk: a survey'],1,,43.0
A theory of the good and the right,b79ddb15b465aa0c966bd77d69c3ef3e8d08aa30,"[{'authorId': '40206276', 'name': 'R. Brandt'}]",1980.0,,['Responses to catastrophic AGI risk: a survey'],1,"What would any rational person believe to be worth wanting or working for? Viewed from the standpoint of ethics and empirical psychology, how would such a person define and explain the morally right and the just?And what system of morals would rational people select as the best for the society? Essential to what is important in traditional philosophical inquiries, these questions and others are pursued in ""A Theory of the Good and the Right"", Richard B. Brandt's now classic work, based on his Oxford lectures.Using a contemporary psychological theory of action and of motivation, Brandt argues that rational people would choose a utilitarian moral code that the purpose of living should be to strive for the greatest good for the largest number of people. He discusses the concept of welfare, the prospects for the interpersonal comparison and measurement of utility, the implications of the relevant form of rule utilitarianism for the theory of distributive justice, and the possibilities of conflict between utilitarian moral codes and the dictates of self-interest.",460.0
Public Choice III,907e89b3be5709f003821bbd8bc908cbf16ab0fa,"[{'authorId': '145709498', 'name': 'D. Mueller'}]",1979.0,,['Responses to catastrophic AGI risk: a survey'],1,"This book represents a considerable revision and expansion of Public Choice II (1989). Six new chapters have been added, and several chapters from the previous edition have been extensively revised. The discussion of empirical work in public choice has been greatly expanded. As in the previous editions, all of the major topics of public choice are covered. These include: why the state exists, voting rules, federalism, the theory of clubs, two-party and multiparty electoral systems, rent seeking, bureaucracy, interest groups, dictatorship, the size of government, voter participation, and political business cycles. Normative issues in public choice are also examined including a normative analysis of the simple majority rule, Bergsonâ€“Samuelson social welfare functions, the Arrow and Sen impossibility theorems, Rawls's social contract theory and the constitutional political economy of Buchanan and Tullock.",2018.0
Can Robots be Moral?,1ab9ca3fc4d9c6c6fe642a51c26910acc9ee2f50,"[{'authorId': '16840758', 'name': 'L. Versenyi'}]",1974.0,"Ethics: An International Journal of Social, Political, and Legal Philosophy",['Responses to catastrophic AGI risk: a survey'],1,"Recent philosophical discussion concerning robots has been largely preoccupied with questions such as ""can robots think, know, feel, or learn?"" ""can they be conscious, teleological, and self-adaptive?""; ""can robots be in principle psychologically and intellectually isomorphic to men?""' Considerably less attention has been paid meanwhile to the question whether robots can be moral. Since the latter problem seems to me rather intimately connected with the ones extensively discussed, I would like to raise it here in an attempt to carry the discussion to its logical conclusion. The thesis of this paper is that if there are no magic descriptive terms-intelligence, consciousness, purposiveness, etc. -predicable exclusively of men but not of robots, then there are no such moral terms either. If men and machines coexist in a natural continuum in which there are no gaps, quantum jumps, or insurmountable barriers preventing the assimilation of the one to the other, then they also coexist in a moral continuum in which only relative but never absolute distinctions can be made between human and machine morality. I will argue this thesis by raising the question whether robots can be moral in two stages: (1) Can robots act morally? (2) Can we, without absurdity, treat robots as moral agents? The answer to these questions will be given, not in terms of a new ""robot morality,"" but in terms of a few traditional ethical theories. To make these questions at least initially plausible our robots will have to be imagined to be much more sophisticated than any single machine already existing. At the same time, for all their complexity, they are not to have any capabilities other than the ones computer scientists and cyberneticists like Turing, Wiener, Ashby, Arbib, Pask, and Uttley, for example, have argued to be, if not already",26.0
A note on the confinement problem,3aa7fa1563467801db724b046df439dc33de2407,"[{'authorId': '2665014', 'name': 'B. Lampson'}]",1973.0,CACM,['Responses to catastrophic AGI risk: a survey'],1,onfining a program during its execution so that it cannot transmit information to any other program except its caller. A set of examples attempts to stake out the boundaries of the problem. Necessary conditions for a solution are stated and informally justified.,1560.0
Does power corrupt?,1c0a369e6abb55379774332f708bddd87eeb6563,"[{'authorId': '116289935', 'name': 'D. Kipnis'}]",1972.0,Journal of Personality and Social Psychology,['Responses to catastrophic AGI risk: a survey'],1,,529.0
The importance of what we care about: Freedom of the will and the concept of a person,b3547085b2e85f807d3ab5e882c845055219bcc5,"[{'authorId': '77662947', 'name': 'H. Frankfurt'}]",1971.0,,['Responses to catastrophic AGI risk: a survey'],1,"What philosophers have lately come to accept as analysis of the concept of a person is not actually analysis of that concept at all. Strawson, whose usage represents the current standard, identifies the concept of a person as “the concept of a type of entity such that both predicates ascribing states of consciousness and predicates ascribing corporeal characteristics...are equally applicable to a single individual of that single type.”1 But there are many entities besides persons that have both mental and physical properties. As it happens—though it seems extraordinary that this should be so—there is no common English word for the type of entity Strawson has in mind, a type that includes not only human beings but animals of various lesser species as well. Still, this hardly justifies the misappropriation of a valuable philosophical term.",3071.0
Some future social repercussions of computers,dc07a577c010b3724df04367f4bd1a886c3b7b58,"[{'authorId': '145179124', 'name': 'I. Good'}]",1970.0,,"['Responses to catastrophic AGI risk: a survey', 'Superintelligence: Paths, Dangers, Strategies']",2,"The prehistory and history of computers and programs are briefly reviewed and are used as a basis for predicting the future. Computers are classified into thirteen generations of which the last five have not yet arrived. A previously published argument is mentioned suggesting that an ultra‐intelligent machine will be built within a few decades and its promise and dangers are emphasized. It is suggested that an association for dealing with the dangers should be started. A proposal for press‐button peace is mentioned, based on computerized international cooperation. Numerous future applications for computers are briefly discussed, some being natural extrapolations from what has already begun.",13.0
Validity of the Single Processor Approach to Achieving Large Scale Computing Capabilities,0a5814c2b61aafdc9bafd14dba4ea6bd9536f236,"[{'authorId': '146767541', 'name': 'G. Amdhal'}]",1967.0,,['Responses to catastrophic AGI risk: a survey'],1,"An instrument for facilitating the calculation of equivalent values includes a plate bearing symbols representing units and dimensions, the plate having a window in which a movable pointer is located. The pointer is movable from a first position representing a first set of units and dimensions to a second position representing an equivalent set of units and dimensions.",4891.0
Treatment of psychopaths.,6b0598b4a3762e91bbb566a675799fe30b31d55d,"[{'authorId': '8188697', 'name': 'T. Gibbens'}]",1961.0,Journal of Mental Science,['Responses to catastrophic AGI risk: a survey'],1,,30.0
Toward some circuitry of ethical robots or an observational science of the genesis of social evaluation in the mind-like behavior of artifacts,1e47ca39c750b6e421ece9da851f31d0b40021db,"[{'authorId': '4605464', 'name': 'W. McCulloch'}]",1956.0,,['Responses to catastrophic AGI risk: a survey'],1,,18.0
“Other Worlds than Ours”,265e563ad391593a98bdd6ae8c0116ebabd39e02,"[{'authorId': '32675999', 'name': 'R. Proctor'}]",,Nature,['Responses to catastrophic AGI risk: a survey'],1,,21.0
Is Power-Seeking AI an Existential Risk?,1c07e314985161ec42ba895eb4869ffc5d360736,"[{'authorId': '116874955', 'name': 'Joseph Carlsmith'}]",2022.0,arXiv.org,['X-Risk Analysis for AI Research'],1,"This report examines what I see as the core argument for concern about existential risk from misaligned artificial intelligence. I proceed in two stages. First, I lay out a backdrop picture that informs such concern. On this picture, intelligent agency is an extremely powerful force, and creating agents much more intelligent than us is playing with fire – especially given that if their objectives are problematic, such agents would plausibly have instrumental incentives to seek power over humans. Second, I formulate and evaluate a more specific six-premise argument that creating agents of this kind will lead to existential catastrophe by 2070. On this argument, by 2070: (1) it will become possible and financially feasible to build relevantly powerful and agentic AI systems; (2) there will be strong incentives to do so; (3) it will be much harder to build aligned (and relevantly powerful/agentic) AI systems than to build misaligned (and relevantly powerful/agentic) AI systems that are still superficially attractive to deploy; (4) some such misaligned systems will seek power over humans in high-impact ways; (5) this problem will scale to the full disempowerment of humanity; and (6) such disempowerment will constitute an existential catastrophe. I assign rough subjective credences to the premises in this argument, and I end up with an overall estimate of ~5% that an existential catastrophe of this kind will occur by 2070. (May 2022 update: since making this report public in April 2021, my estimate here has gone up, and is now at >10%.)",15.0
Hierarchical Text-Conditional Image Generation with CLIP Latents,c57293882b2561e1ba03017902df9fc2f289dea2,"[{'authorId': '1992922591', 'name': 'A. Ramesh'}, {'authorId': '6515819', 'name': 'Prafulla Dhariwal'}, {'authorId': '38967461', 'name': 'Alex Nichol'}, {'authorId': '30414789', 'name': 'Casey Chu'}, {'authorId': '2108828435', 'name': 'Mark Chen'}]",2022.0,arXiv.org,['X-Risk Analysis for AI Research'],1,"Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.",1260.0
Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback,0286b2736a114198b25fb5553c671c33aed5d477,"[{'authorId': '1486307451', 'name': 'Yuntao Bai'}, {'authorId': '2149890773', 'name': 'Andy Jones'}, {'authorId': '1978097132', 'name': 'Kamal Ndousse'}, {'authorId': '119609682', 'name': 'Amanda Askell'}, {'authorId': '2111073313', 'name': 'Anna Chen'}, {'authorId': '2142833890', 'name': 'Nova DasSarma'}, {'authorId': '1943097969', 'name': 'Dawn Drain'}, {'authorId': '30176974', 'name': 'Stanislav Fort'}, {'authorId': '2081806483', 'name': 'Deep Ganguli'}, {'authorId': '103143311', 'name': 'T. Henighan'}, {'authorId': '2117706920', 'name': 'Nicholas Joseph'}, {'authorId': '148070327', 'name': 'Saurav Kadavath'}, {'authorId': '1583434563', 'name': 'John Kernion'}, {'authorId': '2154608209', 'name': 'Tom Conerly'}, {'authorId': '1403602266', 'name': 'S. El-Showk'}, {'authorId': '2866708', 'name': 'Nelson Elhage'}, {'authorId': '1573482302', 'name': 'Zac Hatfield-Dodds'}, {'authorId': '39182747', 'name': 'Danny Hernandez'}, {'authorId': '2162194147', 'name': 'Tristan Hume'}, {'authorId': '2154610174', 'name': 'Scott Johnston'}, {'authorId': '49604482', 'name': 'S. Kravec'}, {'authorId': '2154608229', 'name': 'Liane Lovitt'}, {'authorId': '2051128902', 'name': 'Neel Nanda'}, {'authorId': '2061321863', 'name': 'Catherine Olsson'}, {'authorId': '2698777', 'name': 'Dario Amodei'}, {'authorId': '31035595', 'name': 'Tom B. Brown'}, {'authorId': '2115193883', 'name': 'Jack Clark'}, {'authorId': '52238703', 'name': 'Sam McCandlish'}, {'authorId': '37232298', 'name': 'C. Olah'}, {'authorId': '2056658938', 'name': 'Benjamin Mann'}, {'authorId': '2053807409', 'name': 'Jared Kaplan'}]",2022.0,arXiv.org,['X-Risk Analysis for AI Research'],1,"We apply preference modeling and reinforcement learning from human feedback (RLHF) to ﬁnetune language models to act as helpful and harmless assistants. We ﬁnd this alignment training improves performance on almost all NLP evaluations, and is fully compatible with training for specialized skills such as python coding and summarization. We explore an iterated online mode of training, where preference models and RL policies are updated on a weekly cadence with fresh human feedback data, efﬁciently improving our datasets and models. Finally, we investigate the robustness of RLHF training, and identify a roughly linear relation between the RL reward and the square root of the KL divergence between the policy and its initialization. Alongside our main results, we perform peripheral analyses on calibration, competing objectives, and the use of OOD detection, compare our models with human writers, and provide samples from our models using prompts appearing in recent related work. Figure These plots show that PM accuracy decreases as we focus exclusively on comparisons between pairs of samples with high score. We have normalized all preference models to have the same mean score on a held-out dataset so that they’re directly comparable, and then plotted accuracy for the comparisons where both samples have scores above a speciﬁc threshold.",88.0
Training language models to follow instructions with human feedback,d766bffc357127e0dc86dd69561d5aeb520d6f4c,"[{'authorId': '31793034', 'name': 'Long Ouyang'}, {'authorId': '49387725', 'name': 'Jeff Wu'}, {'authorId': '2115903168', 'name': 'Xu Jiang'}, {'authorId': '2061137049', 'name': 'Diogo Almeida'}, {'authorId': '2064084601', 'name': 'Carroll L. Wainwright'}, {'authorId': '2051714782', 'name': 'Pamela Mishkin'}, {'authorId': '2111387504', 'name': 'Chong Zhang'}, {'authorId': '144517868', 'name': 'Sandhini Agarwal'}, {'authorId': '2117680841', 'name': 'Katarina Slama'}, {'authorId': '2064770039', 'name': 'Alex Ray'}, {'authorId': '47971768', 'name': 'J. Schulman'}, {'authorId': '2052366271', 'name': 'Jacob Hilton'}, {'authorId': '2151735262', 'name': 'Fraser Kelton'}, {'authorId': '2142365973', 'name': 'Luke E. Miller'}, {'authorId': '2151735251', 'name': 'Maddie Simens'}, {'authorId': '119609682', 'name': 'Amanda Askell'}, {'authorId': '2930640', 'name': 'P. Welinder'}, {'authorId': '145791315', 'name': 'P. Christiano'}, {'authorId': '2990741', 'name': 'J. Leike'}, {'authorId': '49407415', 'name': 'Ryan J. Lowe'}]",2022.0,arXiv.org,['X-Risk Analysis for AI Research'],1,"Making language models bigger does not inherently make them better at following a user’s intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.",716.0
Dual use of artificial-intelligence-powered drug discovery,d7eef9b5bb65feda6647440e7727bbcdf0edaebc,"[{'authorId': '34577787', 'name': 'Fabio Urbina'}, {'authorId': '5450685', 'name': 'Filippa Lentzos'}, {'authorId': '2063052193', 'name': 'Cédric Invernizzi'}, {'authorId': '1887610', 'name': 'S. Ekins'}]",2022.0,Nature Machine Intelligence,['X-Risk Analysis for AI Research'],1,,57.0
Predictability and Surprise in Large Generative Models,9cbc044e315cdefe9a255119037ac7c23e9abdd5,"[{'authorId': '2081806415', 'name': 'Deep Ganguli'}, {'authorId': '39182747', 'name': 'Danny Hernandez'}, {'authorId': '2154608229', 'name': 'Liane Lovitt'}, {'authorId': '2142833890', 'name': 'Nova DasSarma'}, {'authorId': '103143311', 'name': 'T. Henighan'}, {'authorId': '2149890773', 'name': 'Andy Jones'}, {'authorId': '2117706920', 'name': 'Nicholas Joseph'}, {'authorId': '1583434563', 'name': 'John Kernion'}, {'authorId': '2056658938', 'name': 'Benjamin Mann'}, {'authorId': '119609682', 'name': 'Amanda Askell'}, {'authorId': '1486307451', 'name': 'Yuntao Bai'}, {'authorId': '2111073313', 'name': 'Anna Chen'}, {'authorId': '2154608209', 'name': 'Tom Conerly'}, {'authorId': '1943097969', 'name': 'Dawn Drain'}, {'authorId': '2866708', 'name': 'Nelson Elhage'}, {'authorId': '2154609053', 'name': 'Sheer El Showk'}, {'authorId': '30176974', 'name': 'Stanislav Fort'}, {'authorId': '1573482302', 'name': 'Zac Hatfield-Dodds'}, {'authorId': '2154610174', 'name': 'Scott Johnston'}, {'authorId': '49604482', 'name': 'S. Kravec'}, {'authorId': '2051128902', 'name': 'Neel Nanda'}, {'authorId': '1978097132', 'name': 'Kamal Ndousse'}, {'authorId': '2061321863', 'name': 'Catherine Olsson'}, {'authorId': '2154608472', 'name': 'Daniela Amodei'}, {'authorId': '2698777', 'name': 'Dario Amodei'}, {'authorId': '31035595', 'name': 'Tom B. Brown'}, {'authorId': '2053807409', 'name': 'Jared Kaplan'}, {'authorId': '52238703', 'name': 'Sam McCandlish'}, {'authorId': '37232298', 'name': 'C. Olah'}, {'authorId': '2115193883', 'name': 'Jack Clark'}]",2022.0,"Conference on Fairness, Accountability and Transparency",['X-Risk Analysis for AI Research'],1,"Large-scale pre-training has recently emerged as a technique for creating capable, general-purpose, generative models such as GPT-3, Megatron-Turing NLG, Gopher, and many others. In this paper, we highlight a counterintuitive property of such models and discuss the policy implications of this property. Namely, these generative models have a paradoxical combination of predictable loss on a broad training distribution (as embodied in their ”scaling laws”), and unpredictable specific capabilities, inputs, and outputs. We believe that the high-level predictability and appearance of useful capabilities drives rapid development of such models, while the unpredictable qualities make it difficult to anticipate the consequences of model deployment. We go through examples of how this combination can lead to socially harmful behavior with examples from the literature and real world observations, and we also perform two novel experiments to illustrate our point about harms from unpredictability. Furthermore, we analyze how these conflicting properties combine to give model developers various motivations for deploying these models, and challenges that can hinder deployment. We conclude with a list of possible interventions the AI community may take to increase the chance of these models having a beneficial impact. We intend for this paper to be useful to policymakers who want to understand and regulate AI systems, technologists who care about the potential policy impact of their work, funders who want to support work addressing these challenges, and academics who want to analyze, critique, and potentially develop large generative models.",48.0
Execute Order 66: Targeted Data Poisoning for Reinforcement Learning,69e961d65b9ed52eec5e545c2021d28b1c51ed07,"[{'authorId': '2047082262', 'name': 'Harrison Foley'}, {'authorId': '120165773', 'name': 'Liam Fowl'}, {'authorId': '1962083', 'name': 'T. Goldstein'}, {'authorId': '2189083', 'name': 'Gavin Taylor'}]",2022.0,arXiv.org,['X-Risk Analysis for AI Research'],1,"Data poisoning for reinforcement learning has historically focused on general performance degradation, and targeted attacks have been successful via perturbations that involve control of the victim's policy and rewards. We introduce an insidious poisoning attack for reinforcement learning which causes agent misbehavior only at specific target states - all while minimally modifying a small fraction of training observations without assuming any control over policy or reward. We accomplish this by adapting a recent technique, gradient alignment, to reinforcement learning. We test our method and demonstrate success in two Atari games of varying difficulty.",4.0
"The Precipice. Existential Risk and the Future of Humanity, de T. Ord",ab524fb6fab7e249ff5b628ddfde527b4337e1c5,"[{'authorId': '2075661945', 'name': 'Fabrizio López de Pomar'}]",2021.0,,['X-Risk Analysis for AI Research'],1,,48.0
Highly accurate protein structure prediction for the human proteome,1fb3ad9969245795f268636eff9a145337144718,"[{'authorId': '2119712884', 'name': 'Kathryn Tunyasuvunakool'}, {'authorId': '40523747', 'name': 'J. Adler'}, {'authorId': '51125091', 'name': 'Zachary Wu'}, {'authorId': '1484039896', 'name': 'Tim Green'}, {'authorId': '2060760382', 'name': 'Michal Zielinski'}, {'authorId': '40501144', 'name': 'Augustin Zídek'}, {'authorId': '1392692054', 'name': 'Alex Bridgland'}, {'authorId': '143964037', 'name': 'A. Cowie'}, {'authorId': '1406288863', 'name': 'Clemens Meyer'}, {'authorId': '2120370895', 'name': 'Agata Laydon'}, {'authorId': '26591876', 'name': 'S. Velankar'}, {'authorId': '2452390', 'name': 'G. Kleywegt'}, {'authorId': '46888415', 'name': 'A. Bateman'}, {'authorId': '152381239', 'name': 'R. Evans'}, {'authorId': '1863250', 'name': 'A. Pritzel'}, {'authorId': '73776617', 'name': 'Michael Figurnov'}, {'authorId': '1737326', 'name': 'O. Ronneberger'}, {'authorId': '2119696840', 'name': 'Russ Bates'}, {'authorId': '2060176064', 'name': 'Simon A A Kohl'}, {'authorId': '13759734', 'name': 'Anna Potapenko'}, {'authorId': '5055381', 'name': 'A. J. Ballard'}, {'authorId': '1403031665', 'name': 'Bernardino Romera-Paredes'}, {'authorId': '48206221', 'name': 'Stanislav Nikolov'}, {'authorId': '22418321', 'name': 'Rishub Jain'}, {'authorId': '2072167521', 'name': 'Ellen Clancy'}, {'authorId': '2066483671', 'name': 'D. Reiman'}, {'authorId': '48348688', 'name': 'Stig Petersen'}, {'authorId': '33666044', 'name': 'A. Senior'}, {'authorId': '2645384', 'name': 'K. Kavukcuoglu'}, {'authorId': '1702103', 'name': 'E. Birney'}, {'authorId': '143967473', 'name': 'Pushmeet Kohli'}, {'authorId': '47921134', 'name': 'J. Jumper'}, {'authorId': '48987704', 'name': 'D. Hassabis'}]",2021.0,Nature,['X-Risk Analysis for AI Research'],1,,1009.0
Well-Being,8cedcf6ece7aa6be3fbfb620b74b7a400bca7144,"[{'authorId': '2213892', 'name': 'Robert Kaplan'}, {'authorId': '153309051', 'name': 'W. Smith'}]",2015.0,Encyclopedia of the UN Sustainable Development Goals,['X-Risk Analysis for AI Research'],1,"Well-being is most commonly used in philosophy to describe what is non-instrumentally or ultimately good for a person. The question of what well-being consists in is of independent interest, but it is of great importance in moral philosophy, especially in the case of utilitarianism, according to which the only moral requirement is that well-being be maximized. Significant challenges to the very notion have been mounted, in particular by G.E. Moore and T.M. Scanlon. It has become standard to distinguish theories of well-being as either hedonist theories, desire theories, or objective list theories. According to the view known as welfarism, well-being is the only value. Also important in ethics is the question of how a person’s moral character and actions relate to their well-being. 1. The Concept 2. Moore’s Challenge 3. Scanlon’s Challenge 4. Theories of Well-being 4.1 Hedonism 4.2 Desire Theories 4.3 Objective List Theories 5. Well-being and Morality 5.1 Welfarism 5.2 Well-being and Virtue Bibliography Academic Tools Other Internet Resources Related Entries",297.0
Dialogue Response Ranking Training with Large-Scale Human Feedback Data,32722fb49e981d0e53b1ff3c064850548f0b54e0,"[{'authorId': '71886367', 'name': 'Xiang Gao'}, {'authorId': '3272356', 'name': 'Yizhe Zhang'}, {'authorId': '1947267', 'name': 'Michel Galley'}, {'authorId': '3125776', 'name': 'Chris Brockett'}, {'authorId': '66648221', 'name': 'Bill Dolan'}]",2020.0,Conference on Empirical Methods in Natural Language Processing,['X-Risk Analysis for AI Research'],1,"Existing open-domain dialog models are generally trained to minimize the perplexity of target human responses. However, some human replies are more engaging than others, spawning more followup interactions. Current conversational models are increasingly capable of producing turns that are context-relevant, but in order to produce compelling agents, these models need to be able to predict and optimize for turns that are genuinely engaging. We leverage social media feedback data (number of replies and upvotes) to build a large-scale training dataset for feedback prediction. To alleviate possible distortion between the feedback and engagingness, we convert the ranking problem to a comparison of response pairs which involve few confounding factors. We trained DialogRPT, a set of GPT-2 based models on 133M pairs of human feedback data and the resulting ranker outperformed several baselines. Particularly, our ranker outperforms the conventional dialog perplexity baseline with a large margin on predicting Reddit feedback. We finally combine the feedback prediction models and a human-like scoring model to rank the machine-generated dialog responses. Crowd-sourced human evaluation shows that our ranking method correlates better with real human preferences than baseline models.",53.0
Generative Language Modeling for Automated Theorem Proving,5fe0a4af3bd1479d5e39fbda2215c86bce54722b,"[{'authorId': '2373714', 'name': 'Stanislas Polu'}, {'authorId': '1701686', 'name': 'Ilya Sutskever'}]",2020.0,arXiv.org,['X-Risk Analysis for AI Research'],1,"We explore the application of transformer-based language models to automated theorem proving. This work is motivated by the possibility that a major limitation of automated theorem provers compared to humans -- the generation of original mathematical terms -- might be addressable via generation from language models. We present an automated prover and proof assistant, GPT-f, for the Metamath formalization language, and analyze its performance. GPT-f found new short proofs that were accepted into the main Metamath library, which is to our knowledge, the first time a deep-learning based system has contributed proofs that were adopted by a formal mathematics community.",111.0
"Mastering Atari, Go, chess and shogi by planning with a learned model",3507bd62a14bd0e8ead28cdedb1c33ba83c39c6b,"[{'authorId': '4337102', 'name': 'Julian Schrittwieser'}, {'authorId': '2460849', 'name': 'Ioannis Antonoglou'}, {'authorId': '2067208983', 'name': 'T. Hubert'}, {'authorId': '34838386', 'name': 'K. Simonyan'}, {'authorId': '2175946', 'name': 'L. Sifre'}, {'authorId': '152380508', 'name': 'Simon Schmitt'}, {'authorId': '35099444', 'name': 'A. Guez'}, {'authorId': '49860549', 'name': 'Edward Lockhart'}, {'authorId': '48987704', 'name': 'D. Hassabis'}, {'authorId': '1686971', 'name': 'T. Graepel'}, {'authorId': '2542999', 'name': 'T. Lillicrap'}, {'authorId': '145824029', 'name': 'David Silver'}]",2019.0,Nature,['X-Risk Analysis for AI Research'],1,,1068.0
Neural Cleanse: Identifying and Mitigating Backdoor Attacks in Neural Networks,42658c812d60d26a0bdad91b4d81e8620b994bf6,"[{'authorId': '2081795', 'name': 'Bolun Wang'}, {'authorId': '3460027', 'name': 'Yuanshun Yao'}, {'authorId': '40959657', 'name': 'Shawn Shan'}, {'authorId': '49403971', 'name': 'Huiying Li'}, {'authorId': '34824488', 'name': 'Bimal Viswanath'}, {'authorId': '2704852', 'name': 'Haitao Zheng'}, {'authorId': '145970007', 'name': 'Ben Y. Zhao'}]",2019.0,IEEE Symposium on Security and Privacy,['X-Risk Analysis for AI Research'],1,"Lack of transparency in deep neural networks (DNNs) make them susceptible to backdoor attacks, where hidden associations or triggers override normal classification to produce unexpected results. For example, a model with a backdoor always identifies a face as Bill Gates if a specific symbol is present in the input. Backdoors can stay hidden indefinitely until activated by an input, and present a serious security risk to many security or safety related applications, e.g. biometric authentication systems or self-driving cars. We present the first robust and generalizable detection and mitigation system for DNN backdoor attacks. Our techniques identify backdoors and reconstruct possible triggers. We identify multiple mitigation techniques via input filters, neuron pruning and unlearning. We demonstrate their efficacy via extensive experiments on a variety of DNNs, against two types of backdoor injection methods identified by prior work. Our techniques also prove robust against a number of variants of the backdoor attack.",747.0
Jury Theorems,5ecd4d6489e7640fc41848fb1b650bbafb2d605e,"[{'authorId': '1731363', 'name': 'F. Dietrich'}, {'authorId': '15471141', 'name': 'Kai P. Spiekermann'}]",2019.0,The Routledge Handbook of Social Epistemology,['X-Risk Analysis for AI Research'],1,"We give a review and critique of jury theorems from a social-epistemology perspective, covering Condorcet’s (1785) classic theorem and several later refinements and departures. We assess the plausibility of the conclusions and premises featuring in jury theorems and evaluate the potential of such theorems to serve as formal arguments for the ‘wisdom of crowds’. In particular, we argue (i) that there is a fundamental tension between voters’ independence and voters’ competence, hence between the two premises of most jury theorems; (ii) that the (asymptotic) conclusion that ‘huge groups are infallible’, reached by many jury theorems, is an artifact of unjustified premises; and (iii) that the (nonasymptotic) conclusion that ‘larger groups are more reliable’, also reached by many jury theorems, is not an artifact and should be regarded as the more adequate formal rendition of the ‘wisdom of crowds’.",11.0
General Purpose Intelligence: Arguing the Orthogonality Thesis,157b724130d3e6b7124bbccdc115cc56476a45bb,"[{'authorId': '2054678912', 'name': 'S. Armstrong'}]",2013.0,,"['X-Risk Analysis for AI Research', 'Superintelligence: Paths, Dangers, Strategies']",2,"ABSTRACT.In his paper ""The Superintelligent Will,"" Nick Bostrom formalized the Orthogonality thesis: the idea that the final goals and intelligence levels of artificial agents are independent of each other. This paper presents arguments for a (narrower) version of the thesis. It proceeds through three steps. First it shows that superintelligent agents with essentially arbitrary goals can exist in our universe - both as theoretical impractical agents such as AIXI and as physically possible real-world agents. Then it argues that if humans are capable of building human-level artificial intelligences, we can build them with an extremely broad spectrum of goals. Finally it shows that the same result holds for any superintelligent agent we could directly or indirectly build. This result is relevant for arguments about the potential motivations of future agents: knowing an artificial agent is of high intelligence does not allow us to presume that it will be moral, we will need to figure out its goals directly.Keywords: AI, Artificial Intelligence, efficiency, intelligence, goals, orthogonality1. The Orthogonality ThesisScientists and mathematicians are the stereotypical examples of high intelligence humans. But their morality and ethics have been all over the map. On modem political scales, they can be left- (Oppenheimer) or right-wing (von Neumann) and historically they have slotted into most of the political groupings of their period (Galois, Lavoisier). Ethically, they have ranged from very humanitarian (Darwin, Einstein outside of his private life), through amoral (von Braun) to commercially belligerent (Edison) and vindictive (Newton). Few scientists have been put in a position where they could demonstrate genuinely evil behavior, but there have been a few of those (Teichmuller, Philipp Lenard, Ted Kaczynski, Shiro Ishii).Of course, many scientists have been absolutely conventional in their views and attitudes given the society of their time. But the above examples hint that their ethics are not strongly impacted by their high intelligence; intelligence and ethics seem 'orthogonal' (varying independently of each other, to some extent). If we turn to the case of (potential) artificial intelligences we can ask whether that relation continues: would high intelligence go along with certain motivations and goals, or are they unrelated?To avoid the implicit anthropomorphisation in terms such as 'ethics,' we will be looking at agents 'final goals' - the ultimate objectives they are aiming for. Then the Orthogonality thesis, due to Nick Bostrom (Bostrom, 2012), states that:Intelligence and final goals are orthogonal axes along which possible agents can freely vary. In other words, more or less any level of intelligence could in principle be combined with more or less any final goal.It is analogous to Hume's thesis about the independence of reason and morality (Hume, 1739), but applied more narrowly, using the normatively thinner concepts 'intelligence' and 'final goals' rather than 'reason' and 'morality'.But even 'intelligence,' as generally used, has too many connotations. A better term would be efficiency, or instrumental rationality, or the ability to effectively solve problems given limited knowledge and resources (Wang, 2011). Nevertheless, we will be sticking with terminology such as 'intelligent agent,' 'artificial intelligence' or 'superintelligence,' as they are well established, but using them synonymously with 'efficient agent,' artificial efficiency' and 'superefficient algorithm.' The relevant criteria is whether the agent can effectively achieve its goals in general situations, not whether its inner process matches up with a particular definition of what intelligence is.Thus an artificial intelligence (AI) is an artificial algorithm, deterministic or probabilistic, implemented on some device, that demonstrates an ability to achieve goals in varied and general situations. …",17.0
"For better or worse, benchmarks shape a field",c8ba4cd47db0c50b544f645b71b68aac66774a55,"[{'authorId': '1701130', 'name': 'D. Patterson'}]",2012.0,Communications of the ACM,['X-Risk Analysis for AI Research'],1,"LiKe oThe r iT fields, computer architects initially reported incomparable results. We quickly saw the folly of this approach. We then went through a sequence of performance metrics, with each being an improvement on its predecessor: average instruction time, millions of instructions per second (MIPS), millions of floating point operations per second (MEGAFLOPS), synthetic program performance (DHRYSTONES), and ultimately average performance improvement relative to a reference computer based on a suite of real programs (SPEC CPU). When a field has good benchmarks, we settle debates and the field makes rapid progress. Indeed, the acceleration in computer performance from 25% to 50% per year starting in the mid1980s is due in part to our ability to fairly compare competing designs as well as to Moore’s Law. Similarly, computer vision made dramatic advances in the last decade after it embraced benchmarks to evaluate innovations in vision algorithms. Sadly, when a field has bad benchmarks, progress can be problematic. For example, despite being discredited in textbooks since 1990, embedded computing still reports DHRYSTONES when making performance claims. How do we know whether a new embedded processor is a genuine breakthrough, or simply the result of cynical benchmarketering, in that it runs the benchmark quickly but real programs slowly? The answer is we cannot know from DHRYSTONE reports. In the following paper, the authors point out that while computer architecture has a glorious past, it has become",24.0
Capitalism and freedom,16871d6d5f9bf380c7a4e145c4ae6b3d0c198e7c,"[{'authorId': '4521305', 'name': 'Frederic L. Pryor'}]",2010.0,,['X-Risk Analysis for AI Research'],1,,3877.0
How Complex Systems Fail,a45fe219296997413a314a26ad17e0b809173943,"[{'authorId': '2429121', 'name': 'John Allspaw'}, {'authorId': '1702291', 'name': 'R. Cook'}]",2010.0,Web Operations,['X-Risk Analysis for AI Research'],1,"1) Complex systems are intrinsically hazardous systems. All of the interesting systems (e.g. transportation, healthcare, power generation) are inherently and unavoidably hazardous by the own nature. The frequency of hazard exposure can sometimes be changed but the processes involved in the system are themselves intrinsically and irreducibly hazardous. It is the presence of these hazards that drives the creation of defenses against hazard that characterize these systems.",84.0
Moving Beyond Normal Accidents and High Reliability Organizations: A Systems Approach to Safety in Complex Systems,203b1066448b8d0b0a236697b0e7d18f08bb6845,"[{'authorId': '1777378', 'name': 'N. Leveson'}, {'authorId': '2656053', 'name': 'Nicolas Dulac'}, {'authorId': '2072212', 'name': 'K. Marais'}, {'authorId': '21235732', 'name': 'J. Carroll'}]",2009.0,,['X-Risk Analysis for AI Research'],1,"In this century society faces increasingly large-scale accidents and risks emerging from our own wondrous technologies. Two prominent organizational approaches to safety, Normal Accident Theory and High Reliability Organizations, have focused attention on a variety of industries that deal with hazardous situations, developed concepts to explicate organizational structure and culture, and debated whether accidents are inevitable in complex systems. We outline these approaches and identify some limitations, including narrow definitions, ambiguity about key concepts, confusion of reliability and safety, and overly pessimistic or optimistic conclusions. We believe that the debate between NAT and HRO can become a more productive three-way conversation by including a systems approach to safety emerging from engineering disciplines. The more comprehensive systems approach clarifies the strengths and weaknesses of NAT and HRO and offers a more powerful repertoire of analytic tools and intervention strategies to manage and control post modern risk in complex, high-tech, systems with their potential for catastrophic disruptions and losses.",348.0
"Guide to Emergency Management and Related Terms, Definitions, Concepts, Acronyms, Organizations, Programs, Guidance, Executive Orders & Legislation: A Tutorial on Emergency Management, Broadly Defined, Past and Present",7650a111a0d5300a0e6d9c30a4f8eab477af8795,"[{'authorId': '69986420', 'name': 'B. Blanchard'}]",2008.0,,['X-Risk Analysis for AI Research'],1,,55.0
Existential risks: analyzing human extinction scenarios and related hazards,8ea30b1b4141d555b455723738a79528ed390424,"[{'authorId': '2193691', 'name': 'N. Bostrom'}]",2002.0,,"['X-Risk Analysis for AI Research', 'Superintelligence: Paths, Dangers, Strategies']",2,"Because of accelerating technological progress, humankind may be rapidly approaching a critical phase in its career. In addition to well-known threats such as nuclear holocaust, the propects of radically transforming technologies like nanotech systems and machine intelligence present us with unprecedented opportunities and risks. Our future, and whether we will have a future at all, may well be determined by how we deal with these challenges. In the case of radically transforming technologies, a better understanding of the transition dynamics from a human to a ""posthuman"" society is needed. Of particular importance is to know where the pitfalls are: the ways in which things could go terminally wrong. While we have had long exposure to various personal, local, and endurable global hazards, this paper analyzes a recently emerging category: that of existential risks. These are threats that could case our extinction or destroy the potential of Earth - originating intelligent life. Some of these threats are relatively well known while others, including some of the gravest, have gone almost unrecognized. Existential risks have a cluster of features that make ordinary risk management ineffective. A final section of this paper discusses several ethical and policy implications. A clearer understanding of the threat picture will enable us to formulate better strategies.",351.0
A bitter lesson.,7bea855e19fd13461590e4f2d44bbf7b807ce3e3,"[{'authorId': '123396149', 'name': 'N. Whitman'}]",1999.0,Academic medicine : journal of the Association of American Medical Colleges,['X-Risk Analysis for AI Research'],1,",",138.0
The Lack of A Priori Distinctions Between Learning Algorithms,604954a3600f749b25a9f52317a42d13a8ec0339,"[{'authorId': '144509099', 'name': 'M. Kauffman'}, {'authorId': '81426350', 'name': 'San Mateo'}, {'authorId': '3011435', 'name': 'E. Prescott'}]",1996.0,,['X-Risk Analysis for AI Research'],1,"Plutowski, M., et al. (1994). Cross-validation estimates integrated mean squared error. In Advances in neural information processing systems 6, Cowan et al. some constant set by k and m. It's also true that E(C OTS | φ, d X) is not drastically different if one considers d X 's with a different m'. Accordingly, our summand doesn't vary drastically between d X 's of one m' and d X 's of another. Since n >> m and π(x) is uniform though, almost all of the terms in the sum have m' = m.",726.0
The Structure of Normative Ethics,fa200345ac9482ac69aa21355b0889b220d2e4c8,"[{'authorId': '39134453', 'name': 'S. Kagan'}]",1992.0,,['X-Risk Analysis for AI Research'],1,"If you open a typical textbook on normative ethics, it will have a discussion of what it thinks of as rival theories: there might be a chapter each on, say, utilitarianism, contractarianism, and virtue theory. The assumption seems to be that these three are alternative attempts to answer the same basic questions. This seems to me exactly incorrect At least as they are ordinarily understood, these three are addressing three different concerns in normative ethics, and in principle-although almost never in practice-they are completely compatible. Failure to see this, I believe, is due to our failure to have an adequate ""map"" of the structure of normative ethics. We lack an adequate account of what the various theories in normative ethics are trying to accomplish. It is not that there is nothing at all like a received view concerning the nature of normative ethics: roughly, normative ethics involves the attempt to state and defend the basic principles of morality. It is concerned with determining which actions are right, which wrong, what is permitted and what forbidden. Similarly, it might be said to treat the basic moral rights, duties, virtues, and so on. So far as it goes, there is nothing wrong with this account. But it does not go very far. It is like a map of a country which only displays its border. Such a map can serve to distinguish one country from another-as our account can help to set off normative ethics from metaethics-but it gives no significant detail about the internal features of the country. What we lack-what the received view does not give us-is a sense of the major regions of normative ethics and how they are related to one another. Lacking an adequate guide to the structure of normative ethics we can fail to recognize what we are doing when we compare and evaluate the specific normative theories that have been offered. In this essay, then, I want to try to lay out the basic outlines of a more",37.0
The bases of social power.,d0c0bccc2b4820310260ffbcb9a347fc54feee7a,"[{'authorId': '2052330889', 'name': 'J. R. French'}, {'authorId': '104041475', 'name': 'B. Raven'}]",1959.0,,['X-Risk Analysis for AI Research'],1,,6799.0
X-Risk Analysis for AI Research,3ba793e937cb90ea3e82b4a6903ee4a95f307ddf,"[{'authorId': '3422872', 'externalIds': {'DBLP': ['Dan Hendrycks']}, 'url': 'https://www.semanticscholar.org/author/3422872', 'name': 'Dan Hendrycks', 'aliases': None, 'affiliations': ['UC Berkeley'], 'homepage': 'danhendrycks.com', 'paperCount': 49, 'citationCount': 11806, 'hIndex': 28}, {'authorId': '16787428', 'externalIds': {'DBLP': ['Mantas Mazeika']}, 'url': 'https://www.semanticscholar.org/author/16787428', 'name': 'Mantas Mazeika', 'aliases': ['Mantas Mažeika'], 'affiliations': [], 'homepage': None, 'paperCount': 18, 'citationCount': 2928, 'hIndex': 12}]",2022.0,arXiv.org,['X-Risk Analysis for AI Research'],1,"Artificial intelligence (AI) has the potential to greatly improve society, but as with any powerful technology, it comes with heightened risks and responsibilities. Current AI research lacks a systematic discussion of how to manage long-tail risks from AI systems, including speculative long-term risks. Keeping in mind the potential benefits of AI, there is some concern that building ever more intelligent and powerful AI systems could eventually result in systems that are more powerful than us; some say this is like playing with fire and speculate that this could create existential risks (x-risks). To add precision and ground these discussions, we provide a guide for how to analyze AI x-risk, which consists of three parts: First, we review how systems can be made safer today, drawing on time-tested concepts from hazard analysis and systems safety that have been designed to steer large processes in safer directions. Next, we discuss strategies for having long-term impacts on the safety of future systems. Finally, we discuss a crucial concept in making AI systems safer by improving the balance between safety and general capabilities. We hope this document and the presented concepts and tools serve as a useful guide for understanding how to analyze AI x-risk.",17.0
Human Compatible: Artificial Intelligence and the Problem of Control,6df2126301ab415aed034b0bcd9589b1897fe983,"[{'authorId': '2055581632', 'externalIds': {'DBLP': ['Stuart Russell']}, 'url': 'https://www.semanticscholar.org/author/2055581632', 'name': 'Stuart Russell', 'aliases': None, 'affiliations': [], 'homepage': None, 'paperCount': 7, 'citationCount': 453, 'hIndex': 4}]",2019.0,,['Human Compatible: Artificial Intelligence and the Problem of Control'],1,"""The most important book I have read in quite some time"" (Daniel Kahneman); ""A must-read"" (Max Tegmark); ""The book we've all been waiting for"" (Sam Harris) LONGLISTED FOR THE 2019 FINANCIAL TIMES AND MCKINSEY BUSINESS BOOK OF THE YEAR; A FINANCIAL TIMES BEST BOOK OF THE YEAR 2019 Humans dream of super-intelligent machines. But what happens if we actually succeed? Creating superior intelligence would be the biggest event in human history. Unfortunately, according to the world's pre-eminent AI expert, it could also be the last. In this groundbreaking book on the biggest question facing humanity, Stuart Russell explains why he has come to consider his own discipline an existential threat to our species, and lays out how we can change course before it's too late. There is no one better placed to assess the promise and perils of the dominant technology of the future than Russell, who has spent decades at the forefront of AI research. Through brilliant analogies and crisp, lucid prose, he explains how AI actually works, how it has an enormous capacity to improve our lives - but why we must ensure that we never lose control of machines more powerful than we are. Here Russell shows how we can avert the worst threats by reshaping the foundations of AI to guarantee that machines pursue our objectives, not theirs. Profound, urgent and visionary, Human Compatible is the one book everyone needs to read to understand a future that is coming sooner than we think.",326.0
"The Magical Number Seven, Plus or Minus Two - PLUNGE",3bf64859abc522258d90984312e88dc7661ef630,"[{'authorId': '46991184', 'name': 'Jeremy Mayall'}, {'authorId': '2054006638', 'name': 'James Sutherland'}]",2016.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"The Magical Number Seven, Plus or Minus Two 
A moving image and sound collaboration with James Sutherland 
“Miller’s Law is the number of objects an average human being can hold in working memory. In this artwork we investigate this to explore Autonoetic Memory and Endel Tulving’s notion of Mental Time Travel, the voluntary or involuntary ability to project one’s self in a past or future situation often triggered by an external stimulus such as a familiar sound, smell or image. The footage is from journeys taken in 2010 and 2013 and will be reminiscent to many but specific to only one. Images and sound work in a generative way, creating moments of synchresis before separating. The quality of the image is painterly, impressionistic and indistinct. A fading memory. The viewer is invited to hold the information and to find the pattern through the multiple layers of image and sound.”",167.0
The Unilateralist’s Curse and the Case for a Principle of Conformity,2085ee72311e79d195a9ebfe85683bdd72131281,"[{'authorId': '2193691', 'name': 'N. Bostrom'}, {'authorId': '115086706', 'name': 'Thomas Douglas'}, {'authorId': '144816231', 'name': 'A. Sandberg'}]",2016.0,Social Epistemology,"['Superintelligence: Paths, Dangers, Strategies']",1,"In some situations a number of agents each have the ability to undertake an initiative that would have significant effects on the others. Suppose that each of these agents is purely motivated by an altruistic concern for the common good. We show that if each agent acts on her own personal judgment as to whether the initiative should be undertaken, then the initiative will be undertaken more often than is optimal. We suggest that this phenomenon, which we call the unilateralist’s curse, arises in many contexts, including some that are important for public policy. To lift the curse, we propose a principle of conformity, which would discourage unilateralist action. We consider three different models for how this principle could be implemented, and respond to an objection that could be raised against it.",22.0
Adaptation And Natural Selection A Critique Of Some Current Evolutionary Thought,05c47e3b830ae7a6dedfe77c041137ab07488340,"[{'authorId': '152156915', 'name': 'Lena Osterhagen'}]",2016.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"Thank you very much for reading adaptation and natural selection a critique of some current evolutionary thought. Maybe you have knowledge that, people have search hundreds times for their favorite readings like this adaptation and natural selection a critique of some current evolutionary thought, but end up in infectious downloads. Rather than reading a good book with a cup of tea in the afternoon, instead they cope with some malicious bugs inside their computer.",477.0
An Introduction To Modern Astrophysics,32404cc7f8214c4a24c719dbf3ce690eb0f72546,"[{'authorId': '101222170', 'name': 'Doreen Schweizer'}]",2016.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,,331.0
The Making Of The Atomic Bomb,a63f3e33364d4c75eecd7fba61fe68916863de36,"[{'authorId': '122036082', 'name': 'Laura Schweitzer'}]",2016.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"Thank you very much for downloading the making of the atomic bomb. Maybe you have knowledge that, people have look numerous times for their chosen books like this the making of the atomic bomb, but end up in malicious downloads. Rather than enjoying a good book with a cup of coffee in the afternoon, instead they juggled with some infectious virus inside their computer. the making of the atomic bomb is available in our book collection an online access to it is set as public so you can download it instantly. Our book servers spans in multiple locations, allowing you to get the most less latency time to download any of our books like this one. Kindly say, the the making of the atomic bomb is universally compatible with any devices to read.",198.0
Fifty Years of Moore's Law,91774682023d0d0e053cbb66b26ad3d32a5adfed,"[{'authorId': '47489981', 'name': 'M. Golio'}]",2015.0,Proceedings of the IEEE,"['Superintelligence: Paths, Dangers, Strategies']",1,,309.0
"If materialism is true, the United States is probably conscious",ea71117fc14ae12e6e82f1376e5483095ec03f45,"[{'authorId': '3687664', 'name': 'Eric Schwitzgebel'}]",2015.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,,84.0
"I, Cyborg",e1d826162645c3e218573397a3ecafd1d96ab327,"[{'authorId': '6201215', 'name': 'E. Pearlman'}]",2015.0,PAJ : Journal of Performance and Art,"['Superintelligence: Paths, Dangers, Strategies']",1,"Cyborgs can be grouped into three basic types: those that use mechanical elements, those that use electronic elements, and those that just use cybernetics as part of their body. These categories are not fixed and can easily overlap. For example, someone with a hand prosthesis could be categorized as mechanical, but if the prosthesis was equipped with a camera it would become electronic. If the prosthesis could sense and relay to the user the difference between feeling a hot or cold surface, the device would be defined as cybernetic.",15.0
Response to Stephen Hawking ’ s Physics-as-Philosophy by,bd8c4203d4ebbd536dffaad0e967f5e69ce58b03,"[{'authorId': '121917862', 'name': 'Wolfgang Smith'}]",2015.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"The Grand Design,1 to be sure, is not simply another “Physics for the Millions” production, nor is Stephen Hawking, its primary author, just another scientist addressing the public at large. What stands at issue is rather to be seen as the crossing of a threshold, an event comparable, in a way, to the publication of Charles Darwin’s magnum opus a century and a half ago. There have always been physicists who make it a point, in the name of science, to dispatch the “God-hypothesis”; what confronts us, however, in The Grand Design is something more. It is the spectacle of a physics, no less, presuming to explain how the universe itself came to be: “why there is something rather than nothing” as Hawking declares. The answer to this supreme conundrum, we are told, can now be given on rigorous mathematical grounds by physics itself: such is the “breakthrough” the treatise proposes to expound in terms simple enough to fall within the purview of the non-specialist.We need also to remind ourselves that following the demise of Albert Einstein, it is Stephen Hawking who has become, in the public eye, the premiere physicist: the lone figure that personifies the wizardry of mathematical physics as such. Add this fact to the brilliance of the book itself, and one begins to sense the magnitude of its likely impact, the effect upon millions of the claim that a mathematical physics has trashed the sacred wisdom of mankind!This contention must not go unanswered. It calls for a definitive response, a rigorous refutation; and such I propose to present in the sequel with the help of Almighty God: the very God whose existence has supposedly been disproved.",1.0
Radical Abundance: How a Revolution In Nanotechnology Will Change Civilization,4780e8610ebb0a0f19ec1fb8f8fecf2072dd3db1,"[{'authorId': '2069586992', 'name': 'J. Cordeiro'}]",2014.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,,14.0
"Embodiment and the inner life: Cognition and consciousness in the space of possible minds, M. Shanahan. Oxford University Press (2010)",73e0b748d01ebe21df4b3480c3f1d228775f0880,"[{'authorId': '52176346', 'name': 'J. Glazebrook'}]",2014.0,Cognitive Systems Research,"['Superintelligence: Paths, Dangers, Strategies']",1,,25.0
The Social and Economic Impacts of Cognitive Enhancement,08dd5bfa505a893defd59c3796f6345f0f7c6507,"[{'authorId': '144816231', 'name': 'A. Sandberg'}, {'authorId': '4159060', 'name': 'J. Savulescu'}]",2014.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,,45.0
"Childhood intelligence is heritable, highly polygenic and associated with FNBP1L",c7574e5156406500b2d88d97a6a5ef91cffdc35f,"[{'authorId': '6300162', 'name': 'B. Benyamin'}, {'authorId': '105049807', 'name': 'B. Pourcain'}, {'authorId': '143860129', 'name': 'O. Davis'}, {'authorId': '50522961', 'name': 'G. Davies'}, {'authorId': '144776648', 'name': 'N. Hansell'}, {'authorId': '34651471', 'name': 'M. Brion'}, {'authorId': '36197129', 'name': 'R. Kirkpatrick'}, {'authorId': '6156110', 'name': 'R. Cents'}, {'authorId': '3855671', 'name': 'S. Franić'}, {'authorId': '2107491884', 'name': 'M. Miller'}, {'authorId': '153028567', 'name': 'C. Haworth'}, {'authorId': '3499841', 'name': 'E. Meaburn'}, {'authorId': '144398457', 'name': 'T. Price'}, {'authorId': '145570180', 'name': 'D. Evans'}, {'authorId': '143937740', 'name': 'N. Timpson'}, {'authorId': '1909038', 'name': 'J. Kemp'}, {'authorId': '143630059', 'name': 'S. Ring'}, {'authorId': '6340612', 'name': 'W. McArdle'}, {'authorId': '145056501', 'name': 'S. Medland'}, {'authorId': '120157155', 'name': 'J. Yang'}, {'authorId': '152807453', 'name': 'S. Harris'}, {'authorId': '4820794', 'name': 'D. Liewald'}, {'authorId': '2655638', 'name': 'P. Scheet'}, {'authorId': '2148844235', 'name': 'X. Xiao'}, {'authorId': '145043122', 'name': 'J. Hudziak'}, {'authorId': '52191289', 'name': 'E. D. Geus'}, {'authorId': '49875026', 'name': 'V. Jaddoe'}, {'authorId': '152614549', 'name': 'J. Starr'}, {'authorId': '34815500', 'name': 'F. Verhulst'}, {'authorId': '90525323', 'name': 'C. Pennell'}, {'authorId': '2882863', 'name': 'H. Tiemeier'}, {'authorId': '82225263', 'name': 'W. Iacono'}, {'authorId': '1850838', 'name': 'L. Palmer'}, {'authorId': '81404249', 'name': 'G. Montgomery'}, {'authorId': '143609115', 'name': 'N. Martin'}, {'authorId': '4155273', 'name': 'D. Boomsma'}, {'authorId': '145866943', 'name': 'D. Posthuma'}, {'authorId': '2941423', 'name': 'M. McGue'}, {'authorId': '145112828', 'name': 'M. Wright'}, {'authorId': '2118649758', 'name': 'G. D. Smith'}, {'authorId': '153805058', 'name': 'I. Deary'}, {'authorId': '2611761', 'name': 'R. Plomin'}, {'authorId': '120318286', 'name': 'P. Visscher'}]",2014.0,Molecular Psychiatry,"['Superintelligence: Paths, Dangers, Strategies']",1,,246.0
Embryo Selection for Cognitive Enhancement: Curiosity or Game-changer?,923f1f265ac44a5d71bc06c3360aea07ec9d4ac4,"[{'authorId': '3389522', 'name': 'Carl Shulman'}, {'authorId': '2193691', 'name': 'N. Bostrom'}]",2014.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"capital is an important determinant of individual and aggregate economic outcomes, and a major input to sci- entific progress. It has been suggested that advances in genomics may open up new avenues to enhance human intel- lectual abilities genetically, complementing environmental interventions such as education and nutrition. One way to do this would be via embryo selection in the context of in vitro fertilization (IVF). In this article, we analyze the feasibil- ity, timescale, and possible societal impacts of embryo selection for cognitive enhancement. We find that embryo selec- tion, on its own, may have significant (but likely not drastic) impacts over the next 50 years, though large effects could accumulate over multiple generations. However, there is a complementary technology - stem cell-derived gametes - which has been making rapid progress and which could amplify the impact of embryo selection, enabling very large changes if successfully applied to humans.",31.0
Clues for Consequentialists,3f4d3d2ebe809a0a02a8f7e94eb089dfa7f85d65,"[{'authorId': '1405403224', 'name': 'Joanna Burch-Brown'}]",2014.0,Utilitas,"['Superintelligence: Paths, Dangers, Strategies']",1,"In an influential paper, James Lenman argues that consequentialism can provide no basis for ethical guidance, because we are irredeemably ignorant of most of the consequences of our actions. If our ignorance of distant consequences is great, he says, we can have little reason to recommend one action over another on consequentialist grounds. In this article, I show that for reasons to do with statistical theory, the cluelessness objection is too pessimistic. We have good reason to believe that certain patterns of action will tend to have better consequences, and we have good reason to recommend acting in accordance with strategies based on those advantageous patterns. I close by saying something about the strategies that this argument should lead us to favour.",23.0
Stem cells: Egg engineers,375ab8c6adaa99ed4d5fbf5891fefd2408accb3d,"[{'authorId': '4754010', 'name': 'D. Cyranoski'}]",2013.0,Nature,"['Superintelligence: Paths, Dangers, Strategies']",1,,13.0
Eternity in six hours: Intergalactic spreading of intelligent life and sharpening the Fermi paradox,800d2e79c64c2dd23247761cfdcc16f678118446,"[{'authorId': '2054678912', 'name': 'S. Armstrong'}, {'authorId': '144816231', 'name': 'A. Sandberg'}]",2013.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,,83.0
"GWAS of 126,559 Individuals Identifies Genetic Variants Associated with Educational Attainment",3c530df4ddbee85e46221ceb9f20e01b2d27d19b,"[{'authorId': '2539949', 'name': 'C. A. Rietveld'}, {'authorId': '145056501', 'name': 'S. Medland'}, {'authorId': '50026725', 'name': 'J. Derringer'}, {'authorId': '46478134', 'name': 'Jian Yang'}, {'authorId': '52200875', 'name': 'T. Esko'}, {'authorId': '152384205', 'name': 'Nicolas W. Martin'}, {'authorId': '28025600', 'name': 'H. Westra'}, {'authorId': '5225506', 'name': 'K. Shakhbazov'}, {'authorId': '145634215', 'name': 'A. Abdellaoui'}, {'authorId': '145972498', 'name': 'A. Agrawal'}, {'authorId': '2821023', 'name': 'E. Albrecht'}, {'authorId': '145794424', 'name': 'B. Alizadeh'}, {'authorId': '143668860', 'name': 'N. Amin'}, {'authorId': '26723379', 'name': 'J. Barnard'}, {'authorId': '2489219', 'name': 'S. Baumeister'}, {'authorId': '2300369', 'name': 'K. Benke'}, {'authorId': '114913336', 'name': 'L. Bielak'}, {'authorId': '50191835', 'name': 'J. Boatman'}, {'authorId': '34221678', 'name': 'P. Boyle'}, {'authorId': '50522961', 'name': 'G. Davies'}, {'authorId': '2131147', 'name': 'C. D. de Leeuw'}, {'authorId': '6657443', 'name': 'N. Eklund'}, {'authorId': '38183065', 'name': 'D. S. Evans'}, {'authorId': '1768777540', 'name': 'Rudolf Ferhmann'}, {'authorId': '143683261', 'name': 'K. Fischer'}, {'authorId': '3281482', 'name': 'C. Gieger'}, {'authorId': '3557081', 'name': 'H. Gjessing'}, {'authorId': '8187690', 'name': 'S. Hägg'}, {'authorId': '2578196', 'name': 'J. Harris'}, {'authorId': '145458897', 'name': 'C. Hayward'}, {'authorId': '50051712', 'name': 'C. Holzapfel'}, {'authorId': '1399846499', 'name': 'C. Ibrahim-Verbaas'}, {'authorId': '2105755', 'name': 'E. Ingelsson'}, {'authorId': '143791709', 'name': 'B. Jacobsson'}, {'authorId': '145603994', 'name': 'P. Joshi'}, {'authorId': '47019214', 'name': 'A. Jugessur'}, {'authorId': '47235381', 'name': 'M. Kaakinen'}, {'authorId': '4811583', 'name': 'S. Kanoni'}, {'authorId': '1382239167', 'name': 'J. Karjalainen'}, {'authorId': '5092515', 'name': 'I. Kolčić'}, {'authorId': '4343816', 'name': 'K. Kristiansson'}, {'authorId': '1813216', 'name': 'Z. Kutalik'}, {'authorId': '143861932', 'name': 'J. Lahti'}, {'authorId': '2152572096', 'name': 'S. Lee'}, {'authorId': '143704655', 'name': 'P. Lin'}, {'authorId': '27973798', 'name': 'P. Lind'}, {'authorId': '2108776122', 'name': 'Yongmei Liu'}, {'authorId': '3531388', 'name': 'K. Lohman'}, {'authorId': '2975182', 'name': 'M. Loitfelder'}, {'authorId': '152202927', 'name': 'George Mcmahon'}, {'authorId': '46784327', 'name': 'P. M. Vidal'}, {'authorId': '46534214', 'name': 'Osorio Meirelles'}, {'authorId': '35264228', 'name': 'L. Milani'}, {'authorId': '5309079', 'name': 'R. Myhre'}, {'authorId': '50986878', 'name': 'Marja-Liisa Nuotio'}, {'authorId': '50241695', 'name': 'C. Oldmeadow'}, {'authorId': '144344066', 'name': 'K. Petrovic'}, {'authorId': '3704328', 'name': 'W. Peyrot'}, {'authorId': '144250569', 'name': 'O. Polašek'}, {'authorId': '47538706', 'name': 'L. Quaye'}, {'authorId': '6393856', 'name': 'E. Reinmaa'}, {'authorId': '2008756', 'name': 'J. Rice'}, {'authorId': '5484863', 'name': 'T. S. Rizzi'}, {'authorId': '145020176', 'name': 'H. Schmidt'}, {'authorId': '145626527', 'name': 'R. Schmidt'}, {'authorId': '152286463', 'name': 'A. Smith'}, {'authorId': '1946691289', 'name': 'Jennifer A. Smith'}, {'authorId': '1612997394', 'name': 'Toshiko Tanaka'}, {'authorId': '3093096', 'name': 'A. Terracciano'}, {'authorId': '3700143', 'name': 'Matthijs J. H. M. van der Loos'}, {'authorId': '145072252', 'name': 'V. Vitart'}, {'authorId': '2771544', 'name': 'H. Völzke'}, {'authorId': '3765343', 'name': 'J. Wellmann'}, {'authorId': '2213788655', 'name': 'Lei Yu'}, {'authorId': '38480589', 'name': 'Wei Zhao'}, {'authorId': '3467657', 'name': 'J. Allik'}, {'authorId': '145400381', 'name': 'J. Attia'}, {'authorId': '7364307', 'name': 'S. Bandinelli'}, {'authorId': '3735762', 'name': 'F. Bastardot'}, {'authorId': '47258565', 'name': 'Jonathan P. Beauchamp'}, {'authorId': '144486770', 'name': 'D. Bennett'}, {'authorId': '144234305', 'name': 'K. Berger'}, {'authorId': '2922928', 'name': 'L. Bierut'}, {'authorId': '4155273', 'name': 'D. Boomsma'}, {'authorId': '5857028', 'name': 'U. Bültmann'}, {'authorId': '145731439', 'name': 'H. Campbell'}, {'authorId': '1950933', 'name': 'C. Chabris'}, {'authorId': '2194586', 'name': 'L. Cherkas'}, {'authorId': '3850893', 'name': 'M. Chung'}, {'authorId': '3549167', 'name': 'F. Cucca'}, {'authorId': '2664309', 'name': 'M. de Andrade'}, {'authorId': '143772783', 'name': 'P. D. De Jager'}, {'authorId': '1741081187', 'name': 'J. De Neve'}, {'authorId': '145805110', 'name': 'I. Deary'}, {'authorId': '89574384', 'name': 'G. Dedoussis'}, {'authorId': '144365082', 'name': 'P. Deloukas'}, {'authorId': '2939668', 'name': 'M. Dimitriou'}, {'authorId': '143951627', 'name': 'G. Eiriksdottir'}, {'authorId': '4090206', 'name': 'Martin F. Elderson'}, {'authorId': '145805757', 'name': 'J. Eriksson'}, {'authorId': '145570180', 'name': 'D. Evans'}, {'authorId': '40209353', 'name': 'J. Faul'}, {'authorId': '1737506', 'name': 'L. Ferrucci'}, {'authorId': '144423560', 'name': 'M. Garcia'}, {'authorId': '7023600', 'name': 'H. Grönberg'}, {'authorId': '3856275', 'name': 'V. Guðnason'}, {'authorId': '144549954', 'name': 'P. Hall'}, {'authorId': '2107819651', 'name': 'J. Harris'}, {'authorId': '1906665', 'name': 'T. Harris'}, {'authorId': '3908103', 'name': 'N. Hastie'}, {'authorId': '143839085', 'name': 'A. Heath'}, {'authorId': '145562703', 'name': 'D. Hernandez'}, {'authorId': '145328545', 'name': 'W. Hoffmann'}, {'authorId': '29841188', 'name': 'A. Hofman'}, {'authorId': '145960690', 'name': 'R. Holle'}, {'authorId': '143930820', 'name': 'E. Holliday'}, {'authorId': '8540513', 'name': 'J. Hottenga'}, {'authorId': '3113827', 'name': 'W. Iacono'}, {'authorId': '2144009', 'name': 'T. Illig'}, {'authorId': '145757905', 'name': 'M. Järvelin'}, {'authorId': '3238967', 'name': 'M. Kähönen'}, {'authorId': '2232640', 'name': 'J. Kaprio'}, {'authorId': '36197129', 'name': 'R. Kirkpatrick'}, {'authorId': '3845681', 'name': 'M. Kowgier'}, {'authorId': '2731507', 'name': 'A. Latvala'}, {'authorId': '144170749', 'name': 'L. Launer'}, {'authorId': '49643913', 'name': 'D. Lawlor'}, {'authorId': '2248433', 'name': 'T. Lehtimäki'}, {'authorId': '49298499', 'name': 'Jingmei Li'}, {'authorId': '145749630', 'name': 'P. Lichtenstein'}, {'authorId': '48775216', 'name': 'P. Lichtner'}, {'authorId': '4820794', 'name': 'D. Liewald'}, {'authorId': '2005348', 'name': 'P. Madden'}, {'authorId': '144740819', 'name': 'P. Magnusson'}, {'authorId': '31810552', 'name': 'T. Mäkinen'}, {'authorId': '49428129', 'name': 'Marco Masala'}, {'authorId': '2941423', 'name': 'M. McGue'}, {'authorId': '2632713', 'name': 'A. Metspalu'}, {'authorId': '5168868', 'name': 'A. Mielck'}, {'authorId': '49929414', 'name': 'Michael B. Miller'}, {'authorId': '145566508', 'name': 'G. Montgomery'}, {'authorId': '3741186', 'name': 'S. Mukherjee'}, {'authorId': '1914037', 'name': 'D. Nyholt'}, {'authorId': '5848075', 'name': 'B. Oostra'}, {'authorId': '1850838', 'name': 'L. Palmer'}, {'authorId': '5355550', 'name': 'A. Palotie'}, {'authorId': '145365790', 'name': 'B. Penninx'}, {'authorId': '2548112', 'name': 'M. Perola'}, {'authorId': '6642027', 'name': 'P. Peyser'}, {'authorId': '6276909', 'name': 'M. Preisig'}, {'authorId': '5306851', 'name': 'K. Räikkönen'}, {'authorId': '1912681', 'name': 'O. Raitakari'}, {'authorId': '3629800', 'name': 'A. Realo'}, {'authorId': '143630059', 'name': 'S. Ring'}, {'authorId': '2407947', 'name': 'S. Ripatti'}, {'authorId': '145069287', 'name': 'F. Rivadeneira'}, {'authorId': '144790343', 'name': 'I. Rudan'}, {'authorId': '1796592', 'name': 'A. Rustichini'}, {'authorId': '2960501', 'name': 'V. Salomaa'}, {'authorId': '38875165', 'name': 'Antti-Pekka Sarin'}, {'authorId': '1831712', 'name': 'D. Schlessinger'}, {'authorId': '34239286', 'name': 'R. Scott'}, {'authorId': '27745062', 'name': 'H. Snieder'}, {'authorId': '7948240', 'name': 'B. St Pourcain'}, {'authorId': '2227355', 'name': 'J. Starr'}, {'authorId': '3239550', 'name': 'J. Sul'}, {'authorId': '2536019', 'name': 'I. Surakka'}, {'authorId': '3119349', 'name': 'R. Svento'}, {'authorId': '2075822', 'name': 'A. Teumer'}, {'authorId': '2071234801', 'name': 'H. Tiemeier'}, {'authorId': '35004233', 'name': 'F. V. van Rooij'}, {'authorId': '10213254', 'name': 'D. V. Van Wagoner'}, {'authorId': '2105906', 'name': 'E. Vartiainen'}, {'authorId': '2376766', 'name': 'J. Viikari'}, {'authorId': '2579385', 'name': 'P. Vollenweider'}, {'authorId': '4183813', 'name': 'J. Vonk'}, {'authorId': '4261209', 'name': 'G. Waeber'}, {'authorId': '1966315', 'name': 'D. Weir'}, {'authorId': '116396754', 'name': 'H. Wichmann'}, {'authorId': '143720222', 'name': 'E. Widén'}, {'authorId': '4091064', 'name': 'G. Willemsen'}, {'authorId': '152239065', 'name': 'James F. Wilson'}, {'authorId': '145243949', 'name': 'A. Wright'}, {'authorId': '145623552', 'name': 'D. Conley'}, {'authorId': '1397415516', 'name': 'G. Davey-Smith'}, {'authorId': '144755531', 'name': 'L. Franke'}, {'authorId': '2594542', 'name': 'P. Groenen'}, {'authorId': '2158355875', 'name': 'A. Hofman'}, {'authorId': '145281248', 'name': 'M. Johannesson'}, {'authorId': '1961726191', 'name': 'S. Kardia'}, {'authorId': '92146758', 'name': 'R. Krueger'}, {'authorId': '3467819', 'name': 'David I. Laibson'}, {'authorId': '145356345', 'name': 'N. Martin'}, {'authorId': '2113593126', 'name': 'M. Meyer'}, {'authorId': '145866943', 'name': 'D. Posthuma'}, {'authorId': '144896812', 'name': 'A. Thurik'}, {'authorId': '143937740', 'name': 'N. Timpson'}, {'authorId': '145613058', 'name': 'A. Uitterlinden'}, {'authorId': '145102347', 'name': 'C. V. van Duijn'}, {'authorId': '2211652', 'name': 'P. Visscher'}, {'authorId': '34802004', 'name': 'D. Benjamin'}, {'authorId': '1817212', 'name': 'D. Cesarini'}, {'authorId': '6990559', 'name': 'P. Koellinger'}]",2013.0,Science,"['Superintelligence: Paths, Dangers, Strategies']",1,"Genetic College Many genomic elements in humans are associated with behavior, including educational attainment. In a genome-wide association study including more than 100,000 samples, Rietveld et al. (p. 1467, published online 30 May; see the Perspective by Flint and Munafò) looked for genes related to educational attainment in Caucasians. Small genetic effects at three loci appeared to impact educational attainment. Three genetic loci are found to explain variation associated with educational achievement. [Also see Perspective by Flint and Munafò] A genome-wide association study (GWAS) of educational attainment was conducted in a discovery sample of 101,069 individuals and a replication sample of 25,490. Three independent single-nucleotide polymorphisms (SNPs) are genome-wide significant (rs9320913, rs11584700, rs4851266), and all three replicate. Estimated effects sizes are small (coefficient of determination R2 ≈ 0.02%), approximately 1 month of schooling per allele. A linear polygenic score from all measured SNPs accounts for ≈2% of the variance in both educational attainment and cognitive function. Genes in the region of the loci have previously been associated with health, cognitive, and central nervous system phenotypes, and bioinformatics analyses suggest the involvement of the anterior caudate nucleus. These findings provide promising candidate SNPs for follow-up work, and our effect size estimates can anchor power analyses in social-science genetics.",802.0
Very long range global Population Scenarios to 2300 and the Implications of Sustained low Fertility,9454f52403abe4d89f2cbf9de2c88a48f0c0cdb9,"[{'authorId': '5961641', 'name': 'S. Basten'}, {'authorId': '143615959', 'name': 'W. Lutz'}, {'authorId': '3990297', 'name': 'S. Scherbov'}]",2013.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"Depending on whether the global level of fertility is assumed to converge to the current European TFR (~1.5) or that of Southeast Asia or Central America (~2.5), global population will either decline to 2.3-2.9 billion by 2200 or increase to 33-37 billion, if mortality continues to decline. Furthermore, sizeable human populations exist when the 'voluntary chosen' ideal family size is heavily concentrated around one child per woman with TFRs as low as 0.6-0.8. However, the UN population projections to 2300 use a much narrower band of possible future TFRs. 
 
If the two-child norm is not necessarily the end-point transition, what would be the consequences of the currently reported low fertility rates being sustained and becoming widespread? 
 
We present new projections for 13 IPCC world regions with scenarios calculated on the basis of regular cohort-component projections by age and sex in single-year time steps up to 2300, each based upon a much broader set of fertility assumptions than currently employed. We create three mortality scenarios based upon maximum life expectancies of 90, 100, 110, as well as a series of ""Special"" scenarios. 
 
Even under conditions of further substantial increases in life expectancy, world population size would decline significantly if the world in the longer run followed the examples of Europe and East Asia. 
In contrast to Malthusian disaster scenarios, our exercise illustrates the distinct possibility of significant population shrinking associated with increasing life expectancy and human well-being.",38.0
In vitro eugenics,9b57c068eb8a005be496b307f725f482bbcc2c46,"[{'authorId': '52504962', 'name': 'R. Sparrow'}]",2013.0,Journal of Medical Ethics,"['Superintelligence: Paths, Dangers, Strategies']",1,"A series of recent scientific results suggest that, in the not-too-distant future, it will be possible to create viable human gametes from human stem cells. This paper discusses the potential of this technology to make possible what I call ‘in vitro eugenics’: the deliberate breeding of human beings in vitro by fusing sperm and egg derived from different stem-cell lines to create an embryo and then deriving new gametes from stem cells derived from that embryo. Repeated iterations of this process would allow scientists to proceed through multiple human generations in the laboratory. In vitro eugenics might be used to study the heredity of genetic disorders and to produce cell lines of a desired character for medical applications. More controversially, it might also function as a powerful technology of ‘human enhancement’ by allowing researchers to use all the techniques of selective breeding to produce individuals with a desired genotype.",41.0
Who Rises to the Top? Early Indicators,9dd22663e2cb19e874fddc0977c42bfac720c950,"[{'authorId': '4005187', 'name': 'H. Kell'}, {'authorId': '50516908', 'name': 'D. Lubinski'}, {'authorId': '4670685', 'name': 'C. Benbow'}]",2013.0,Psychology Science,"['Superintelligence: Paths, Dangers, Strategies']",1,"Youth identified before age 13 (N = 320) as having profound mathematical or verbal reasoning abilities (top 1 in 10,000) were tracked for nearly three decades. Their awards and creative accomplishments by age 38, in combination with specific details about their occupational responsibilities, illuminate the magnitude of their contribution and professional stature. Many have been entrusted with obligations and resources for making critical decisions about individual and organizational well-being. Their leadership positions in business, health care, law, the professoriate, and STEM (science, technology, engineering, and mathematics) suggest that many are outstanding creators of modern culture, constituting a precious human-capital resource. Identifying truly profound human potential, and forecasting differential development within such populations, requires assessing multiple cognitive abilities and using atypical measurement procedures. This study illustrates how ultimate criteria may be aggregated and longitudinally sequenced to validate such measures.",150.0
Ethical machines,6a402b98c96c8e53812cd1f55050c7496503fc8f,"[{'authorId': '145179124', 'name': 'I. Good'}]",2013.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"The notion of an ethical machine can be interpreted in more than one way. Perhaps the most important interpretation is a machine that can generalize from existing literature to infer one or more consistent ethical systems and can work out their consequences. A n ultra - in te ll ig en t machine should be able to do this, and that is one reason for not fearing it.",8.0
"Shall We Vote on Values , But Bet on Beliefs ?",e52025d1704b50e4fccd4c16d9901528e0621950,"[{'authorId': '145624941', 'name': 'G. Mason'}]",2013.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"Policy disputes arise at all scales of governance: in clubs, non-profits, firms, nations, and alliances of nations. Both the means and ends of policy are disputed. While many, perhaps most, policy disputes arise from conflicting ends, important disputes also arise from differing beliefs on how to achieve shared ends. In fact, according to many experts in economics and development, governments often choose policies that are "" inefficient "" in the sense that most everyone could expect to gain from other feasible policies. Many other kinds of experts also see existing policies as often clearly inferior to known alternatives. If inferior policies would not have been adopted had most everyone known they are inferior, and if someone somewhere knew or could have learned that they are inferior, then we can blame inferior policies on a failure of our "" info "" institutions. By "" info "" here I just mean clues and analysis that should change our beliefs. Our info institutions are those within which we induce, express, and evaluate the acquiring and sharing of info. policies happen because our info institutions fail to induce people to acquire and share relevant info with properly-motivated decision makers.",8.0
Intelligence Explosion Microeconomics,74cda7d4006b26bd623f39c8ac16a8151d56a0e5,"[{'authorId': '2542795', 'name': 'Eliezer Yudkowsky'}]",2013.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"I. J. Good’s thesis of the “intelligence explosion” states that a sufficiently advanced machine intelligence could build a smarter version of itself, which could in turn build an even smarter version, and that this process could continue to the point of vastly exceeding human intelligence. As Sandberg (2010) correctly notes, there have been several attempts to lay down return on investment formulas intended to represent sharp speedups in economic or technological growth, but very little attempt has been made to deal formally with Good’s intelligence explosion thesis as such. I identify the key issue as returns on cognitive reinvestment—the ability to invest more computing power, faster computers, or improved cognitive algorithms to yield cognitive labor which produces larger brains, faster brains, or better mind designs. There are many phenomena in the world which have been argued to be evidentially relevant to this question, from the observed course of hominid evolution, to Moore’s Law, to the competenceovertimeofmachinechess-playingsystems, andmanymore. Igointosome depth on some debates which then arise on how to interpret such evidence. I propose that the next step in analyzing positions on the intelligence explosion would be to formalize return on investment curves, so that each stance can formally state which possible microfoundations they hold to be falsified by historical observations. More generally,",39.0
The Better Angels of Our Nature: Why Violence Has Declined,e7cdea2e7a82a9daaaa884c926057481d37ca091,"[{'authorId': '41159837', 'name': 'Kayla Jordan'}, {'authorId': '14591495', 'name': 'Geoffrey W. Sutton'}]",2012.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"THE BETTER ANGELS OF OUR NATURE: WHY VIOLENCE HAS DECLINED. Steven Pinker, Penguin, New York, NY 2011. ISBN 978-0-670-02295-3.There's been a shooting in a Sikh Temple this morning. A lone gunman entered a Colorado theater and opened fire. Syrians are now engaged in civil war. Faced with daily news stories of death and destruction, it is easy to believe that things are getting worse. Not so, explains Harvard psychologist, Steven Pinker in his new work, The Better Angels of Our Nature: Why Violence has Declined.Pinker combines in-depth historical research with rigorous psychological research to argue the case for a decline in global violence. As Pinker aptly points out, many people look at our age as one of unprecedented violence and terror to be viewed with pessimism. Drawing on historical analysis, psychological research and findings from related sciences such as anthropology, sociology, and economics Pinker argues that the data paint a very different picture. In the first chapter, Pinker takes the reader on a quick journey through the history of the world pointing out that the ancient and medieval worlds were very different than the world we live in today. Numerous prehistoric skeletons bear evidence of very violent deaths. Ancient people destroyed entire tribes. Romans carried out violent executions. Medieval Knights led lies of violence and other Europeans meted out horrendous punishments for acts which might not even be judged worthy of condemnation in today's democracies. Finally, the early 20th century saw two World Wars before the long peace ensued. In light of that history, Pinker argues that perhaps we should reconsider our assumptions about our own world.In the first section of the book, Pinker identifies six historical trends which could have led to declines in violence. The first trend he calls the Pacification Process by which people gave up nomadic hunting and gathering lives for lives of agriculture in cities. Competition and anarchy in the prehistoric world made violence necessary for survival. The development of agriculture called for greater cooperation between individuals and the formation of governments to impose order created a world where violence was not always in one's best interest. Statistical analysis supports the idea that the emergence of states lead to a decline in violence. The second trend, the Civilizing Process, is an idea he developed from the work of Norbert Elias. In the late medieval and early modern periods, etiquette and social norms began to be important in social interactions, economics and technology began to advance, and governments began to become more centralized. This trend was also accompanied by a decline in violence. The third trend is the Humanitarian Revolution during which people began to increasingly find practices, such as torture, capital punishment, war and slavery, morally questionable. Empathy, compassion, and peace became important characteristics. The fourth trend is the Long Peace, which stems from the realization that since World War II no two major world powers have gone to war and, in spite of predictions to the contrary, nuclear weapons have never been used. …",1070.0
Rate of de novo mutations and the importance of father’s age to disease risk,0ed1900ac200ce5bf2d4d4e88cbcfb92977afd58,"[{'authorId': '145032143', 'name': 'A. Kong'}, {'authorId': '3807258', 'name': 'M. Frigge'}, {'authorId': '145016671', 'name': 'G. Másson'}, {'authorId': '145780968', 'name': 'S. Besenbacher'}, {'authorId': '2659228', 'name': 'P. Sulem'}, {'authorId': '35148284', 'name': 'Gísli Magnússon'}, {'authorId': '40558520', 'name': 'S. A. Gudjonsson'}, {'authorId': '2052608', 'name': 'A. Sigurdsson'}, {'authorId': '4779075', 'name': 'Á. Jónasdóttir'}, {'authorId': '35445175', 'name': 'A. Jonasdottir'}, {'authorId': '143664500', 'name': 'Wendy S. W. Wong'}, {'authorId': '47858702', 'name': 'G. Sigurdsson'}, {'authorId': '143915625', 'name': 'G. Walters'}, {'authorId': '40095443', 'name': 'S. Steinberg'}, {'authorId': '5388289', 'name': 'H. Helgason'}, {'authorId': '4714342', 'name': 'G. Thorleifsson'}, {'authorId': '2068997', 'name': 'D. Gudbjartsson'}, {'authorId': '145002379', 'name': 'A. Helgason'}, {'authorId': '40597604', 'name': 'O. Magnusson'}, {'authorId': '3857821', 'name': 'U. Thorsteinsdóttir'}, {'authorId': '145074995', 'name': 'K. Stefánsson'}]",2012.0,Nature,"['Superintelligence: Paths, Dangers, Strategies']",1,,1720.0
Entity Linking at Web Scale,c5c08e6dec3bf8a036607593e11e389697e03f45,"[{'authorId': '144739109', 'name': 'Thomas Lin'}, {'authorId': '2674444', 'name': 'Mausam'}, {'authorId': '1741101', 'name': 'Oren Etzioni'}]",2012.0,AKBC-WEKEX@NAACL-HLT,"['Superintelligence: Paths, Dangers, Strategies']",1,"This paper investigates entity linking over millions of high-precision extractions from a corpus of 500 million Web documents, toward the goal of creating a useful knowledge base of general facts. This paper is the first to report on entity linking over this many extractions, and describes new opportunities (such as corpus-level features) and challenges we found when entity linking at Web scale. We present several techniques that we developed and also lessons that we learned. We envision a future where information extraction and entity linking are paired to automatically generate knowledge bases with billions of assertions over millions of linked entities.",117.0
3D segmentation of SBFSEM images of neuropil by a graphical model over supervoxel boundaries,9a645f47ba018ea91b99657fa300ac942e26361e,"[{'authorId': '2064022407', 'name': 'Björn Andres'}, {'authorId': '1708103', 'name': 'U. Köthe'}, {'authorId': '1908018', 'name': 'Thorben Kröger'}, {'authorId': '2495450', 'name': 'M. Helmstaedter'}, {'authorId': '1997979', 'name': 'K. Briggman'}, {'authorId': '144922222', 'name': 'W. Denk'}, {'authorId': '1685187', 'name': 'F. Hamprecht'}]",2012.0,Medical Image Anal.,"['Superintelligence: Paths, Dangers, Strategies']",1,,49.0
"An existing, ecologically-successful genus of collectively intelligent artificial creatures",c0cea197bd0827f5d5e864b91ff77ccecc43a21d,"[{'authorId': '145585296', 'name': 'B. Kuipers'}]",2012.0,arXiv.org,"['Superintelligence: Paths, Dangers, Strategies']",1,"People sometimes worry about the Singularity (Vinge 1993, Kurzweil 2005), or about the world being taken over by articially intelligent robots. I believe the risks of these are very small. However, few people recognize that we already share our world with articial creatures that participate as intelligent agents in our society: corporations. Our planet is inhabited by two distinct kinds of intelligent beings | individual humans and corporate entities | whose natures and interests are intimately linked. To co-exist well, we need to nd ways to dene the rights and responsibilities of both individual humans and corporate entities, and to nd ways to ensure that corporate entities behave as responsible members of society.",4.0
A Nonlinear Model for Hippocampal Cognitive Prosthesis: Memory Facilitation by Hippocampal Ensemble Stimulation,7961e7ce8549e88f1e258dbd28c12b9560ce6ea2,"[{'authorId': '1720595', 'name': 'R. Hampson'}, {'authorId': '145910948', 'name': 'D. Song'}, {'authorId': '1766423', 'name': 'Rosa H. M. Chan'}, {'authorId': '145589830', 'name': 'A. Sweatt'}, {'authorId': '3981764', 'name': 'M. R. Riley'}, {'authorId': '1855680', 'name': 'G. Gerhardt'}, {'authorId': '2107132632', 'name': 'D. C. Shin'}, {'authorId': '1695632', 'name': 'V. Marmarelis'}, {'authorId': '1733834', 'name': 'T. Berger'}, {'authorId': '21556164', 'name': 'S. Deadwyler'}]",2012.0,IEEE transactions on neural systems and rehabilitation engineering,"['Superintelligence: Paths, Dangers, Strategies']",1,"Collaborative investigations have characterized how multineuron hippocampal ensembles encode memory necessary for subsequent successful performance by rodents in a delayed nonmatch to sample (DNMS) task and utilized that information to provide the basis for a memory prosthesis to enhance performance. By employing a unique nonlinear dynamic multi-input/multi-output (MIMO) model, developed and adapted to hippocampal neural ensemble firing patterns derived from simultaneous recorded CA1 and CA3 activity, it was possible to extract information encoded in the sample phase necessary for successful performance in the nonmatch phase of the task. The extension of this MIMO model to online delivery of electrical stimulation delivered to the same recording loci that mimicked successful CA1 firing patterns, provided the means to increase levels of performance on a trial-by-trial basis. Inclusion of several control procedures provides evidence for the specificity of effective MIMO model generated patterns of electrical stimulation. Increased utility of the MIMO model as a prosthesis device was exhibited by the demonstration of cumulative increases in DNMS task performance with repeated MIMO stimulation over many sessions on both stimulation and nonstimulation trials, suggesting overall system modification with continued exposure. Results reported here are compatible with and extend prior demonstrations and further support the candidacy of the MIMO model as an effective cortical prosthesis.",60.0
"IQ tests are not for machines, yet",0b174015283b9744f351795621d4b29c10df4aca,"[{'authorId': '1745871', 'name': 'D. Dowe'}, {'authorId': '1398777358', 'name': 'J. Hernández-Orallo'}]",2012.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,,59.0
Advances in auditory prostheses.,f3f981213e2b8840ea42a6ed125640fabb4f234d,"[{'authorId': '32648075', 'name': 'R. Shannon'}]",2012.0,Current Opinion in Neurology,"['Superintelligence: Paths, Dangers, Strategies']",1,"PURPOSE OF REVIEW
Auditory prostheses use electric currents on multiple electrodes to stimulate auditory neurons and recreate auditory sensations in deaf people. Cochlear implants have restored hearing in more than 200  000 deaf adults and children to a level that allows most to understand speech. Here we review the reasons underlying these results and describe new directions in restoring hearing to additional patient populations and the design of new devices.


RECENT FINDINGS
From their early development about 50 years ago, cochlear implants have been well received and beneficial to people who had lost their hearing. Although those first implants did not allow high levels of speech understanding, they provided auditory information that worked synergistically with lip reading to improve communication. Present day cochlear implants provide excellent speech understanding in children and in postlingually deafened adults. Research is focused on improved signal processing and new electrode designs. Electric stimulation of the auditory brainstem can also produce excellent hearing in some children and adults.


SUMMARY
Auditory prostheses, both at the level of the sensory nerve and at the brainstem, can restore patterns of neural activation that are sufficient for high levels of speech understanding. These prostheses are not only clinically successful but also important tools for understanding sensory processing in the brain.",49.0
Retinal implants: emergence of a multidisciplinary field.,ab78fb0fc16b4125a807ce0d7fe38dc4ce1576db,"[{'authorId': '3187893', 'name': 'G. Dagnelie'}]",2012.0,Current Opinion in Neurology,"['Superintelligence: Paths, Dangers, Strategies']",1,"PURPOSE OF REVIEW
This review summarizes the current status of retinal prostheses, recent accomplishments, and major remaining research, engineering, and rehabilitation challenges.


RECENT FINDINGS
Retinal research, materials and biocompatibility studies, and clinical trials in patients blind from retinitis pigmentosa are representative of an emerging field with considerable promise and sobering challenges. A summary of progress in dozens of laboratories, companies, and clinics around the world is presented through a synopsis of relevant studies, not only to summarize the progress but also to convey the remarkable increase in interest, effort, and outside funding this field has enjoyed.


SUMMARY
At present, clinical applications of retinal implant technology are dominated by one or two groups/companies, but the field is wide open for others to take the lead through novel approaches in technology, tissue interfacing, information transfer paradigms, and rehabilitation. Where the field will go in the next few years is almost anybody's guess, but that it will move forward is a certainty.",64.0
Less Income Inequality and More Growth – Are they Compatible? Part 6. The Distribution of Wealth,252b8e7d517514aa0e8ebcdc812e1328c06ff59d,"[{'authorId': '100775335', 'name': 'Kaja Fredriksen'}]",2012.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"The wealth distribution within OECD countries is very concentrated and much more so than the income distribution. Wealth dispersion is especially high in the United States and Sweden. The latter illustrates that the most wealth unequal countries are not necessarily the most income unequal. Wealth inequality came down since the beginning of the 20th century until the 1970s, but has since been on the rise. Major explanations for this development are soaring financial markets in the aftermath of financial market deregulation in the 1970s, a lighter taxation of top incomes and wealth, which has favoured the accumulation of wealth, and the rising importance of inheritances and inter vivos gifts. Moins d'inegalites de revenu et plus de croissance – Ces deux objectifs sont-ils compatibles ?Partie 6. La repartition de la richesse Dans les pays de l’OCDE, la repartition de la richesse est tres concentree, bien plus encore que celle des revenus. Elle est particulierement asymetrique aux Etats-Unis et en Suede. Ce dernier pays est l’exemple d’un pays ou les inegalites de richesse sont fortes alors que les inegalites de revenu y sont plus faibles que dans la plupart des pays de l’OCDE. Les inegalites de richesse se sont resorbees du debut du 20e siecle aux annees 70 pour se creuser de nouveau depuis lors. L’essor des marches financiers qui a fait suite a la dereglementation des marches de capitaux dans les annees 70, l’allegement de l’imposition des plus hauts revenus et de la fortune, qui a favorise l’accumulation de richesses, et l’importance croissante des heritages et des donations expliquent principalement cette evolution.",34.0
Intelligence: new findings and theoretical developments.,a3041bd3bc674eb894476a86dcb707d2b58fc88b,"[{'authorId': '2518186', 'name': 'R. Nisbett'}, {'authorId': '144066806', 'name': 'Joshua Aronson'}, {'authorId': '3381695', 'name': 'C. Blair'}, {'authorId': '6817278', 'name': 'W. Dickens'}, {'authorId': '49512862', 'name': 'J. Flynn'}, {'authorId': '2355479', 'name': 'D. Halpern'}, {'authorId': '3176133', 'name': 'E. Turkheimer'}]",2012.0,American Psychologist,"['Superintelligence: Paths, Dangers, Strategies']",1,"We review new findings and new theoretical developments in the field of intelligence. New findings include the following: (a) Heritability of IQ varies significantly by social class. (b) Almost no genetic polymorphisms have been discovered that are consistently associated with variation in IQ in the normal range. (c) Much has been learned about the biological underpinnings of intelligence. (d) ""Crystallized"" and ""fluid"" IQ are quite different aspects of intelligence at both the behavioral and biological levels. (e) The importance of the environment for IQ is established by the 12-point to 18-point increase in IQ when children are adopted from working-class to middle-class homes. (f) Even when improvements in IQ produced by the most effective early childhood interventions fail to persist, there can be very marked effects on academic achievement and life outcomes. (g) In most developed countries studied, gains on IQ tests have continued, and they are beginning in the developing world. (h) Sex differences in aspects of intelligence are due partly to identifiable biological factors and partly to socialization factors. (i) The IQ gap between Blacks and Whites has been reduced by 0.33 SD in recent years. We report theorizing concerning (a) the relationship between working memory and intelligence, (b) the apparent contradiction between strong heritability effects on IQ and strong secular effects on IQ, (c) whether a general intelligence factor could arise from initially largely independent cognitive skills, (d) the relation between self-regulation and cognitive skills, and (e) the effects of stress on intelligence.",795.0
Artificial intelligence,ccbd57a694615fe188eab7bf8b9f5e4a928f3b4a,"[{'authorId': '9083969', 'name': 'Moshe Y. Vardi'}]",2012.0,Communications of the ACM,"['Superintelligence: Paths, Dangers, Strategies', 'Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",2,"coming years, “You’re highly educated. You make a lot of money. You should still be afraid.” In fact, worries about the impact of technology on the job market are not only about the far, but also the not too far future. In a recent book, Race Against The Machine: How the Digital Revolution is Accelerating Innovation, Driving Productivity, and Irreversibly Transforming Employment and the Economy, by Erik Brynjolfsson and Andrew McAfee, the authors argue that “technological progress is accelerating innovation even as it leaves many types of workers behind.” Indeed, over the past 30 years, as we saw the personal computer morph into tablets, smartphones, and cloud computing, we also saw income inequality grow worldwide. While the loss of millions of jobs over the past few years has been attributed to the Great Recession, whose end is not yet in sight, it now seems that technology-driven productivity growth is at least a major factor. The fundamental question, I believe, is whether Herbert Simon was right, even if his timing was off, when he said “Machines will be capable ... of doing any work a man can do.” While AI has been proven to be much more difficult than early pioneers believed, its inexorable progress over the past 50 years suggests that Simon may have been right. Bill Joy’s question, therefore, deserves not to be ignored. Does the future need us?",24.0
World happiness report,a0d527f8fe4bcfeeccc5e95a4434eb7d80199508,"[{'authorId': '5142735', 'name': 'J. Helliwell'}, {'authorId': '70849631', 'name': 'R. Layard'}, {'authorId': '2355669', 'name': 'J. Sachs'}]",2012.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"The report, published by the Earth Institute and co-edited by the institute’s director, Jeffrey Sachs, reflects a new worldwide demand for more attention to happiness and absence of misery as criteria for government policy. It reviews the state of happiness in the world today and shows how the new science of happiness explains personal and national variations in happiness.",1014.0
Infinite Ethics,2076741966e208d692898a7c570571d105b7985e,"[{'authorId': '2085642383', 'name': 'Infinite Ethics'}, {'authorId': '2193691', 'name': 'N. Bostrom'}]",2012.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"Aggregative consequentialism and several other popular moral theories are threatened with paralysis: when coupled with some plausible assumptions, they seem to imply that it is always ethically indifferent what you do. Modern cosmology teaches that the world might well contain an infinite number of happy and sad people and other candidate value-bearing locations. Aggregative ethics implies that such a world contains an infinite amount of positive value and an infinite amount of negative value. You can affect only a finite amount of good or bad. In standard cardinal arithmetic, an infinite quantity is unchanged by the addition or subtraction of any finite quantity. So it appears you cannot change the value of the world. Modifications of aggregationism aimed at resolving the paralysis are only partially effective and cause severe side effects, including problems of “fanaticism”, “distortion”, and erosion of the intuitions that originally motivated the theory. Is the infinitarian challenge fatal?",43.0
The Singularity Institute Whole Brain Emulation and the Evolution of Superorganisms,fc6f1bcaa955e63b34815fffe0c754c3dc0ae760,"[{'authorId': '3389522', 'name': 'Carl Shulman'}]",2012.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"Many scientists expect the eventual development of intelligent software programs capable of closely emulating human brains, to the point of substituting for human labor in almost every economic niche. As software, such emulations could be cheaply copied, with copies subsequently diverging and interacting with their copy-relatives. This paper examines a set of evolutionary pressures on interaction between related emulations, pressures favoring the emergence of superorganisms, groups of emulations ready to selfsacrifice in service of the superorganism. We argue that the increased capacities and internal coordination of such superorganisms could pose increased risks of overriding human values, but also could facilitate the solution of global coordination problems. Shulman, Carl. 2010. Whole brain emulation and the evolution of superorganism. The Singularity Institute, San Francisco, CA.",9.0
How Hard is Artificial Intelligence? Evolutionary Arguments and Selection Effects,b77ff182ad8f8d42789368fe800c6481d372d60a,"[{'authorId': '3389522', 'name': 'Carl Shulman'}, {'authorId': '2193691', 'name': 'N. Bostrom'}]",2012.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"Several authors have made the argument that because blind evolutionary processes produced human intelligence on Earth, it should be feasible for clever human engineers to create human-level artificial intelligence in the not-too-distant future. This evolutionary argument, however, has ignored the observation selection effect that guarantees that observers will see intelligent life having arisen on their planet no matter how hard it is for intelligent life to evolve on any given Earth-like planet. We explore how the evolutionary argument might be salvaged from this objection, using a variety of considerations from observation selection theory and analysis of specific timing features and instances of convergent evolution in the terrestrial evolutionary record. We find that, depending on the resolution of disputed questions in observation selection theory, the objection can be either be wholly or moderately defused, although other challenges for the evolutionary argument remain.",20.0
Optimal Rules for Patent Races,ea974abce529767d8e8eb5fc13fbbe87f1d613e4,"[{'authorId': '35080173', 'name': 'K. Judd'}, {'authorId': '2319660', 'name': 'K. Schmedders'}, {'authorId': '2103625', 'name': 'Sevin Yeltekin'}]",2011.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"There are two important rules to patent races: minimal accomplishment necessary to receive the patent and the allocation of the innovation benefits. We study the optimal combination of these rules. A planner, who cannot distinguish between competing firms in a multistage innovation race, chooses the patent rules by maximizing either consumer or social surplus. We show that efficiency cost of prizes is a key consideration. Races are undesirable only when efficiency costs are low, firms are similar, and social surplus is maximized. Otherwise, the optimal policy involves a race of nontrivial duration to spur innovation and filter out inferior innovators.",32.0
Epigenetics and assisted reproductive technology,46fa02cebef4a60161ad782ee3949881ad424a6d,"[{'authorId': '81825171', 'name': 'A. Iliadou'}, {'authorId': '33306160', 'name': 'P. Janson'}, {'authorId': '3775273', 'name': 'S. Cnattingius'}]",2011.0,Journal of Internal Medicine,"['Superintelligence: Paths, Dangers, Strategies']",1,"Iliadou AN, Janson PCJ, Cnattingius S (Karolinska Institutet, Stockholm; Sweden). Epigenetics and assisted reproductive technology (Review). J Intern Med 2011; 270: 414–420.",50.0
The Quest for Artificial Intelligence: A History of Ideas and Achievements,39942c1a78aa11a187858bbb2e8b0e0be5ea3657,"[{'authorId': '16210036', 'name': 'D. Hutton'}]",2011.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,,15.0
Measuring Agent Intelligence via Hierarchies of Environments,3361aba22c8b7bea8578bce84afd681b55126a0d,"[{'authorId': '39109261', 'name': 'B. Hibbard'}]",2011.0,Artificial General Intelligence,"['Superintelligence: Paths, Dangers, Strategies']",1,,35.0
High-accuracy neurite reconstruction for high-throughput neuroanatomy,2781da92e831d09488c3e1f38ca6d783429a2eb2,"[{'authorId': '2495450', 'name': 'M. Helmstaedter'}, {'authorId': '1997979', 'name': 'K. Briggman'}, {'authorId': '144922222', 'name': 'W. Denk'}]",2011.0,Nature Neuroscience,"['Superintelligence: Paths, Dangers, Strategies']",1,,270.0
Deep brain stimulation: current and future clinical applications.,2bfaf27ceecbb43bcb723912b3d85abd366944c5,"[{'authorId': '7629247', 'name': 'M. Lyons'}]",2011.0,Mayo Clinic proceedings,"['Superintelligence: Paths, Dangers, Strategies']",1,"Deep brain stimulation (DBS) has developed during the past 20 years as a remarkable treatment option for several different disorders. Advances in technology and surgical techniques have essentially replaced ablative procedures for most of these conditions. Stimulation of the ventralis intermedius nucleus of the thalamus has clearly been shown to markedly improve tremor control in patients with essential tremor and tremor related to Parkinson disease. Symptoms of bradykinesia, tremor, gait disturbance, and rigidity can be significantly improved in patients with Parkinson disease. Because of these improvements, a decrease in medication can be instrumental in reducing the disabling features of dyskinesias in such patients. Primary dystonia has been shown to respond well to DBS of the globus pallidus internus. The success of these procedures has led to application of these techniques to multiple other debilitating conditions such as neuropsychiatric disorders, intractable pain, epilepsy, camptocormia, headache, restless legs syndrome, and Alzheimer disease. The literature analysis was performed using a MEDLINE search from 1980 through 2010 with the term deep brain stimulation, and several double-blind and larger case series were chosen for inclusion in this review. The exact mechanism of DBS is not fully understood. This review summarizes many of the current and potential future clinical applications of this technology.",192.0
Social effects of oxytocin in humans: context and person matter,3244535ad29829300583e78d47f704cfea515e57,"[{'authorId': '34605352', 'name': 'Jennifer A. Bartz'}, {'authorId': '2268731', 'name': 'J. Zaki'}, {'authorId': '5539614', 'name': 'N. Bolger'}, {'authorId': '2669604', 'name': 'K. Ochsner'}]",2011.0,Trends in Cognitive Sciences,"['Superintelligence: Paths, Dangers, Strategies']",1,,1322.0
Genome-wide association studies establish that human intelligence is highly heritable and polygenic,613dd8c2f0175e235753653103172e1005828d1d,"[{'authorId': '50522961', 'name': 'G. Davies'}, {'authorId': '1892590', 'name': 'A. Tenesa'}, {'authorId': '144352373', 'name': 'A. Payton'}, {'authorId': '46478134', 'name': 'Jian Yang'}, {'authorId': '1867093', 'name': 'S. Harris'}, {'authorId': '4820794', 'name': 'D. Liewald'}, {'authorId': '2702506', 'name': 'X. Ke'}, {'authorId': '4904017', 'name': 'S. Le Hellard'}, {'authorId': '46424566', 'name': 'A. Christoforou'}, {'authorId': '144895887', 'name': 'M. Luciano'}, {'authorId': '2586837', 'name': 'K. McGhee'}, {'authorId': '2370678', 'name': 'L. Lopez'}, {'authorId': '3396321', 'name': 'A. Gow'}, {'authorId': '143792523', 'name': 'J. Corley'}, {'authorId': '144631333', 'name': 'P. Redmond'}, {'authorId': '2072738785', 'name': 'H. Fox'}, {'authorId': '5417707', 'name': 'P. Haggarty'}, {'authorId': '2272065', 'name': 'L. Whalley'}, {'authorId': '145660067', 'name': 'G. Mcneill'}, {'authorId': '143759426', 'name': 'M. Goddard'}, {'authorId': '2874733', 'name': 'T. Espeseth'}, {'authorId': '144657046', 'name': 'A. Lundervold'}, {'authorId': '2551417', 'name': 'I. Reinvang'}, {'authorId': '2066480', 'name': 'A. Pickles'}, {'authorId': '144453962', 'name': 'V. Steen'}, {'authorId': '144690660', 'name': 'W. Ollier'}, {'authorId': '143653561', 'name': 'D. Porteous'}, {'authorId': '50277290', 'name': 'M. Horan'}, {'authorId': '2227355', 'name': 'J. Starr'}, {'authorId': '5241575', 'name': 'N. Pendleton'}, {'authorId': '2211652', 'name': 'P. Visscher'}, {'authorId': '145805110', 'name': 'I. Deary'}]",2011.0,Molecular Psychiatry,"['Superintelligence: Paths, Dangers, Strategies']",1,,632.0
Ontological Crises in Artificial Agents' Value Systems,0bd23d6d555cb931e82e323dcb71045774b6f93e,"[{'authorId': '39727043', 'name': 'Peter de Blanc'}]",2011.0,arXiv.org,"['Superintelligence: Paths, Dangers, Strategies']",1,"Decision-theoretic agents predict and evaluate the results of their actions using a model, or ontology, of their environment. An agent's goal, or utility function, may also be specified in terms of the states of, or entities within, its ontology. If the agent may upgrade or replace its ontology, it faces a crisis: the agent's original goal may not be well-defined with respect to its new ontology. This crisis must be resolved before the agent can make plans towards achieving its goals. 
We discuss in this paper which sorts of agents will undergo ontological crises and why we may want to create such agents. We present some concrete examples, and argue that a well-defined procedure for resolving ontological crises is needed. We point to some possible approaches to solving this problem, and evaluate these methods on our examples.",21.0
Control of a Visual Keyboard Using an Electrocorticographic Brain–Computer Interface,419f326faf00afa96d5d12731647f4f495628dcd,"[{'authorId': '2644844', 'name': 'D. Krusienski'}, {'authorId': '31595731', 'name': 'J. Shih'}]",2011.0,Neurorehabilitation and Neural Repair,"['Superintelligence: Paths, Dangers, Strategies']",1,Objective. Brain–computer interfaces (BCIs) are devices that enable severely disabled people to communicate and interact with their environments using their brain waves. Most studies investigating BCI in humans have used scalp EEG as the source of electrical signals and focused on motor control of prostheses or computer cursors on a screen. The authors hypothesize that the use of brain signals obtained directly from the cortical surface will more effectively control a communication/spelling task compared to scalp EEG. Methods. A total of 6 patients with medically intractable epilepsy were tested for the ability to control a visual keyboard using electrocorticographic (ECOG) signals. ECOG data collected during a P300 visual task paradigm were preprocessed and used to train a linear classifier to subsequently predict the intended target letters. Results. The classifier was able to predict the intended target character at or near 100% accuracy using fewer than 15 stimulation sequences in 5 of the 6 people tested. ECOG data from electrodes outside the language cortex contributed to the classifier and enabled participants to write words on a visual keyboard. Conclusions. This is a novel finding because previous invasive BCI research in humans used signals exclusively from the motor cortex to control a computer cursor or prosthetic device. These results demonstrate that ECOG signals from electrodes both overlying and outside the language cortex can reliably control a visual keyboard to generate language output without voice or limb movements.,48.0
Beyond Deep Blue: Chess in the Stratosphere,0c54174bfab62254839fd0d89e162e34ac1cd486,"[{'authorId': '1795816', 'name': 'M. Newborn'}]",2011.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"More than a decade has passed since IBMs Deep Blue computer stunned the world by defeating Garry Kasparov, the world chess champion at that time. Beyond Deep Blue tells the continuing story of the chess engine and its steady improvement. The book provides analysis of the games alongside a detailed examination of the remarkable technological progress made by the engines asking which one is best, how good is it, and how much better can it get. Features: presents a total of 118 games, played by 17 different chess engines, collected together for the first time in a single reference; details the processor speeds, memory sizes, and the number of processors used by each chess engine; includes games from 10 World Computer Chess Championships, and three computer chess tournaments of the Internet Chess Club; covers the man-machine matches between Fritz and Kramnik, and Kasparov and Deep Junior; describes three historical matches between leading engines Hydra vs. Shredder, Junior vs. Fritz, and Zappa vs. Rybka.",9.0
Neural control of cursor trajectory and click by a human with tetraplegia 1000 days after implant of an intracortical microelectrode array,77af8c9f4a0c5f2a4d2ad9ae671f84aeea086a03,"[{'authorId': '2954233', 'name': 'J. Simeral'}, {'authorId': '2109587229', 'name': 'S-P Kim'}, {'authorId': '2105795', 'name': 'Michael J. Black'}, {'authorId': '2945810', 'name': 'J. Donoghue'}, {'authorId': '2109871', 'name': 'L. Hochberg'}]",2011.0,Journal of Neural Engineering,"['Superintelligence: Paths, Dangers, Strategies']",1,"The ongoing pilot clinical trial of the BrainGate neural interface system aims in part to assess the feasibility of using neural activity obtained from a small-scale, chronically implanted, intracortical microelectrode array to provide control signals for a neural prosthesis system. Critical questions include how long implanted microelectrodes will record useful neural signals, how reliably those signals can be acquired and decoded, and how effectively they can be used to control various assistive technologies such as computers and robotic assistive devices, or to enable functional electrical stimulation of paralyzed muscles. Here we examined these questions by assessing neural cursor control and BrainGate system characteristics on five consecutive days 1000 days after implant of a 4 × 4 mm array of 100 microelectrodes in the motor cortex of a human with longstanding tetraplegia subsequent to a brainstem stroke. On each of five prospectively-selected days we performed time-amplitude sorting of neuronal spiking activity, trained a population-based Kalman velocity decoding filter combined with a linear discriminant click state classifier, and then assessed closed-loop point-and-click cursor control. The participant performed both an eight-target center-out task and a random target Fitts metric task which was adapted from a human-computer interaction ISO standard used to quantify performance of computer input devices. The neural interface system was further characterized by daily measurement of electrode impedances, unit waveforms and local field potentials. Across the five days, spiking signals were obtained from 41 of 96 electrodes and were successfully decoded to provide neural cursor point-and-click control with a mean task performance of 91.3% ± 0.1% (mean ± s.d.) correct target acquisition. Results across five consecutive days demonstrate that a neural interface system based on an intracortical microelectrode array can provide repeatable, accurate point-and-click control of a computer interface to an individual with tetraplegia 1000 days after implantation of this sensor.",444.0
Computer poker: A review,43f8e76de51b81396faacbd61c91a5ac03fab9f3,"[{'authorId': '145557817', 'name': 'Jonathan Rubin'}, {'authorId': '2056400654', 'name': 'I. Watson'}]",2011.0,Artificial Intelligence,"['Superintelligence: Paths, Dangers, Strategies']",1,,99.0
A decade’s perspective on DNA sequencing technology,9bb910631a8e126401cde7c2919d2f9966259126,"[{'authorId': '3286001', 'name': 'E. Mardis'}]",2011.0,Nature,"['Superintelligence: Paths, Dangers, Strategies']",1,,880.0
Feasibility of Whole Brain Emulation,375b1b4540121f6be993591d0e5ba43cb84708f1,"[{'authorId': '144816231', 'name': 'A. Sandberg'}]",2011.0,Conference on Philosophy and Theory of Artificial Intelligence,"['Superintelligence: Paths, Dangers, Strategies']",1,,35.0
Parasite prevalence and the worldwide distribution of cognitive ability,2ac26896cdecca0213407cada2de76ca31a934a2,"[{'authorId': '10032526', 'name': 'Christopher Eppig'}, {'authorId': '2379794', 'name': 'C. Fincher'}, {'authorId': '145716877', 'name': 'R. Thornhill'}]",2010.0,Proceedings of the Royal Society B: Biological Sciences,"['Superintelligence: Paths, Dangers, Strategies']",1,"In this study, we hypothesize that the worldwide distribution of cognitive ability is determined in part by variation in the intensity of infectious diseases. From an energetics standpoint, a developing human will have difficulty building a brain and fighting off infectious diseases at the same time, as both are very metabolically costly tasks. Using three measures of average national intelligence quotient (IQ), we found that the zero-order correlation between average IQ and parasite stress ranges from r = −0.76 to r = −0.82 (p < 0.0001). These correlations are robust worldwide, as well as within five of six world regions. Infectious disease remains the most powerful predictor of average national IQ when temperature, distance from Africa, gross domestic product per capita and several measures of education are controlled for. These findings suggest that the Flynn effect may be caused in part by the decrease in the intensity of infectious diseases as nations develop.",302.0
Anthropic Shadow: Observation Selection Effects and Human Extinction Risks,19ab5ac63c032f6184128fb0bcca09e24a10c8b2,"[{'authorId': '2547770', 'name': 'M. Ćirković'}, {'authorId': '144816231', 'name': 'A. Sandberg'}, {'authorId': '2193691', 'name': 'N. Bostrom'}]",2010.0,Risk Analysis,"['Superintelligence: Paths, Dangers, Strategies']",1,"We describe a significant practical consequence of taking anthropic biases into account in deriving predictions for rare stochastic catastrophic events. The risks associated with catastrophes such as asteroidal/cometary impacts, supervolcanic episodes, and explosions of supernovae/gamma‐ray bursts are based on their observed frequencies. As a result, the frequencies of catastrophes that destroy or are otherwise incompatible with the existence of observers are systematically underestimated. We describe the consequences of this anthropic bias for estimation of catastrophic risks, and suggest some directions for future work.",54.0
Modafinil and methylphenidate for neuroenhancement in healthy individuals: A systematic review.,936caa4dc213536595604ff74a0402f3c7fe1370,"[{'authorId': '9962504', 'name': 'D. Repantis'}, {'authorId': '145850614', 'name': 'P. Schlattmann'}, {'authorId': '2782502', 'name': 'Oona Laisney'}, {'authorId': '1999690', 'name': 'I. Heuser'}]",2010.0,Pharmacological Research,"['Superintelligence: Paths, Dangers, Strategies']",1,,493.0
Total Recall: How the E-memory Revolution Will Change Everything; DELETE: The Virtue of Forgetting in the Digital Age,1e0675185175b92b28aa46449b6181621bc9df9c,"[{'authorId': '4720217', 'name': 'Jean-François Blanchette'}]",2010.0,J. Assoc. Inf. Sci. Technol.,"['Superintelligence: Paths, Dangers, Strategies']",1,"Oblivion is enjoying a moment in the spotlight these days, with the simultaneous publication of two books predicting its imminent demise at the hands of digital media. According to both Total Recall and Delete, forgetting will soon become a thing of the past, a quirky feature of a bygone technological age, like cars that need cranking and very large cell phones. While Total Recall rejoices in the innumerable benefits—from increased productivity to immortality—the “ememory revolution” will bring, Delete is concerned that “perfect remembering” will cast a chilling shadow on individuals’ ability to think and act in the present.",141.0
A Three-Stage Genome-Wide Association Study of General Cognitive Ability: Hunting the Small Effects,f38fbcf45dfdc841f8c7db34975311e4892b19fe,"[{'authorId': '143860129', 'name': 'O. Davis'}, {'authorId': '144826042', 'name': 'L. Butcher'}, {'authorId': '39072304', 'name': 'S. Docherty'}, {'authorId': '3499841', 'name': 'E. Meaburn'}, {'authorId': '117581983', 'name': 'C. Curtis'}, {'authorId': '144888545', 'name': 'M. Simpson'}, {'authorId': '2773639', 'name': 'L. Schalkwyk'}, {'authorId': '2611761', 'name': 'R. Plomin'}]",2010.0,Behavior Genetics,"['Superintelligence: Paths, Dangers, Strategies']",1,,92.0
The neuroscience of human intelligence differences,5c485d798c1e79e74ce418f07cc456f748d2ca85,"[{'authorId': '145805110', 'name': 'I. Deary'}, {'authorId': '5579520', 'name': 'L. Penke'}, {'authorId': '1757156', 'name': 'W. Johnson'}]",2010.0,Nature Reviews Neuroscience,"['Superintelligence: Paths, Dangers, Strategies']",1,,918.0
"Rate, molecular spectrum, and consequences of human mutation",9546177984079e6b75193e6cc605fd6d4d28d682,"[{'authorId': '144302338', 'name': 'M. Lynch'}]",2010.0,Proceedings of the National Academy of Sciences of the United States of America,"['Superintelligence: Paths, Dangers, Strategies']",1,"Although mutation provides the fuel for phenotypic evolution, it also imposes a substantial burden on fitness through the production of predominantly deleterious alleles, a matter of concern from a human-health perspective. Here, recently established databases on de novo mutations for monogenic disorders are used to estimate the rate and molecular spectrum of spontaneously arising mutations and to derive a number of inferences with respect to eukaryotic genome evolution. Although the human per-generation mutation rate is exceptionally high, on a per-cell division basis, the human germline mutation rate is lower than that recorded for any other species. Comparison with data from other species demonstrates a universal mutational bias toward A/T composition, and leads to the hypothesis that genome-wide nucleotide composition generally evolves to the point at which the power of selection in favor of G/C is approximately balanced by the power of random genetic drift, such that variation in equilibrium genome-wide nucleotide composition is largely defined by variation in mutation biases. Quantification of the hazards associated with introns reveals that mutations at key splice-site residues are a major source of human mortality. Finally, a consideration of the long-term consequences of current human behavior for deleterious-mutation accumulation leads to the conclusion that a substantial reduction in human fitness can be expected over the next few centuries in industrialized societies unless novel means of genetic intervention are developed.",681.0
Bootstrapping Safe AGI Goal Systems,a96592f2bc3829e74ee5819cad6d9e1bf05122f9,[],2010.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"The field of machine ethics seeks methods to ensure that future intelligent machines will act in ways beneficial to human beings. Machine ethics is relevant to a wide range of possible artificial agents, but becomes especially difficult and especially important when the agents in question have at least humanlevel intelligence. This paper describes a solution, originally proposed by Yudkowsky (2004), to the problem of what goals to give such agents: rather than attempt to explicitly program in any specific normative theory (a project which would face numerous philosophical and immediate ethical difficulties), we should implement a system to discover what goals we would, upon reflection, want such agents to have. We discuss the motivations for and details of this approach, comparing it to other suggested methods for creating 'artificial moral agents' (Wallach & Collin 2007), and describe underspecified and uncertain areas for further research.",1.0
"The 10,000 Year Explosion: How Civilization Accelerated Human Evolution",54b2a1d7101da98ef6927fd15fd7e8a3cf6ffc97,"[{'authorId': '39641762', 'name': 'G. Gorelik'}, {'authorId': '5926998', 'name': 'T. Shackelford'}]",2010.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"n The 10,000 Year Explosion, biological anthropologists Gregory Cochran and Henry Harpending attempt to prove that evolution within our species, Homo sapiens sapiens, has accelerated over the past 10,000 years. The notion that our species has been subject to the same mechanisms of evolution (natural selection, nonrandom mating, mutation, genetic drift and gene flow) over the past 10,000 years as all other organisms on the planet is an important hypothesis to defend. Several evolutionary anthropologists and biologists over the past few decades have supported an idea that evolution ceased to affect our species after ‘biological modernity’ was achieved during ‘The Great-Leap Forward’ 40,000 years ago (Diamond 1992:32; Gould 2000:18-19; Cronin 2003:53). This hypothesis is not only unsupported by recent genetic evidence (e.g., Cochran and Harpending 2009:18-19; Jablonski and Chaplin 2010:8962; Gerbault et al. 2011:863), but it also reinforces the idea that because humans are cultural animals we have somehow managed to become immune to universal evolutionary processes. The central purpose of The 10,000 Year Explosion is to dismantle this",150.0
The group selection controversy.,41ea7c105dae5d97e980e64995969460f0625919,"[{'authorId': '144684493', 'name': 'E. Leigh'}]",2010.0,Journal of Evolutionary Biology,"['Superintelligence: Paths, Dangers, Strategies']",1,"Many thought Darwinian natural selection could not explain altruism. This error led Wynne-Edwards to explain sustainable exploitation in animals by selection against overexploiting groups. Williams riposted that selection among groups rarely overrides within-group selection. Hamilton showed that altruism can evolve through kin selection. How strongly does group selection influence evolution? Following Price, Hamilton showed how levels of selection interact: group selection prevails if Hamilton's rule applies. Several showed that group selection drove some major evolutionary transitions. Following Hamilton's lead, Queller extended Hamilton's rule, replacing genealogical relatedness by the regression on an actor's genotypic altruism of interacting neighbours' phenotypic altruism. Price's theorem shows the generality of Hamilton's rule. All instances of group selection can be viewed as increasing inclusive fitness of autosomal genomes. Nonetheless, to grasp fully how cooperation and altruism evolve, most biologists need more concrete concepts like kin selection, group selection and selection among individuals for their common good.",111.0
Clinical Applications of Brain-Computer Interfaces: Current State and Future Prospects,f2c4082faeff5d63b0144ef371c8964621ee33bf,"[{'authorId': '3118472', 'name': 'J. Mak'}, {'authorId': '2707196', 'name': 'J. Wolpaw'}]",2009.0,IEEE Reviews in Biomedical Engineering,"['Superintelligence: Paths, Dangers, Strategies']",1,"Braincomputer interfaces (BCIs) allow their users to communicate or control external devices using brain signals rather than the brain's normal output pathways of peripheral nerves and muscles. Motivated by the hope of restoring independence to severely disabled individuals and by interest in further extending human control of external systems, researchers from many fields are engaged in this challenging new work. BCI research and development has grown explosively over the past two decades. Efforts have begun recently to provide laboratory-validated BCI systems to severely disabled individuals for real-world applications. In this paper, we discuss the current status and future prospects of BCI technology and its clinical applications. We will define BCI, review the BCI-relevant signals from the human brain, and describe the functional components of BCIs. We will also review current clinical applications of BCI technology and identify potential users and potential applications. Lastly, we will discuss current limitations of BCI technology, impediments to its widespread clinical use, and expectations for the future.",433.0
"The DARPA Urban Challenge: Autonomous Vehicles in City Traffic, George Air Force Base, Victorville, California, USA",213f46b27a8ea9a113b2be776695cfdbf064b3fd,"[{'authorId': '143629205', 'name': 'M. Buehler'}, {'authorId': '1792303', 'name': 'K. Iagnemma'}, {'authorId': '2118413567', 'name': 'Sanjiv Singh'}]",2009.0,The DARPA Urban Challenge,"['Superintelligence: Paths, Dangers, Strategies']",1,,592.0
The Humean Theory of Motivation Reformulated and Defended,d59963e935b6dde19f9fb30c945459ab213ba8cd,"[{'authorId': '103616223', 'name': 'Neil Sinhababu'}]",2009.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,,64.0
Guest Editors' Introduction: The New Frontier of Human-Level Artificial Intelligence,1bc3dc58064b9fe08092a29db03a69d1e9167bfe,"[{'authorId': '144811753', 'name': 'J. Beal'}, {'authorId': '2060896', 'name': 'P. Winston'}]",2009.0,IEEE Intelligent Systems,"['Superintelligence: Paths, Dangers, Strategies']",1,"Within the field of human-level intelligence, researchers are combining a variety of approaches toward the goals of human-like breadth, flexibility, and resilience for artificial intelligence systems. Each of the four papers in this special issue brings a different background and perspective on the subject, and hence a different technical approach.",10.0
Late Pleistocene Demography and the Appearance of Modern Human Behavior,91fa9dc4b433909893e42d62ac003035d91f3851,"[{'authorId': '50484864', 'name': 'Adam Powell'}, {'authorId': '49338442', 'name': 'S. Shennan'}, {'authorId': '2017856', 'name': 'Mark George Thomas'}]",2009.0,Science,"['Superintelligence: Paths, Dangers, Strategies']",1,"War and Peace? Modern behavior, including the development of advanced tools, musical instruments, and art, seems to have arisen in humans in stages. The earliest hints are seen in Africa about 70 to 90,000 years ago, but later in Europe about 45,000 years ago. An ongoing discussion centers on the origins and significance of human prosociality. During early human development, could the benefits of altruistic behavior have outweighed its costs (see the Perspective by Mace)? Bowles (p. 1293) constructed a model of conflict between groups of humans and extracted estimates of the critical parameters from archaeological and ethnographic data sets. Provocatively, it appears that warfare might have enhanced the emergence and persistence of altruistic behavior. Powell et al. (p. 1298) present a population model that shows that the development of modern behaviors may rely on the attainment of critical population densities and migratory patterns required for stable cultural transmission. The model is consistent with genetic inferences of population dynamics in Africa and Europe and suggests that these cultural changes may not solely reflect increased cognitive evolution. Population size and migration account for modern human behavior appearing in Africa about 90,000 years ago but much later across Europe. The origins of modern human behavior are marked by increased symbolic and technological complexity in the archaeological record. In western Eurasia this transition, the Upper Paleolithic, occurred about 45,000 years ago, but many of its features appear transiently in southern Africa about 45,000 years earlier. We show that demography is a major determinant in the maintenance of cultural complexity and that variation in regional subpopulation density and/or migratory activity results in spatial structuring of cultural skill accumulation. Genetic estimates of regional population size over time show that densities in early Upper Paleolithic Europe were similar to those in sub-Saharan Africa when modern behavior first appeared. Demographic factors can thus explain geographic variation in the timing of the first appearance of modern behavior without invoking increased cognitive capacity.",864.0
The Future of Humanity,df4885751476e028cae817e4de897deed873ce7b,"[{'authorId': '2193691', 'name': 'N. Bostrom'}]",2009.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,,109.0
Equal numbers of neuronal and nonneuronal cells make the human brain an isometrically scaled‐up primate brain,8144a2a93531af58c87418054380aa31d14fd763,"[{'authorId': '40373716', 'name': 'Frederico A. C. Azevedo'}, {'authorId': '1935325360', 'name': 'Ludmila R.B. Carvalho'}, {'authorId': '7291859', 'name': 'L. Grinberg'}, {'authorId': '3186373', 'name': 'J. Farfel'}, {'authorId': '94279254', 'name': 'R. E. Ferretti'}, {'authorId': '144195063', 'name': 'R. Leite'}, {'authorId': '2105654860', 'name': 'W. J. Filho'}, {'authorId': '2599002', 'name': 'R. Lent'}, {'authorId': '1398972191', 'name': 'S. Herculano‐Houzel'}]",2009.0,The Journal of comparative neurology,"['Superintelligence: Paths, Dangers, Strategies']",1,"The human brain is often considered to be the most cognitively capable among mammalian brains and to be much larger than expected for a mammal of our body size. Although the number of neurons is generally assumed to be a determinant of computational power, and despite the widespread quotes that the human brain contains 100 billion neurons and ten times more glial cells, the absolute number of neurons and glial cells in the human brain remains unknown. Here we determine these numbers by using the isotropic fractionator and compare them with the expected values for a human‐sized primate. We find that the adult male human brain contains on average 86.1 ± 8.1 billion NeuN‐positive cells (“neurons”) and 84.6 ± 9.8 billion NeuN‐negative (“nonneuronal”) cells. With only 19% of all neurons located in the cerebral cortex, greater cortical size (representing 82% of total brain mass) in humans compared with other primates does not reflect an increased relative number of cortical neurons. The ratios between glial cells and neurons in the human brain structures are similar to those found in other primates, and their numbers of cells match those expected for a primate of human proportions. These findings challenge the common view that humans stand out from other primates in their brain composition and indicate that, with regard to numbers of neuronal and nonneuronal cells, the human brain is an isometrically scaled‐up primate brain. J. Comp. Neurol. 513:532–541, 2009. © 2009 Wiley‐Liss, Inc.",1831.0
How Good Can It Get? Analysing and Improving OCR Accuracy in Large Scale Historic Newspaper Digitisation Programs,4e96ae0f84e47df5c8709e8d57275d98a55c1c0b,"[{'authorId': '2393255', 'name': 'R. Holley'}]",2009.0,D-Lib Magazine,"['Superintelligence: Paths, Dangers, Strategies']",1,"This article details the work undertaken by the National Library of Australia Newspaper Digitisation Program on identifying and testing solutions to improve OCR accuracy in large scale newspaper digitisation programs. In 2007 and 2008 several different solutions were identified, applied and tested on digitised material now available in the Australian Newspapers Digitisation Program beta service http://ndpbeta.nla.gov.au/ndp/del/home. This article gives a state of the art overview of how OCR software works on newspapers, factors that effect OCR accuracy, methods of measuring accuracy, methods of improving accuracy, and testing methods and results for specific solutions that were considered viable for large scale text digitisation projects.",174.0
The PhilPapers surveys,ded90a47fc9efaf89e61b93f978a83f36fceac2a,"[{'authorId': '3133904', 'name': 'D. Bourget'}, {'authorId': '2091082624', 'name': 'D. Chalmers'}]",2009.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,,17.0
"‘ Cognitive Enhancement : Methods , Ethics , Regulatory Challenges ’ ’",b000c5bdb7564cbbb64b63e858a7a8a12d88557f,"[{'authorId': '1968549', 'name': 'Andrew Askland'}]",2009.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"In an age of scientific breakthroughs whose potential applications rival the most imaginative science fiction and are regularly sensationalized for entertainment profits or political gain, recurrent efforts to forbid various kinds of scientific inquiry do not surprise. Those efforts raise concerns for many observers. The concerns largely focus on interventions by national governments, though international regulation is also relevant (Bostrom and Sandberg 2009). One family of concerns is that any attempt to limit scientific inquiry would be fraught with errors of administration. The debate about which inquiries should be limited would be manipulated too easily to serve partisan political purposes so that the substantive issues would be buried beneath a morass of subterfuge and pretense. Further, whatever agreements might be reached would suffer bureaucratic inefficiencies as the parties tasked to monitor the agreement would inevitably misconstrue its terms and intentions and seriously hinder the work of those being monitored. It is a tale of many grey suits interfering with important work on the basis of ineptly devised regulations and caution-begging incentives. A more comprehensive response attacks the presumption of legitimacy for government intervention and refuses to concede that science ought to be managed by government. This response probably acknowledges exceptions in the egregious instances where threats to public health and safety are clear and convincing. This view likely recognizes the social dimension of science, i.e., that it is a function of the interests and perspectives of the human beings who do the work, and so acknowledges that governments will fund the inquiries that match their ideological interests. However, it posits the baseline view that scientific inquiry qua science presumptively should be free and that efforts to limit it must satisfy substantial burdens before they can be endorsed.",62.0
Dutch Book Arguments,160b2df0f34da961d593769bcd4ea7d495029bbb,"[{'authorId': '145033113', 'name': 'A. Hájek'}]",2009.0,The Handbook of Rational and Social Choice,"['Superintelligence: Paths, Dangers, Strategies']",1,,63.0
SMART POLICY: COGNITIVE ENHANCEMENT AND THE PUBLIC INTEREST,eeb35a685a5f7c86415bfb5140adffd4e58b58ad,"[{'authorId': '2193691', 'name': 'N. Bostrom'}, {'authorId': '47335668', 'name': 'Rebecca Roache'}, {'authorId': '2110111212', 'name': 'James Martin'}]",2009.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"Cognitive enhancement may be defined as the amplification or extension of core capacities of the mind through improvement or augmentation of internal or external information processing systems. Cognition refers to the processes an organism uses to organize information. These include acquiring information (perception), selecting (attention), representing (understanding) and retaining (memory) information, and using it to guide behavior (reasoning and coordination of motor outputs). Interventions to improve cognitive function may be directed at any of these core faculties. Many methods for enhancing cognition are of a quite mundane nature, and some have been practiced for thousands of years. The prime example is education and training, where the goal is often not only to impart specific skills or information but also to improve general mental faculties such as concentration, memory, and critical thinking. Other forms of mental training, such as yoga, martial arts, meditation, and creativity courses are also in common use. Caffeine is widely used to improve alertness. Herbal extracts reputed to improve memory are popular, with sales of Ginko biloba alone on the order of several hundred million dollars per year in the U.S. In an ordinary supermarket or health food store we can find a veritable cornucopia of energy drinks and similar preparations, vying for consumers hoping to turbo-charge their brains. As cognitive neuroscience has advanced, the list of prospective biomedical enhancements has steadily expanded. Yet to date, the most dramatic advances in our",66.0
"Graphical Models, Exponential Families, and Variational Inference",d98d0d1900b13b87aa4ffd6b69c046beb63f0434,"[{'authorId': '1721860', 'name': 'M. Wainwright'}, {'authorId': '1694621', 'name': 'Michael I. Jordan'}]",2008.0,Found. Trends Mach. Learn.,"['Superintelligence: Paths, Dangers, Strategies']",1,"The formalism of probabilistic graphical models provides a unifying framework for capturing complex dependencies among random variables, and building large-scale multivariate statistical models. Graphical models have become a focus of research in many statistical, computational and mathematical fields, including bioinformatics, communication theory, statistical physics, combinatorial optimization, signal and image processing, information retrieval and statistical machine learning. Many problems that arise in specific instances — including the key problems of computing marginals and modes of probability distributions — are best studied in the general setting. Working with exponential family representations, and exploiting the conjugate duality between the cumulant function and the entropy for exponential families, we develop general variational representations of the problems of computing likelihoods, marginal probabilities and most probable configurations. We describe how a wide variety of algorithms — among them sum-product, cluster variational methods, expectation-propagation, mean field methods, max-product and linear programming relaxation, as well as conic programming relaxations — can all be understood in terms of exact or approximate forms of these variational representations. The variational approach provides a complementary alternative to Markov chain Monte Carlo as a general source of approximation methods for inference in large-scale statistical models.",4162.0
Brain–computer interface in paralysis,a7c437a2cc7d44b0bea8ef4af94359b21f8cc85a,"[{'authorId': '145198983', 'name': 'N. Birbaumer'}, {'authorId': '3704483', 'name': 'A. Murguialday'}, {'authorId': '2141461', 'name': 'L. Cohen'}]",2008.0,Current Opinion in Neurology,"['Superintelligence: Paths, Dangers, Strategies']",1,"Purpose of reviewCommunication with patients suffering from locked-in syndrome and other forms of paralysis is an unsolved challenge. Movement restoration for patients with chronic stroke or other brain damage also remains a therapeutic problem and available treatments do not offer significant improvements. This review considers recent research in brain–computer interfaces (BCIs) as promising solutions to these challenges. Recent findingsExperimentation with nonhuman primates suggests that intentional goal directed movements of the upper limbs can be reconstructed and transmitted to external manipulandum or robotic devices controlled from a relatively small number of microelectrodes implanted into movement-relevant brain areas after some training, opening the door for the development of BCI or brain–machine interfaces in humans. Although noninvasive BCIs using electroencephalographic recordings or event-related-brain-potentials in healthy individuals and patients with amyotrophic lateral sclerosis or stroke can transmit up to 80 bits/min of information, the use of BCIs – invasive or noninvasive – in severely or totally paralyzed patients has met some unforeseen difficulties. SummaryInvasive and noninvasive BCIs using recordings from nerve cells, large neuronal pools such as electrocorticogram and electroencephalography, or blood flow based measures such as functional magnetic resonance imaging and near-infrared spectroscopy show potential for communication in locked-in syndrome and movement restoration in chronic stroke, but controlled phase III clinical trials with larger populations of severely disturbed patients are urgently needed.",269.0
Brain-Computer Interfaces: An international assessment of research and development trends,de1d9ac8834da960fc71aa8c0299f4d78fd0ea28,"[{'authorId': '1733834', 'name': 'T. Berger'}, {'authorId': '33959477', 'name': 'J. Chapin'}, {'authorId': '1855680', 'name': 'G. Gerhardt'}, {'authorId': '143931490', 'name': 'D. McFarland'}, {'authorId': '143961030', 'name': 'J. Príncipe'}, {'authorId': '1862720', 'name': 'W. Soussou'}, {'authorId': '121168849', 'name': 'Dawn M. Taylor'}, {'authorId': '3138468', 'name': 'P. Tresco'}]",2008.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,,59.0
A Review of Gregory Clark's A Farewell to Alms: A Brief Economic History of the World,1fefeaf988dc63102b244f643e6acf955a0fe4bc,"[{'authorId': '143622405', 'name': 'R. Allen'}]",2008.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"A Farewell to Alms advances striking claims about the economic history of the world. These include (1) the preindustrial world was in a Malthusian preventive check equilibrium, (2) living standards were unchanging and above subsistence for the last 100,000 years, (3) bad institutions were not the cause of economic backwardness, (4) successful economic growth was due to the spread of ""middle class"" values from the elite to the rest of society for ""biological"" reasons, (5) workers were the big gainers in the British Industrial Revolution, and (6) the absence of middle class values, for biological reasons, explains why most of the world is poor. The empirical support for these claims is examined, and all are questionable.",57.0
"Intricate Ethics: Rights, Responsibilities, and Permissible Harm",8a664e5e8964fd5b74b5f1563fddba57ea70ba8d,"[{'authorId': '13058651', 'name': 'H. Lillehammer'}]",2008.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,,45.0
Neurotrophic electrode: Method of assembly and implantation into human motor speech cortex,942dd18751686cc6462fe6935947c8127c01b3a7,"[{'authorId': '39967562', 'name': 'Jess Bartels'}, {'authorId': '3422518', 'name': 'D. Andreasen'}, {'authorId': '144655625', 'name': 'Princewill Ehirim'}, {'authorId': '116047153', 'name': 'H. Mao'}, {'authorId': '2075609607', 'name': 'S. Seibert'}, {'authorId': '47316838', 'name': 'E. J. Wright'}, {'authorId': '29894035', 'name': 'P. Kennedy'}]",2008.0,Journal of Neuroscience Methods,"['Superintelligence: Paths, Dangers, Strategies']",1,,103.0
"The Developing World is Poorer than We Thought, But No Less Successful in the Fight Against Poverty",1ed800c8ec09ab84656c5a0e437a565338b8c845,"[{'authorId': '48848243', 'name': 'Shaohua Chen'}, {'authorId': '108772572', 'name': 'M. Ravallion'}]",2008.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"The paper presents a major overhaul to the World Bank's past estimates of global poverty, incorporating new and better data. Extreme poverty-as judged by what""poverty""means in the world's poorest countries-is found to be more pervasive than we thought. Yet the data also provide robust evidence of continually declining poverty incidence and depth since the early 1980s. For 2005 we estimate that 1.4 billion people, or one quarter of the population of the developing world, lived below our international line of $1.25 a day in 2005 prices; 25 years earlier there were 1.9 billion poor, or one half of the population. Progress was uneven across regions. The poverty rate in East Asia fell from almost 80 percent to under 20 percent over this period. By contrast it stayed at around 50 percent in Sub-Saharan Africa, though with signs of progress since the mid 1990s. Because of lags in survey data availability, these estimates do not yet reflect the sharp rise in food prices since 2005.",1481.0
Iodine deficiency—way to go yet,2b6cb640f8fdd4a329eaa876faaf6d17fc195147,"[{'authorId': '7662473', 'name': 'T. Lancet'}]",2008.0,The Lancet,"['Superintelligence: Paths, Dangers, Strategies']",1,,11.0
Economica Coase Lecture: Reference Points and the Theory of the Firm,548bc3d89685b3e3a4fdfac59965026d292cba57,"[{'authorId': '145648594', 'name': 'O. Hart'}]",2008.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"I argue that it has been hard to make progress on Coase's theory of the firm agenda because of the difficulty of formalizing haggling costs. I propose an approach that tries to move things forward using the idea of aggrievement costs, and apply it to the question of whether a transaction should be placed inside a firm (in-house production) or in the market place (outsourcing).",53.0
In the Theater of Consciousness: The Workspace of the Mind,acae256c1318eecc2b1d2ccf0e88afef6bb213ff,"[{'authorId': '3184597', 'name': 'Bradford McCall'}]",2008.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"A review of In the Theater of Consciousness: The Workspace of the Mind, by Bernard J. Baars, 1997. New York: Oxford University, xiv + 193 pp. ISBN 0195102657. $45.00 USD.",66.0
A Comprehensive Comparison of the Sun to Other Stars: Searching for Self-Selection Effects,d1a38a295a38d089f39dcc77fdfa315b566ac6c1,"[{'authorId': '121837445', 'name': 'J. Robles'}, {'authorId': '6938387', 'name': 'C. Lineweaver'}, {'authorId': '3690705', 'name': 'D. Grether'}, {'authorId': '100552170', 'name': 'C. Flynn'}, {'authorId': '46176612', 'name': 'Chas A. Egan'}, {'authorId': '8561176', 'name': 'M. Pracy'}, {'authorId': '145128412', 'name': 'J. Holmberg'}, {'authorId': '102718076', 'name': 'E. Gardner'}]",2008.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"If the origin of life and the evolution of observers on a planet is favored by atypical properties of a planet’s host star, we would expect our Sun to be atypical with respect to such properties. The Sun has been described by previous studies as both typical and atypical. In an effort to reduce this ambiguity and quantify how typical the Sun is, we identify 11 maximally independent properties that have plausible correlations with habitability and that have been observed by, or can be derived from, sufficiently large, currently available, and representative stellar surveys. By comparing solar values for the 11 properties to the resultant stellar distributions, we make the most comprehensive comparison of the Sun to other stars. The two most atypical properties of the Sun are its mass and orbit. The Sun is more massive than 95% ± 2% of nearby stars, and its orbit around the Galaxy is less eccentric than 93% ± 1% of FGK stars within 40 pc. Despite these apparently atypical properties, a χ2 analysis of the Sun’s values for 11 properties, taken together, yields a solar χ2☉ = 8.39 ± 0.96. If a star is chosen at random, the probability that it will have a lower value (i.e., be more typical) than the Sun, with respect to the 11 properties analyzed here, is only 29% ± 11% . These values quantify, and are consistent with, the idea that the Sun is a typical star. If we have sampled all reasonable properties associated with habitability, our result suggests that there are no special requirements for a star to host a planet with life.",35.0
Towards Ultra-High Resolution Models of Climate and Weather,fb4294e3e7adf1c9c39deb6b97005d49ee62211f,"[{'authorId': '35339491', 'name': 'M. Wehner'}, {'authorId': '1757847', 'name': 'L. Oliker'}, {'authorId': '1746446', 'name': 'J. Shalf'}]",2008.0,The international journal of high performance computing applications,"['Superintelligence: Paths, Dangers, Strategies']",1,"We present a speculative extrapolation of the performance aspects of an atmospheric general circulation model to ultra-high resolution and describe alternative technological paths to realize integration of such a model in the relatively near future. Due to a superlinear scaling of the computational burden dictated by stability criterion, the solution of the equations of motion dominate the calculation at ultra-high resolutions. From this extrapolation, it is estimated that a credible kilometer scale atmospheric model would require at least a sustained ten petaflop computer to provide scientifically useful climate simulations. Our design study portends an alternate strategy for practical power-efficient implementations of petaflop scale systems. Embedded processor technology could be exploited to tailor a custom machine designed to ultra-high climate model specifications at relatively affordable cost and power considerations. The major conceptual changes required by a kilometer scale climate model are certain to be difficult to implement. Although the hardware, software, and algorithms are all equally critical in conducting ultra-high climate resolution studies, it is likely that the necessary petaflop computing technology will be available in advance of a credible kilometer scale climate model",69.0
Letter from Utopia,b9aea076bce0df8291733b834870c973dc4adbec,"[{'authorId': '2193691', 'name': 'N. Bostrom'}]",2008.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"The good life: just how good could it be? A vision of the future, from the future.",56.0
Heritability in the genomics era — concepts and misconceptions,8dad831936474c5d0f534c3449c7af884e9abe8a,"[{'authorId': '2211652', 'name': 'P. Visscher'}, {'authorId': '144079122', 'name': 'W. G. Hill'}, {'authorId': '144607039', 'name': 'N. Wray'}]",2008.0,Nature reviews genetics,"['Superintelligence: Paths, Dangers, Strategies']",1,,1674.0
Noise in the nervous system,013bd8ad2963c901895bf76e4765b607f2c8f0b5,"[{'authorId': '144683767', 'name': 'A. Faisal'}, {'authorId': '1796527', 'name': 'L. Selen'}, {'authorId': '1960857', 'name': 'D. Wolpert'}]",2008.0,Nature Reviews Neuroscience,"['Superintelligence: Paths, Dangers, Strategies']",1,,1272.0
Secular declines in cognitive test scores: A reversal of the Flynn Effect,d6e4083901452d584c31bf34a35222711e857bb6,"[{'authorId': '3999106', 'name': 'T. Teasdale'}, {'authorId': '144609710', 'name': 'D. R. Owen'}]",2008.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,,172.0
Do You See What We See? An Investigation of an Argument Against Collective Representation,857de790cc6587017ccda2f9980100dc44d0d425,"[{'authorId': '2857250', 'name': 'Bryce Huebner'}]",2008.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"Collectivities (states, club, unions, teams, etc.) are often fruitfully spoken of as though they possessed representational capacities. Despite this fact, many philosophers reject the possibility that collectivities might be thought of as genuinely representational. This paper addresses the most promising objection to the possibility of collective representation, the claim that there is no explanatory value to positing collective representations above and beyond the representational states of the individuals that compose a particular collectivity. I claim that this argument either proves too much, also giving us reason to abandon person-level representations, or it proves too little, demonstrating precisely the sort of continuity between individual and collective representations that would warrant the positing of genuine collective representations. I conclude with a brief sketch of two promising cases of collective representation that lend credence to my claim that individual representations and collective representations are analogous in a way that warrants the study of collective mentality from within the cognitive sciences.",23.0
Are we now living in the Anthropocene,7abd351368f0160021c2af3651d30925e84158bb,"[{'authorId': '90135870', 'name': 'J. Zalasiewicz'}, {'authorId': '1995826324', 'name': 'M. Williams'}, {'authorId': '152286413', 'name': 'Alan Smith'}, {'authorId': '145850393', 'name': 'T. Barry'}, {'authorId': '104064306', 'name': 'A. Coe'}, {'authorId': '72042505', 'name': 'P. Bown'}, {'authorId': '67131166', 'name': 'P. Brenchley'}, {'authorId': '48471688', 'name': 'D. Cantrill'}, {'authorId': '47814329', 'name': 'A. Gale'}, {'authorId': '4136065', 'name': 'P. Gibbard'}, {'authorId': '71281608', 'name': 'F. Gregory'}, {'authorId': '3673713', 'name': 'M. Hounslow'}, {'authorId': '144259117', 'name': 'A. Kerr'}, {'authorId': '3751536', 'name': 'P. Pearson'}, {'authorId': '105035381', 'name': 'R. Knox'}, {'authorId': '145617427', 'name': 'J. Powell'}, {'authorId': '35055466', 'name': 'C. Waters'}, {'authorId': '144050567', 'name': 'J. Marshall'}, {'authorId': '49167802', 'name': 'M. Oates'}, {'authorId': '15936115', 'name': 'P. Rawson'}, {'authorId': '40006590', 'name': 'P. Stone'}]",2008.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"The term Anthropocene, proposed and increasingly employed to denote the current interval of anthropogenic global environmental change, may be discussed on stratigraphic grounds. A case can be made for its consideration as a formal epoch in that, since the start of the Industrial Revolution, Earth has endured changes sufficient to leave a global stratigraphic signature distinct from that of the Holocene or of previous Pleistocene interglacial phases, encompassing novel biotic, sedimentary, and geochemical change. These changes, although likely only in their initial phases, are sufficiently distinct and robustly established for suggestions of a Holocene–Anthropocene boundary in the recent historical past to be geologically reasonable. The boundary may be defined either via Global Stratigraphic Section and Point (“golden spike”) locations or by adopting a numerical date. Formal adoption of this term in the near future will largely depend on its utility, particularly to earth scientists working on late Holocene successions. This datum, from the perspective of the far future, will most probably approximate a distinctive stratigraphic boundary.",604.0
Brain–computer symbiosis,a9052c1a5189497ad55e7a95c6196c3698b9fa28,"[{'authorId': '2369552', 'name': 'G. Schalk'}]",2008.0,Journal of Neural Engineering,"['Superintelligence: Paths, Dangers, Strategies']",1,"The theoretical groundwork of the 1930s and 1940s and the technical advance of computers in the following decades provided the basis for dramatic increases in human efficiency. While computers continue to evolve, and we can still expect increasing benefits from their use, the interface between humans and computers has begun to present a serious impediment to full realization of the potential payoff. This paper is about the theoretical and practical possibility that direct communication between the brain and the computer can be used to overcome this impediment by improving or augmenting conventional forms of human communication. It is about the opportunity that the limitations of our body's input and output capacities can be overcome using direct interaction with the brain, and it discusses the assumptions, possible limitations and implications of a technology that I anticipate will be a major source of pervasive changes in the coming decades.",70.0
Bayesian networks : a practical guide to applications,1cf290f0d2ca6d3d85c010b234e1f6b1cd0d32f6,"[{'authorId': '2216415', 'name': 'O. Pourret'}, {'authorId': '94735948', 'name': 'Patrick. Naïm'}, {'authorId': '3317643', 'name': 'B. Marcot'}]",2008.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"Foreword. Preface. 1 Introduction to Bayesian networks. 1.1 Models. 1.2 Probabilistic vs. deterministic models. 1.3 Unconditional and conditional independence. 1.4 Bayesian networks. 2 Medical diagnosis. 2.1 Bayesian networks in medicine. 2.2 Context and history. 2.3 Model construction. 2.4 Inference. 2.5 Model validation. 2.6 Model use. 2.7 Comparison to other approaches. 2.8 Conclusions and perspectives. 3 Clinical decision support. 3.1 Introduction. 3.2 Models and methodology. 3.3 The Busselton network. 3.4 The PROCAMnetwork. 3.5 The PROCAMBusselton network. 3.6 Evaluation. 3.7 The clinical support tool: TakeHeartII. 3.8 Conclusion. 4 Complex genetic models. 4.1 Introduction. 4.2 Historical perspectives. 4.3 Complex traits. 4.4 Bayesian networks to dissect complex traits. 4.5 Applications. 4.6 Future challenges. 5 Crime risk factors analysis. 5.1 Introduction. 5.2 Analysis of the factors affecting crime risk. 5.3 Expert probabilities elicitation. 5.4 Data preprocessing. 5.5 A Bayesian network model. 5.6 Results. 5.7 Accuracy assessment. 5.8 Conclusions. 6 Spatial dynamics in the coastal region. 6.1 Introduction. 6.2 An indicator-based analysis. 6.3 The Bayesian network model. 6.4 Conclusions. 7 Inference problems in forensic science. 7.1 Introduction. 7.2 Building Bayesian networks for inference. 7.3 Applications of Bayesian networks in forensic science. 7.4 Conclusions. 8 Conservation of marbled murrelets in British Columbia. 8.1 Context/history. 8.2 Model construction. 8.3 Model calibration, validation and use. 8.4 Conclusions/perspectives. 9 Classifiers for modeling of mineral potential. 9.1 Mineral potential mapping. 9.2 Classifiers for mineral potential mapping. 9.3 Bayesian network mapping of base metal deposit. 9.4 Discussion. 9.5 Conclusions. 10 Student modeling. 10.1 Introduction. 10.2 Probabilistic relational models. 10.3 Probabilistic relational student model. 10.4 Case study. 10.5 Experimental evaluation. 10.6 Conclusions and future directions. 11 Sensor validation. 11.1 Introduction. 11.2 The problem of sensor validation. 11.3 Sensor validation algorithm. 11.4 Gas turbines. 11.5 Models learned and experimentation. 11.6 Discussion and conclusion. 12 An information retrieval system. 12.1 Introduction. 12.2 Overview. 12.3 Bayesian networks and information retrieval. 12.4 Theoretical foundations. 12.5 Building the information retrieval system. 12.6 Conclusion. 13 Reliability analysis of systems. 13.1 Introduction. 13.2 Dynamic fault trees. 13.3 Dynamic Bayesian networks. 13.4 A case study: The Hypothetical Sprinkler System. 13.5 Conclusions. 14 Terrorism risk management. 14.1 Introduction. 14.2 The Risk Influence Network. 14.3 Software implementation. 14.4 Site Profiler deployment. 14.5 Conclusion. 15 Credit-rating of companies. 15.1 Introduction. 15.2 Naive Bayesian classifiers. 15.3 Example of actual credit-ratings systems. 15.4 Credit-rating data of Japanese companies. 15.5 Numerical experiments. 15.6 Performance comparison of classifiers. 15.7 Conclusion. 16 Classification of Chilean wines. 16.1 Introduction. 16.2 Experimental setup. 16.3 Feature extraction methods. 16.4 Classification results. 16.5 Conclusions. 17 Pavement and bridge management. 17.1 Introduction. 17.2 Pavement management decisions. 17.3 Bridge management. 17.4 Bridge approach embankment - case study. 17.5 Conclusion. 18 Complex industrial process operation. 18.1 Introduction. 18.2 A methodology for Root Cause Analysis. 18.3 Pulp and paper application. 18.4 The ABB Industrial IT platform. 18.5 Conclusion. 19 Probability of default for large corporates. 19.1 Introduction. 19.2 Model construction. 19.3 BayesCredit. 19.4 Model benchmarking. 19.5 Benefits from technology and software. 19.6 Conclusion. 20 Risk management in robotics. 20.1 Introduction. 20.2 DeepC. 20.3 The ADVOCATE II architecture. 20.4 Model development. 20.5 Model usage and examples. 20.6 Benefits from using probabilistic graphical models. 20.7 Conclusion. 21 Enhancing Human Cognition. 21.1 Introduction. 21.2 Human foreknowledge in everyday settings. 21.3 Machine foreknowledge. 21.4 Current application and future research needs. 21.5 Conclusion. 22 Conclusion. 22.1 An artificial intelligence perspective. 22.2 A rational approach of knowledge. 22.3 Future challenges. Bibliography. Index.",380.0
Machine super intelligence,0d175746355f293187ed491665563018ee690cfa,"[{'authorId': '34313265', 'name': 'S. Legg'}]",2008.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,,87.0
Development of artificial gametes.,14f0d545058acc45f0dfdee5684f3869a401b0c8,"[{'authorId': '39237804', 'name': 'Z. Nagy'}, {'authorId': '4364838', 'name': 'I. Kerkis'}, {'authorId': '5965293', 'name': 'Ching-Chien Chang'}]",2008.0,Reproductive Biomedicine Online,"['Superintelligence: Paths, Dangers, Strategies']",1,,15.0
Why I Want to be a Posthuman when I Grow Up,f701051baf49b69b7b60758c24e4b9c03b7d01df,"[{'authorId': '2193691', 'name': 'N. Bostrom'}]",2013.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,,180.0
Derivation of human embryonic stem cell lines from parthenogenetic blastocysts,a98425d01a87196fdec6980a682a2e3e1be4f66c,"[{'authorId': '1873024', 'name': 'QingYun Mai'}, {'authorId': '2152846840', 'name': 'Yang Yu'}, {'authorId': '2149198601', 'name': 'Tao Li'}, {'authorId': '46659832', 'name': 'Liu Wang'}, {'authorId': '2108608336', 'name': 'Mei-jue Chen'}, {'authorId': '3560868', 'name': 'Shu‐zhen Huang'}, {'authorId': '145455443', 'name': 'Canquan Zhou'}, {'authorId': '144179288', 'name': 'Qi Zhou'}]",2007.0,Cell Research,"['Superintelligence: Paths, Dangers, Strategies']",1,,192.0
A Farewell to Alms: A Brief Economic History of the World,fed44d5583955082d2f2c252038b09c40590e82a,"[{'authorId': '34638195', 'name': 'J. Goodman'}]",2007.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,,176.0
An empirical note on factor shares,b3918718c12e7562d8f57fdf3657472ac0fad455,"[{'authorId': '113199440', 'name': 'Hernando Zuleta'}]",2007.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"In this study, we propose an explanation for why labor and capital shares do not seem to have a trend: an increasing trend in physical capital share is compensated by a decreasing trend in land share. Similarly, an increasing trend in human capital share is compensated by a decreasing trend in raw labor share. We also find empirical support for the claim that the elasticity of output with respect to reproducible factors, human and physical capital, is positively correlated with the income level. This result has important implications for economic growth theory and for empirical exercises related to economic growth.",33.0
Checkers Is Solved,fc436e3566b48c50424881f852d77a13f5ed8bde,"[{'authorId': '1694170', 'name': 'J. Schaeffer'}, {'authorId': '2625574', 'name': 'Neil Burch'}, {'authorId': '1722156', 'name': 'Y. Björnsson'}, {'authorId': '1710401', 'name': 'Akihiro Kishimoto'}, {'authorId': '144054003', 'name': 'Martin Müller'}, {'authorId': '2071802387', 'name': 'R. Lake'}, {'authorId': '2093016', 'name': 'P. Lu'}, {'authorId': '2568072', 'name': 'S. Sutphen'}]",2007.0,Science,"['Superintelligence: Paths, Dangers, Strategies']",1,"The game of checkers has roughly 500 billion billion possible positions (5 × 1020). The task of solving the game, determining the final result in a game with no mistakes made by either player, is daunting. Since 1989, almost continuously, dozens of computers have been working on solving checkers, applying state-of-the-art artificial intelligence techniques to the proving process. This paper announces that checkers is now solved: Perfect play by both sides leads to a draw. This is the most challenging popular game to be solved to date, roughly one million times as complex as Connect Four. Artificial intelligence technology has been used to generate strong heuristic-based game-playing programs, such as Deep Blue for chess. Solving a game takes this to the next level by replacing the heuristics with perfection.",448.0
Reflection and Disagreement,7fb98f13dda3c48cfe27252bb112af40bfc6039f,"[{'authorId': '52192352', 'name': 'A. Elga'}]",2007.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"How should you take into account the opinions of an advisor? When you completely defer to the advisor's judgment (the manner in which she responds to her evidence), then you should treat the advisor as a guru. Roughly, that means you should believe what you expect she would believe, if supplied with your extra evidence. When the advisor is your own future self, the resulting principle amounts to a version of the Reflection Principle---a version amended to handle cases of information loss. When you count an advisor as an epistemic peer, you should give her conclusions the same weight as your own. Denying that view---call it the ``equal weight view''---leads to absurdity: the absurdity that you could reasonably come to believe yourself to be an epistemic superior to an advisor simply by noting cases of disagreement with her, and taking it that she made most of the mistakes. Accepting the view seems to lead to another absurdity: that one should suspend judgment about everything that one's smart and well-informed friends disagree on, which means suspending judgment about almost everything interesting. But despite appearances, the equal weight view does not have this absurd consequence. Furthermore, the view can be generalized to handle cases involving not just epistemic peers, but also epistemic superiors and inferiors.",632.0
Quantifying and mapping the human appropriation of net primary production in earth's terrestrial ecosystems,336586b4aa2b687a4ec8b5e904781b2dc2cc3962,"[{'authorId': '6255998', 'name': 'H. Haberl'}, {'authorId': '4121890', 'name': 'K. Erb'}, {'authorId': '6851465', 'name': 'F. Krausmann'}, {'authorId': '2854698', 'name': 'V. Gaube'}, {'authorId': '3904480', 'name': 'A. Bondeau'}, {'authorId': '5403159', 'name': 'C. Plutzar'}, {'authorId': '4832651', 'name': 'S. Gingrich'}, {'authorId': '145369034', 'name': 'W. Lucht'}, {'authorId': '1398950892', 'name': 'M. Fischer-Kowalski'}]",2007.0,Proceedings of the National Academy of Sciences of the United States of America,"['Superintelligence: Paths, Dangers, Strategies']",1,"Human appropriation of net primary production (HANPP), the aggregate impact of land use on biomass available each year in ecosystems, is a prominent measure of the human domination of the biosphere. We present a comprehensive assessment of global HANPP based on vegetation modeling, agricultural and forestry statistics, and geographical information systems data on land use, land cover, and soil degradation that localizes human impact on ecosystems. We found an aggregate global HANPP value of 15.6 Pg C/yr or 23.8% of potential net primary productivity, of which 53% was contributed by harvest, 40% by land-use-induced productivity changes, and 7% by human-induced fires. This is a remarkable impact on the biosphere caused by just one species. We present maps quantifying human-induced changes in trophic energy flows in ecosystems that illustrate spatial patterns in the human domination of ecosystems, thus emphasizing land use as a pervasive factor of global importance. Land use transforms earth's terrestrial surface, resulting in changes in biogeochemical cycles and in the ability of ecosystems to deliver services critical to human well being. The results suggest that large-scale schemes to substitute biomass for fossil fuels should be viewed cautiously because massive additional pressures on ecosystems might result from increased biomass harvest.",1296.0
Distribution of Power and Military R&D,f40c11e7c5dc0a09654dca62009dd9c8689c41c1,"[{'authorId': '46230088', 'name': 'V. Koubi'}, {'authorId': '14112895', 'name': 'D. Lalman'}]",2007.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"Military technology plays an important role for national security and the international distribution of power. Yet it has received little formal attention in the literature. This article aims at filling this gap. We build a dynamic model that links the properties of military technology races (intensity and persistence) to the characteristics of the international environment (the distribution of power) and the types of weapons under development. The model is also used to discuss the implications of military technology races for power transition, the incentives countries may have to engage in arms control agreements and the circumstances under which it pays to engage in deception strategies. We find that the intensity of military R&D is higher when a dominant nation faces a potential challenger than under conditions of actual competition. The intensity of weapons development and also the speed of power transition depend on whether the challenger can be ‘intimidated’ through aggressive weapon development or not. We also find that when the stakes associated with the development of a weapon are high, then the optimal policy is to underplay one’s own success and exaggerate foreign accomplishments (threats).",1.0
Two Centuries of Productivity Growth in Computing,0962b6858030a6673387b4846edb0c37aa201890,"[{'authorId': '69862380', 'name': 'W. Nordhaus'}]",2007.0,Journal Economic History,"['Superintelligence: Paths, Dangers, Strategies']",1,"The present study analyzes computer performance over the last century and a half. Three results stand out. First, there has been a phenomenal increase in computer power over the twentieth century. Depending upon the standard used, computer performance has improved since manual computing by a factor between 1.7 trillion and 76 trillion. Second, there was a major break in the trend around World War II. Third, this study develops estimates of the growth in computer power relying on performance rather than components; the price declines using performance-based measures are markedly larger than those reported in the official statistics.",229.0
Counterfactual Thought Experiments: A Necessary Teaching Tool,7f0c5a64adf072df0df31ee3a7140bbe32e06c3d,"[{'authorId': '66866780', 'name': 'R. Lebow'}]",2007.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"COUNTERFACTUALS are routinely used in physical and biological sciences to develop and evaluate sophisticated, non-linear models. They have been used with telling effect in the study of economic history and American politics.' For some historians, counterfactual arguments have no scholarly standing. They consider them flights of fancy, fun over a beer or two in the faculty club, but not the stuff of serious research.2 This dismissive attitude may be encouraged by the emergence and popularity of counterfactual historical works as a fictional genre, and the uncomfortable similarities between some recent works ofcounterfactual scholarship and such fiction. Nevertheless, counterfactuals are an effective research tool, but comprehending this requires a clear understanding of their nature, the circumstances to which they are best suited, and robust protocols for conducting thought experiments. With these ends in mind, I begin by exploring the differences between counterfactual and so-called ""factual"" arguments and offer the proposition that the difference between them is greatly exaggerated; it is one of degree, not of kind. I go on to discuss different uses ofcounterfactual thought experiments for historians. I distinguish between miracle and plausible world and scholarly and folk counterfactuals, and their respective uses, and I propose several criteria to guide plausible world counterfactuals. I conclude by looking at the special problems of applying counterfactual analysis to a problem",32.0
Nutrition and the developing brain: nutrient priorities and measurement.,c1aba43964e11d960b0c7e16616e10acd8d13573,"[{'authorId': '35483944', 'name': 'M. Georgieff'}]",2007.0,American Journal of Clinical Nutrition,"['Superintelligence: Paths, Dangers, Strategies']",1,"Nutrients and growth factors regulate brain development during fetal and early postnatal life. The rapidly developing brain is more vulnerable to nutrient insufficiency yet also demonstrates its greatest degree of plasticity. Certain nutrients have greater effects on brain development than do others. These include protein, energy, certain fats, iron, zinc, copper, iodine, selenium, vitamin A, choline, and folate. The effect of any nutrient deficiency or overabundance on brain development will be governed by the principle of timing, dose, and duration. The ability to detect the specific effects of nutrient deficiencies is dependent on knowing which area of the brain is preferentially affected and on having neurologic assessments that tap into the functions of those specific areas. As examples, protein-energy malnutrition causes both global deficits, which are testable by general developmental testing, and area-specific effects on the hippocampus and the cortex. Iron deficiency alters myelination, monoamine neurotransmitter synthesis, and hippocampal energy metabolism in the neonatal period. Assessments of these effects could include tests for speed of processing (myelination), changes in motor and affect (monoamines), and recognition memory (hippocampus). Zinc deficiency alters autonomic nervous system regulation and hippocampal and cerebellar development. Long-chain polyunsaturated fatty acids are important for synaptogenesis, membrane function, and, potentially, myelination. Overall, circuit-specific behavioral and neuroimaging tests are being developed for use in progressively younger infants to more accurately assess the effect of nutrient deficits both while the subject is deficient and after recovery from the deficiency.",739.0
Productivity and aggregate growth : a global picture,d60980c749711c09528a15c3ebd31a4033646d61,"[{'authorId': '145216318', 'name': 'Anders Isaksson'}]",2007.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"This paper describes the world in terms of income and productivity, levels and growth. It shows that the world is becoming increasingly unequal, in terms of income per worker, TFP and technical efficiency. Although some developing countries have managed to catch up with the world technology frontier, countries that were poor in 1960 generally stayed poor in 2000. Growth analysis tends to confirm the bleak picture and shows little indication of a forthcoming reversal of income polarization and divergence. Taken together, it appears that in early stages of development, countries rely on factor accumulation for their growth, but as they advance, productivity growth starts to contribute to output growth.",7.0
The Oxford Handbook of Rational and Social Choice,130698917a9a5d8261df61a9cf4056f48c13df55,"[{'authorId': '35488355', 'name': 'P. Anand'}, {'authorId': '2978292', 'name': 'P. Pattanaik'}, {'authorId': '1923417', 'name': 'C. Puppe'}]",2007.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,Ambiguity refers to a decision situation under uncertainty when there is incomplete information about the likelihood of events. Different formal models of this notion have been developed with differing implications about the representation of ambiguity and ambiguity aversion.,51.0
Arsenals of Folly: The Making of the Nuclear Arms Race,babc8a6834af3af60412cc074363735743d8af55,"[{'authorId': '143905751', 'name': 'R. Rhodes'}]",2007.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"From the Pulitzer Prize-winning author of The Making of the Atomic Bomb comes this brilliant account of the post-war superpower arms race, climaxing during the Reagan-Gorbachev decade when the United States and the Soviet Union came within scant hours of nuclear war -- and then nearly agreed to abolish nuclear weapons. In a narrative that reads like a thriller, Rhodes reveals how the Reagan administration's unprecedented arms build-up in the early 1980s led the Soviets to conclude that the U.S. must be preparing for a nuclear war -- only for Reagan, out of deep conviction, to launch the arms-reduction campaign of his second presidential term and set the stage for the famous 1986 summit with Gorbachev in Reykjavik, and the breakthroughs that followed. Drawing on personal interviews with both Soviet and U.S. participants, and on a wealth of new documentation that has become available only in the past ten years, Rhodes recounts what actually happened in the final years of the Cold War. The story is new, compelling, and continually surprising -- a revelatory re-creation of a hugely important era of our recent history.",54.0
"Contours of the World Economy, 1-2030AD: Essays in Macro-Economic History",a26fc0c266b117cbd6c8b2f1f73f6e13bbf0892c,"[{'authorId': '115873380', 'name': 'A. Maddison'}]",2007.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"This book seeks to identify the forces which explain how and why some parts of the world have grown rich and others have lagged behind. Encompassing 2000 years of history, part 1 begins with the Roman Empire and explores the key factors that have influenced economic development in Africa, Asia, the Americas and Europe. Part 2 covers the development of macroeconomic tools of analysis from the 17th century to the present. Part 3 looks to the future and considers what the shape of the world economy might be in 2030. Combining both the close quantitative analysis for which Professor Maddison is famous with a more qualitative approach that takes into account the complexity of the forces at work, this book provides students and all interested readers with a totally fascinating overview of world economic history. Professor Maddison has the unique ability to synthesise vast amounts of information into a clear narrative flow that entertains as well as informs, making this text an invaluable resource for all students and scholars, and anyone interested in trying to understand why some parts of the World are so much richer than others.",491.0
"Some Limits to Global Ecophagy by Biovorous Nanoreplicators , with Public Policy Recommendations",8d04cb724fddfaa742048d7fb5804ba26e8958cf,"[{'authorId': '2360590', 'name': 'R. Freitas'}]",2007.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"The maximum rate of global ecophagy by biovorous selfreplicating nanorobots is fundamentally restricted by the replicative strategy employed; by the maximum dispersal velocity of mobile replicators; by operational energy and chemical element requirements; by the homeostatic resistance of biological ecologies to ecophagy; by ecophagic thermal pollution limits (ETPL); and most importantly by our determination and readiness to stop them. Assuming current and foreseeable energy-dissipative designs requiring ~100 MJ/kg for chemical transformations (most likely for biovorous systems), ecophagy that proceeds slowly enough to add ~4°C to global warming (near the current threshold for immediate climatological detection) will require ~20 months to run to completion; faster ecophagic devices run hotter, allowing quicker detection by policing authorities. All ecophagic scenarios examined appear to permit early detection by vigilant monitoring, thus enabling rapid deployment of effective defensive instrumentalities.",18.0
From Computer Power and Human Reason From Judgment to Calculation,e033f0f7576a9b4b62d1837cbeaa4f6a1dc579b5,"[{'authorId': '2215426', 'name': 'J. Weizenbaum'}]",2007.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,,531.0
Levels of Organization in General Intelligence,5e87ea116950bd2a29f5e24f9e465c5b8a0b4a0d,"[{'authorId': '2542795', 'name': 'Eliezer Yudkowsky'}]",2007.0,Artificial General Intelligence,"['Superintelligence: Paths, Dangers, Strategies']",1,,35.0
Converging Cognitive Enhancements,48f68920399ecb06e388254695c6485b8e6cc6c3,"[{'authorId': '144816231', 'name': 'A. Sandberg'}, {'authorId': '2193691', 'name': 'N. Bostrom'}]",2006.0,Annals of the New York Academy of Sciences,"['Superintelligence: Paths, Dangers, Strategies']",1,"Abstract:  Cognitive enhancement, the amplification or extension of core capacities of the mind, has become a major topic in bioethics. But cognitive enhancement is a prime example of a converging technology where individual disciplines merge and issues transcend particular local discourses. This article reviews currently available methods of cognitive enhancement and their likely near‐term prospects for convergence.",112.0
A multifaceted approach to understanding the botnet phenomenon,da094c4a916b3ca6d4607a0110084464edefba8a,"[{'authorId': '2269282', 'name': 'M. Rajab'}, {'authorId': '3154339', 'name': 'J. Zarfoss'}, {'authorId': '1792232', 'name': 'F. Monrose'}, {'authorId': '1763579', 'name': 'A. Terzis'}]",2006.0,ACM/SIGCOMM Internet Measurement Conference,"['Superintelligence: Paths, Dangers, Strategies']",1,"The academic community has long acknowledged the existence of malicious botnets, however to date, very little is known about the behavior of these distributed computing platforms. To the best of our knowledge, botnet behavior has never been methodically studied, botnet prevalence on the Internet is mostly a mystery, and the botnet life cycle has yet to be modeled. Uncertainty abounds. In this paper, we attempt to clear the fog surrounding botnets by constructing a multifaceted and distributed measurement infrastructure. Throughout a period of more than three months, we used this infrastructure to track 192 unique IRC botnets of size ranging from a few hundred to several thousand infected end-hosts. Our results show that botnets represent a major contributor to unwanted Internet traffic - 27% of all malicious connection attempts observed from our distributed darknet can be directly attributed to botnet-related spreading activity. Furthermore, we discovered evidence of botnet infections in 11% of the 800,000 DNS domains we examined, indicating a high diversity among botnet victims. Taken as a whole, these results not only highlight the prominence of botnets, but also provide deep insights that may facilitate further research to curtail this phenomenon.",691.0
Uncommon Priors Require Origin Disputes,a5fc6ccf9adda9fd2060689a78a66197e6d6d730,"[{'authorId': '145447707', 'name': 'R. Hanson'}]",2006.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,,18.0
The Singularity Is Near: When Humans Transcend Biology,84fb458d203612139cee7df732bcf2b196577a45,"[{'authorId': '2186634', 'name': 'R. Kurzweil'}]",2006.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"A radical and optimistic view of the future course of human development from Ray Kurzweil, whom Bill Gates calls ""the best person I know at predicting the future of artificial intelligence.""",2331.0
Stanley: The robot that won the DARPA Grand Challenge,9e938cf8a74c2e7415bfd10d66a9ea4b1c3e0e15,"[{'authorId': '144867807', 'name': 'S. Thrun'}]",2006.0,The AI Magazine,"['Superintelligence: Paths, Dangers, Strategies']",1,"This article describes the robot Stanley, which won the 2005 DARPA Grand Challenge. Stanley was developed for high‐speed desert driving without manual intervention. The robot's software system relied predominately on state‐of‐the‐art artificial intelligence technologies, such as machine learning and probabilistic reasoning. This paper describes the major components of this architecture, and discusses the results of the Grand Challenge race. © 2006 Wiley Periodicals, Inc.",2285.0
"Brain–machine interfaces: past, present and future",eb06f203a0b21f8d7e23efed26bdeb9fc7def49a,"[{'authorId': '2057713', 'name': 'M. Lebedev'}, {'authorId': '144052463', 'name': 'M. Nicolelis'}]",2006.0,Trends in Neurosciences,"['Superintelligence: Paths, Dangers, Strategies']",1,,1621.0
How Much the Eye Tells the Brain,7afc7403ff99063cf3aad731d7dbf3c327a9b66c,"[{'authorId': '49800080', 'name': 'Kristin Koch'}, {'authorId': '152138711', 'name': 'J. McLean'}, {'authorId': '1874289', 'name': 'R. Segev'}, {'authorId': '5130730', 'name': 'M. A. Freed'}, {'authorId': '145839070', 'name': 'M. Berry'}, {'authorId': '144037301', 'name': 'V. Balasubramanian'}, {'authorId': '2169666', 'name': 'P. Sterling'}]",2006.0,Current Biology,"['Superintelligence: Paths, Dangers, Strategies']",1,,251.0
The Reversal Test: Eliminating Status Quo Bias in Applied Ethics*,a5088eccfb9efc69d6df64dc151aba0b4658023f,"[{'authorId': '2193691', 'name': 'N. Bostrom'}, {'authorId': '46277517', 'name': 'Toby Ord'}]",2006.0,"Ethics: An International Journal of Social, Political, and Legal Philosophy","['Superintelligence: Paths, Dangers, Strategies']",1,"Suppose that we develop a medically safe and affordable means of enhancing human intelligence. For concreteness, we shall assume that the technology is genetic engineering (either somatic or germ line), although the argument we will present does not depend on the technological implementation. For simplicity, we shall speak of enhancing “intelligence” or “cognitive capacity,” but we do not presuppose that intelligence is best conceived of as a unitary attribute. Our considerations could be applied to specific cognitive abilities such as verbal fluency, memory, abstract reasoning, social intelligence, spatial cognition, numerical ability, or musical talent. It will emerge that the form of argument that we use can be applied much more generally to help assess other kinds of enhancement technologies as well as other kinds of reform. However, to give a detailed illustration of how the argument form works, we will focus on the prospect of cognitive enhancement. Many ethical questions could be asked with regard to this prospect, but we shall address only one: do we have reason to believe that the long-term consequences of human cognitive enhancement would be, on balance, good? This may not be the only morally relevant question—",194.0
Deep brain stimulation.,c6b337bb9879a22bd3043e1b9d652f8d9d36027a,"[{'authorId': '143747005', 'name': 'J. Perlmutter'}, {'authorId': '6370443', 'name': 'J. Mink'}]",2006.0,Annual Review of Neuroscience,"['Superintelligence: Paths, Dangers, Strategies']",1,"Deep brain stimulation (DBS) has provided remarkable benefits for people with a variety of neurologic conditions. Stimulation of the ventral intermediate nucleus of the thalamus can dramatically relieve tremor associated with essential tremor or Parkinson disease (PD). Similarly, stimulation of the subthalamic nucleus or the internal segment of the globus pallidus can substantially reduce bradykinesia, rigidity, tremor, and gait difficulties in people with PD. Multiple groups are attempting to extend this mode of treatment to other conditions. Yet, the precise mechanism of action of DBS remains uncertain. Such studies have importance that extends beyond clinical therapeutics. Investigations of the mechanisms of action of DBS have the potential to clarify fundamental issues such as the functional anatomy of selected brain circuits and the relationship between activity in those circuits and behavior. Although we review relevant clinical issues, we emphasize the importance of current and future investigations on these topics.",617.0
Boswell's Life of Johnson,cbe023dd7bf35b6538d6349b611ebeab48a3705b,"[{'authorId': '3735809', 'name': 'J. Boswell'}]",2006.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,,113.0
Quantity of experience: brain-duplication and degrees of consciousness,6e3db129d19e40655b43df1c7e1adad26eb249f9,"[{'authorId': '2193691', 'name': 'N. Bostrom'}]",2006.0,Minds and Machines,"['Superintelligence: Paths, Dangers, Strategies']",1,,27.0
What Is Thought? (Bradford Books),3856a67832a81105c3fb3208063245e56f8502ef,"[{'authorId': '2403454', 'name': 'E. Baum'}]",2006.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,,8.0
Do Diseconomies of Scale Impact Firm Size and Performance? A Theoretical and Empirical Overview,e78d292e243d8b5865d80df2627efafd5e497b0b,"[{'authorId': '118720262', 'name': 'Staffan Canback'}, {'authorId': '69883333', 'name': 'P. Samouel'}, {'authorId': '153614934', 'name': 'D. Price'}]",2006.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"This article tests Oliver Williamson's proposition that transaction cost economics can explain the limits of firm size. Williamson suggests that diseconomies of scale are manifested through four interrelated factors: atmospheric consequences due to specialization, bureaucratic insularity, incentive limits of the employment relation and communication distortion due to bounded rationality. Furthermore, Williamson argues that diseconomies of scale are counteracted by economies of scale and can be moderated by adoption of the multidivisional organization form and by high internal asset specificity. Combined, these influences tend to cancel out and thus there is not a strong, directly observable, relationship between a large firm's size and performance. A review of the relevant literature, including transaction cost economics, sociological studies of bureaucracy, information-processing perspectives on the firm, agency theory, and studies of incentives and motivation within firms, as well as empirical studies of trends in firm size and industry concentration, corroborates Williamson's theoretical framework. The framework translates into five hypotheses: (1) Bureaucratic failure, in the form of atmospheric consequences, bureaucratic insularity, incentive limits and communication distortion, increases with firm size; (2) Large firms exhibit economies of scale; (3) Diseconomies of scale from bureaucratic failure have a negative impact on firm performance; (4) Economies of scale increase the relative profitability of large firms over smaller firms; and (5) Diseconomies of scale are moderated by two transaction cost-related factors: organisation form and asset specificity. The hypotheses were tested by applying structural equation models to primary and secondary cross-sectional data from 784 large US manufacturing firms. The statistical analyzes confirm the hypotheses. Thus, diseconomies of scale influence the growth and profitability of firms negatively, while economies of scale and the moderating factors have positive influences. This implies that executives and directors of large firms should pay attention to bureaucratic failure.",69.0
The Blue Brain Project,fd399c0dd19b587e1b4e76fefa854c90dde64172,"[{'authorId': '1754307', 'name': 'H. Markram'}]",2006.0,Nature Reviews Neuroscience,"['Superintelligence: Paths, Dangers, Strategies']",1,,1320.0
Agreeing to disagree.,b38215588df93e174512a242cf3e058288c11bfa,"[{'authorId': '2380340', 'name': 'R. Aumann'}]",1976.0,Nature Cell Biology,"['Superintelligence: Paths, Dangers, Strategies']",1,"Two people, 1 and 2, are said to have common knowledge of an event E if both know it, 1 knows that 2 knows it, 2 knows that 1 knows it, 1 knows that 2 knows that 1 knows it, and so on.",1003.0
Good and Real: Demystifying Paradoxes from Physics to Ethics,e7423ffdf1057d7e8fcb5e74b2f07fbfb7d87666,"[{'authorId': '2393791', 'name': 'G. Drescher'}]",2006.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"In Good and Real, Gary Drescher examines a series of provocative paradoxes about consciousness, choice, ethics, quantum mechanics, and other topics, in an effort to reconcile a purely mechanical view of the universe with key aspects of our subjective impressions of our own existence.Many scientists suspect that the universe can ultimately be described by a simple (perhaps even deterministic) formalism; all that is real unfolds mechanically according to that formalism. But how, then, is it possible for us to be conscious, or to make genuine choices? And how can there be an ethical dimension to such choices? Drescher sketches computational models of consciousness, choice, and subjunctive reasoning--what would happen if this or that were to occur?--to show how such phenomena are compatible with a mechanical, even deterministic universe. Analyses of Newcomb's Problem (a paradox about choice) and the Prisoner's Dilemma (a paradox about self-interest vs. altruism, arguably reducible to Newcomb's Problem) help bring the problems and proposed solutions into focus. Regarding quantum mechanics, Drescher builds on Everett's relative-state formulation--but presenting a simplified formalism, accessible to laypersons--to argue that, contrary to some popular impressions, quantum mechanics is compatible with an objective, deterministic physical reality, and that there is no special connection between quantum phenomena and consciousness.In each of several disparate but intertwined topics ranging from physics to ethics, Drescher argues that a missing technical linchpin can make the quest for objectivity seem impossible, until the elusive technical fix is at hand.",28.0
What is a Singleton,0890ec1b4a26b133c3d456317b29fb35416751d0,"[{'authorId': '2193691', 'name': 'N. Bostrom'}]",2006.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,This note introduces the concept of a “singleton” and suggests that this concept is useful for formulating and analyzing possible scenarios for the future of humanity.,47.0
The long-term future of space travel,f1e8080805f72e65f145b92cc363c489802d567f,"[{'authorId': '10780453', 'name': 'J. Heyl'}]",2005.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"The fact that we apparently live in an accelerating universe places limitations on where humans might visit. If the current energy density of the universe is dominated by a cosmological constant, a rocket could reach a galaxy observed today at a redshift of 1.7 on a one-way journey or merely 0.65 on a round trip. Unfortunately these maximal trips are impractical as they require an infinite proper time to traverse. However, calculating the rocket trajectory in detail shows that a rocketeer could nearly reach such galaxies within a lifetime (a long lifetime admittedly--about 100 years). For less negative values of w the maximal redshift increases becoming infinite for w{>=}-1/3.",15.0
NATURAL HISTORY OF ASHKENAZI INTELLIGENCE,4839d43b427cc8f3973d57257fbdfc159c8c40f4,"[{'authorId': '11860375', 'name': 'G. Cochran'}, {'authorId': '2053892038', 'name': 'Jason Hardy'}, {'authorId': '5178699', 'name': 'H. Harpending'}]",2005.0,Journal of Biosocial Science,"['Superintelligence: Paths, Dangers, Strategies']",1,"This paper elaborates the hypothesis that the unique demography and sociology of Ashkenazim in medieval Europe selected for intelligence. Ashkenazi literacy, economic specialization, and closure to inward gene flow led to a social environment in which there was high fitness payoff to intelligence, specifically verbal and mathematical intelligence but not spatial ability. As with any regime of strong directional selection on a quantitative trait, genetic variants that were otherwise fitness reducing rose in frequency. In particular we propose that the well-known clusters of Ashkenazi genetic diseases, the sphingolipid cluster and the DNA repair cluster in particular, increase intelligence in heterozygotes. Other Ashkenazi disorders are known to increase intelligence. Although these disorders have been attributed to a bottleneck in Ashkenazi history and consequent genetic drift, there is no evidence of any bottleneck. Gene frequencies at a large number of autosomal loci show that if there was a bottleneck then subsequent gene flow from Europeans must have been very large, obliterating the effects of any bottleneck. The clustering of the disorders in only a few pathways and the presence at elevated frequency of more than one deleterious allele at many of them could not have been produced by drift. Instead these are signatures of strong and recent natural selection.",148.0
The baryonic mass function of galaxies,4dc488c8eb91123e53deee996c33705365a23a0f,"[{'authorId': '49214168', 'name': 'J. Read'}, {'authorId': '3525305', 'name': 'N. Trentham'}]",2005.0,"Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences","['Superintelligence: Paths, Dangers, Strategies']",1,"In the Big Bang about 5% of the mass that was created was in the form of normal baryonic matter (neutrons and protons). Of this about 10% ended up in galaxies in the form of stars or of gas (that can be in molecules, can be atomic, or can be ionized). In this work, we measure the baryonic mass function of galaxies, which describes how the baryonic mass is distributed within galaxies of different types (e.g. spiral or elliptical) and of different sizes. This can provide useful constraints on our current cosmology, convolved with our understanding of how galaxies form. This work relies on various large astronomical surveys, e.g. the optical Sloan Digital Sky Survey (to observe stars) and the HIPASS radio survey (to observe atomic gas). We then perform an integral over our mass function to determine the cosmological density of baryons in galaxies: Ωb,gal=0.0035. Most of these baryons are in stars: Ω*=0.0028. Only about 20% are in gas. The error on the quantities, as determined from the range obtained between different methods, is ca 10%; systematic errors may be much larger. Most (ca 90%) of the baryons in the Universe are not in galaxies. They probably exist in a warm/hot intergalactic medium. Searching for direct observational evidence and deeper theoretical understanding for this will form one of the major challenges for astronomy in the next decade.",52.0
Growth and Interaction in the World Economy: The Roots of Modernity,1588671106f359a6dfc65943e71e11ac2538225e,"[{'authorId': '115873380', 'name': 'A. Maddison'}]",2005.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"In Growth and Interaction in the World Economy, Angus Maddison explores the causes of the West's economic growth over the last two thousand years and contrasts it with the economic history of the rest of the world. While these topics have been the subject of many studies - from Adam Smith's Wealth of Nations in 1776 to David Landes's Wealth and Poverty of Nations in 1998 - Maddison's approach is the first to use quantitative evidence in a systematic way within a macroeconomic framework. Maddison explores the impact of western conquest on the Americas and analyzes the indigenous and external forces that hindered advance in Asia and Africa. His unique analysis suggests that Western Europe overtook Chinese levels of per capita income in the fourteenth century, in sharp contrast to the prevailing scholarship asserting that China was ahead of Europe until 1800. His assessment of European performance also challenges the neo-Malthusian judgment that the French pessimism of the real wage approach suggesting that English living standards in 1800 were well below their level in 1260 and 60 percent lower than in 1500. Maddison debunks the notion that the Western ascension originated in the Industrial revolution in England in the late eighteenth century. Growth and Interaction in the World Economy provides guidance on the broad contours of development, which complements qualitative analysis that, on its own, cannot clearly identify the timing and scope of economic change.",73.0
The effects of iodine on intelligence in children: a meta-analysis of studies conducted in China.,6f0bd48e87e20d162ac988ec1c739b0a02ccce5a,"[{'authorId': '2061356656', 'name': 'M. Qian'}, {'authorId': '2152691137', 'name': 'Dong Wang'}, {'authorId': '38470959', 'name': 'W. E. Watkins'}, {'authorId': '5834954', 'name': 'V. Gebski'}, {'authorId': '4361965', 'name': 'Yu-qin Yan'}, {'authorId': '47629016', 'name': 'Mu Li'}, {'authorId': '2109415781', 'name': 'Zu-pei Chen'}]",2005.0,Asia Pacific Journal of Clinical Nutrition,"['Superintelligence: Paths, Dangers, Strategies']",1,"This study quantifies the effects of iodine on the intellectual development of children using a systematic manual literature search of Chinese publications related to iodine deficiency disorders. The Chinese Medical Reference Database, Medline, and Cochrane library were searched electronically in Chinese and English. Inclusion criteria included: studies conducted in China, comparing children (<16 ys) living in naturally iodine sufficient (IS) with those in severely iodine deficient (ID) areas, or children in ID areas born before and after the introduction of iodine supplementation. Intelligent Quotient (IQ) was measured using Binet or Raven Scales. The iodine sufficient control groups were comparable socially, economically, and educationally with the study groups. Random effects models were used in the meta-analysis. Effect size was the standard deviation IQ point (SIQP), which is equivalent to 15 IQ. Thirty-seven reported studies, total 12,291 children, were analysed. The effect size was an increase of 0.83, 0.82, and 0.32 SIQP respectively, for the children living in IS communities compared with those living in ID areas with no iodine supplementation, with inadequate iodine supplementation, or children who had received iodine during their mothers' pregnancy and after birth. These equal to 12.45, 12.3, 4.8 IQ points. Compared with that of children whose mothers were persistently exposed to ID, the total effect size of the 21 entries was an increase of 0.58 SIQP (8.7 IQ points) in the group receiving iodine supplementation during pregnancy. Furthermore, there was an increase on 1.15 SIQP of Binet or 0.8 SIQP on Raven Scale (17.25 or 12 IQ points) for children born more than 3.5 years after iodine supplementation program was introduced. The level of iodine nutrition plays a crucial role in the intellectual development of children. The intelligence damage of children exposed to severe ID was profound, demonstrated by 12.45 IQ points loss and they recovered 8.7 IQ points with iodine supplementation or IS before and during pregnancy. Iodine supplementation before and during pregnancy to women living in severe ID areas could prevent their children from intelligence deficit. This effect becomes evident in children born 3.5 years after the iodine supplementation program was introduced.",272.0
Optimal Agents ∗,cdfabcd09c896ab2e34aa5cfe835e6a52cd1fc19,"[{'authorId': '1685717', 'name': 'Cristian S. Calude'}]",2005.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"This paper studies a principal’s hiring decision when different agents generate different probability distributions of output under effort, subject to moral hazard. Contracting is subject to canonical frictions: Agents enjoy limited liability and can manipulate output ex post. The main insight is that the contracting problem determines not only optimal contract design but also the type of agent the principal hires. Different but equally productive agents require different optimal contracts, implying different agency rents. This generates a pecking order among agents with the same productivity. Moreover, the contracting problem can bias the principal towards hiring less productive agents. The results suggest a novel link between incentives and hiring, with implications for firms’ hiring decisions, the level, shape, and dispersion of incentive pay, human capital formation, the choice of corporate strategy, delegation, and firms’ production technologies.",6.0
Genetic Influence on Human Psychological Traits,28a36eaebfa9c6e2268e8bb0af9875b77d4851b6,"[{'authorId': '7885726', 'name': 'T. Bouchard'}]",2004.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"There is now a large body of evidence that supports the conclusion that individual differences in most, if not all, reliably measured psychological traits, normal and abnormal, are substantively influenced by genetic factors. This fact has important implications for research and theory building in psychology, as evidence of genetic influence unleashes a cascade of questions regarding the sources of variance in such traits. A brief list of those questions is provided, and representative findings regarding genetic and environmental influences are presented for the domains of personality, intelligence, psychological interests, psychiatric illnesses, and social attitudes. These findings are consistent with those reported for the traits of other species and for many human physical traits, suggesting that they may represent a general biological phenomenon.",446.0
The end of the Flynn effect?: A study of secular trends in mean intelligence test scores of Norwegian conscripts during half a century,af8fd4266349b2fa7b8fe879e85d633071e2129a,"[{'authorId': '4956669', 'name': 'J. Sundet'}, {'authorId': '3810778', 'name': 'Dag Barlaug'}, {'authorId': '14213486', 'name': 'T. Torjussen'}]",2004.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,,306.0
Forecast for the Next Eon: Applied Cosmology and the Long-Term Fate of Intelligent Beings,629dc71a20a1b2bd20fcd692d456ee2e7cff96e2,"[{'authorId': '2547770', 'name': 'M. Ćirković'}]",2002.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,,22.0
Reproductive Genetic Testing: What America Thinks,fabdcf025df353717374098f152c69ffe64be184,"[{'authorId': '10181695', 'name': 'A. Kalfoglou'}, {'authorId': '35119924', 'name': 'K. Suthers'}, {'authorId': '50573656', 'name': 'Joan Scott'}, {'authorId': '2715776', 'name': 'K. Hudson'}]",2004.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,,44.0
"United Nations, UN reform, atomic control",9c79be2a77fbdbca5a68af412efd5b839059e159,"[{'authorId': '52021296', 'name': 'J. Baratta'}]",2004.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"This volume traces the influence of a generation of internationalists on policy, particularly on Winston Churchill's proposal of Anglo-French union of June 16, 1940, deliberations in the U.S. State Department on the shape of a postwar international security organization until October 1943, the Baruch plan for the international control of atomic energy in l946, and early efforts at UN reform.",2.0
Kinematic Self-Replicating Machines,c0bdd6d03f270d105f593e4e70e939caa07287f7,"[{'authorId': '2360590', 'name': 'R. Freitas'}, {'authorId': '1725731', 'name': 'R. Merkle'}]",2004.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,,201.0
Global Economic History: a Personal View on the Agenda for future Research,560986168e8b272aab49dacb884e295c7e7fad96,"[{'authorId': '145304433', 'name': 'J. L. Zanden'}, {'authorId': '118125058', 'name': 'S. Heikkinen'}]",2004.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"This essay is an attempt to do different things at the same time – therefore it is almost as chaotic as life itself. One of the most important reasons for writing it was to bring some order into the research I am doing – it is an attempt to convince myself that behind the apparent chaos of projects and papers there is structure and meaning. It also an attempt to start thinking about ‘global economic history’ as the conceptual framework for the research I am doing (I could not find anything bigger). This (perhaps unhappy) combination is clear from its structure: it starts as a ‘real’ academic essay, and later on becomes more ‘personal’, outlining my personal research programme, and concluding with some personal hobbies.",5.0
"Information Theory, Inference, and Learning Algorithms",f7f15848cd0fbb3d08f351595da833b1627de9c3,"[{'authorId': '145852650', 'name': 'D. MacKay'}]",2004.0,IEEE Transactions on Information Theory,"['Superintelligence: Paths, Dangers, Strategies']",1,Fun and exciting textbook on the mathematics underpinning the most dynamic areas of modern science and engineering.,9177.0
Astronomical Waste: The Opportunity Cost of Delayed Technological Development,2a35693a534b927b6a2304dbd4fa0270178a3c90,"[{'authorId': '2193691', 'name': 'N. Bostrom'}]",2003.0,Utilitas,"['Superintelligence: Paths, Dangers, Strategies']",1,"With very advanced technology, a very large population of people living happy lives could be sustained in the accessible region of the universe. For every year that development of such technologies and colonization of the universe is delayed, there is therefore a corresponding opportunity cost: a potential good, lives worth living, is not being realized. Given some plausible assumptions, this cost is extremely large. However, the lesson for standard utilitarians is not that we ought to maximize the pace of technological development, but rather that we ought to maximize its safety, i.e. the probability that colonization will eventually occur. This goal has such high utility that standard utilitarians ought to focus all their efforts on it. Utilitarians of a ‘person-affecting’ stripe should accept a modified version of this conclusion. Some mixed ethical views, which combine utilitarian considerations with other criteria, will also be committed to a similar bottom line.",153.0
Socioeconomic Status Modifies Heritability of IQ in Young Children,4621a1b603d5389e874aab00a60efa588140ccea,"[{'authorId': '3176133', 'name': 'E. Turkheimer'}, {'authorId': '3922609', 'name': 'A. Haley'}, {'authorId': '2070330661', 'name': 'M. Waldron'}, {'authorId': '1384088400', 'name': 'B. D’Onofrio'}, {'authorId': '4494812', 'name': 'I. Gottesman'}]",2003.0,Psychology Science,"['Superintelligence: Paths, Dangers, Strategies']",1,"Scores on the Wechsler Intelligence Scale for Children were analyzed in a sample of 7-year-old twins from the National Collaborative Perinatal Project. A substantial proportion of the twins were raised in families living near or below the poverty level. Biometric analyses were conducted using models allowing for components attributable to the additive effects of genotype, shared environment, and non-shared environment to interact with socioeconomic status (SES) measured as a continuous variable. Results demonstrate that the proportions of IQ variance attributable to genes and environment vary nonlinearly with SES. The models suggest that in impoverished families, 60% of the variance in IQ is accounted for by the shared environment, and the contribution of genes is close to zero; in affluent families, the result is almost exactly the reverse.",1253.0
A Map of the Universe,87095199a746d33a646a76ab50b4065aefad2e3f,"[{'authorId': '2177603060', 'name': 'J. R. Gott III'}, {'authorId': '2066411622', 'name': ""Mario Juri'c""}, {'authorId': '48767993', 'name': 'D. Schlegel'}, {'authorId': '153849686', 'name': 'F. Hoyle'}, {'authorId': '2428279', 'name': 'M. Vogeley'}, {'authorId': '49944508', 'name': 'M. Tegmark'}, {'authorId': '6460231', 'name': 'N. Bahcall'}, {'authorId': '145465591', 'name': 'J. Brinkmann'}]",2003.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"We have produced a new conformal map of the universe illustrating recent discoveries, ranging from Kuiper Belt objects in the solar system to the galaxies and quasars from the Sloan Digital Sky Survey. This map projection, based on the logarithm map of the complex plane, preserves shapes locally and yet is able to display the entire range of astronomical scales from the Earth's neighborhood to the cosmic microwave background. The conformal nature of the projection, preserving shapes locally, may be of particular use for analyzing large-scale structure. Prominent in the map is a Sloan Great Wall of galaxies 1.37 billion light-years long, 80% longer than the Great Wall discovered by Geller and Huchra and therefore the largest observed structure in the universe.",382.0
Learning to Control a Brain–Machine Interface for Reaching and Grasping by Primates,23bd1da0ad4d2ec956346655f0bb0206e13556b8,"[{'authorId': '2016191', 'name': 'J. Carmena'}, {'authorId': '2057713', 'name': 'M. Lebedev'}, {'authorId': '1826610', 'name': 'R. Crist'}, {'authorId': '1397219405', 'name': 'J. E. O’Doherty'}, {'authorId': '39819116', 'name': 'David M. Santucci'}, {'authorId': '38928339', 'name': 'D. Dimitrov'}, {'authorId': '8004223', 'name': 'P. Patil'}, {'authorId': '1845021', 'name': 'C. Henriquez'}, {'authorId': '144052463', 'name': 'M. Nicolelis'}]",2003.0,PLoS Biology,"['Superintelligence: Paths, Dangers, Strategies']",1,"Reaching and grasping in primates depend on the coordination of neural activity in large frontoparietal ensembles. Here we demonstrate that primates can learn to reach and grasp virtual objects by controlling a robot arm through a closed-loop brain–machine interface (BMIc) that uses multiple mathematical models to extract several motor parameters (i.e., hand position, velocity, gripping force, and the EMGs of multiple arm muscles) from the electrical activity of frontoparietal neuronal ensembles. As single neurons typically contribute to the encoding of several motor parameters, we observed that high BMIc accuracy required recording from large neuronal ensembles. Continuous BMIc operation by monkeys led to significant improvements in both model predictions and behavioral performance. Using visual feedback, monkeys succeeded in producing robot reach-and-grasp movements even when their arms did not move. Learning to operate the BMIc was paralleled by functional reorganization in multiple cortical areas, suggesting that the dynamic properties of the BMIc were incorporated into motor and sensory cortical representations.",1760.0
Sperm Whales: Social Evolution in the Ocean,fc4b7f4bb0512008cb4f768085eb7b6eb5f34aa3,"[{'authorId': '48590242', 'name': 'H. Whitehead'}]",2003.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"Famed in story as ""the great leviathans"" sperm whales are truly creatures of extremes. Giants among all whales, they also have the largest brains of any creature on Earth. Males can reach a length of 62 feet and can weigh upwards of 50 tons. With this book, Hal Whitehead gives us a clearer picture of the ecology and social life of sperm whales than we have ever had before. Based on almost two decades of field research, Whitehead describes sperm whale biology, behaviour and habitat; how they organize their societies; and how their complex lifestyles may have evolved in this unique environment. Among the many fascinating topics he explores is the crucial role that culture plays in the life of the sperm whale, and he traces the consequences of this argument for both evolution and conservation. Finally, drawing on these findings, Whitehead builds a general model of how the ocean environment influences social behaviour and cultural evolution among mammals as well as other animals. The definitive portrait of a provocative creature ""sperm whales"" should interest animal behaviourists, conservationists, ecologists and evolutionary biologists as well as marine mammalogists.",371.0
Genetic Programming IV: Routine Human-Competitive Machine Intelligence,5ef6bcc28e109a076ea3a30677b349b68889dc44,"[{'authorId': '1732302', 'name': 'J. Koza'}]",2003.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"1. Introduction. 2. Background on Genetic Programming. 3. Automatic Synthesis of Controllers. 4. Automatic Synthesis of Circuits. 5. Automatic Synthesis of Circuit Topology, Sizing, Placement, and Routing. 6. Automatic Synthesis of Antennas. 7. Automatic Synthesis of Genetic Networks. 8. Automatic Synthesis of Metabolic Pathways. 9. Automatic Synthesis of Parameterized Topologies for Controllers. 10. Automatic Synthesis of Parameterized Topologies for Circuits. 11. Automatic Synthesis of Parameterized Topologies with Conditional Developmental Operators for Circuits. 12. Automatic Synthesis of Improved Tuning Rules for PID Controllers. 13. Automatic Synthesis of Parameterized Topologies for Improved Controllers. 14. Reinvention of Negative Feedback. 15. Automated Re-Invention of Six Post-2000 Patented Circuits. 16. Problems for Which Genetic Programming May Be Well Suited. 17. Parallel Implementation and Computer Time. 18. Historical Perspective on Moore's Law and the Progression of Qualitatively More Substantial Results Produced by Genetic Programming. 19. Conclusion. Appendix A: Functions and Terminals. Appendix B: Control Parameters. Appendix C: Patented or Patentable Inventions Generated by Genetic Programming. Bibliography.",794.0
Are We Living in a Computer Simulation,857d288ea7a2336886cc652a6ecdb242ebcabb4b,"[{'authorId': '2193691', 'name': 'N. Bostrom'}]",2003.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"This paper argues that at least one of the following propositions is true: (1) the human species is very likely to go extinct before reaching a ""posthuman"" stage; (2) any posthuman civilization is extremely unlikely to run a significant number of simulations of their evolutionary history (or variations thereof); (3) we are almost certainly living in a computer simulation. It follows that the belief that there is a significant chance that we will one day become posthumans who run ancestor-simulations is false, unless we are currently living in a simulation. A number of other consequences of this result are also discussed.",397.0
Cognitive Architecture of a Mini-Brain,b01d1f8b7febf98824ab192afac735f39a46c018,"[{'authorId': '5028825', 'name': 'M. Giurfa'}, {'authorId': '144020211', 'name': 'R. Menzel'}]",2003.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,,40.0
The Doomsday Argument and the Self–Indication Assumption: Reply to Olum,7b859310e23045d8f524d013e591732c90ac883c,"[{'authorId': '2193691', 'name': 'N. Bostrom'}, {'authorId': '2547770', 'name': 'M. Ćirković'}]",2003.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"In a recent paper in this journal, Ken Olum attempts to refute the doomsday argument by appealing to the self-indication assumption (SIA) that your very existence gives you reason to think that there are many observers. Unlike earlier users of this strategy, Olum tries to counter objections that have been made against (SIA). We argue that his defence of (SIA) is unsuccessful. This does not, however, mean that one has to accept the doomsday argument (or the other counter-intuitive results that flow from related thought-experiments). A developed theory of observation selection efects shows why the doomsday argument is inconclusive, and how one can consistently reect both it and (SIA). I. THE RELATION BETWEEN THE DOOMSDAY ARGUMENT AND (SIA) The doomsday argument purports to show that we have systematically underestimated the probability that humankind will become extinct relatively soon. Originated by Brandon Carter and developed at length by John Leslie,' the doomsday argument argues that we have neglected to take fully into account the indexical information residing in the fact about when in the history of the human species we exist. In a nutshell, your birth rank (i.e., your position in the sequence of all humans) is roughly 70o billion. That you should have such a low birth rank is less surprising, and more probable, if the total number of humans that will ever have existed is, say, 2oo billion rather than, say, 200 trillion. (By 'probability' we here mean rational subjective credence.) Given these unequal conditional probabilities, one can derive from Bayes' theorem that the probability of impending doom goes up after conditionalizing on your birth rank. That is, after realizing the full evidential import",28.0
Prediction Markets as Decision Support Systems,e9e45d1c5767a3f6e3e62d1c7488090c3b22dbe5,"[{'authorId': '50201370', 'name': 'Joyce E. Berg'}, {'authorId': '2363796', 'name': 'Thomas A. Rietz'}]",2003.0,Inf. Syst. Frontiers,"['Superintelligence: Paths, Dangers, Strategies']",1,,270.0
Long-term in vivo imaging of experience-dependent synaptic plasticity in adult cortex,761f48502d86089fdf264a01bb685697e97d8901,"[{'authorId': '3222224', 'name': 'J. Trachtenberg'}, {'authorId': '2108342650', 'name': 'B. Chen'}, {'authorId': '144237098', 'name': 'G. Knott'}, {'authorId': '145660766', 'name': 'G. Feng'}, {'authorId': '90286693', 'name': 'J. Sanes'}, {'authorId': '144653463', 'name': 'E. Welker'}, {'authorId': '145919007', 'name': 'K. Svoboda'}]",2002.0,Nature,"['Superintelligence: Paths, Dangers, Strategies']",1,,1886.0
"Innovations, patent races and endogenous growth",e517b46da60a1a72178c9f4b0e4fb746e3bdfa0d,"[{'authorId': '2609384', 'name': 'Joseph Zeira'}]",2002.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"This paper presents a model of innovations and endogenous economic growth with two main assumptions: first, the cost of searching for innovations differs across innovations, and second, innovations take time to find. The paper shows that given these two assumptions together, competition leads to patent races and to duplication of innovative activity. The paper then shows that duplication significantly reduces the effect of scale on growth. It also shows that competitive R&D creates too much research on easy innovations, and too little research on the difficult ones. Finally, the paper shows that risk sharing might increase duplication and reduce growth.",37.0
Heuristics and Biases: The Psychology of Intuitive Judgment,7e0b715b6c49e68c516d5658d5ddad3ac993202a,"[{'authorId': '6030414', 'name': 'T. Gilovich'}, {'authorId': '49701838', 'name': 'D. Griffin'}, {'authorId': '3683465', 'name': 'D. Kahneman'}]",2002.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"A review is presented of the book “Heuristics and Biases: The Psychology of Intuitive Judgment,” edited by Thomas Gilovich, Dale Griffin, and Daniel Kahneman.",3745.0
Exercise: a behavioral intervention to enhance brain health and plasticity,61c52e360baff60c83d3b168e69c801c234a9dff,"[{'authorId': '145975173', 'name': 'C. Cotman'}, {'authorId': '3567138', 'name': 'N. Berchtold'}]",2002.0,Trends in Neurosciences,"['Superintelligence: Paths, Dangers, Strategies']",1,,2535.0
The evolved radio and its implications for modelling the evolution of novel sensors,ee6aeb06d477555f3e46b69486988f423c667ba0,"[{'authorId': '2144275972', 'name': 'Jon Bird'}, {'authorId': '1683086', 'name': 'P. Layzell'}]",2002.0,Proceedings of the 2002 Congress on Evolutionary Computation. CEC'02 (Cat. No.02TH8600),"['Superintelligence: Paths, Dangers, Strategies']",1,"Sensor evolution research typically uses evolutionary algorithms (EAs) to generate sensors that near-optimally satisfy large numbers of constraints. This is qualitatively different from the phylogenetic process found in nature that has resulted, for example, in the mammalian auditory ossicles evolving from the jaw bones of amphibians and reptiles, that in turn had previously acted as gill arches in fish. This paper describes an evolvable hardware experiment that resulted in a network of transistors sensing and utilising the radio waves emanating from nearby PCs. We argue that this evolved 'radio' is only the second device ever whose sensors were constructed in a way that in key aspects is analogous to that found in nature. We highlight the advantages and disadvantages of this approach and show why it is practically impossible to implement a similar process in simulation.",65.0
G: Highly general and highly practical,208dba9d99309acc0ba612e8de4e504e33f5ae1a,"[{'authorId': '116554629', 'name': 'L. Gottfredson'}]",2002.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,,168.0
DART: Revolutionizing Logistics Planning,8c6fbb2fdb8a10d27aaa4ad4f4af06b57006af12,"[{'authorId': '1687827', 'name': 'S. Hedberg'}]",2002.0,IEEE Intelligent Systems,"['Superintelligence: Paths, Dangers, Strategies']",1,"For the last two hundred years, the dominant force in international affairs has been the nation state. Most wars have been caused by attempts to create or expand such states. In contrast, over the next twenty years, the risks to international stability seem as likely to come from other factors: ethnic and religious conflict; population and environmental pressures; competition for scarce resources; drugs, terrorism and crime ... the consequences of initially local crises may spread dramatically throughout an ever more interdependent world. Planning and implementing an effective military response to a crisis is a highly complex problem. There are a host of interdependent factors to consider from the high-level strategic planning of an evolving crisis situation, to the nuts and bolts of moving people, machinery, and supplies.",9.0
World-championship-caliber Scrabble,5acbb3f169bc13a0e6b3848adabf856c20edf9c2,"[{'authorId': '2075623070', 'name': 'B. Sheppard'}]",2002.0,Artificial Intelligence,"['Superintelligence: Paths, Dangers, Strategies']",1,,155.0
Anthropic Bias: Observation Selection Effects in Science and Philosophy,07c231ff89f5d65f7541383e08e5bdcec2154f61,"[{'authorId': '2193691', 'name': 'N. Bostrom'}]",2002.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"Preface Content Acknowledgements Chapter1: Introduction Observation selection effects A brief history of anthropic reasoning Synopsis of this book Chapter 2: Fine- Tuning Arguments in Cosmology Does fine-tuning need explaining? No ""Inverse Gambler's Fallacy"" Roger White and Phil Dowe's analysis Surprising vs. unsurprising improbable events Modeling observation selection effects: the angel parable Preliminary conclusions Chapter 3: Anthropic Principles, the Motley Family The anthropic principle as expressing an observation selection effect Anthropic hodgepodge Freak observers and why earlier formulations are inadequate The Self-Sampling Assumption Chapter 4: Thought Experiments Supporting the Self-Sampling Assumption The Dungeon gedanken Two thought experiments by John Leslie The Incubator gedanken The reference class problem Chapter 5: The Self-Sampling Assumption in Science SSA in cosmology SSA in thermodynamics SSA in evolutionary biology SSA in traffic analysis SSA in quantum physics Summary of the case for SSA Chapter 6: The Doomsday Argument Background Doomsday a la Gottv The incorrectness of Gott's argument Doomsday a la Leslie The premisses of DA, and the Old evidence problem Leslie's views on the reference class problem Alternative conclusions of DA Chapter 7: Invalid Objections Against the Doomsday Argument Doesn't the Doomsday argument fail to ""target the truth""? (Korb and Oliver) The ""baby-paradox"" (Delahaye Korb and Oliver) Isn't a sample size of one too small? (Korb and Oliver) Couldn't a Cro-Magnon man have used the Doomsday argument? (Various) We can make the effect go away simply by considering a larger hypothesis space (Dieks Eastmond Korb and Oliver) Aren't we necessarily alive now? (Mark Greenberg) Sliding reference of ""soon"" and ""late""? (Mark Greenberg) How could I have been a 16th century human? (Mark Greenberg) Doesn't your theory presuppose that what happens in causally disconnected regions affects what happens here? (Ken Olum) But we know so much more about ourselves than our birth ranks! (Various) The Self-Indication Assumption - Is there safety in numbers? (Various) Chapter 8: Observer-Relative Chances in Anthropic Reasoning? Leslie's argument, and why it fails Observer-relative chances: another go Discussion: indexical facts - no conflict with physicalism In conclusion Appendix: the no-betting results Chapter 9: Paradoxes of the Self-Sampling Assumption The Adam & Eve experiments Analysis of Lazy Adam: predictions and counterfactuals The UN++ gedanken: reasons and abilities Quantum Joe: SSA and the Principal Principle Upshot Appendix: The Meta-Newcomb problem Chapter 10: Observation Selection Theory: A Methodology for Anthropic Reasoning Building blocks, theory constraints and desiderata The outline of a solution SSSA: Taking account of indexical information of observer-moments Reassessing Incubator How the reference class may be observer-moment relative Formalizing the theory: the Observation Equation A quantum generalization of OE Non-triviality of the reference class: why must be rejected A subjective factor in the choice of reference class? Chapter 11: Observation Selection Theory Applied Cosmological theorizing: fine-tuning and freak observers The freak-observer problem places only lax demands on the reference class The Sleeping Beauty problem: modeling imperfect recall The case of no outsiders The case with outsiders Synthesis of the 1/2- and the 1/3-views Observation selection theory applied to other scientific problems Robustness of reference class and scientific solidity Wrap-up References",294.0
Functional electrical stimulation in tetraplegic patients to restore hand function.,130faa339cb794a43e5d13aedae0b0d78eee8f33,"[{'authorId': '6022846', 'name': 'G. Degnan'}, {'authorId': '37368217', 'name': 'Tyler C Wind'}, {'authorId': '49886962', 'name': 'E. V. Jones'}, {'authorId': '5902116', 'name': 'R. Edlich'}]",2002.0,Journal of long-term effects of medical implants,"['Superintelligence: Paths, Dangers, Strategies']",1,"The purpose of this collective review is to describe a new form of functional electrical stimulation called neuroprosthesis. This unique technology has been devised to produce lateral pinch and palmar grasp in persons with C5 and C6 motor level spinal cord injuries. This neuroprosthesis includes external as well as implanted components. First, a receiver is surgically implanted into the patient's chest above a pectoralis major muscle. The receiver stimulator is then connected to 8 surgically implanted epimysial or intramuscular electrodes. Restoration of upper extremity function can greatly improve the lives of people affected with tetraplegia. When contralateral shoulder movements trigger an external transmitting coil, it sends a radio wave impulse to the stimulator inducing contraction of the muscles. Many tetraplegics are regaining hand function using implanted functional electrical stimulation. One major limitation is that the key muscles to be stimulated may have lower motor neuron damage, but this obstacle has been successfully overcome using surgical modifications of the biomechanics of the hand.",12.0
The Lives and the Death of Moore ’ s Law Ilkka Tuomi,2496782920276762d6a7eacbdcbea156ca118e8e,"[{'authorId': '3251136', 'name': 'I. Tuomi'}]",2002.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"Moore’s Law has been an important benchmark for developments in microelectronics and information processing for over three decades. During this time, its applications and interpretations have proliferated and expanded, often far beyond the validity of the original assumptions made by Moore. Technical considerations of optimal chip manufacturing costs have been expanded to processor performance, economics of computing, and social development. It is therefore useful to review the various interpretations of Moore’s Law and empirical evidence that could support them.",3.0
PROLEGOMENA TO ANY FUTURE PHILOSOPHY,89bdb1cb17dd1d91dc3218777972f50a34383bbd,"[{'authorId': '49523503', 'name': 'M. Walker'}]",2002.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"Since its inception, philosophy has struggled to reconcile the apparent finitude of humans with the traditional telos of philosophy—the attempt to unite thought and Being, to arrive at absolute knowledge, at a final theory of everything. In response, some pragmatists, positivists, and philosophical naturalists have offered a deflationary account of philosophy: the ambitions of philosophy ought to be scaled back to something much more modest. Inflationism is offered as an alternative: it is conjectured that philosophy might make more progress towards the traditional telos if we attempt to create beings (through the application of technology) who are as far removed from us in intelligence as we are from apes. Rather than deflating the ambitions of philosophy we ought to consider inflating the abilities of philosophers.",14.0
Investigating New Product Diffusion Across Products and Countries,4e45fc1a8ff9b92d142f8b8c34c75ae6834357f5,"[{'authorId': '2063037', 'name': 'Debabrata Talukdar'}, {'authorId': '145834267', 'name': 'K. Sudhir'}, {'authorId': '2064712994', 'name': 'A. Ainslie'}]",2002.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"As firms jockey to position themselves in emerging markets, firms need to evaluate the relative attractiveness of market expansion in different countries. Since the attractiveness of a market is a function of the eventual market potential and the speed at which the product diffuses through the market, a better understanding of the determinants of market potential and diffusion speed across different countries is of particular relevance to firms deliberating their market expansion strategies. Despite a recent spurt in research on multinational diffusion, there exist significant gaps in the literature. First, existing studies tend to limit their analysis to industrialized countries, thus reducing the ability to generalize the insights to many emerging markets. Second, these studies tend to focus on the coefficients of external and internal influence in the Bass diffusion model but do not analyze the determinants of market potential. Third, the choice of variables that affect the parameters of the Bass diffusion model has been rather limited. 
 
In this paper, we seek to address these gaps in the literature. To address the scope issue, we assembled a novel dataset that captures the diffusion of 6 products in 31 developed and developing countries from Europe, Asia, and North and South America. The set of countries in our dataset encompasses 60% of the world population and includes such emerging economies as China, India, Brazil, and Thailand. This should provide us with a stronger basis to make empirical generalizations about the diffusion process. 
 
For firms seeking to expand into emerging international markets, our findings about penetration potential have considerable significance. For example, we find that for the set of products that we analyze the average penetration potential for developing countries is about one-third 0.17 versus 0.52 of that for developed countries. We also find that it takes developing countries on average 17.9% 19.25 versus 16.33 years longer to achieve peak sales. Thus, despite the well-known positive effect of product introduction delays on diffusion speed, we find that developing countries still continue to experience a slower adoption rate, compared to that of developed countries. 
 
Our study also investigated the impact of several new macroenvironmental variables on penetration potential and speed. For example, our findings indicate that a 1% change in international trade or urbanization level can potentially change the penetration potential by about 0.5% and 0.2% respectively. These are some of the key variables projected to change significantly over the coming years for developing countries. While business managers have relatively little influence on such variables, our findings can still serve as valuable empirical guide for the variables that they should consider in evaluating diverse international markets and in performing sensitivity analysis with respect to their projected trends. 
 
Finally, our study also holds implications for managers seeking to combine information about past diffusion patterns across products and countries for better prediction. We pool information efficiently across multiple products and countries using a Hierarchical Bayes estimation methodology. By sharing information across countries and products in a single, coherent framework, we find that this pooling approach leads to substantial improvements in prediction accuracy. Our technique is particularly superior in predicting sales and BDM parameter values in the early years of new product introduction in a new country, when forecast estimates are managerially most useful. We also decompose the variance in the BDM model parameters into product, country, and product-country components. These results give guidelines to managers about which market experience they should weigh more to arrive at forecasts of market potential and diffusion speed. We find that while past experiences of other products in a country country effects are relatively more useful to explain penetration level cumulative sales, past experiences in other countries where a product was earlier introduced product effects are more useful to explain the coefficients of external and internal influence and thus the speed with which the product will attain peak sales.",311.0
Human intelligence differences: a recent history,d61651890772369892e005f24fce7f2110511780,"[{'authorId': '145805110', 'name': 'I. Deary'}]",2001.0,Trends in Cognitive Sciences,"['Superintelligence: Paths, Dangers, Strategies']",1,,113.0
The magical number 4 in short-term memory: A reconsideration of mental storage capacity,c8f359b3967ddef8e6d7f6ad58213a543d33ea22,"[{'authorId': '144225308', 'name': 'N. Cowan'}]",2001.0,Behavioral and Brain Sciences,"['Superintelligence: Paths, Dangers, Strategies']",1,"Miller (1956) summarized evidence that people can remember about seven chunks in short-term memory (STM) tasks. However, that number was meant more as a rough estimate and a rhetorical device than as a real capacity limit. Others have since suggested that there is a more precise capacity limit, but that it is only three to five chunks. The present target article brings together a wide variety of data on capacity limits suggesting that the smaller capacity limit is real. Capacity limits will be useful in analyses of information processing only if the boundary conditions for observing them can be carefully described. Four basic conditions in which chunks can be identified and capacity limits can accordingly be observed are: (1) when information overload limits chunks to individual stimulus items, (2) when other steps are taken specifically to block the recoding of stimulus items into larger chunks, (3) in performance discontinuities caused by the capacity limit, and (4) in various indirect effects of the capacity limit. Under these conditions, rehearsal and long-term memory cannot be used to combine stimulus items into chunks of an unknown size; nor can storage mechanisms that are not capacity-limited, such as sensory memory, allow the capacity-limited storage mechanism to be refilled during recall. A single, central capacity limit averaging about four chunks is implicated along with other, noncapacity-limited sources. The pure STM capacity limit expressed in chunks is distinguished from compound STM limits obtained when the number of separately held chunks is unclear. Reasons why pure capacity estimates fall within a narrow range are discussed and a capacity limit for the focus of attention is proposed.",5768.0
世界经济千年史=The world economy : a millennial perspective,3ac8ca8637764758b5eda5852bd3832d598d499a,"[{'authorId': '115873380', 'name': 'A. Maddison'}, {'authorId': '1403087876', 'name': '伍 晓鹰'}, {'authorId': '95695799', 'name': '許 憲春'}]",2001.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,,2708.0
Luxury Fever: Why Money Fails to Satisfy in an Era of Excess,3890254ea495eda31e60bc3ea417241c9aa895a8,"[{'authorId': '3923529', 'name': 'R. Nesse'}]",2001.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,,436.0
Toward a Fourth Generation of Revolutionary Theory,f422905e414686aaa30405302d1142791b0f69b6,"[{'authorId': '48789243', 'name': 'J. Goldstone'}]",2001.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"Third-generation theories of revolution pointed to the structural vulnerabilities of regimes as the basic causes of revolutions. In the last decade, critics of structural theories have argued for the need to incorporate leadership, ideology, and processes of identification with revolutionary movements as key elements in the production of revolution. Analyses of revolutions in developing countries and in communist regimes have further argued for incorporating these factors and for the inadequacy of structural theories to account for these events. Rather than try to develop a list of the “causes” of revolutions, it may be more fruitful for the fourth generation of revolutionary theory to treat revolutions as emergent phenomena, and to start by focusing on factors that cement regime stability. Weakness in those factors then opens the way for revolutionary leadership, ideology, and identification, along with structural factors such as international pressure and elite conflicts, to create revolutions.",424.0
The Future of Human Evolution,92d16554c322abb5769737937802ecbf19e697d0,"[{'authorId': '2193691', 'name': 'N. Bostrom'}]",2001.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"Evolutionary development is sometimes thought of as exhibiting an inexorable trend towards higher, more complex, and normatively worthwhile forms of life. This paper explores some dystopian scenarios where freewheeling evolutionary developments, while continuing to produce complex and intelligent forms of organization, lead to the gradual elimination of all forms of being that we care about. We then consider how such catastrophic outcomes could be avoided and argue that under certain conditions the only possible remedy would be a globally coordinated policy to control human evolution by modifying the fitness function of future intelligent life forms. 1. The Panglossian view Can we trust evolutionary development to take our species in broadly desirable directions? Starting from primitive, unconscious life, biological evolution has led to the development of ever more advanced organisms, including creatures that have minds, consciousness, language, and reason. More recently, cultural and technological development, which exhibit some parallels with biological evolution, have enabled our species to progress at a vastly accelerated pace. The past few hundred years have seen enormous improvements in human life‐span, labor productivity, scientific knowledge, and social and political organization, which have enabled billions of people to enjoy unprecedented opportunities for enjoyment and personal development. On a historical as well as on a geological timescale, the big picture shows an overarching trend towards increasing levels of complexity, knowledge, 1",66.0
Long-Term Growth As A Sequence of Exponential Modes,af8ec1aeefc9efbeb27da9781bc18b1f5db55d36,"[{'authorId': '145447707', 'name': 'R. Hanson'}]",2001.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"A world product time series covering two million years is well fit by either a sum of four exponentials, or a constant elasticity of substitution (CES) combination of three exponential growth modes: “hunting,” “farming,” and “industry.” The CES parameters suggest that farming substituted for hunting, while industry complemented farming, making the industrial revolution a smoother transition. Each mode grew world product by a factor of a few hundred, and grew a hundred times faster than its predecessor. This weakly suggests that within the next century a new mode might appear with a doubling time measured in days, not years. JEL Categories: O00, N10",32.0
In search of ant ancestors.,d2479169c5cdae9efb40007fd8d62993cbe2f70f,"[{'authorId': '39498634', 'name': 'T. Schultz'}]",2000.0,Proceedings of the National Academy of Sciences of the United States of America,"['Superintelligence: Paths, Dangers, Strategies']",1,"Ants are arguably the greatest success story in the history of terrestrial metazoa. On average, ants monopolize 15–20% of the terrestrial animal biomass, and in tropical regions where ants are especially abundant, they monopolize 25% or more. But ants did not always run the world. They do not appear in the fossil record until the mid-Cretaceous, and for more than the first half of their history—a period spanning 60 to 80 million years—ants occupied a relatively modest position in the terrestrial biosphere. To understand the factors, both ecological and historical, that contributed to the rise of the ants, we require a clearer picture of the stepwise evolution of the major ant lineages. Now, Grimaldi and Agosti (1) report in a recent issue of PNAS the remarkable discovery of a worker ant, preserved in amber for over 90 million years, that is clearly assignable to a modern ant subfamily that contains many familiar extant species, including carpenter ants. Combined with other paleontological and phylogenetic information, this unexpected fossil strongly indicates that the diversification of many ant subfamilies occurred earlier and more rapidly than previously suspected.",132.0
Preimplantation genetic diagnosis: polar body and embryo biopsy.,a4694d08f166081a02963b27159e6adf952b483e,"[{'authorId': '2963451', 'name': 'L. Gianaroli'}]",2000.0,Human Reproduction,"['Superintelligence: Paths, Dangers, Strategies']",1,,54.0
Minimization of Boolean complexity in human concept learning,5b81efddb8f201ad2b147642eaa12aaac36e9231,"[{'authorId': '143617733', 'name': 'J. Feldman'}]",2000.0,Nature,"['Superintelligence: Paths, Dangers, Strategies']",1,,457.0
Consequentialism and Cluelessness,3ac29be2feac5ca1235d9fe71247047e736be8a8,"[{'authorId': '117907079', 'name': 'J. Lenman'}]",2000.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,,135.0
"The origins, patterns and implications of human spontaneous mutation",5bfbff90c1b43c36f3c3b40b41b2ce91fcd8d426,"[{'authorId': '12269780', 'name': 'J. Crow'}]",2000.0,Nature reviews genetics,"['Superintelligence: Paths, Dangers, Strategies']",1,,857.0
Direct and Indirect Bargaining Costs and the Scope of the Firm,aa92f0848f548d0521bc4130bd1204dbb80a85da,"[{'authorId': '100783193', 'name': 'Marc Knez'}, {'authorId': '2387376', 'name': 'D. Simester'}]",2000.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"We compare bargaining costs with internal and external suppliers using a unique data set describing internal and external transactions for the same categories of parts at a single firm. The findings confirm that direct bargaining costs are higher with external suppliers, at least in part because there is more to bargain over. We also observed higher indirect bargaining costs with external suppliers. Information that may hinder contractual negotiations is often suppressed or delayed. To enforce these restrictions, all communication with external suppliers passes through procurement personnel, greatly hindering coordination and contributing to the determination of which parts are made internally.",76.0
Labor- and Capital- Augmenting Technical Change,99468878a795f411f24d95fd7d98456f906109ba,"[{'authorId': '2660799', 'name': 'D. Acemoglu'}]",2000.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"I analyze an economy in which profit-maximizing firms can undertake both labor- or capital-augmenting technological improvements. In the long run, the economy looks like the standard growth model with purely labor-augmenting technical change, and the share of labor in GDP is constant. Along the transition path, however, there is capital-augmenting technical change and factor shares change. A range of policies may have counterintuitive implications due to their effect on the direction of technical change. For example, taxes on capital income reduce the labor share in the short run, but increase it in the medium/long run.",651.0
Theory of quantum error correction for general noise,0d32ab5e558cc5091b03f3e74a6e517a78d2b7c6,"[{'authorId': '119628689', 'name': 'Knill'}, {'authorId': '104776388', 'name': 'Laflamme'}, {'authorId': '97872131', 'name': 'Viola'}]",1999.0,Physical Review Letters,"['Superintelligence: Paths, Dangers, Strategies']",1,"A measure of quality of an error-correcting code is the maximum number of errors that it is able to correct. We show that a suitable notion of ""number of errors"" e makes sense for any quantum or classical system in the presence of arbitrary interactions. Thus, e-error-correcting codes protect information without requiring the usual assumptions of independence. We prove the existence of large codes for both quantum and classical information. By viewing error-correcting codes as subsystems, we relate codes to irreducible representations of operator algebras and show that noiseless subsystems are infinite-distance error-correcting codes.",596.0
Ultimate physical limits to computation,a253b6c2d8f7e5f653e17fc27a97125cfe66168d,"[{'authorId': '145762777', 'name': 'S. Lloyd'}]",1999.0,Nature,"['Superintelligence: Paths, Dangers, Strategies']",1,,920.0
Economic Growth Given Machine Intelligence,f00de689ec0e93bc27d5e721cad99f32829d7ffb,"[{'authorId': '145447707', 'name': 'R. Hanson'}]",2000.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"A simple exogenous growth model gives conservative estimates of the economic implications of machine intelligence. Machines complement human labor when they become more productive at the jobs they perform, but machines also substitute for human labor by taking over human jobs. At first, expensive hardware and software does only the few jobs where computers have the strongest advantage over humans. Eventually, computers do most jobs. At first, complementary effects dominate, and human wages rise with computer productivity. But eventually substitution can dominate, making wages fall as fast as computer prices now do. An intelligence population explosion makes per-intelligence consumption fall this fast, while economic growth rates rise by an order of magnitude or more. These results are robust to automating incrementally, and to distinguishing hardware, software, and human capital from other forms of capital.",43.0
PROVERB: The Probabilistic Cruciverbalist,bd417d347a18e52598b3ccafcf5d8031f45c0f41,"[{'authorId': '2612932', 'name': 'Greg A. Keim'}, {'authorId': '1846258', 'name': 'Noam M. Shazeer'}, {'authorId': '144885169', 'name': 'M. Littman'}, {'authorId': '2114357210', 'name': 'Sushant Agarwal'}, {'authorId': '2687014', 'name': 'C. M. Cheves'}, {'authorId': '2052690663', 'name': 'Joseph Fitzgerald'}, {'authorId': '1996180', 'name': 'Jason Grosland'}, {'authorId': '2106723041', 'name': 'Fan Jiang'}, {'authorId': '34553772', 'name': 'S. Pollard'}, {'authorId': '2237516', 'name': 'Karl Weinmeister'}]",1999.0,AAAI/IAAI,"['Superintelligence: Paths, Dangers, Strategies']",1,"We attacked the problem of solving crossword puzzles by computer: given a set of clues and a crossword grid, try to maximize the number of words correctly filled in. In our system, ""expert modules"" specialize in solving specific types of clues, drawing on ideas from information retrieval, database search, and machine learning. Each expert module generates a (possibly empty) candidate list for each clue, and the lists are merged together and placed into the grid by a centralized solver. We used a probabilistic representation throughout the system as a common interchange language between subsystems and to drive the search for an optimal solution. PROVERB, the complete system, averages 95.3% words correct and 98.1 % letters correct in under 15 minutes per puzzle on a sample of 370 puzzles taken from the New York Times and several other puzzle sources. This corresponds to missing roughly 3 words or 4 letters on a daily 15 × 15 puzzle, making PROVERB a better-than-average cruciverbalist (crossword solver).",45.0
Military Technology Races,1b1be1ed6fbf882189579c43ce0dd981b13ea3a0,"[{'authorId': '46230088', 'name': 'V. Koubi'}]",1999.0,International Organization,"['Superintelligence: Paths, Dangers, Strategies']",1,"Because of the nature of modern weapons, significant innovations in arms technology have the potential to induce dramatic changes in the international distribution of power. Consider, for example, the “strategic defense initiative” (SDI), a program initiated by the United States in the early 1980s. Had the program been successfully completed, it might have led to a substantial devaluation of Soviet nuclear capabilities and put the United States in a very dominant position. It should not then come as a surprise that interstate rivalry, especially among super powers, often takes the form of a race for technological superiority. Mary Acland-Hood claims that although the United States and the Soviet Union together accounted for roughly half of the world's military expenditures in the early 1980s, their share of world military research and development (R&D) expenditures was about 80 percent. As further proof of the perceived importance of R&D, note that whereas the overall U.S. defense budget increased by 38 percent (from $225.1 billion to $311.6 billion in real terms) from 1981 to 1987, military R&D spending increased by 100 percent (from $20.97 billion to $41.96 billion). Moreover, before World War II military R&D absorbed on average less than 1 percent of the military expenditure of major powers, but since then it has grown to 11–13 percent. The emphasis on military technology is bound to become more pronounced in the future as R&D becomes the main arena for interstate competition.",16.0
The Foundations of Causal Decision Theory,1d32f7f1b89fda00962b8cf5649e25c37315e74b,"[{'authorId': '32328080', 'name': 'James M. Joyce'}]",1999.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,Preface Introduction: a chance to reconsider 1. Prudential rationality as expected utility maximization 2. Decision problems 3. Savage's theory 4. Evidential decision theory 5. Causal decision theory 6. A general theory of conditional beliefs 7. A representation theorem for causal decision theory 8. Where things stand Notes References.,618.0
"Chinese Glazes: Their Origins, Chemistry, and Recreation",07a5beada687dd4d26ad37ee277dec8bfa1de9cb,"[{'authorId': '47110371', 'name': 'N. Wood'}]",1999.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"Chinese ceramics and their glazes have delighted and enthralled the world for centuries. In this book, Nigel Wood traces the development of Chinese glazes from the Bronze Age to the present day. He carefully describes how Chinese glazes were made, and how they evolved over some 3000 years of continuous production. He provides analyses and shows how their superb qualities can be reproduced with common Western raw materials. The book is lavishly illustrated with hundreds of colour plates of Chinese potters and Chinese kilns.",100.0
UNITED STATES GOVERNMENT PRINTING OFFICE. WASHINGTON: 1999,de44e1133dc90d0d843ae9ad49651e54b348076f,"[{'authorId': '5458929', 'name': 'C. Groat'}]",1999.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,,20.0
Burning the Cosmic Commons : Evolutionary Strategies for Interstellar Colonization,62d001823d39bcb69ddd6ddd8cf09c712a682c27,"[{'authorId': '145447707', 'name': 'R. Hanson'}]",1999.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"Attempts to model interstellar colonization may seem hopelessly compromised by uncertainties regarding the technologies and preferences of advanced civilizations. If light speed limits travel speeds, however, then a selection effect may eventually determine frontier behavior. Making weak assumptions about colonization technology, we use this selection effect to predict colonists’ behavior, including which oases they colonize, how long they stay there, how many seeds they then launch, how fast and far those seeds fly, and how behavior changes with increasing congestion. This colonization model explains several astrophysical puzzles, predicting lone oases like ours, amid large quiet regions with vast unused resources. ∗For their comments, I thank Curt Adams, William Bainbridge, Forrest Bishop, Nick Bostrom, Russell Brand, Alexander Chislenko, Wei Dai, Carl Feymnan, Hal Finney, Sandy Hodges, Al Globus, Geoffrey Landis, Eugene Leitl, Tim May, Peter McCluskey, Ralph Merkle, Perry Metzger, William Newman, James Nolan, Anders Sandberg, Richard Schroeppel, Damien Sullivan, Nick Szabo, Frank Tipler, Ed Turner, Dan Werthimer, and participants of the University of West Virginia economics department seminars. This paper was inspired by a comment by Carl Feynman that a selection effect might determine asymptotic frontier colonization behavior. †rhanson@gmu.edu http://hanson.gmu.edu 704-993-2326 FAX: 704-993-2323 MSN 1D3, Carow Hall, Fairfax VA 22030",28.0
Nonzero: The Logic of Human Destiny,9ab5f193b8f87153c8096f3e21325cfb4eaa5958,"[{'authorId': '2070607466', 'name': 'R. Wright'}]",1999.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"A ssume that you are reading this book review from the comforts of your office chair or some stationary position How fast are you moving? Come on, quickly close your eyes and guess without peaking at the answer in the next paragraphl When I ask this question of my students, the typical response is zero miles an hour, which is the wrong answer. When our bodies are fully stationary, we are actually travelling at approximately 67,000 miles an hour as spaceship earth orbits the sun. Simultaneously, we are rotating on the earth's axis at 1,000 miles an hour. Yet, we don't feel dizzy, do we? Next, how many breaths do you take in a day? Come on, guess again! Several years ago I participated in a week long silent Buddhist meditation retreat where we counted our breaths for several hours and then extrapolated the amount over 24 hours. The average rate was 18,000 breaths a day, with a range from 15,000 to 21,000. In order to take one breath, many bodily functions must harmoniously interact. Otherwise, we die. How many of those breaths are you",452.0
The Physics of Information Processing Superobjects : Daily Life Among the Jupiter Brains,e03f5d8e1f1e5680b50c1ed0b0aee71f163cb4b4,"[{'authorId': '144816231', 'name': 'A. Sandberg'}]",1999.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"Physics limits information processing, and hence the possible forms of intelligent beings and their civilizations. In this review I discuss physical limitations on density, speed, size, energy dissipation and communication, sketching the constraints on very powerful information processing objects.",39.0
Psychological and Cardiovascular Effects and Short-Term Sequelae of MDMA (“Ecstasy”) in MDMA-Naïve Healthy Volunteers,a1a6ba6e9ee8802d74155dde0c862431944590f8,"[{'authorId': '1733129', 'name': 'F. Vollenweider'}, {'authorId': '145411166', 'name': 'A. Gamma'}, {'authorId': '3493681', 'name': 'M. Liechti'}, {'authorId': '2057126638', 'name': 'T. Huber'}]",1998.0,Neuropsychopharmacology,"['Superintelligence: Paths, Dangers, Strategies']",1,,423.0
Human cognitive abilities: A survey of factor analytic studies,d8a7c3f71f364f0b96deede8d14694438873d04c,"[{'authorId': '98618223', 'name': 'Michael C. Pyryt'}]",1998.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"This long awaited work surveys and summarizes the results of more than seventy years of investigation, by factor analysis, of a variety of cognitive abilities, with particular attention to language, thinking, memory, visual and auditory perception, creativity and the production of ideas, and the speed and accuracy of mental processing. The author describes his detailed finThis long awaited work surveys and summarizes the results of more than seventy years of investigation, by factor analysis, of a variety of cognitive abilities, with particular attention to language, thinking, memory, visual and auditory perception, creativity and the production of ideas, and the speed and accuracy of mental processing. The author describes his detailed findings resulting from reanalysis of more than 460 data sets from the factor-analytic literature, followed by a presentation of a hierarchical, three-stratum theory of cognitive ability and its implications for further research. A set of three computer disks (IBM 3-1/2 1.4 megabytes, ASCII format) containing the numerical data sets and Dr. Carroll's statistical results is also available. Representing over 4 megabytes of data or roughly 2000 printed pages the disks are major resources for the interested researcher.",2991.0
Prokaryotes: the unseen majority.,321e4c5e6031364c39db3cfdb151dcc2deaac1ba,"[{'authorId': '2592873', 'name': 'W. Whitman'}, {'authorId': '48284299', 'name': 'D. Coleman'}, {'authorId': '6375354', 'name': 'W. Wiebe'}]",1998.0,Proceedings of the National Academy of Sciences of the United States of America,"['Superintelligence: Paths, Dangers, Strategies']",1,"The number of prokaryotes and the total amount of their cellular carbon on earth are estimated to be 4-6 x 10(30) cells and 350-550 Pg of C (1 Pg = 10(15) g), respectively. Thus, the total amount of prokaryotic carbon is 60-100% of the estimated total carbon in plants, and inclusion of prokaryotic carbon in global models will almost double estimates of the amount of carbon stored in living organisms. In addition, the earth's prokaryotes contain 85-130 Pg of N and 9-14 Pg of P, or about 10-fold more of these nutrients than do plants, and represent the largest pool of these nutrients in living organisms. Most of the earth's prokaryotes occur in the open ocean, in soil, and in oceanic and terrestrial subsurfaces, where the numbers of cells are 1.2 x 10(29), 2.6 x 10(29), 3.5 x 10(30), and 0. 25-2.5 x 10(30), respectively. The numbers of heterotrophic prokaryotes in the upper 200 m of the open ocean, the ocean below 200 m, and soil are consistent with average turnover times of 6-25 days, 0.8 yr, and 2.5 yr, respectively. Although subject to a great deal of uncertainty, the estimate for the average turnover time of prokaryotes in the subsurface is on the order of 1-2 x 10(3) yr. The cellular production rate for all prokaryotes on earth is estimated at 1.7 x 10(30) cells/yr and is highest in the open ocean. The large population size and rapid growth of prokaryotes provides an enormous capacity for genetic diversity.",4422.0
Anatomy of a revolution.,86c878f09521f6c55130bbb23e8e8141c080a0da,"[{'authorId': '144129407', 'name': 'G. Fink'}]",1998.0,Genetics,"['Superintelligence: Paths, Dangers, Strategies']",1,"WITH this volume, Genetics announces that Arabidopsis has joined the Security Council of Model Genetic Organisms. These favored few form the standard to which all other organisms are compared. Like the Security Council of the United Nations, where there is broad geographical representation, the",251.0
The Extended Mind,1be3feeb17c3520fbac6b5b652aa41feea596b2f,"[{'authorId': '37700983', 'name': 'A. Clark'}, {'authorId': '2072252', 'name': 'D. Chalmers'}]",1998.0,Analysis,"['Superintelligence: Paths, Dangers, Strategies']",1,"Where does the mind stop and the rest of the world begin? The question invites two standard replies. Some accept the intuitive demarcations of skin and skull, and say that what is outside the body is outside the mind. Others are impressed by arguments suggesting that the meaning of our words ""just ain't in the head"", and hold that this externalism about meaning carries over into an externalism about mind. We propose to pursue a third position. We will advocate an externalism about mind, but one that is in no way grounded in the debatable role of external reference in fixing the contents of our mental states. Rather, we advocate an *active externalism*, based on the active role of the environment in driving cognitive processes.",4569.0
Must Early Life Be Easy? The Rhythm of Major Evolutionary Transitions,9305ddd5782bed5b14360869a538241707b00bb8,"[{'authorId': '145447707', 'name': 'R. Hanson'}]",1998.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"If we are not to conclude that most planets like Earth have evolved life as intelligent as we are, we must presume Earth is not random. This selection effect, however, also implies that the origin of life need not be as easy as the early appearance of life on Earth suggests. If a series of major evolutionary transitions were required to produce intelligent life, selection implies that a subset of these were “critical steps,” with durations that are similarly distributed. The time remaining from now until simple life is no longer possible on Earth must also be similarly distributed. I show how these results provide timing tests to constrain models of critical evolutionary transitions. ∗I thank Brandon Carter, Jeff Dominitz, David Grether, Peggy Jackson, John Ledyard, John Leslie, David Porter, and Robert Sherman for comments on this paper. I thank the New Millennium Program Office of the Jet Propulsion Lab of NASA and the Robert Wood Johnson Foundation for financial support. †rhanson@gmu.edu http://hanson.gmu.edu 704-993-2326 FAX: 704-993-2323MSN 1D3, Carow Hall, Fairfax VA 22030",23.0
The Handicap Principle: A Missing Piece of Darwin's Puzzle,9fd8edb7b18bd5e6666e798e5c7679b2b4b0993b,"[{'authorId': '1422292642', 'name': 'Amots Zehavi'}, {'authorId': '5389688', 'name': 'A. Zahavi'}]",1997.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"Ever since Darwin, animal behaviour has intrigued and perplexed human observers. The elaborate mating rituals, lavish decorative displays, complex songs, calls, dances and many other forms of animal signalling raise fascinating questions. To what degree can animals communicate within their own species and even between species? What evolutionary purpose do such communications serve? Perhaps most importantly, what can animal signalling tell us about our own non-verbal forms of communication? In The Handicap Principle, Amotz and Ashivag Zahavi offer a unifying theory that brilliantly explains many previously baffling aspects of animal signalling and holds up a mirror in which ordinary human behaviours take on surprising new significance. The wide-ranging implications of the Zahavis' new theory make it arguably the most important advance in animal behaviour in decades. Based on 20 years of painstaking observation, the Handicap Principle illuminates an astonishing variety of signalling behaviours in animals ranging from ants and ameba to peacocks amd gazelles. Essentially, the theory asserts that for animal signals to be effective they must be reliable, and to be reliable they must impose a cost, or handicap, on the signaller. When a gazelle sights a wolf, for instance, and jumps high into the air several times before fleeing, it is signalling, in a reliable way, that it is in tip-top condition, easily able to outrun the wolf. (A human parallel occurs in children's games of tag, where faster children will often taunt their pursuer before running). By momentarily handicapping itself-expending precious time and energy in this display-the gazelle underscores the truthfulness of its signal. Such signalling, the authors suggest, serves the interests of both predator and prey, sparing each the exhaustion of a pointless chase. Similarly, the enormous cost a peacock incurs by carrying its elaborate and weighty tail-feathers, which interfere with food gathering, reliably communicates its value as a mate able to provide for its offspring. Perhaps the book's most important application of the Handicap Principle is to the evolutionary enigma of animal altruism. The authors convincingly demonstrate that when an animal acts altruistically, it handicaps itself-assumes a risk or endures a sacrifice-not primarily to benefit its kin or social group but to increase its own prestige within the group and thus signal its status as a partner or rival. Finally, the Zahavis' show how many forms of non-verbal communication among humans can also be explained by the Handicap Principle. Indeed, the authors suggest that non-verbal signals-tones of voice, facial expressions, body postures-are quite often more reliable indicators of our intentions than is language. Elegantly written, exhaustively researched, and consistently enlivened by equal measures of insight and example, The Handicap Principle illuminates virtually every kind of animal communication. It not only allows us to hear what animals are saying to each other-and to understand why they are saying it-but also to see the enormously important role non-verbal behaviour plays in human communication.",1355.0
The heritability of IQ,c9e7c2477f830a1651ba337fbca2c5f17282e752,"[{'authorId': '108596863', 'name': 'B. Devlin'}, {'authorId': '2074470844', 'name': 'Michael J. Daniels'}, {'authorId': '144750484', 'name': 'K. Roeder'}]",1997.0,Nature,"['Superintelligence: Paths, Dangers, Strategies']",1,,251.0
An Empirical Exploration of a Technology Race,ed54e5ce5963f2ba97771a4e4870d59de17ba822,"[{'authorId': '2674836', 'name': 'J. Lerner'}]",1997.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"An extensive theoretical literature examines technological competition, and in particular whether leaders maintain their standing. These models, however, have received little empirical support. I examine innovation in the disk drive industry, an environment particularly conducive to identifying racing behavior. Strategic variables prove significant in explaining the decision to innovate. The patterns are in accord with Reinganum's work: firms that trail the leader innovate more. I add controls for technological opportunity, financing constraints, and firm turnover. When firms manufacture drives for internal use or there are many entrants, and strategic interactions may be less important, the effects are less pronounced.",209.0
One jump ahead - challenging human supremacy in checkers,5fb356de5280f09ad638475f1bad9574df49bae3,"[{'authorId': '1694170', 'name': 'J. Schaeffer'}]",1997.0,J. Int. Comput. Games Assoc.,"['Superintelligence: Paths, Dangers, Strategies']",1,"Marion Tinsley, World checkers champion This extraordinary book tells the story of the creation of the world champion checkers computer program, Chinook. From its beginnings in 1988, Chinook became a worthy opponent to the world champion by 1990 and by 1992 had defeated all the world's top human players. In this fascinating account, Jonathan Schaeffer, the originator and leader of the Chinook team, provides an engrossing story of failures and successes. He describes the human story behind the program and his own feelings in learning from mistakes and technical problems in a continuous effort to improve Chinook's performance. Over the ten year period beginning in 1988, we follow the development of Chinook from an innocent question asked over lunch through to the final match against the then world champion, Marion Tinsley. As the story unfolds, readers are introduced to the rules of checkers and the basics of computer game programs, as well as to the key figures of the story. As a result, all those interested in computing and games will enjoy this book. "" Schaeffer's personal involvement in the Chinook project, along with his engaging and open story-telling makes the book surprisingly gripping."" A.K. Dewdney",163.0
A Neural Substrate of Prediction and Reward,12b9019f99a315a137400389ee7c6faa4cceef35,"[{'authorId': '145401965', 'name': 'W. Schultz'}, {'authorId': '1790646', 'name': 'P. Dayan'}, {'authorId': '144895942', 'name': 'P. Montague'}]",1997.0,Science,"['Superintelligence: Paths, Dangers, Strategies']",1,"The capacity to predict future events permits a creature to detect, model, and manipulate the causal structure of its interactions with its environment. Behavioral experiments suggest that learning is driven by changes in the expectations about future salient events such as rewards and punishments. Physiological work has recently complemented these studies by identifying dopaminergic neurons in the primate whose fluctuating output apparently signals changes or errors in the predictions of future salient and rewarding events. Taken together, these findings can be understood through quantitative theories of adaptive optimizing control.",8088.0
The end of the world: The science and ethics of human extinction,ab2b32e462f3c90967f6fea6f04e50a9bfb68cae,"[{'authorId': '50851624', 'name': 'Stephen F. Haller'}]",1997.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,,116.0
Mutually Assured Destruction Revisited. Strategic Doctrine in Question,e5fa7755610d7c462a7cc36341168a109e1df0a8,"[{'authorId': '95277750', 'name': 'Alan J. Parrington'}]",1997.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"Abstract : On 3 December 1996, Gen Lee Butler, USAF, Retired, the last commander in chief of the Strategic Air Command, stunned a National Public Radio audience by calling for the near-term elimination of all nuclear weapons. General Butler was joined on the rostrum by Gen Andrew J. Goodpaster, the former NATO commander and advisor to a half-dozen presidents during his 70 years of national service. They were there to announce the release of the ""Statement on Nuclear Weapons by International Generals and Admirals, a document signed by 63 former flag officers advocating the abolition of nuclear weapons. The signatories read like a Who's Who of cold-war militaries, including such notables as Bernard Rogers, John Galvin, Chuck Horner, Lord Carver, Vladimir Belous, and Alexander Lebed--20 Americans, 18 Russians, and 17 nations in all from every corner of the globe. They were not the first to make such a recommendation, however. As General Good every US president since Dwight Eisenhower has taken a similar position with respect to atomic weapons. But the generals seemed perplexed. Despite the long widespread questions about the utility of atomic weapons, the world was steadily marching along the path towards nuclear proliferation while the perceived window of opportunity brought about by the end of the cold war slipped away. It was as if the lessons of the past 50 years were too hard to swallow and the elimination of nuclear weapons just too hard to do. Other than garnering a few small articles in the national press, their warnings seemed to have little impact. Where the generals erred was in simply challenging the nuclear bombs, rather than the strategy behind the weapons--a strategy oddly known as mutually assured destruction (MAD).",8.0
Artificial Evolution in the Physical World,36c432dd4dc44e78601b9431bfcddd46c53d9baf,"[{'authorId': '145720639', 'name': 'A. Thompson'}]",1997.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,,40.0
From Here to Human-Level AI,35759a54f37d0a3612e248706d9f64faac5ca254,"[{'authorId': '143805236', 'name': 'J. McCarthy'}]",1996.0,International Conference on Principles of Knowledge Representation and Reasoning,"['Superintelligence: Paths, Dangers, Strategies']",1,,169.0
Chattering Cells: Superficial Pyramidal Neurons Contributing to the Generation of Synchronous Oscillations in the Visual Cortex,36e65e4e4df46c97f481b7a5f46d0fa3389cc93b,"[{'authorId': '145941291', 'name': 'C. Gray'}, {'authorId': '34873916', 'name': 'D. McCormick'}]",1996.0,Science,"['Superintelligence: Paths, Dangers, Strategies']",1,"In response to visual stimulation, a subset of neurons in the striate and prestriate cortex displays synchronous rhythmic firing in the gamma frequency band (20 to 70 hertz). This finding has raised two fundamental questions: What is the functional significance of synchronous gamma-band activity and how is it generated? This report addresses the second of these two questions. By means of intracellular recording and staining of single cells in the cat striate cortex in vivo, a biophysically distinct class of pyramidal neuron termed “chattering cells” is described. These neurons are located in the superficial layers of the cortex, intrinsically generate 20- to 70-hertz repetitive burst firing in response to suprathreshold depolarizing current injection, and exhibit pronounced oscillations in membrane potential during visual stimulation that are largely absent during periods of spontaneous activity. These properties suggest that chattering cells may make a substantial intracortical contribution to the generation of synchronous cortical oscillations and thus participate in the recruitment of large populations of cells into synchronously firing assemblies.",910.0
River out of eden : a Darwinian view of life,1fbddffc99013abeaec4fd05a9eda2d1631e7185,"[{'authorId': '143916126', 'name': 'R. Dawkins'}, {'authorId': '2056152800', 'name': 'L. Ward'}]",1996.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"The Number One SUNDAY TIMES bestseller. A fascinating explanation of how evolution works, from bestselling author Richard Dawkins. The river of Dawkins's title is a river of DNA, flowing through time from the beginning of life on earth to the present - and onwards. Dawkins explains that DNA must be thought of as the most sophisticated information system imaginable: 'Life is just bytes and bytes of information,' he writes. Using this perspective, he describes the mechanisms by which evolution has taken place, gradually but inexorably, over a period of three thousand million years. It is the story of how evolution happens, rather than a narrative of what has actually happened in evolution. He discusses current views on the process of human evolution, including the idea that we all trace back to a comparatively recent African 'Eve', and speculates that the 'information explosion' that was unleashed on Earth when DNA came into being has almost certainly happened in other places in the universe.",274.0
Experiments In Musical Intelligence,ecf79fcaba4eac5628ac1315d1c612952672a8f2,"[{'authorId': '143874526', 'name': 'D. Cope'}]",1996.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,,285.0
THE FACE OF TOMORROW,5d419194283e7d581e4727ce9f43e0c24642c863,"[{'authorId': '2069262640', 'name': 'Jenny McCune'}]",1995.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"In the next 10 years, service industries will dominate the list of fast‐growing, small‐business fields. Take a look at what factors are behind the trends, as well as who's taking advantage of them—and how.",18.0
Could gambling save science? Encouraging an honest consensus,bc7f184ffc7816ac53cba4bce8fc26ac3d812c87,"[{'authorId': '145447707', 'name': 'R. Hanson'}]",1995.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"Abstract The pace of scientific progress may be hindered by the tendency of our academic institutions to reward being popular rather than being right. A market‐based alternative, where scientists can more formally ‘stake their reputation’, is presented here. It offers clear incentives to be careful and honest while contributing to a visible, self‐consistent consensus on controversial (or routine) scientific questions. In addition, it allows patrons to choose questions to be researched without choosing people or methods. The bulk of this paper is spent in examining potential problems with the proposed approach. After this examination, the idea still seems to be plausible and worthy of further study.",87.0
The Roots of Backpropagation: From Ordered Derivatives to Neural Networks and Political Forecasting,d181a84258bd1249768bfaaa580be5dd1338c596,"[{'authorId': '2319833', 'name': 'P. Werbos'}]",1994.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"THESIS. Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences. Dynamic Feedback, Statistical Estimation, and Systems Optimization: General Techniques. The Multivariate ARMA(1,1) Model: Its Significance and Estimation. Simulation Studies of Techniques of Time--Series Analysis. General Applications of These Ideas: Practical Hazards and New Possibilities. Nationalism and Social Communications: A Test Case for Mathematical Approaches. APPLICATIONS AND EXTENSIONS. Forms of Backpropagation for Sensitivity Analysis, Optimization, and Neural Networks. Backpropagation Through Time: What It Does and How to Do It. Neurocontrol: Where It Is Going and Why It Is Crucial. Neural Networks and the Human Mind: New Mathematics Fits Humanistic Insight. Index.",625.0
Population Growth and Technological Change: One Million B.C. to 1990,73163a879f3c6fc04edd51edb95e6f33b9e88fd6,"[{'authorId': '1514969562', 'name': 'M. Kremer'}]",1993.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"The nonrivalry of technology, as modeled in the endogenous growth Uterature, implies that high population spurs technological change. This paper constructs and empirically tests a model of long-run world population growth combining this implication with the Malthusian assumption that technology limits population. The model predicts that over most of history, the growth rate of population will be proportional to its level. Empirical tests support this prediction and show that historically, among societies with no possibility for technological contact, those with larger initial populations have had faster technological change and population growth.",1393.0
The anthropic selection principle and the ultra-Darwinian synthesis.,19e71e58bf0316e33c8bfd185f1b9b4ff43e177a,"[{'authorId': '86922500', 'name': 'B. Carter'}]",1993.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,,22.0
An Introduction to Kolmogorov Complexity and Its Applications,a222764ed0176fc1d32ab5cab6588f6e9e426b45,"[{'authorId': '49140741', 'name': 'Ming Li'}, {'authorId': '46641506', 'name': 'P. Vitányi'}]",1993.0,Texts and Monographs in Computer Science,"['Superintelligence: Paths, Dangers, Strategies']",1,,3447.0
The Selected Letters of Bertrand Russell: The Public Years 1914-1970,857e6fbf5bdc72d46c50902e60d1d5d3ad33b7d7,"[{'authorId': '94008979', 'name': 'B. Russell'}, {'authorId': '143884004', 'name': 'N. Griffin'}]",1992.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"Preface Introduction 1. War (1914-1918) 2. Children, Companionship, and Joint Work (1918-1927) 3. Starting a School and Ending a Marriage (1927-1935) 4. Marriage, Poverty, and Exile (1936-1944) 5. Respectability at Last (1944-1954) 6. Peace (1955-1970)",10.0
"Nanosystems - molecular machinery, manufacturing, and computation",7950cd1c2d87bc936914cfa2b197ce64a274ce5e,"[{'authorId': '143977849', 'name': 'K. Drexler'}]",1992.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"PHYSICAL PRINCIPLES. Classical Magnitudes and Scaling Laws. Potential Energy Surfaces. Molecular Dynamics. Positional Uncertainty. Transitions, Errors, and Damage. Energy Dissipation. Mechanosynthesis. COMPONENTS AND SYSTEMS. Nanoscale Structural Components. Mobile Interfaces and Moving Parts. Intermediate Subsystems. Nanomechanical Computational Systems. Molecular Sorting, Processing, and Assembly. Molecular Manufacturing Systems. IMPLEMENTATION STRATEGIES. Macromolecular Engineering. Paths to Molecular Manufacturing. Appendices. Afterword. Symbols, Units, and Constants. Glossary. References. Index.",1265.0
Executive Summary 1,cf7cf02628ed00222a850a380db43756d850583b,"[{'authorId': '36993656', 'name': 'A. Dodd'}, {'authorId': '47200135', 'name': 'W. Webb'}, {'authorId': '3554865', 'name': 'M. Lichtveld'}, {'authorId': '15626260', 'name': 'Pamela D. Tucker'}, {'authorId': '2113476161', 'name': 'Deborah White'}]",1992.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,,1.0
Agency and Probabilistic Causality,2b753ed8d7446a6a6ad1433246feb68506c6b8e7,"[{'authorId': '143724728', 'name': 'H. Price'}]",1991.0,British Journal for the Philosophy of Science,"['Superintelligence: Paths, Dangers, Strategies']",1,"Probabilistic accounts of causality have long had trouble with ‘spurious’ evidential correlations. Such correlations are also central to the case for causal decision theory—the argument that evidential decision theory is inadequate to cope with certain sorts of decision problem. However, there are now several strong defences of the evidential theory. Here I present what I regard as the best defence, and apply it to the probabilistic approach to causality. I argue that provided a probabilistic theory appeals to the notions of agency and effective strategy, it can avoid the problem of spurious causes. I show that such an appeal has other advantages; and argue that it is not illegitimate, even for a causal realist.",126.0
"Wonderful Life: The Burgess Shale and the Nature of History, Stephen Jay Gould. W. W. Norton, New York (1989), 347, Price $19.95 (U.S.A.), $27.95 (Canada)",0842879f5c071157733333a641647cc6b1411a34,"[{'authorId': '47465702', 'name': 'J. Hailman'}]",1991.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,,731.0
"A Craft in Antiquity. (Book Reviews: Prehistoric Textiles. The Development of Cloth in the Neolithic and Bronze Ages, with Special Reference to the Aegean)",bdf84d1fc5e2ca145913f1eed3bd8439d9d32709,"[{'authorId': '88676666', 'name': 'B. Barber'}]",1991.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"This pioneering work revises our notions of the origins and early development of textiles in Europe and the Near East. Using innovative linguistic techniques, along with methods from palaeobiology and other fields, it shows that spinning and pattern weaving began far earlier than has been supposed. Prehistoric Textiles made an unsurpassed leap in the social and cultural understanding of textiles in humankind's early history. Cloth making was an industry that consumed more time and effort, and was more culturally significant to prehistoric cultures, than anyone assumed before the book's publication. The textile industry is in fact older than pottery--and perhaps even older than agriculture and stockbreeding. It probably consumed far more hours of labor per year, in temperate climates, than did pottery and food production put together. And this work was done primarily by women. Up until the Industrial Revolution, and into this century in many peasant societies, women spent every available moment spinning, weaving, and sewing. The author, Elizabeth Wayland Barber, demonstrates command of an almost unbelievably disparate array of disciplines--from historical linguistics to archaeology and paleobiology, from art history to the practical art of weaving. Her passionate interest in the subject matter leaps out on every page. Barber, a professor of linguistics and archaeology, developed expert sewing and weaving skills as a small girl under her mother's tutelage. One could say she had been born and raised to write this book. Because modern textiles are almost entirely made by machines, we have difficulty appreciating how time-consuming and important the premodern textile industry was. This book opens our eyes to this crucial area of prehistoric human culture.",294.0
Limits of Artificial Intelligence@@@The Emperor's New Mind,63ab0846f5a15378b5a611388abee2ce3365957b,"[{'authorId': '100716461', 'name': 'Louis F. Weschler'}, {'authorId': '3110190', 'name': 'R. Penrose'}]",1990.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,,3.0
Desire as Belief,4923a4e5b53e49f0a8fa3c68222d101bc3332946,"[{'authorId': '2116423750', 'name': 'D. Lewis'}]",1988.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"A Humean thesis about motivation says that we are moved entirely by desire: we are disposed to do what will serve our desires according to our beliefs. If there were no desires to serve, we would never be moved more to do one thing than another. Whatever might happen then would be entirely unmotivated. Here I shall uphold Humeanism against one sort of opponent. Our Anti-Humean challenges us with this case. The Department must choose between two candidates for a job, Meane and Neiss. Neiss is your old friend, affable, sensible, fair-minded, co-operative, moderate, . Meane is quite the opposite. But it is clear that Meane is just a little bit better at philosophy. Gritting your teeth and defying all desire, you vote for Meane, because you believe that Meane getting the job instead of Neiss would, all things considered, be good. Your belief about what's good has moved you to go against your desire to have Neiss for a colleague and to have nothing to do with Meane. We Humeans reply that there are desires and there are desires. Some desires, for instance your desire to have Neiss for a colleague, are warmyou feel enthusiasm, you take pleasure in the prospect of fulfilment. Other desires, for instance your desire to hire the best available candidate, are cold. Nobody ever said that only the warm desires can move us. It is not so that you defied all desire when you voted for Meane. You were moved entirely by your desires, however the cold desire outweighed the warm one. We are within our rights to construe 'desire' inclusively, to cover the entire range of states that move us, including for instance the state that moved you to vote for Meane. Humeanism understood in this inclusive way is surely true-maybe a trivial truth, but a trivial truth is still a truth. Let our Anti-Humean grant that the state that moved you was after all, inclusively speaking, a desire. He may insist, however, that it was also a belief: the belief (as he said before) that Meane getting the job would be good. Although it may be true-trivially, he sneers-that all motivation is by desire, it is also true that some motivation is by belief. Sometimes, what happens is that we do what will serve the good according to our beliefs about what would be good together with our other beliefs-no desire, other than desires which are identical with beliefs, need enter into it. More cautiously, he might say that some beliefs are, at least, necessarily",149.0
The Humean Theory of Motivation,0e31fe01bca61edadbb20d2afa0b13c6e8203d74,"[{'authorId': '2116645812', 'name': 'Michael Smith'}]",1987.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,,390.0
The structure of the nervous system of the nematode Caenorhabditis elegans.,62d36f23580ae0c822ebc7de69ae603d85441bfc,"[{'authorId': '152370438', 'name': 'J. White'}, {'authorId': '87124908', 'name': 'E. Southgate'}, {'authorId': '144083582', 'name': 'J. Thomson'}, {'authorId': '144284186', 'name': 'S. Brenner'}]",1986.0,"Philosophical transactions of the Royal Society of London. Series B, Biological sciences","['Superintelligence: Paths, Dangers, Strategies']",1,"The structure and connectivity of the nervous system of the nematode Caenorhabditis elegans has been deduced from reconstructions of electron micrographs of serial sections. The hermaphrodite nervous system has a total complement of 302 neurons, which are arranged in an essentially invariant structure. Neurons with similar morphologies and connectivities have been grouped together into classes; there are 118 such classes. Neurons have simple morphologies with few, if any, branches. Processes from neurons run in defined positions within bundles of parallel processes, synaptic connections being made en passant. Process bundles are arranged longitudinally and circumferentially and are often adjacent to ridges of hypodermis. Neurons are generally highly locally connected, making synaptic connections with many of their neighbours. Muscle cells have arms that run out to process bundles containing motoneuron axons. Here they receive their synaptic input in defined regions along the surface of the bundles, where motoneuron axons reside. Most of the morphologically identifiable synaptic connections in a typical animal are described. These consist of about 5000 chemical synapses, 2000 neuromuscular junctions and 600 gap junctions.",5269.0
"A Strategic Analysis of Science and Technology Policy@@@Science, Technology, and Policy Decisions@@@Tradeoffs: Imperatives of Choice in a High-Tech World",ace80aefa76f5022448e1bf20f85f771c2a86d9c,"[{'authorId': '2942299', 'name': 'A. Teich'}, {'authorId': '73705554', 'name': 'H. Averch'}, {'authorId': '150904887', 'name': 'Anne L. Hiskes'}, {'authorId': '70432039', 'name': 'Richard P. Hiskes'}, {'authorId': '97795525', 'name': 'E. Wenk'}]",1986.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,,34.0
How Much do People Remember? Some Estimates of the Quantity of Learned Information in Long-Term Memory,f39fcc3a36d6813721bc1c8254f9a2b292d12901,"[{'authorId': '1836606', 'name': 'T. Landauer'}]",1986.0,Cognitive Sciences,"['Superintelligence: Paths, Dangers, Strategies']",1,How much information from experience does a normal adult remember? The “functional informotion content” of humon memory wos estimated in several ways. The methods depend on measured rotes of input and loss from very longterm memory and on onolyses of the informotionol demands of human memotybosed performance. Estimates ranged around 10’ bits. It is speculated that the flexible ond creative retrieval of facts by humans is a function of a large ratio of “hardware” capacity to functional storage requirements.,208.0
Manhattan: The Army and the Atomic Bomb,686a441b1881a627cb040af019bc25d3765ff430,"[{'authorId': '72736259', 'name': 'V. C. Jones'}]",1985.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,,44.0
Robots and Empire,6d6f1db564df8564eebf94cc400fdf01e372abcd,"[{'authorId': '12636907', 'name': 'I. Asimov'}]",1985.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"Isaac Asmiov's classic novel about the decline and fall of Solaria. Gladia Delmarre's homeworld, the Spacer planet Solaria, has been abandoned - by its human population. Countless robots remain there. And when traders from Settler worlds attempt to salvage them, the robots of Solaria turn to killing...in defiance of the Three Laws of Robotics. Pax Robotica Long ago, Gladia's robots Daneel and Giskard played a vital role in opening the worlds beyond the Solar system to Settlers from Earth. Now the conscience-stricken robots are faced with an even greater challenge. Either the sacred Three Laws of Robotics are in ruins - or a new, superior Law must be established to bring peace to the galaxy. With Madam Gladia and D.G. Baley - the captain of the Settler traders and a descendant of the robots' friend Elijah Baley - Daneel and Giskard travel to the robot stronghold of Solaria...where they uncover a sinister Spacer plot to destroy Earth itself.",78.0
Reasons and Persons,5ff1bc29cc526dfcfcb345327c3bf34ccc48d6c2,"[{'authorId': '115796161', 'name': 'D. Parfit'}]",1986.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"This book challenges, with several powerful arguments, some of our deepest beliefs about rationality, morality, and personal identity. The author claims that we have a false view of our own nature; that it is often rational to act against our own best interests; that most of us have moral views that are directly self-defeating; and that, when we consider future generations the conclusions will often be disturbing. He concludes that moral non-religious moral philosophy is a young subject, with a promising but unpredictable future.",3812.0
Strategies of containment: a critical appraisal of postwar American national security policy,1f98e9bee8336d4a2a9f2bf98bc1fac43816cd7c,"[{'authorId': '104569933', 'name': 'R. Garson'}]",1984.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,,209.0
The anthropic principle and its implications for biological evolution,635e0b02a23583a3067cc8058ac1050c30bf001d,"[{'authorId': '143680248', 'name': 'B. Carter'}]",1983.0,Philosophical transactions of the Royal Society of London. Series A: Mathematical and physical sciences,"['Superintelligence: Paths, Dangers, Strategies']",1,"In the form in which it was originally expounded, the anthropic principle was presented as a warning to astrophysical and cosmological theorists of the risk of error in the interpretation of astronomical and cosmological information unless due account is taken of the biological restraints under which the information was acquired. However, the converse message is also valid: biological theorists also run the risk of error in the interpretation of the evolutionary record unless they take due heed of the astrophysical restraints under which evolution took place. After an introductory discussion of the ordinary (‘weak’) anthropic principle and of its more contestable (‘strong’) analogue, a new application of the former to the problem of the evolution of terrestrial life is presented. It is shown that the evidence suggests that the evolutionary chain included at least one but probably not more than two links that were highly improbable (a priori) in the available time interval.",309.0
EURISKO: A Program That Learns New Heuristics and Domain Concepts,0e19bdb1dba273b920e2b0c70d5d40fe9996a67a,"[{'authorId': '1704635', 'name': 'D. Lenat'}]",1983.0,Artificial Intelligence,"['Superintelligence: Paths, Dangers, Strategies']",1,,274.0
ELIZA — a computer program for the study of natural language communication between man and machine,de42b848775f9fa1e4bff758ae04a54099c0c381,"[{'authorId': '2215426', 'name': 'J. Weizenbaum'}]",1983.0,CACM,"['Superintelligence: Paths, Dangers, Strategies']",1,"ELIZA is a program operating within the MAC time-sharing system of MIT which makes certain kinds of natural language conversation between man and computer possible. Input sentences are analyzed on the basis of decomposition rules which are triggered by key words appearing in the input text. Responses are generated by reassembly rules associated with selected decomposition rules. The fundamental technical problems with which ELIZA is concerned are: (1) the identification of key words, (2) the discovery of minimal context, (3) the choice of appropriate transformations, (4) generation of responses in the absence of key words, and (5) the provision of an editing capability for ELIZA “scripts”. A discussion of some psychological issues relevant to the ELIZA approach as well as of future developments concludes the paper.",1616.0
Connectionist Models and Their Properties,0c8e33b624df1ef0da3eb6d9011ce710b5aa275b,"[{'authorId': '2165973', 'name': 'J. Feldman'}, {'authorId': '1691804', 'name': 'D. Ballard'}]",1982.0,Cognitive Sciences,"['Superintelligence: Paths, Dangers, Strategies']",1,"Much of the progress in the fields constituting cognitive science has been based upon the use of explicit information processing models, almost exclusively patterned after conventional serial computers. An extension of these ideas to massively parallel, connectionist models appears to offer a number of advantages. After a preliminary discussion, this paper introduces a general connectionist model and considers how it might be used in cognitive science. Among the issues addressed are: stability and noise-sensitivity, distributed decision-making, time and sequence problems, and the representation of complex concepts.",1527.0
Isaac Asimov: The Foundations of Science Fiction,9cdfc6f8083cf526732cd05f59c114adf1edf6af,"[{'authorId': '144544194', 'name': 'J. Gunn'}]",1982.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"Updates and expands science fiction scholar James Gunn's definitive, Hugo Award-winning critical volume about Isaac Asimov and his contributions to the science fiction genre.",12.0
Learning program helps win national fleet wargame tournament,e8e3da2acb0d4aa199d2528a305e7007eba41dba,"[{'authorId': '1704635', 'name': 'D. Lenat'}]",1982.0,SGAR,"['Superintelligence: Paths, Dangers, Strategies']",1,"During the month of June 1981, the EURISKO program was set the task of exploring the design of naval fleets conforming to a body of (several hundreds of) rules and constraints as set forward in Traveller: The Trillion Credit Squadron. EURISKO designed a fleet of ships suitable for entry in the 1981 Origins national wargame tournament, held at Dunfey's Hotel, in San Mateo, Ca., over July 4 weekend. The Traveller tournament, run by Game Designers Workshop (based in Normal, Illinois), was single elimination, six rounds. EURISKO's fleet won that tournament, thereby becoming the ranking player in the United States (and also an honorary Admiral in the Traveller navy). This win is made more significant by the fact that the program's creator, Professor Douglas Lenat of Stanford University's Heuristic Programming Project, had never played this game before, nor any miniatures battle game of this type.",3.0
Machines Who Think: A Personal Inquiry Into the History and Prospects of Artificial Intelligence.,c6b064f62f4ab1141aa5afe9b7b4c7d2300c5229,"[{'authorId': '49901168', 'name': 'G. Mandler'}]",1981.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,,47.0
Positive reinforcement produced by noradrenergic stimulation of the hypothalamus in rats,a2936d8b66702ae15f571ee19acbbc1a7acca1e4,"[{'authorId': '2411634', 'name': 'J. Cytawa'}, {'authorId': '6407067', 'name': 'E. Jurkowlaniec'}, {'authorId': '1924788', 'name': 'J. Bialowas'}]",1980.0,Physiology and Behavior,"['Superintelligence: Paths, Dangers, Strategies']",1,,11.0
Backgammon Computer Program Beats World Champion,3d0ca13fc00fe82c189764cf7c022f1131153bf0,"[{'authorId': '16759604', 'name': 'H. Berliner'}]",1980.0,Artificial Intelligence,"['Superintelligence: Paths, Dangers, Strategies']",1,,95.0
Theories of Revolution: The Third Generation,cbc56b16cb464dbc1e7a496626160dba65901b87,"[{'authorId': '48789243', 'name': 'J. Goldstone'}]",1980.0,World Politics,"['Superintelligence: Paths, Dangers, Strategies']",1,"The work of Ted Robert Gurr, Chalmers Johnson, Neil Smelser, Samuel P. Huntington, and Charles Tilly has dominated the recent study of revolutions. However, Jeffrey Paige, Ellen Kay Trimberger, S. N. Eisenstadt, and Theda Skocpol have lately produced theories of revolution that are far better grounded historically than those in earlier works. Five major points were neglected by earlier theorists: (1) the variable goals and structures of states; (2) the systematic intrusion of international pressures on the domestic political and economic organization of societies; (3) the structure of peasant communities; (4) the coherence or weakness of the armed forces; and (5) the variables affecting elite behavior. Starting from these points, Paige, Trimberger, Eisenstadt, and Skocpol have produced analyses of the causes and outcomes of a variety of revolutions. Yet significant challenges to the theory of revolutions—such as extending the range of cases analyzed, clarifying the grounds of peasant behavior, and tying theoretical analysis to demographic data—still remain.",169.0
Obstacle avoidance and navigation in the real world by a seeing robot rover,93b376bd451db8ed94a18c556da16f25a3e7961b,"[{'authorId': '2066894', 'name': 'Hans P. Moravec'}]",1980.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"Abstract : The Stanford AI Lab cart is a card-table sized mobile robot controlled remotely through a radio link, and equipped with a TV camera and transmitter. A computer has been programmed to drive the cart through cluttered indoor and outdoor spaces, gaining its knowledge of the world entirely from images broadcast by the onboard TV system. The cart uses several kinds of stereo to locate objects around it in 3D and to deduce its own motion. It plans an obstacle avoiding path to a desired destination on the basis of a model built with this information. The plan changes as the cart perceives new obstacles on its journey. The system is reliable for short runs, but slow. The cart moves one meter every ten to fifteen minutes, in lurches. After rolling a meter it stops, takes some pictures and thinks about them for a long time. Then it plans a new path, executes a little of it, and pauses again. The program has successfully driven the cart through several 20 meter indoor courses (each taking about five hours) complex enough to necessitate three or four avoiding swerves. A less successful outdoor run, in which the cart skirted two obstacles but collided with a third, was also done. Harsh lighting (very bright surfaces next to very dark shadows) giving poor pictures and movement of shadows during the cart's creeping progress were major reasons for the poorer outdoor performance. The action portions of these runs were filmed by computer controlled cameras. (Author)",1089.0
Backgammon program beats world champ,d0131ef855d501c7c201c5f33a8f73ef55d9c449,"[{'authorId': '16759604', 'name': 'H. Berliner'}]",1980.0,SGAR,"['Superintelligence: Paths, Dangers, Strategies']",1,"On July 15,1979 in Monte Carlo world history was made as a computer program for the first time beat a recognized world champion at his own game. BKG 9.8, authored by Hans Berliner of the Computer Science Department, Carnegie-Mellon University, Pittsburgh, 15213, beat Luigi Villa, the newly crowned Backgammon World Champion in a seven point match 7-1. It should be noted that during the match both opponents palyed well; however, Villa played clearly better. However, as an examination of the game records below will confirm, on this occasion BKG 9.8 was the beneficiary of the element of luck in the game of backgammon, and this accounts for the lop-sided score.",6.0
"Applied Optimal Control: Optimization, Estimation, and Control",864e4a227bbd097a4155b17d956b9fd1f5f8981b,"[{'authorId': '5721376', 'name': 'A. Bryson'}, {'authorId': '145452404', 'name': 'Y. Ho'}, {'authorId': '1703072', 'name': 'G. Siouris'}]",1979.0,"IEEE Transactions on Systems, Man and Cybernetics","['Superintelligence: Paths, Dangers, Strategies']",1,"This best-selling text focuses on the analysis and design of complicated dynamics systems. CHOICE called it ""a high-level, concise book that could well be used as a reference by engineers, applied mathematicians, and undergraduates. The format is good, the presentation clear, the diagrams instructive, the examples and problems helpful...References and a multiple-choice examination are included.""",3072.0
Group selections among laboratory populations of Tribolium.,cd00a10b32185068da538a983619094eaac95e17,"[{'authorId': '5399888', 'name': 'M. Wade'}]",1976.0,Proceedings of the National Academy of Sciences of the United States of America,"['Superintelligence: Paths, Dangers, Strategies']",1,"Selection at the population level or group selection is defined as genetic change that is brought about or maintained by the differential extinction and/or proliferation of populations. Group selection for both increased and decreased adult population size was carried out among laboratory populations of Tribolium castaneum at 37-day intervals. The effect of individual selection within populations on adult population size was evaluated in an additional control series of populations. The response in the group selection treatments occurred rapidly, within three or four generations, and was large in magnitude, at times differing from the controls by over 200%. This response to selection at the populational level occurred despite strong individual selection which caused a decline in the mean size of the control populations from over 200 adults to near 50 adults in nine 37-day intervals. ""Assay"" experiments indicated that selective changes in fecundity, developmental time, body weight, and cannibalism rates were responsible in part for the observed treatment differences in adult population size. These findings have implications in terms of speciation in organisms whose range is composed of many partially isolated local populations.",194.0
The pharynx of Caenorhabditis elegans.,ffbd8e58c19d184f76d4bc502a7709b98b265a18,"[{'authorId': '1720093', 'name': 'D. Albertson'}, {'authorId': '144083582', 'name': 'J. Thomson'}]",1976.0,"Philosophical transactions of the Royal Society of London. Series B, Biological sciences","['Superintelligence: Paths, Dangers, Strategies']",1,"The anatomy of the pharynx of Caenorhabditis elegans has been reconstructed from electron micrographs of serial sections. The pharynx is used for pumping food into the gut, and is composed of 34 muscle cells, 9 marginal cells, 9 epithelial cells, 5 gland cells and 20 neurones. Three regions of specialization in the cuticle lining of the pharyngeal lumen may aid in the accumulation of food particles. A basement membrane isolates the pharynx from the rest of the animal, making the pharyngeal nervous system a nearly self-contained unit which is composed primarily of five classes of motor neurones and six classes of interneurones. Three other classes have also been described, which by their morphology appear to be neurosecretory and motor, motor and interneuronal, and lastly one pair that only innervates three of the marginal cells. Some classes of neurone have free endings just under the cuticle lining the lumen of the pharynx, suggesting that these are mechano- or proprio-receptive endings. The connectivity of these neurones has been described at the level of individual synaptic regions, and after combining this information with video taped observations of the pharynx pumping, some interpretations of how these neurones function have been offered.",651.0
Fundamentals of Error-Correcting Codes,f9af90839147654d7cffab025b35a3e652ee0ac9,"[{'authorId': '144075943', 'name': 'W. C. Huffman'}, {'authorId': '3001725', 'name': 'V. Pless'}]",1975.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,Preface 1. Basic concepts of linear codes 2. Bounds on size of codes 3. Finite fields 4. Cyclic codes 5. BCH and Reed-Soloman codes 6. Duadic codes 7. Weight distributions 8. Designs 9. Self-dual codes 10. Some favourite self-dual codes 11. Covering radius and cosets 12. Codes over Z4 13. Codes from algebraic geometry 14. Convolutional codes 15. Soft decision and iterative decoding Bibliography Index.,2122.0
THE COLONIZATION OF SPACE,a8cd1551efb870d965d60193cfef24044b1263d7,"[{'authorId': '2070994800', 'name': 'G. Oneill'}]",1974.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"New ideas are controversial when they challenge orthodoxy, but orthodoxy changes with time, often surprisingly fast. It is orthodox, for example, to believe that Earth is the only practical habitat for Man, and that the human race is close to its ultimate size limits. But I believe we have now reached the point where we can, if we so choose, build new habitats far more comfortable, productive and attractive than is most of Earth.",197.0
Artificial Intelligence,b886f2c097b635ee9550ca29fff7dcbbb7727ff7,"[{'authorId': '144497046', 'name': 'N. Nilsson'}]",1974.0,IFIP Congress,"['Superintelligence: Paths, Dangers, Strategies']",1,"This paper is a survey of Artifici'al Intelligence (AI). It divides the field into four cor~ topics (embodying the base fo·r a science of intelligence) and eight applications topics (in which research has been contributing to core ideas).. The paper discusses the history, the major landmarks, and some of the controversies in each of these twelve topics. Each topic is represented by a chart citing the major references. These references are contained in an extensive bibliography. The paper concludes with a discussion of some of the criticisms of 'AI and with some predictions about the course of future research.",6525.0
Understanding natural language,bb20f121c979b535bbeade5ac06676d627d4ad7d,"[{'authorId': '1699245', 'name': 'T. Winograd'}]",1974.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"Abstract This paper describes a computer system for understanding English. The system answers questions, executes commands, and accepts information in an interactive English dialog. It is based on the belief that in modeling language understanding, we must deal in an integrated way with all of the aspects of language—syntax, semantics, and inference. The system contains a parser, a recognition grammar of English, programs for semantic analysis, and a general problem solving system. We assume that a computer cannot deal reasonably with language unless it can understand the subject it is discussing. Therefore, the program is given a detailed model of a particular domain. In addition, the system has a simple model of its own mentality. It can remember and discuss its plans and actions as well as carrying them out. It enters into a dialog with a person, responding to English sentences with actions and English replies, asking for clarification when its heuristic programs cannot understand a sentence through the use of syntactic, semantic, contextual, and physical knowledge. Knowledge in the system is represented in the form of procedures, rather than tables of rules or lists of patterns. By developing special procedural representations for syntax, semantics, and inference, we gain flexibility and power. Since each piece of knowledge can be a procedure, it can call directly on any other piece of knowledge in the system.",2533.0
The web of belief,57f1d9744b841944e9d7dd67001d88e706037319,"[{'authorId': '145260350', 'name': 'W. Quine'}, {'authorId': '2224178', 'name': 'J. Ullian'}]",1970.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"Introduction Belief and Change of Belief Observation Self-Evidence Testimony Hypothesis Induction, Analogy, and Intuition Confirmation and Refutation Explanation Persuasion and Evaluation Suggested Readings Glossary Index",528.0
Time Without Change,f32c5b6ca6d36f5376ea61f0127c34159d434142,"[{'authorId': '47037440', 'name': 'S. Shoemaker'}]",1969.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,,137.0
Handbook of Textile Fibres,3a1348dc4138ae411382631ac9f584c484a6cd9a,"[{'authorId': '2115139954', 'name': 'J. Cook'}]",1968.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,,196.0
The Making of a Scientist,b82835f51bdc39963ae8bc8e4aea10889ca1184e,"[{'authorId': '1820034', 'name': 'H. Krebs'}]",1967.0,Nature,"['Superintelligence: Paths, Dangers, Strategies']",1,,295.0
Information Processing,e4b8569a11dc29e82952c72cdb296bc2a6a75588,"[{'authorId': '37151900', 'name': 'S. Harris'}]",1977.0,Nature,"['Superintelligence: Paths, Dangers, Strategies']",1,,565.0
The shape of automation for men and management,b1ea523e555f484e5c44f893c0f9df0c25e64bea,"[{'authorId': '94059053', 'name': 'H. Simon'}]",1965.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"THIS report is the second in a series dealing with the employment problems of older workers, a topic which is becoming increasingly important as new technological advances carry in their wake inevitable problems of manpower reallocation. Traditional notions about the age limits for training, and the techniques appropriate for the development of industrial skills require serious review if we are to make the most economical use of our human resources. The aim of this report is to dispel some of the mistaken preconceptions which often appear in discussions concerning adult training.",332.0
Natural Language Input for a Computer Problem Solving System,9fc77941297522cc420ce9292193dd04ed2ed1af,"[{'authorId': '1753394', 'name': 'D. Bobrow'}]",1964.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,,353.0
John Von Neumann,87cc5b343c681ac071ae1ae4847c2a8df6979a2d,"[{'authorId': '92781029', 'name': 'W. Penney'}]",1964.0,Nature,"['Superintelligence: Paths, Dangers, Strategies']",1,,39.0
A Heuristic Program that Solves Symbolic Integration Problems in Freshman Calculus,a9388c1101f83f4a882aa6bf7aa96ae9cab1900e,"[{'authorId': '1930474', 'name': 'J. Slagle'}]",1963.0,JACM,"['Superintelligence: Paths, Dangers, Strategies']",1,"A large high-speed general-purpose digital computer (IBM 7090) was programmed to solve elementary symbolic integration problems at approximately the level of a good college freshman. The program is called SAINT, an acronym for ""Symbolic Automatic INTegrator."" This paper discusses the SAINT program and its performance. SAINT performs indefinite integration. I t also performs definite and multiple integration when these are trivial extensions of indefinite integration. I t uses many of the methods and heuristics of students attacking the same problems. SAINT took an average of two minutes each to solve 52 of the 54 attempted problems taken from the Massachusetts Institute of Technology freshman calculus final examinations. Based on this and other experiments with SAINT, some conclusions concerning computer solution of such problems are: (1) Pattern recognition is of fundamental importance. (2) Great benefit would have been derived from a larger memory and more convenient symbol manipulating facilities. (3) The solution of a symbolic integration problem by a commercially available computer is far cheaper and faster than by man.",186.0
The Structure of Scientific Revolutions,0fc425a8004830fdd7f207efd4fa7a2331d56d3f,"[{'authorId': '46341118', 'name': 'T. Kuhn'}, {'authorId': '2067794782', 'name': 'David Hawkins'}]",1963.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"A good book may have the power to change the way we see the world, but a great book actually becomes part of our daily consciousness, pervading our thinking to the point that we take it for granted, and we forget how provocative and challenging its ideas once were-and still are. ""The Structure of Scientific Revolutions"" is that kind of book. When it was first published in 1962, it was a landmark event in the history and philosophy of science. And fifty years later, it still has many lessons to teach. With ""The Structure of Scientific Revolutions"", Kuhn challenged long-standing linear notions of scientific progress, arguing that transformative ideas don't arise from the day-to-day, gradual process of experimentation and data accumulation, but that revolutions in science, those breakthrough moments that disrupt accepted thinking and offer unanticipated ideas, occur outside of ""normal science,"" as he called it. Though Kuhn was writing when physics ruled the sciences, his ideas on how scientific revolutions bring order to the anomalies that amass over time in research experiments are still instructive in our biotech age. This new edition of Kuhn's essential work in the history of science includes an insightful introductory essay by Ian Hacking that clarifies terms popularized by Kuhn, including paradigm and incommensurability, and applies Kuhn's ideas to the science of today. Usefully keyed to the separate sections of the book, Hacking's essay provides important background information as well as a contemporary context. Newly designed, with an expanded index, this edition will be eagerly welcomed by the next generation of readers seeking to understand the history of our perspectives on science.",53361.0
The Strategy of Conflict.,e9e0ffb65fb5a0f95e506d01c0eca42bfdd6a0e4,"[{'authorId': '48184297', 'name': 'A. Rapoport'}, {'authorId': '5068532', 'name': 'T. Schelling'}]",1961.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"I. Elements of a Theory of Strategy 1. The Retarded Science of International Strategy 2. An Essay on Bargaining 3. Bargaining, Communication, and Limited War II. A Reorientation of Game Theory 4. Toward a Theory of Interdependent Decision 5. Enforcement, Communication, and Strategic Moves 6. Game Theory and Experimental Research III. Strategy with a Random Ingredient 7. Randomization of Promises and Threats 8. The Threat That Leaves Something to Chance IV. Surprise Attack: A Study in Mutual Distrust 9. The Reciprocal Fear of Surprise Attack 10. Surprise Attack and Disarmament Appendices A. Nuclear Weapons and Limited War B. For the Abandonment of Symmetry in Game Theory C. Re-interpretation of a Solution Concept for ""Noncooperative"" Games Index",9513.0
"Fact, Fiction, and Forecast",67187948c6da7e78215efce46dcd1435faca9de7,"[{'authorId': '50481099', 'name': 'N. Goodman'}]",1955.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"liELsoN Goodman's second book,1 which represents?excepting the first chapter, a reprint of the well-known paper, ""The Prob lem of Counterfactual Conditionals""?the ""Special Lectures in Philosophy"" he delivered at the University of London in 1953, is small in volume but rich in content. There is a clear thread run ning through its four chapters: counterf actual conditionals (which in recent years have given such headaches to several ana lytic philosophers) speak about possible non-actual 2 events. But Goodman cannot accept talk about possible events or entities in good philosophical conscience; according to his ""actualist"" onto logy, as we might call it, there are no possibilities, just as for a nominalist there are no classes and other abstract entities. To put it in the formal mode of speech: he wants to find a way of trans lating all modal talk into a non-modal language. Guided by this motivation, he proceeds to examine the distinction between ""law like"" and ""accidental"" statements and, after an impressive display of analytical acumen, comes to the conclusion that the solution of this problem of explication is one with the solution of the problem of induction: what is the criterion of reasonableness of an inductive inference? How are warranted predictions to be dis tinguished from unwarranted ones? There seem to me, however, to be serious flaws in Goodman's intricate chain of argument, which I wish to examine in some detail.",1560.0
Computing Machinery and Intelligence,2d5673caa9e6af3a7b82a43f19ee920992db07ad,"[{'authorId': '2262347', 'name': 'A. Turing'}]",1950.0,The Philosophy of Artificial Intelligence,"['Superintelligence: Paths, Dangers, Strategies']",1,"I propose to consider the question, “Can machines think?”♣ This should begin with definitions of the meaning of the terms “machine” and “think”. The definitions might be framed so as to reflect so far as possible the normal use of the words, but this attitude is dangerous. If the meaning of the words “machine” and “think” are to be found by examining how they are commonly used it is difficult to escape the conclusion that the meaning and the answer to the question, “Can machines think?” is to be sought in a statistical survey such as a Gallup poll.",8887.0
Young Children,692a1b4effb08cd1882656eda1efa3d308b2efb0,"[{'authorId': '46504894', 'name': 'J. Towner'}, {'authorId': '69340804', 'name': 'Theodore A. Mork'}, {'authorId': '1665221144', 'name': 'Young Children'}]",1949.0,Nature,"['Superintelligence: Paths, Dangers, Strategies']",1,,284.0
The Nature of the Firm,39f0e78225386d7da0ec2f0308792c15c5cab23e,"[{'authorId': '69060284', 'name': 'R. Coase'}]",1937.0,,"['Superintelligence: Paths, Dangers, Strategies']",1,"Economic theory has suffered in the past from a failure to state clearly its assumptions. Economists in building up a theory have often omitted to examine the foundations on which it was erected. This examination is, however, essential not only to prevent the misunderstanding and needless controversy which arise from a lack of knowledge of the assumptions on which a theory is based, but also because of the extreme importance for economics of good judgement in choosing between rival sets of assumptions. For instance, it is suggested that the use of the word “firm” in economics may be different from the use of the term by the “plain man.”1 Since there is apparently a trend in economic theory towards starting analysis with the individual firm and not with the industry,2 it is all the more necessary not only that a clear definition of the word “firm” should be given but that its difference from a firm in the “real world,” if it exists, should be made clear. Mrs. Robinson has said that “the two questions to be asked of a set of assumptions in economics are: Are they tractable? and: Do they correspond with the real world?”3 Though, as Mrs. Robinson points out, “more often one set will be manageable and the other realistic,” yet there may well be branches of theory where assumptions may be both manageable and realistic. It is hoped to show in the following paper that a definition of a firm may be obtained which is not only realistic in that it corresponds to what is meant by a firm in the real world, but is tractable by two of the most powerful instruments of economic analysis developed by Marshall, the idea of the margin and that of substitution, together giving the idea of substitution at the margin.",15823.0
Life 3.0: Being Human in the Age of Artificial Intelligence,64fda6dc4dfe08f515439c4f255c9766c7487880,"[{'authorId': '2011933', 'externalIds': {'DBLP': ['Max Tegmark']}, 'url': 'https://www.semanticscholar.org/author/2011933', 'name': 'Max Tegmark', 'aliases': ['M Tegmark', 'M. Tegmark'], 'affiliations': [], 'homepage': None, 'paperCount': 274, 'citationCount': 28979, 'hIndex': 79}]",2017.0,,['Life 3.0: Being Human in the Age of Artificial Intelligence'],1,"New York Times Best Seller How will Artificial Intelligence affect crime, war, justice, jobs, society and our very sense of being human? The rise of AI has the potential to transform our future more than any other technologyand theres nobody better qualified or situated to explore that future than Max Tegmark, an MIT professor whos helped mainstream research on how to keep AI beneficial. How can we grow our prosperity through automation without leaving people lacking income or purpose? What career advice should we give todays kids? How can we make future AI systems more robust, so that they do what we want without crashing, malfunctioning or getting hacked? Should we fear an arms race in lethal autonomous weapons? Will machines eventually outsmart us at all tasks, replacing humans on the job market and perhaps altogether? Will AI help life flourish like never before or give us more power than we can handle? What sort of future do you want? This book empowers you to join what may be the most important conversation of our time. It doesnt shy away from the full range of viewpoints or from the most controversial issuesfrom superintelligence to meaning, consciousness and the ultimate physical limits on life in the cosmos.",173.0
Algorithmic Research on Exploring Neural Networks with Activation Atlases,0cd07060c3598f671a3ad20ea3571d6110059671,"[{'authorId': '2194875297', 'name': '慧慧 周'}]",2022.0,Software Engineering and Applications,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,27.0
Four Futures: Life After Capitalism,749e1c9a835e3be1b320fb46ecafebfe34c40ca7,"[{'authorId': '13277637', 'name': 'Adam Szetela'}]",2020.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,45.0
Social media and entrepreneurship research: A literature review,c545002182cef29163bfd48d580af83f2cc2598b,"[{'authorId': '143782711', 'name': 'A. Olanrewaju'}, {'authorId': '144267722', 'name': 'Mohammad Alamgir Hossain'}, {'authorId': '51460818', 'name': 'Naomi Whiteside'}, {'authorId': '2792523', 'name': 'P. Mercieca'}]",2020.0,International Journal of Information Management,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,187.0
HUMAN + MACHINE: REIMAGINING WORK IN THE AGE OF AI,488f4bed57ac98e03826e46faff8093f6bdd4b75,"[{'authorId': '12066635', 'name': 'Paul R. Daugherty'}, {'authorId': '2167073479', 'name': 'H.'}, {'authorId': '2168626680', 'name': 'James Wilson'}]",2020.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"Technology advances are making tech more...human. This changes everything you thought you knew about innovation and strategy. In their groundbreaking book Human + Machine, Accenture technology leaders Paul Daugherty and H. James Wilson showed how leading organizations use the power of humanmachine collaboration to transform their processes and their bottom lines. Now, as AI continues to rapidly impact both life and work, those companies and other pioneers across industries are tipping the balance even more strikingly toward the human with technology-led strategy that is reshaping the very nature of innovation. In Radically Human, Daugherty and Wilson show this profound shift, fast-forwarded by the pandemic, toward more human—and more humane—technology. Artificial intelligence is becoming less artificial and more intelligent. Instead of data-hungry approaches to AI, innovators are pursuing dataefficient approaches that enable machines to learn as humans do. Instead of replacing workers with machines, they are unleashing human expertise to create human-centered AI. In place of lumbering legacy IT systems, they are building cloud-first IT architectures able to continuously adapt to a world of billions of connected devices. And they are pursuing strategies that will take their place alongside classic winning business formulas like disruptive innovation. These against-the-grain approaches to the basic building blocks of business—Intelligence, Data, Experience, Architecture, and Strategy (IDEAS)—are transforming competition. Industrial giants and startups alike are drawing on this radically human IDEAS framework to create new business models, optimize postpandemic approaches to work and talent, rebuild trust with their stakeholders, and show the way toward a sustainable future. With compelling insights and fresh examples from a variety of industries, Radically Human will forever change the way you think about, practice, and win with innovation. Artificial intelligence threatens to disrupt the professions as it has manufacturing. Frank Pasquale argues that law and policy can avert this outcome and promote better ones: instead of replacing humans, technology can make our labor more valuable. Through regulation, we can ensure that AI promotes inclusive prosperity. Two statistics professors describe how intelligent machines are changing the world and use stories, rather than equations, to explain the mathematical language they use and provide a better grasp on concepts in data and probability. AI is radically transforming business. Are you ready? Look around you. Artificial intelligence is no longer just a futuristic notion. It's here right now--in software that senses what we need, supply chains that ""think"" in real time, and robots that respond to changes in their environment. Twenty-first-century pioneer companies are already using AI to innovate and grow fast. The bottom line is this: Businesses that understand how to harness AI can surge ahead. Those that",183.0
What to Expect From Artificial Intelligence in Business,ac0d253c90f4d79343be44dd945d04aa78b0c436,"[{'authorId': '113411033', 'name': 'Peter Verhezen'}]",2020.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"We are increasingly living in a digital world, where companies attempt to adapt to a new context of Industry 4.0. The authors believe that artificial intelligence and the use of logarithms will alter the game of competition. Digitization is moving our economy away from “financial capitalism” to “data capitalism,” and companies and their boards need to adopt the way they operate and steer the organization to new ecosystems where personalized service becomes part of the new digital strategy. Basically, it is not a battle of AI versus humans, but rather finding a way to enhance the collaboration of AI and humans in organizations. Despite the enormous potential benefits of AI, boards should not ignore the darker side of AI, namely the potential biasedness and sometimes unfairness of algorithms and privacy concerns and the ubiquitous cyberthreats. This is why proper data governance at the board level is needed. The authors suggest that this becomes a critical success factor to be addressed at boards, either as part of the risk management or strategic committee or as a separated digitization committee.",87.0
The Risk of Machine Learning Bias (And How to Prevent It),51ff493e223bdedf0ceb48860a36c6beec1c5802,"[{'authorId': '2080868963', 'name': 'Chris DeBrusk'}, {'authorId': '20668468', 'name': 'O. Wyman'}]",2020.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,24.0
"Artificial intelligence for decision making in the era of Big Data - evolution, challenges and research agenda",0574a3abc98f0e6bd035a4ff4ea107cfba45d3d4,"[{'authorId': '144521858', 'name': 'Y. Duan'}, {'authorId': '143698898', 'name': 'J. Edwards'}, {'authorId': '1724490', 'name': 'Yogesh Kumar Dwivedi'}]",2019.0,International Journal of Information Management,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,725.0
Ethics of Artificial Intelligence,fe68fdd7af4e27d42fc336833fced0e217dca255,"[{'authorId': '46197839', 'name': 'J. Horvat'}]",2019.0,Research Library Issues,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"Summary There are many ethical questions relating the issue of developing an intelligent system. There is strong and increasing pressure to raise capabilities of the artificial intelligence at least to the human levelled intelligence as the ultimate goal. This essay describes possible paths of development of the artificial intelligence. It is discussed how this changes will affect our society and challenges that humanity will have to face. Principles, guideways and modern viewpoints are presented and confirmed with the statements of the renowned scientists and experts in the field of the artificial intelligence ethics.",384.0
Digital business ecosystem: Literature review and a framework for future research,cae5a25e0f1a6b7808620ca7a9d981029df61d33,"[{'authorId': '3425015', 'name': 'P. K. Senyo'}, {'authorId': '40938681', 'name': 'Kecheng Liu'}, {'authorId': '2475947', 'name': 'J. Effah'}]",2019.0,International Journal of Information Management,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,161.0
The Impact of Investments in New Digital Technologies on Wages – Worker-Level Evidence from Germany,174bda7db004c551c4891221edee845bde1f50cf,"[{'authorId': '1661110393', 'name': 'Genz Sabrina'}, {'authorId': '1661110399', 'name': 'Janser Markus'}, {'authorId': '1661110397', 'name': 'Lehmer Florian'}]",2019.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"The strong rise of digitalization, automation, machine learning and other related new digital technologies has led to an intense debate about their societal impacts. The transitions of occupations and the effects on labor demand and workers’ wages are still open questions. Research projects dealing with this issue often face a lack of data on the usage of new digital technologies. This paper uses a novel linked employer–employee data set that contains detailed information on establishments’ technological upgrading between 2011 and 2016, a recent period of rapid technological progress. Furthermore, we are the first to develop a digital tools index based on the German expert database BERUFENET. The new index contains detailed information on the work equipment that is used by workers. Hence, we observe the degree of digitalization on both the establishment level and the worker level. The data allow us to investigate the impact of technology investments on the wage growth of employees within establishments. Overall, the results from individual level fixed effects estimates suggest that investments in new digital technologies at the establishment level positively affect the wages of the establishments’ workers. Sector-specific results show that investments in new digital technologies increase wages in knowledge intensive production establishments and non-knowledge intensive services. The wage growth effects of employees in digital pioneer establishments relative to the specific reference group of workers in digital latecomer establishments are most pronounced for low- and medium-skilled workers.",5.0
The Impact of Investments in New Digital Technologies on Wages – Worker-Level Evidence from Germany,0b3bf6b6b0645de9be9a7032313ead358722aec8,"[{'authorId': '134716380', 'name': 'Sabrina Genz'}, {'authorId': '115917130', 'name': 'Markus Janser'}, {'authorId': '104778440', 'name': 'Florian Lehmer'}]",2019.0,Jahrbücher für Nationalökonomie und Statistik,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"Abstract The strong rise of digitalization, automation, machine learning and other related new digital technologies has led to an intense debate about their societal impacts. The transitions of occupations and the effects on labor demand and workers’ wages are still open questions. Research projects dealing with this issue often face a lack of data on the usage of new digital technologies. This paper uses a novel linked employer–employee data set that contains detailed information on establishments’ technological upgrading between 2011 and 2016, a recent period of rapid technological progress. Furthermore, we are the first to develop a digital tools index based on the German expert database BERUFENET. The new index contains detailed information on the work equipment that is used by workers. Hence, we observe the degree of digitalization on both the establishment level and the worker level. The data allow us to investigate the impact of technology investments on the wage growth of employees within establishments. Overall, the results from individual level fixed effects estimates suggest that investments in new digital technologies at the establishment level positively affect the wages of the establishments’ workers. Sector-specific results show that investments in new digital technologies increase wages in knowledge intensive production establishments and non-knowledge intensive services. The wage growth effects of employees in digital pioneer establishments relative to the specific reference group of workers in digital latecomer establishments are most pronounced for low- and medium-skilled workers.",10.0
"The battle of Brain vs. Heart: A literature review and meta-analysis of ""hedonic motivation"" use in UTAUT2",00515bc1e2ec197fcab078b0d507f24a2b5e3a98,"[{'authorId': '2191954', 'name': 'K. Tamilmani'}, {'authorId': '1688149', 'name': 'N. Rana'}, {'authorId': '96700071', 'name': 'Naveena Prakasam'}, {'authorId': '1724490', 'name': 'Yogesh Kumar Dwivedi'}]",2019.0,International Journal of Information Management,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,151.0
"Unpacking knowledge management practices in China: do institution, national and organizational culture matter?",7dcf0717f4b1389cbb969ed7b4f8647c7c9892f9,"[{'authorId': '94754590', 'name': 'Yi Liu'}, {'authorId': '77594358', 'name': 'Christopher C. A. Chan'}, {'authorId': '2000561995', 'name': 'Chenhui Zhao'}, {'authorId': '2152504512', 'name': 'Chao Liu'}]",2019.0,Journal of Knowledge Management,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"
Purpose
This study aims to empirically examine knowledge management practices in China with the purpose to provide a holistic view regarding the current status of knowledge management at both national and organizational levels.


Design/methodology/approach
Using a survey method, this study collected primary data from organizations across several regions in China. The data were analyzed to detect possible relationships among institutional force, organizational culture and knowledge management process in Chinese organizations. More specifically, to what extent are these relationships moderated by national culture?


Findings
While knowledge management practices in China were partly influenced by institutional forces, most of the predicted connections between organizational culture and knowledge management were supported. In addition, the dynamic nature of national culture is predominant, that pervasively influencing knowledge management processes and thus contextualization determines how knowledge is being managed in China. Indeed, the ideologies of relationships and trust are key vehicles for knowledge management in the Chinese organizations.


Practical implications
This study comprehensively reviews existing literature to form an integrative framework, which is under explored in a Chinese context. Such initiative helps scholars and practitioners to gain a full understanding of knowledge management, in general, in the Chinese business environment in particular.


Originality/value
This paper provides a detailed and empirical insight into the knowledge management practices in Chinese organizations and suggests that knowledge management in a distinctive and yet diverse cultural context should be considered with caution.
",46.0
Artificial intelligence‐augmented ECG assessment: The promise and the challenge,056482fefbabf85e62de9e3d8336514b766d4b22,"[{'authorId': '3581600', 'name': 'K. Anderson'}]",2019.0,Cardiovascular Electrophysiology,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"Since Einthoven developed the electrocardiogram (ECG) investigators and clinicians have sought to expand the diagnostic potential of this convenient, widely available and relatively inexpensive noninvasive examination [1]. In this issue of the Journal of Cardiovascular Electrophysiology, Attia and colleagues demonstrate the ability of a novel algorithm based on the standard 12 lead ECG developed with deep learning artificial intelligence (AI) to detect patients with reduced left ventricular ejection fraction (rLVEF) [2]. This article is protected by copyright. All rights reserved.",6.0
Hello world: how to be human in the age of the machine,07aeea9db2a0eacf506f8eef58424ff4df3647a6,"[{'authorId': '48583395', 'name': 'H. Fry'}]",2019.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,55.0
Mapping the challenges of Artificial Intelligence in the public sector: Evidence from public healthcare,1913f41dc43f657edc37574583cf50a01940bed8,"[{'authorId': '47278248', 'name': 'Tara Qian Sun'}, {'authorId': '2575165', 'name': 'R. Medaglia'}]",2019.0,Government Information Quarterly,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,273.0
The rise of motivational information systems: A review of gamification research,4fe996c924516e8fdbb3ab3e212206e676b4a535,"[{'authorId': '2791914', 'name': 'Jonna Koivisto'}, {'authorId': '2095817', 'name': 'Juho Hamari'}]",2019.0,International Journal of Information Management,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,655.0
A human-centric perspective exploring the readiness towards smart warehousing: The case of a large retail distribution warehouse,b27b3a08313b4fa373765d02276e1f5920ec6e73,"[{'authorId': '79411214', 'name': 'Kamran Mahroof'}]",2019.0,International Journal of Information Management,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,86.0
Machine learning and the physical sciences,a9cbbef8f4426329d0687025b34287c35bdd8b38,"[{'authorId': '50666189', 'name': 'G. Carleo'}, {'authorId': '119628938', 'name': 'I. Cirac'}, {'authorId': '11638962', 'name': 'K. Cranmer'}, {'authorId': '1742040', 'name': 'L. Daudet'}, {'authorId': '3048564', 'name': 'M. Schuld'}, {'authorId': '1777660', 'name': 'Naftali Tishby'}, {'authorId': '1445956232', 'name': 'Leslie Vogt-Maranto'}, {'authorId': '2065813820', 'name': ""Lenka Zdeborov'a""}]",2019.0,Reviews of Modern Physics,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"Machine learning (ML) encompasses a broad range of algorithms and modeling tools used for a vast array of data processing tasks, which has entered most scientific disciplines in recent years. This article reviews in a selective way the recent research on the interface between machine learning and the physical sciences. This includes conceptual developments in ML motivated by physical insights, applications of machine learning techniques to several domains in physics, and cross fertilization between the two fields. After giving a basic notion of machine learning methods and principles, examples are described of how statistical physics is used to understand methods in ML. This review then describes applications of ML methods in particle physics and cosmology, quantum many-body physics, quantum computing, and chemical and material physics. Research and development into novel computing architectures aimed at accelerating ML are also highlighted. Each of the sections describe recent successes as well as domain-specific methodology and challenges.",969.0
"Five simultaneous artificial intelligence data challenges on ultrasound, CT, and MRI.",d2f4e9cdcba280f46bc03d16da1dade225876493,"[{'authorId': '2174276', 'name': 'N. Lassau'}, {'authorId': '69330195', 'name': 'T. Estienne'}, {'authorId': '87947975', 'name': 'P. de Vomecourt'}, {'authorId': '2047475642', 'name': 'M. Azoulay'}, {'authorId': '2476613', 'name': 'John Cagnol'}, {'authorId': '2072697508', 'name': 'G. Garcia'}, {'authorId': '1904627636', 'name': 'M. Majer'}, {'authorId': '87638366', 'name': 'E. Jehanno'}, {'authorId': '1397596481', 'name': 'R. Renard-Penna'}, {'authorId': '2069802324', 'name': 'C. Balleyguier'}, {'authorId': '145696044', 'name': 'F. Bidault'}, {'authorId': '38890511', 'name': 'C. Caramella'}, {'authorId': '1397935094', 'name': 'T. Jacques'}, {'authorId': '4256697', 'name': 'F. Dubrulle'}, {'authorId': '143716864', 'name': 'J. Behr'}, {'authorId': '8140974', 'name': 'N. Poussange'}, {'authorId': '145357572', 'name': 'J. Bocquet'}, {'authorId': '87742793', 'name': 'S. Montagne'}, {'authorId': '144698794', 'name': 'F. Cornelis'}, {'authorId': '77801673', 'name': 'M. Faruch'}, {'authorId': '2079931154', 'name': 'B. Bresson'}, {'authorId': '143884038', 'name': 'S. Brunelle'}, {'authorId': '2078862991', 'name': 'A. Jalaguier-Coudray'}, {'authorId': '8429814', 'name': 'N. Amoretti'}, {'authorId': '47790819', 'name': 'A. Blum'}, {'authorId': '15004944', 'name': 'A. Paisant'}, {'authorId': '2044782772', 'name': 'V. Herreros'}, {'authorId': '2130488', 'name': 'O. Rouvière'}, {'authorId': '1401690545', 'name': 'S. Si-Mohamed'}, {'authorId': '32937001', 'name': 'L. Di Marco'}, {'authorId': '6774311', 'name': 'O. Hauger'}, {'authorId': '7650601', 'name': 'M. Garetier'}, {'authorId': '152337768', 'name': 'F. Pigneur'}, {'authorId': '15324365', 'name': 'A. Bergère'}, {'authorId': '2069183913', 'name': 'C. Cyteval'}, {'authorId': '40442671', 'name': 'L. Fournier'}, {'authorId': '2276841', 'name': 'C. Malhaire'}, {'authorId': '4828980', 'name': 'J. Drapé'}, {'authorId': '3807948', 'name': 'E. Poncelet'}, {'authorId': '2074442118', 'name': 'C. Bordonné'}, {'authorId': '87788630', 'name': 'H. Cauliez'}, {'authorId': '5353592', 'name': 'J. Budzik'}, {'authorId': '86988781', 'name': 'M. Boisserie'}, {'authorId': '47230221', 'name': 'T. Willaume'}, {'authorId': '6627454', 'name': 'S. Molière'}, {'authorId': '88347513', 'name': 'N. Peyron Faure'}, {'authorId': '87066302', 'name': 'S. Caius Giurca'}, {'authorId': '144315094', 'name': 'V. Juhan'}, {'authorId': '14145316', 'name': 'T. Caramella'}, {'authorId': '35158308', 'name': 'A. Perrey'}, {'authorId': '40557212', 'name': 'F. Desmots'}, {'authorId': '1435622133', 'name': 'M. Faivre-Pierre'}, {'authorId': '2047136682', 'name': 'M. Abitbol'}, {'authorId': '40895249', 'name': 'R. Lotte'}, {'authorId': '13442977', 'name': 'D. Istrati'}, {'authorId': '4588818', 'name': 'D. Guenoun'}, {'authorId': '144732598', 'name': 'A. Luciani'}, {'authorId': '3623015', 'name': 'M. Zins'}, {'authorId': '115253298', 'name': 'J. Meder'}, {'authorId': '4053753', 'name': 'A. Cotten'}]",2019.0,Diagnostic and Interventional Imaging,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,30.0
Combining the Power of Artificial Intelligence with the Richness of Healthcare Claims Data: Opportunities and Challenges,e19cbab9fb23d5170e65bda39926dd5da2d1c138,"[{'authorId': '81628901', 'name': 'D. Thesmar'}, {'authorId': '79810970', 'name': 'David Sraer'}, {'authorId': '2065306742', 'name': 'L. Pinheiro'}, {'authorId': '82264100', 'name': 'N. Dadson'}, {'authorId': '1834614', 'name': 'Razvan Veliche'}, {'authorId': '14853468', 'name': 'P. Greenberg'}]",2019.0,PharmacoEconomics (Auckland),"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,28.0
Artificial Intelligence Hits the Barrier of Meaning,aa8577451427108bc6f063c3d607cef5e476cfb2,"[{'authorId': '144380037', 'name': 'Melanie Mitchell'}]",2019.0,Inf.,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"Today’s AI systems sorely lack the essence of human intelligence: Understanding the situations we experience, being able to grasp their meaning. The lack of humanlike understanding in machines is underscored by recent studies demonstrating lack of robustness of state-of-the-art deep-learning systems. Deeper networks and larger datasets alone are not likely to unlock AI’s “barrier of meaning”; instead the field will need to embrace its original roots as an interdisciplinary science of intelligence.",36.0
A Call to Action: Moving Forward with the Governance of Artificial Intelligence in Canada,454f37e7ea78f420933ba91da10be16766b958ed,"[{'authorId': '101823544', 'name': 'A. Gaon'}, {'authorId': '119982924', 'name': 'I. Stedman'}]",2019.0,Alberta Law Review,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"The Government of Canada has committed to accelerating the growth of the country’s world-class artificial intelligence (AI) sector. This emerging technology has the potential to impact nearly every segment of Canada’s economy, including national security, health care, and government services. To prepare for the key challenges and opportunities that AI will give rise to, we offer an innovative governance model for Canadian governments to adopt. This model recognizes the uncertainty ahead and prioritizes oversight and accountability while also encouraging a flexible policy-first approach. This approach fosters responsible AI innovation and supports Canada’s emergence as a leader in AI technology and governance.",14.0
Next generation smart manufacturing and service systems using big data analytics,f03abd3616a3778bbf6d8d3ca6b65cca32874347,"[{'authorId': '34287537', 'name': 'N. Shukla'}, {'authorId': '1900756', 'name': 'M. Tiwari'}, {'authorId': '1700598', 'name': 'G. Beydoun'}]",2019.0,Computers & industrial engineering,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,40.0
Industry 4.0: A bibliometric analysis and detailed overview,8fe2dd8d12f5f4864dbee7cc65365b48a55917c6,"[{'authorId': '2851339', 'name': 'Pranab K. Muhuri'}, {'authorId': '39461779', 'name': 'Amit K. Shukla'}, {'authorId': '145731499', 'name': 'A. Abraham'}]",2019.0,Engineering applications of artificial intelligence,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,289.0
A Domain-Oriented Analysis of the Impact of Machine Learning - The Case of Retailing,ec0f99e3ca067494a91bb0aeeba47aa7b2d7d3ca,"[{'authorId': '2057569557', 'name': 'Felix Weber'}, {'authorId': '2091011278', 'name': 'Reinhard Schütte'}]",2019.0,Big Data and Cognitive Computing,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"Information technologies in general and artifical intelligence (AI) in particular try to shift operational task away from a human actor. Machine learning (ML) is a discipline within AI that deals with learning improvement based on data. Subsequently, retailing and wholesaling, which are known for their high proportion of human work and at the same time low profit margins, can be regarded as a natural fit for the application of AI and ML tools. This article examines the current prevalence of the use of machine learning in the industry. The paper uses two disparate approaches to identify the scientific and practical state-of-the-art within the domain: a literature review on the major scientific databases and an empirical study of the 10 largest international retail companies and their adoption of ML technologies in the domain are combined with each other. This text does not present a prototype using machine learning techniques. Instead of a consideration and comparison of the particular algorythms and approaches, the underling problems and operational tasks that are elementary for the specific domain are identified. Based on a comprehensive literature review the main problem types that ML can serve, and the associated ML techniques, are evaluated. An empirical study of the 10 largest retail companies and their ML adoption shows that the practical market adoption is highly variable. The pioneers have extensively integrated applications into everyday business, while others only show a small set of early prototypes. However, some others show neither active use nor efforts to apply such a technology. Following this, a structured approach is taken to analyze the value-adding core processes of retail companies. The current scientific and practical application scenarios and possibilities are illustrated in detail. In summary, there are numerous possible applications in all areas. In particular, in areas where future forecasts and predictions are needed (like marketing or replenishment), the use of ML today is both scientifically and practically highly developed.",25.0
"Translating cancer genomics into precision medicine with artificial intelligence: applications, challenges and future perspectives",00ecb66af391138002070b271ce7f8e6b4d51964,"[{'authorId': '2110562007', 'name': 'Jia Xu'}, {'authorId': '48220410', 'name': 'Pengwei Yang'}, {'authorId': '2057237159', 'name': 'Shang Xue'}, {'authorId': '2082454521', 'name': 'Bhuvan Sharma'}, {'authorId': '1398032853', 'name': 'M. Sanchez-Martin'}, {'authorId': '40351392', 'name': 'F. Wang'}, {'authorId': '2603897', 'name': 'K. Beaty'}, {'authorId': '2808340', 'name': 'E. Dehan'}, {'authorId': '2060339350', 'name': 'Baiju Parikh'}]",2019.0,Human Genetics,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,114.0
"Logged out: Ownership, exclusion and public value in the digital data and information commons",3e859a363400cbb6fed51da377172d965cd6a174,"[{'authorId': '6707207', 'name': 'B. Prainsack'}]",2019.0,Big Data & Society,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"In recent years, critical scholarship has drawn attention to increasing power differentials between corporations that use data and people whose data is used. A growing number of scholars see digital data and information commons as a way to counteract this asymmetry. In this paper I raise two concerns with this argument: First, because digital data and information can be in more than one place at once, governance models for physical common-pool resources cannot be easily transposed to digital commons. Second, not all data and information commons are suitable to address power differentials. In order to create digital commons that effectively address power asymmetries we must pay more systematic attention to the issue of exclusion from digital data and information commons. Why and how digital data and information commons exclude, and what the consequences of such exclusion are, decide whether commons can change power asymmetries or whether they are more likely to perpetuate them.",55.0
"New ethical challenges of digital technologies, machine learning and artificial intelligence in public health: a call for papers",7d7d7868c3c04c873956016a4246b02ba174a0b3,"[{'authorId': '41065825', 'name': 'D. Zandi'}, {'authorId': '145942054', 'name': 'A. Reis'}, {'authorId': '145104974', 'name': 'E. Vayena'}, {'authorId': '13141317', 'name': 'K. Goodman'}]",2019.0,Bulletin of the World Health Organization,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,17.0
Machine Learning as a universal tool for quantitative investigations of phase transitions,99c0747350e0aae4080a49510149ed9e864bc36d,"[{'authorId': '152680673', 'name': 'C. Giannetti'}, {'authorId': '3094343', 'name': 'B. Lucini'}, {'authorId': '102454053', 'name': 'Davide Vadacchino'}]",2018.0,Nuclear Physics B,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,35.0
"Exploring expanding expertise: artificial intelligence as an existential threat and the role of prestigious commentators, 2014–2018",313502eb47a03e508d65a71cbfdbe62633466203,"[{'authorId': '26898704', 'name': 'Vassilis Galanos'}]",2018.0,Technology Analysis & Strategic Management,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"ABSTRACT The post-2010 technological resurgence in artificial intelligence (AI) was followed by a series of public warnings by prestigious intellectuals, scientists, and entrepreneurs presenting AI as an existential threat. While their expertise in other fields is undeniable, their knowledge of AI remains questionable. With expert studies as a theoretical point of departure, the empirical data collected narrate the events which systematically shaped this view between 2014 and 2018. This chronology captures the interplay between such statements by ‘expanding experts’ in the press and traces their impact on governmental policy documents in the EU, UK, and US, while highlighting the overall absence of experts in these debates. The conclusions recommend the inclusion of AI experts in relevant policymaking schemes and the further exploration of the networks between such prestigious individuals, the purposes and origins of emerging future/risk studies institutions, and their impact on AI R&D in the long run via empirical means.",21.0
Human Rights and Artificial Intelligence: An Urgently Needed Agenda,b26ed64288420e368d9783d797330be951cc567e,"[{'authorId': '33804827', 'name': 'Mathias Risse'}]",2018.0,Human Rights Quarterly,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"ABSTRACT:The increasing presence of artificial intelligence creates enormous challenges for human rights. Among the short-term challenges are ways in which technology engages just about all rights on the UDHR, as exemplified through use of effectively discriminatory algorithms. Medium-term challenges include changes in the nature of work that could call into question many people’s status as participants in society. In the long-term humans may have to live with machines that are intellectually and possibly morally superior, even though this is highly speculative. Artificial intelligence also gives a new relevance to moral debates that used to strike many as arcane.",45.0
Explanation in Artificial Intelligence: Insights from the Social Sciences,e89dfa306723e8ef031765e9c44e5f6f94fd8fda,"[{'authorId': '144658641', 'name': 'Tim Miller'}]",2017.0,Artificial Intelligence,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,2573.0
Cronfa - Swansea University Open Access Repository,8415eef21d53eb9f1836d15bcf0c23ad090f53ab,"[{'authorId': '153314894', 'name': 'M. Chen'}]",2009.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"Based on a critical review of the Unified Theory of Acceptance and Use of Technology (UTAUT), this study first formalized an alternative theoretical model for explaining the acceptance and use of information system (IS) and information technology (IT) innovations. The revised theoretical model was then empirically examined using a combination of meta-analysis and structural equation modelling (MASEM) techniques. The meta-analysis was based on 1600 observations on 21 relationships coded from 162 prior studies on IS/ IT acceptance and use. The SEM analysis showed that attitude: was central to behavioural intentions and usage behaviours, partially mediated the effects of exogenous constructs on behavioural intentions, and had a direct influence on usage behaviours. A number of implications for theory and practice are derived based on the findings.",2.0
"Blockchain research, practice and policy: Applications, benefits, limitations, emerging research themes and research agenda",bdac5dc5d501189b8910232c32ef2a8b298a4f99,"[{'authorId': '2098810803', 'name': 'D. L. Hughes'}, {'authorId': '1724490', 'name': 'Yogesh Kumar Dwivedi'}, {'authorId': '47033159', 'name': 'S. Misra'}, {'authorId': '1688149', 'name': 'N. Rana'}, {'authorId': '35098701', 'name': 'Vishnupriya Raghavan'}, {'authorId': '1404314413', 'name': 'Viswanadh Akella'}]",2019.0,International Journal of Information Management,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,418.0
ARTIFICIAL INTELLIGENCE IN CONSTRUCTION INDUSTRY: LEGAL ISSUES AND REGULATORY CHALLENGES,443fbee667a0a713b2486cf66cd356b8ab2707f5,"[{'authorId': '2064239126', 'name': 'Dr. Rehana Parveen'}]",2019.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"Artificial intelligence is rapidly developing and is transforming the world around us and providing unparalleled opportunities in many sectors including construction industry. Artificial Intelligence, Automation and Robotics are the recent technological advances that are likely to have a significant effect on the construction industry. In the era of technology artificial intelligence is developing at a speed faster than ever in many different fields. No doubt, new technology presents exiting opportunities but it also presents lots of uncertainty and challenges in the areas of the law. It is essential to be aware of the legal risks and issues to make informed decisions when considering the use of artificial intelligence in various industries. Artificial Intelligence requires the immediate attention of regulators due to the challenges they present to existing legal frameworks and the new legal and ethical questions they raise. This paper discusses major regulatory dilemmas in the field of Artificial Intelligence in construction industry: how to keep up with these technological advances: how to strike a balance between innovation and the protection of individual rights. Due consideration should be given to the legal issues arising from the use of artificial intelligence (AI) and autonomous vehicles on sites. There is currently a lack of legislation governing the use and development of Artificial Intelligence and autonomous vehicles in construction industries. All these issues and legal challenges require the attention of regulators and law makers that must be addressed. Keyword: Artificial Industries, construction industry, regulation, regulatory dilemmas, technology regulation, smart regulation, Automation. Cite this Article: Dr.Rehana Parveen, Artificial Intelligence in Construction Industry: Legal Issues and Regulatory Challenges, International Journal of Civil Engineering and Technology, 9(13), 2018, pp. 957-962 http://www.iaeme.com/IJCIET/issues.asp?JType=IJCIET&VType=9&IType=13",11.0
Using AI to Personalise Emotionally Appealing Advertisement,f83746e7e3d5a7076ad40b76008c03e5e97f0111,"[{'authorId': '52563419', 'name': 'Emmanuel Mogaji'}, {'authorId': '30809265', 'name': 'S. Olaleye'}, {'authorId': '8303460', 'name': 'D. Ukpabi'}]",2019.0,Digital and Social Media Marketing,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,19.0
"Siri, Siri, in my hand: Who’s the fairest in the land? On the interpretations, illustrations, and implications of artificial intelligence",da75921084a1fe49c43023fe5a2b15cb2b9a36e8,"[{'authorId': '35064842', 'name': 'A. Kaplan'}, {'authorId': '2852308', 'name': 'M. Haenlein'}]",2019.0,Business Horizons,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,911.0
Smart cities: Advances in research - An information systems perspective,41c6ead3158db6e1e7675716bf35f3fbe06572b8,"[{'authorId': '3446873', 'name': 'Elvira Ismagilova'}, {'authorId': '2098810803', 'name': 'D. L. Hughes'}, {'authorId': '1724490', 'name': 'Yogesh Kumar Dwivedi'}, {'authorId': '33916686', 'name': 'K. Raman'}]",2019.0,International Journal of Information Management,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,409.0
Using privacy calculus theory to explore entrepreneurial directions in mobile location-based advertising: Identifying intrusiveness as the critical risk factor,4ada42d4f6f21fa92b3c731155d1495f0d8c9fea,"[{'authorId': '119030920', 'name': 'A. Gutierrez'}, {'authorId': '1404459005', 'name': 'S. O’Leary'}, {'authorId': '1688149', 'name': 'N. Rana'}, {'authorId': '1724490', 'name': 'Yogesh Kumar Dwivedi'}, {'authorId': '52183748', 'name': 'Tatiana Calle'}]",2019.0,Computers in Human Behavior,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,100.0
Artificial Intelligence and Black-Box Medical Decisions: Accuracy versus Explainability.,ddb451cb57f2ed71004fdc218e0b23cc90b8233c,"[{'authorId': '2329084', 'name': 'A. London'}]",2019.0,The Hastings center report,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"Although decision-making algorithms are not new to medicine, the availability of vast stores of medical data, gains in computing power, and breakthroughs in machine learning are accelerating the pace of their development, expanding the range of questions they can address, and increasing their predictive power. In many cases, however, the most powerful machine learning techniques purchase diagnostic or predictive accuracy at the expense of our ability to access ""the knowledge within the machine."" Without an explanation in terms of reasons or a rationale for particular decisions in individual cases, some commentators regard ceding medical decision-making to black box systems as contravening the profound moral responsibilities of clinicians. I argue, however, that opaque decisions are more common in medicine than critics realize. Moreover, as Aristotle noted over two millennia ago, when our knowledge of causal systems is incomplete and precarious-as it often is in medicine-the ability to explain how results are produced can be less important than the ability to produce such results and empirically verify their accuracy.",288.0
The HeartMath coherence model: implications and challenges for artificial intelligence and robotics,b8f900d719d1171e00890df4ba96ad264d523e26,"[{'authorId': '39610698', 'name': 'S. Edwards'}]",2019.0,Ai & Society,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,9.0
Artificial Intelligence in Organizations: New Opportunities for Phenomenon-Based Theorizing,9083485d05b00e72377a6d2ea3ddbc6a97c746c1,"[{'authorId': '8354543', 'name': 'G. Krogh'}]",2018.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,128.0
Artificial Intelligence and the Limitations of Information,04b9b7a968095202cc9eaca09ab5e639e3658900,"[{'authorId': '2066256739', 'name': 'Paul Walton'}]",2018.0,Inf.,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"Artificial intelligence (AI) and machine learning promise to make major changes to the relationship of people and organizations with technology and information. However, as with any form of information processing, they are subject to the limitations of information linked to the way in which information evolves in information ecosystems. These limitations are caused by the combinatorial challenges associated with information processing, and by the tradeoffs driven by selection pressures. Analysis of the limitations explains some current difficulties with AI and machine learning and identifies the principles required to resolve the limitations when implementing AI and machine learning in organizations. Applying the same type of analysis to artificial general intelligence (AGI) highlights some key theoretical difficulties and gives some indications about the challenges of resolving them.",9.0
Artificial Intelligence and the Biofield: New Opportunities and Challenges,8476a1352ae949ab7b0429b989ebf1ca20d473d3,"[{'authorId': '2768080', 'name': 'B. Rubik'}, {'authorId': '32749460', 'name': 'H. Jabs'}]",2018.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"There is an organizing field of energy intimately connected with each person, the human biofield, which holds information central to a higher order of being. It has been proposed as having mind-like properties as super-regulator of the biochemistry and physiology of the organism, coordinating all life functions, promoting homeodynamics, and key to understanding life's integral wholeness. Although brainwaves and heart waves are well characterized and clinically useful, the biofield has not yet been mapped.  Artificial intelligence (AI) is essential to handle the data processing from biofield mapping of a large database of humans to elucidate the electromagnetic fields, acoustic fields, and subtle energy field components of human life.   Moreover, AI could monitor health and well-being through the biofield via a variety of sensors and indicate on a daily basis which lifestyle choices would improve the biofield and enhance well-being.  AI could also be programmed to manipulate the biofield to directly enhance well-being.  Once the biofield is decoded, then direct communication between humans and AI through the biofield would be possible. Thus, a number of positive applications of AI to the biofield to enhance human well-being are possible. Nonetheless, the presence of a biofield around humans presents a dilemma for AI robots, which would not possess a biofield other than the electromagnetic properties of their electronic components.  So, even though robots may well exceed humans in certain cognitive tasks, robots would not possess a biofield, emotions, or an interior experience.  Although they may be able to emulate emotions with certain facial expressions and vocal patterns, they may always be distinguished from humans as lacking the complex dynamic biofield of human beings that reflects the living state. Normal 0 false false false EN-US X-NONE X-NONE /* Style Definitions */ 
 table.MsoNormalTable 
 {mso-style-name:""Table Normal""; 
 mso-tstyle-rowband-size:0; 
 mso-tstyle-colband-size:0; 
 mso-style-noshow:yes; 
 mso-style-priority:99; 
 mso-style-parent:""""; 
 mso-padding-alt:0cm 5.4pt 0cm 5.4pt; 
 mso-para-margin:0cm; 
 mso-para-margin-bottom:.0001pt; 
 mso-pagination:widow-orphan; 
 font-size:10.0pt; 
 font-family:""Times New Roman"",serif; 
 mso-ansi-language:EN-US; 
 mso-fareast-language:EN-US;} 
 Normal 0 false false false EN-US X-NONE X-NONE /* Style Definitions */ 
 table.MsoNormalTable 
 {mso-style-name:""Table Normal""; 
 mso-tstyle-rowband-size:0; 
 mso-tstyle-colband-size:0; 
 mso-style-noshow:yes; 
 mso-style-priority:99; 
 mso-style-parent:""""; 
 mso-padding-alt:0cm 5.4pt 0cm 5.4pt; 
 mso-para-margin:0cm; 
 mso-para-margin-bottom:.0001pt; 
 mso-pagination:widow-orphan; 
 font-size:10.0pt; 
 font-family:""Times New Roman"",serif; 
 mso-ansi-language:EN-US; 
 mso-fareast-language:EN-US;}",4.0
The impact of knowledge management processes on information systems: A systematic review,da6be2aa18b5926e119aaee97675667f05e865ae,"[{'authorId': '1399262006', 'name': 'M. Al-Emran'}, {'authorId': '3049066', 'name': 'V. Mezhuyev'}, {'authorId': '9304813', 'name': 'Adzhar Kamaludin'}, {'authorId': '40241708', 'name': 'K. Shaalan'}]",2018.0,International Journal of Information Management,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,129.0
Designing Out Stereotypes in Artificial Intelligence: Involving users in the personality design of a digital assistant,1aceac0d4c0e714a56114cf05a44bf6e1b996f23,"[{'authorId': '66781543', 'name': 'Jak Spencer'}, {'authorId': '145065049', 'name': 'J. Poggi'}, {'authorId': '2874770', 'name': 'R. Gheerawo'}]",2018.0,International Conference on Smart Objects and Technologies for Social Good,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"Rapid recent advancements in new technologies and artificial intelligence has led to huge developments in science, technology, medicine and engineering over the last few decades. Recently, developments in the field have tried to create machines that go beyond solving rational, logic based challenges to those that mimic human interaction and in particular emotional intelligence. One of the most mainstream applications of this technology is in digital assistants; tools designed to achieve a result using natural language.
 In this paper, we present the findings from a short design research project with a major global accounting firm that aimed to design the personality of a digital assistant. We will present how design methods such as people centred and inclusive design can be used as a tool to build a non stereotypical digital character.",12.0
Radiology and artificial intelligence: An opportunity for our specialty.,bf8f463582eda0083c65986774da8a0595b97d25,"[{'authorId': '144781656', 'name': 'J. Beregi'}, {'authorId': '3623015', 'name': 'M. Zins'}, {'authorId': '144330410', 'name': 'J. Masson'}, {'authorId': '11305316', 'name': 'P. Cart'}, {'authorId': '145728539', 'name': 'J. Bartoli'}, {'authorId': '50541886', 'name': 'B. Silberman'}, {'authorId': '2464681', 'name': 'F. Boudghène'}, {'authorId': '115253298', 'name': 'J. Meder'}]",2018.0,Diagnostic and Interventional Imaging,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,23.0
Demographic and dwelling models by artificial intelligence: urban renewal opportunities in Spanish coast,9daf45e5e4f7507e7d63b9eff7ed83454ef900e8,"[{'authorId': '2083351272', 'name': 'Francisco Javier Abarca-Álvarez'}, {'authorId': '1404781535', 'name': 'F. S. Campos-Sánchez'}, {'authorId': '1405148992', 'name': 'Rafael Reinoso-Bellido'}]",2018.0,International Journal of Sustainable Development and Planning,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"the Spanish Mediterranean coast has undergone intense urban development in recent decades. It has often focused on building a property patrimony based more on real estate, business expectations and consuming resources than on its actual use. Similarly, its functionality and need to adapt to social needs and the requirements of the certain demographic profiles of its time have largely been ignored. the purpose of this study is to shed light on the Spanish Mediterranean coast’s existing residential models and the relationship with the local demographic reality of users. Its aim is to be part of a Decision Support System which focuses on urban regeneration and functional recovery. this study uses heuristic methodologies to demonstrate the coherence of an abundance of open access data. Such methodologies do not necessarily require specific hypotheses or formulations to generate useful knowledge. the 2011 Population and Housing Census (INE) is used as a knowledge source, on which data mining techniques based on Artificial Intelligence techniques are applied. We specifically use Self-Organising Maps (SOM) through Artificial Neural Networks (ANN), subsequently mapping the results through a Geographic Information System (GIS). these techniques permit an exploration of the different residential profiles in this territory. Each profile exposes very different levels of sustainability and resilience, identifying the groups or social collectives that singularly inhabit them, which are at times authentic drivers of the maintenance and growth of these models. to the extent that they are linked to demographic profiles, the knowledge obtained in this study is evidence of the different residential profiles’ territorial location, and highlights the opportunities and weaknesses of urban regeneration.",3.0
Artificial Intelligence Does Not Exist: Lessons from Shared Cognition and the Opposition to the Nature/Nurture Divide,f915b80c83f09a41fcc39e823bf897427d073770,"[{'authorId': '26898704', 'name': 'Vassilis Galanos'}]",2018.0,International Conference on Human Centered Computing,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,6.0
Clinical Implications and Challenges of Artificial Intelligence and Deep Learning.,21412d1d0a8db97fa04e7ff44c65c44f89fa0680,"[{'authorId': '2113153', 'name': 'W. Stead'}]",2018.0,Journal of the American Medical Association (JAMA),"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"Artificial intelligence (AI) and deep learning are entering the mainstream of clinical medicine. For example, in December 2016, Gulshan et al1 reported development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs. An accompanying editorial by Wong and Bressler2 pointed out limits of the study, the need for further validation of the algorithm in different populations, and unresolved challenges (eg, incorporating the algorithm into clinical work flows and convincing clinicians and patients to “trust a ‘black box’”). Sixteen months later, the Food and Drug Administration (FDA)3 permitted marketing of the first medical device to use AI to detect diabetic retinopathy. FDA reduced the risk of releasing the device by limiting the indication for use to screening adults who do not have visual symptoms for greater than mild retinopathy, to refer them to an eye care specialist. This issue of JAMA contains 2 Viewpoints on deep learning in health care. Hinton4 explains the technology underlying AI and deep learning, using clinical examples. AI is the general term for imitating human intelligence with computer systems. Early AI systems represented human reasoning with symbolic logic. As computer processing and storage became more powerful, researchers developed machine-learning techniques to imitate the way the human brain learns. The first machine learning continued to rely on human experts to label the data the system trained on (eg, the diagnosis) and to identify the significant features (eg, findings). Machine learning weighted the features from the data. With continued advances in computational power and with larger data sets, researchers began to develop deep learning techniques. The first deep learning algorithms were “supervised” in that human experts continued to label the training data, and the deep learning algorithms learned the features and weights directly from the data. The retinopathy screening algorithms are an example of supervised deep learning. Hinton4 describes continuing development of new deep learning techniques, including ones that are completely unsupervised. He also points out that it is not feasible to see the features learned by deep learning to explain how the system reaches a conclusion. Naylor5 identifies 7 factors driving adoption of AI and deep learning in health care: (1) the strengths of digital imaging over human interpretation; (2) the digitization of health-related records and data sharing; (3) the adaptability of deep learning to analysis of heterogeneous data sets; (4) the capacity of deep learning for hypothesis generation in research; (5) the promise of deep learning to streamline clinical workflows and empower patients; (6) the rapid-diffusion open-source and proprietary deep learning programs; and (7) of the adequacy of today’s basic deep learning technology to deliver improved performance as data sets get larger. Factors 3, 4, and 6 are specific to deep learning; the other factors apply to other AI techniques as well. Artificial intelligence is a family of technical techniques in the same way the radiologic imaging tool kit includes flat images, computed tomography scans, and functional imaging such as magnetic resonance imaging. Advances in computational technology, computer science, informatics, and statistics improve existing techniques and make new techniques possible. The addition of deep learning to the AI family of techniques represents an advance similar in magnitude to the addition of the computed tomography scanner to the radiology tool kit. Each AI technique has strengths and weaknesses. Symbolic logic is self-explaining but difficult to scale.6 For example, knowledge engineers extract the logic by interviewing or observing human experts. Statistical techniques such as supervised deep learning scale, but are subject to bias in the training data, and the reasoning cannot be explained. Since deep learning systems are trained on data from the past, they are not prepared to reason in the way humans do about conditions that have not been seen before. In the future, unsupervised deep learning may reduce this gap between human intelligence and AI. The potential applications of AI in health care present a range of computational difficulty. Narrow tasks, in which the context is predefined, are relatively easy. Imageprocessing tasks such as recognizing the border of an organ to suggest where to cut off a scan, or highlighting a suspicious area in an image for the radiologist or pathologist, are examples of narrow tasks. Image analysis and diagnostic prediction tasks such as the diabetic retinopathy example are broader and harder, but doable with today’s technology. Very broad data analysis and pattern prediction tasks such as analyzing heterogeneous data sets from diverse sources to suggest novel associations are feasible today because the purpose is limited to hypothesis generation. Thinking in the way humans do—reasoning, for example, from a few observations to suggest a novel scientific framework as Einstein did with the theory of relativity—is beyond technology on the horizon. Clinicians should view the output of AI programs or devices as statistical predictions. They should maintain an index of suspicion that the prediction may be wrong, just as they Viewpoint pages 1099 and 1101 Opinion",148.0
Artifictional Intelligence: Against Humanity's Surrender to Computers,994302bec72db9e020a2b8352f57ee1d470d255f,"[{'authorId': '83595901', 'name': 'H. M. Collins'}]",2018.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"Recent startling successes in machine intelligence using a technique called ‘deep learning’ seem to blur the line between human and machine as never before. Are computers on the cusp of becoming so intelligent that they will render humans obsolete? Harry Collins argues we are getting ahead of ourselves, caught up in images of a fantastical future dreamt up in fictional portrayals. The greater present danger is that we lose sight of the very real limitations of artificial intelligence and readily enslave ourselves to stupid computers: the ‘Surrender’. By dissecting the intricacies of language use and meaning, Collins shows how far we have to go before we cannot distinguish between the social understanding of humans and computers. When the stakes are so high, we need to set the bar higher: to rethink ‘intelligence’ and recognize its inherent social basis. Only if machine learning succeeds on this count can we congratulate ourselves on having produced artificial intelligence.",5.0
"Hello marketing, what can artificial intelligence help you with?",901d3a07d09dfcc22cad212f301156420e942332,"[{'authorId': '2053004098', 'name': 'Norbert Wirth'}]",2018.0,International Journal of Market Research,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"Artificial intelligence based applications are emerging in a broad range of expert domains. News about AI based solutions in medicine, industrial production processes, logistics, mobility and digital marketing trigger discussions and a lot of speculation. The market research industry seems hesitant and at the same time eager to embrace this new technology. In this article the author provides a definition of artificial intelligence and its different forms: narrow AI, hybrid AI and strong AI. He concludes his reflection on the question whether it’s feasible to develop AI based marketing insights solutions with the recommendation: it’s time to embrace AI.",78.0
Artificial intelligence for the public sector: opportunities and challenges of cross-sector collaboration,9c3dc1f34746baf471b031dc0d30078225294795,"[{'authorId': '144923658', 'name': 'Slava Mikhaylov Jankin'}, {'authorId': '37950806', 'name': 'M. Esteve'}, {'authorId': '82664470', 'name': 'Averill Campion'}]",2018.0,"Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences","['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"Public sector organizations are increasingly interested in using data science and artificial intelligence capabilities to deliver policy and generate efficiencies in high-uncertainty environments. The long-term success of data science and artificial intelligence (AI) in the public sector relies on effectively embedding it into delivery solutions for policy implementation. However, governments cannot do this integration of AI into public service delivery on their own. The UK Government Industrial Strategy is clear that delivering on the AI grand challenge requires collaboration between universities and the public and private sectors. This cross-sectoral collaborative approach is the norm in applied AI centres of excellence around the world. Despite their popularity, cross-sector collaborations entail serious management challenges that hinder their success. In this article we discuss the opportunities for and challenges of AI for the public sector. Finally, we propose a series of strategies to successfully manage these cross-sectoral collaborations. This article is part of a discussion meeting issue ‘The growing ubiquity of algorithms in society: implications, impacts and innovations’.",73.0
Weapons of math destruction,e83e2b314192304583162e2de5c4e8299f31a1d3,"[{'authorId': '49225972', 'name': 'Thomas S. Woodson'}]",2018.0,Journal of Responsible Innovation,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,258.0
Big data and business analytics ecosystems: paving the way towards digital transformation and sustainable societies,08f767340bedd7d63523dab0816b0fcc612d1ee2,"[{'authorId': '2423108', 'name': 'I. Pappas'}, {'authorId': '2424691', 'name': 'Patrick Mikalef'}, {'authorId': '1761403', 'name': 'M. Giannakos'}, {'authorId': '1716522', 'name': 'J. Krogstie'}, {'authorId': '3357578', 'name': 'G. Lekakos'}]",2018.0,Inf. Syst. E Bus. Manag.,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,213.0
Outline for a German Strategy for Artificial Intelligence,9acc43731195cd3234f9e198331996161e8b39aa,"[{'authorId': '1697838', 'name': 'D. Harhoff'}, {'authorId': '102169951', 'name': 'Stefan Heumann'}, {'authorId': '2252409', 'name': 'N. Jentzsch'}, {'authorId': '101979490', 'name': 'Philippe Lorenz'}]",2018.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"Numerous countries like China, France and Japan have declared Artificial Intelligence (AI) a key technology and announced comprehensive plans to promote research and development in AI. The German government has also begun to work on an AI strategy and just published a first blueprint with the core themes. In this paper, we argue that such a strategy needs to be broad and comprehensive, focusing on the development of an internationally-competitive AI ecosystem in Germany. 
A strong AI ecosystem is characterized by strong networks between science, economic actors (big companies and startups alike) and society at large. Innovations arise in particular from close exchanges and collaboration between researchers, developers, universities, companies, investors and startups. To promote such an ecosystem, a wide range of different political measures on different levels have to be integrated into a broader, comprehensive strategy. 
This paper discusses the central building blocks of an AI ecosystem in Germany, and offers concrete ideas and recommendations for an AI strategy based on this ecosystem approach. 
1. AI research: Compared to other countries, Germany is lagging behind in research expenditures and must drastically increase them. Research support needs to be open to different technological approaches within AI. It also needs to be more agile to better react to emerging trends and new opportunities in AI research. Better work conditions overall are needed to compete for the best AI talent worldwide as well as clear benchmarks to measure progress in AI research. 
2. Development of AI competencies across society: We do not only need top research. We also need broadly distributed AI competencies in society. Thus AI should not only be taught in computer sciences, but core AI modules should also be integrated into engineering and natural science programs, and be taught at schools of applied sciences. 
3. Data as a basic resource for AI development: A strong AI ecosystem needs data for research and for the development of AI applications in industry, particularly with regard to deep learning (DL). This dimension of the ecosystem needs far more attention in Germany. Possible approaches to mobilize data for AI include the development of data pools and more advanced methods of anonymizing or synthesizing data. It is hard to compete with the big Internet platforms from the United States and China in terms of quantity of data. Instead, special emphasis on machine data, quality of data and alternative approaches to AI that can work with little data could be the cornerstones of an alternative path to a strong AI ecosystem. 4. Infrastructure demands for AI: Deep learning requires not only huge amounts of data but also great computing power. A national AI strategy would address the question of how we can ensure middle- and long-term access to the most powerful processing hardware possible for German AI research and applications. 
5. AI development and AI application in the economy: The German economy and industry already struggles with digitalization. AI exacerbates this issue because it represents the next step of digitalization. Small- and medium-sized businesses in Germany, known as the Mittelstand, especially need support. This support could be, for example, through state-funded AI laboratories, in which companies can experiment with AI with little risk and at low costs. Mobilizing venture capital through public funds and providing better incentives for AI investments represent two more critical challenges. 6. Societal dimension of AI: The ethical and regulatory questions regarding AI need to be openly discussed and require input from many different stakeholders in German society. Here, we already see numerous initiatives and approaches, representing the topic’s arrival on the political agenda. However, more has to be done to make AI competencies and technologies more familiar within society. 
7. A national AI strategy in an international context: Germany can only succeed in the international competition in the long-term as part of an EU-wide approach. Striving for cooperation with France offers the chance to push for a comprehensive European AI strategy. Germany, and Europe as a whole, have to become more conscious of their strategic interests in AI and act accordingly. 
A German AI strategy should focus on the ecosystem approach and propose concrete ideas and recommendations. The strategy also needs to address how we can identify and, if possible, measure how the strategy is being implemented, and how the AI ecosystem in Germany is developing. There are many important indicators that policy makers should consult in order to evaluate the effect of their actions: the attractiveness of German institutes and universities for leading international AI researchers, the number and quality of AI patents, achievements in publishing and visibility at the most important international AI conferences, venture capital investments, the founding of firms, or the number of companies with strong AI competencies and their growth. The good thing is that Germany does not have to start from scratch; numerous countries have already published national AI strategies in which many good ideas can be found. Now is the time for Germany to follow suit. Only then can Germany become a leader of AI development.",12.0
Artificial Intelligence and the Public Sector—Applications and Challenges,522c3f62cb117bb2928b8bcfc95d7430eb749115,"[{'authorId': '2662685', 'name': 'B. Wirtz'}, {'authorId': '107876419', 'name': 'Jan C. Weyerer'}, {'authorId': '120370630', 'name': 'Carolin Geyer'}]",2018.0,International Journal of Public Administration,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"ABSTRACT Advances in artificial intelligence (AI) have attracted great attention from researchers and practitioners and have opened up a broad range of beneficial opportunities for AI usage in the public sector. Against this background, there is an emerging need for a holistic understanding of the range and impact of AI-based applications and associated challenges. However, previous research considers AI applications and challenges only in isolation and fragmentarily. Given the lack of a comprehensive overview of AI-based applications and challenges for the public sector, our conceptual approach analyzes and compiles relevant insights from scientific literature to provide an integrative overview of AI applications and related challenges. Our results suggest 10 AI application areas, describing their value creation and functioning as well as specific public use cases. In addition, we identify four major dimensions of AI challenges. We finally discuss our findings, deriving implications for theory and practice and providing suggestions for future research.",236.0
Data supply chain (DSC): research synthesis and future directions,8ccc0d69fcbc8219f26421bdfec168dcff3321c2,"[{'authorId': '2843165', 'name': 'Konstantina Spanaki'}, {'authorId': '103833744', 'name': 'Zeynep Gürgüç'}, {'authorId': '144118186', 'name': 'R. Adams'}, {'authorId': '50303482', 'name': 'C. Mulligan'}]",2018.0,International Journal of Production Research,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"In the digital economy, the volume, variety and availability of data produced in myriad forms from a diversity of sources has become an important resource for competitive advantage, innovation opportunity as well as source of new management challenges. Building on the theoretical and empirical foundations of the traditional manufacturing Supply Chain (SC), which describes the flow of physical artefacts as raw materials through to consumption, we propose the Data Supply Chain (DSC) along which data are the primary artefact flowing. The purpose of this paper is to outline the characteristics and bring conceptual distinctiveness to the context around DSC as well as to explore the associated and emergent management challenges and innovation opportunities. To achieve this, we adopt the systematic review methodology drawing on the operations management and supply chain literature and, in particular, taking a framework synthetic approach which allows us to build the DSC concept from the pre-existing SC template. We conclude the paper by developing a set of propositions and outlining an agenda for future research that the DSC concept implies.",33.0
Artificial intelligence and the future of work: Human-AI symbiosis in organizational decision making,7ff3f12f27ef8e9e7d754c23b1eeb09b664223bd,"[{'authorId': '34891162', 'name': 'M. H. Jarrahi'}]",2018.0,Business Horizons,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,546.0
Deep Learning and Its Application to LHC Physics,1f135e98e867ffcde5b359e7b817bbe21f80cfce,"[{'authorId': '104284017', 'name': 'D. Guest'}, {'authorId': '11638962', 'name': 'K. Cranmer'}, {'authorId': '104317636', 'name': 'D. Whiteson'}]",2018.0,Annual Review of Nuclear and Particle Science,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"Machine learning has played an important role in the analysis of high-energy physics data for decades. The emergence of deep learning in 2012 allowed for machine learning tools which could adeptly handle higher-dimensional and more complex problems than previously feasible. This review is aimed at the reader who is familiar with high-energy physics but not machine learning. The connections between machine learning and high-energy physics data analysis are explored, followed by an introduction to the core concepts of neural networks, examples of the key results demonstrating the power of deep learning for analysis of LHC data, and discussion of future prospects and concerns.",250.0
Artificial Intelligence and Machine Learning: Opportunities for Radiologists in Training.,7daaab3be6a806d61b83096b5ee235dc61f2db3b,"[{'authorId': '145570580', 'name': 'G. K. Nguyen'}, {'authorId': '35051071', 'name': 'A. Shetty'}]",2018.0,Journal of the American College of Radiology,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,20.0
"Advances in Social Media Research: Past, Present and Future",52dda1cf0b5d88c8dedbd83406fe3bb9abd8144f,"[{'authorId': '37232810', 'name': 'Kawal Kapoor'}, {'authorId': '2191954', 'name': 'K. Tamilmani'}, {'authorId': '1688149', 'name': 'N. Rana'}, {'authorId': '27745084', 'name': 'Pushp P. Patil'}, {'authorId': '1724490', 'name': 'Yogesh Kumar Dwivedi'}, {'authorId': '2290083', 'name': 'S. Nerur'}]",2018.0,Inf. Syst. Frontiers,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,642.0
"‘Negotiating the Algorithm’: Automation, Artificial Intelligence and Labour Protection",7bcc6b1d204aff4a2f0966e025ae9a29cd69b445,"[{'authorId': '2075736391', 'name': 'Valerio De Stefano'}]",2018.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"This paper aims at filling some gaps in the mainstream debate on automation, the introduction of new technologies at the workplace and the future of work. This debate has concentrated, so far, on how many jobs will be lost as a consequence of technological innovation. This paper examines instead issues related to the quality of jobs in future labour markets. It addresses the detrimental effects on workers of awarding legal capacity and rights and obligation to robots. It examines the implications of practices such as People Analytics and the use of big data and artificial intelligence to manage the workforce. It stresses on an oft-neglected feature of the contract of employment, namely the fact that it vests the employer with authority and managerial prerogatives over workers. It points out that a vital function of labour law is to limit these authority and prerogatives to protect the human dignity of workers. In light of this, it argues that even if a Universal Basic Income were introduced, the existence of managerial prerogatives would still warrant the existence of labour regulation since this regulation is about much more than protecting workers’ income. It then highlights the benefits of human- rights based approaches to labour regulation to protect workers’ privacy against invasive electronic monitoring. It concludes by highlighting the crucial role of collective regulation and social partners in governing automation and the impact of technology at the workplace. It stresses that collective dismissal regulation and the involvement of workers’ representatives in managing and preventing job losses is crucial and that collective actors should actively participate in the governance of technology-enhanced management systems, to ensure a vital “human- in-command” approach.",90.0
The Book of Why: The New Science of Cause and Effect,b98ddaf22e53211917c0e84a25c48b61e8c1de2f,"[{'authorId': '145430701', 'name': 'J. Pearl'}, {'authorId': '2082473026', 'name': 'D. Mackenzie'}]",2018.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"A Turing Award-winning computer scientist and statistician shows how understanding causality has revolutionized science and will revolutionize artificial intelligence""Correlation is not causation."" This mantra, chanted by scientists for more than a century, has led to a virtual prohibition on causal talk. Today, that taboo is dead. The causal revolution, instigated by Judea Pearl and his colleagues, has cut through a century of confusion and established causality--the study of cause and effect--on a firm scientific basis. His work explains how we can know easy things, like whether it was rain or a sprinkler that made a sidewalk wet; and how to answer hard questions, like whether a drug cured an illness. Pearl's work enables us to know not just whether one thing causes another: it lets us explore the world that is and the worlds that could have been. It shows us the essence of human thought and key to artificial intelligence. Anyone who wants to understand either needs The Book of Why.",293.0
Artificial Intelligence in Health Care: Brave New World or Golden Opportunity?,8fda57be8e7dd83d49a1e978b2d89c56f9ba3bf6,"[{'authorId': '48398296', 'name': 'Keith Dreyer'}, {'authorId': '40221223', 'name': 'Bibb Allen'}]",2018.0,Journal of the American College of Radiology,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,19.0
"Artificial Intelligence and Machine Learning in Radiology: Opportunities, Challenges, Pitfalls, and Criteria for Success.",e7a24925d166768de9e8b6b97b33c93e03a1f00d,"[{'authorId': '145466601', 'name': 'J. Thrall'}, {'authorId': '47057650', 'name': 'Xiang Li'}, {'authorId': '1762919', 'name': 'Quanzheng Li'}, {'authorId': '123922859', 'name': 'Cinthia Cruz'}, {'authorId': '1788855', 'name': 'Synho Do'}, {'authorId': '26654844', 'name': 'K. Dreyer'}, {'authorId': '144685698', 'name': 'J. Brink'}]",2018.0,Journal of the American College of Radiology,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,377.0
Artificial Intelligence in Service,573e6814c16178186daf537b1e1a5d3c840eef2f,"[{'authorId': '50474058', 'name': 'Ming-Hui Huang'}, {'authorId': '1813939', 'name': 'R. Rust'}]",2018.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"Artificial intelligence (AI) is increasingly reshaping service by performing various tasks, constituting a major source of innovation, yet threatening human jobs. We develop a theory of AI job replacement to address this double-edged impact. The theory specifies four intelligences required for service tasks—mechanical, analytical, intuitive, and empathetic—and lays out the way firms should decide between humans and machines for accomplishing those tasks. AI is developing in a predictable order, with mechanical mostly preceding analytical, analytical mostly preceding intuitive, and intuitive mostly preceding empathetic intelligence. The theory asserts that AI job replacement occurs fundamentally at the task level, rather than the job level, and for “lower” (easier for AI) intelligence tasks first. AI first replaces some of a service job’s tasks, a transition stage seen as augmentation, and then progresses to replace human labor entirely when it has the ability to take over all of a job’s tasks. The progression of AI task replacement from lower to higher intelligences results in predictable shifts over time in the relative importance of the intelligences for service employees. An important implication from our theory is that analytical skills will become less important, as AI takes over more analytical tasks, giving the “softer” intuitive and empathetic skills even more importance for service employees. Eventually, AI will be capable of performing even the intuitive and empathetic tasks, which enables innovative ways of human–machine integration for providing service but also results in a fundamental threat for human employment.",1011.0
Consolidate IoT Edge Computing with Lightweight Virtualization,55435ad7d48a347a94275ebeb8b41fd8ec4c97bc,"[{'authorId': '143802931', 'name': 'Roberto Morabito'}, {'authorId': '2922541', 'name': 'Vittorio Cozzolino'}, {'authorId': '1934437', 'name': 'A. Ding'}, {'authorId': '2543361', 'name': 'N. Beijar'}, {'authorId': '144380603', 'name': 'J. Ott'}]",2018.0,IEEE Network,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"Lightweight virtualization (LV) technologies have refashioned the world of software development by introducing flexibility and new ways of managing and distributing software. Edge computing complements today's powerful centralized data centers with a large number of distributed nodes that provide virtualization close to the data source and end users. This emerging paradigm offers ubiquitous processing capabilities on a wide range of heterogeneous hardware characterized by different processing power and energy availability. The scope of this article is to present an in-depth analysis on the requirements of edge computing from the perspective of three selected use cases that are particularly interesting for harnessing the power of the Internet of Things. We discuss and compare the applicability of two LV technologies, containers and unikernels, as platforms for enabling the scalability, security, and manageability required by such pervasive applications that soon may be part of our everyday lives. To inspire further research, we identify open problems and highlight future directions to serve as a road map for both industry and academia.",205.0
Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification,18858cc936947fc96b5c06bbe3c6c2faa5614540,"[{'authorId': '38222513', 'name': 'Joy Buolamwini'}, {'authorId': '2076288', 'name': 'Timnit Gebru'}]",2018.0,FAT,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"Recent studies demonstrate that machine learning algorithms can discriminate based on classes like race and gender. In this work, we present an approach to evaluate bias present in automated facial analysis algorithms and datasets with respect to phenotypic subgroups. Using the dermatologist approved Fitzpatrick Skin Type classiﬁcation system, we characterize the gender and skin type distribution of two facial analysis benchmarks, IJB-A and Adience. We ﬁnd that these datasets are overwhelm-ingly composed of lighter-skinned subjects (79 . 6% for IJB-A and 86 . 2% for Adience) and introduce a new facial analysis dataset which is balanced by gender and skin type. We evaluate 3 commercial gender classiﬁcation systems using our dataset and show that darker-skinned females are the most misclassiﬁed group (with error rates of up to 34 . 7%). The maximum error rate for lighter-skinned males is 0 . 8%. The substantial disparities in the accuracy of classifying darker females, lighter females, darker males, and lighter males in gender classiﬁcation systems require urgent attention if commercial companies are to build genuinely fair, transparent and accountable facial analysis algorithms.",2624.0
Machine learning action parameters in lattice quantum chromodynamics,d5ab5f15f27ca1e55e7cd25b9664b75678f89e8f,"[{'authorId': '118463703', 'name': 'P. Shanahan'}, {'authorId': '92651362', 'name': 'D. Trewartha'}, {'authorId': '3535702', 'name': 'W. Detmold'}]",2018.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"Numerical lattice quantum chromodynamics studies of the strong interaction are important in many aspects of particle and nuclear physics. Such studies require significant computing resources to undertake. A number of proposed methods promise improved efficiency of lattice calculations, and access to regions of parameter space that are currently computationally intractable, via multi-scale action-matching approaches that necessitate parametric regression of generated lattice datasets. The applicability of machine learning to this regression task is investigated, with deep neural networks found to provide an efficient solution even in cases where approaches such as principal component analysis fail. The high information content and complex symmetries inherent in lattice QCD datasets require custom neural network layers to be introduced and present opportunities for further development.",52.0
Artificial Intelligence and Digital Pathology: Challenges and Opportunities,e4082122602b47d986de04cdbbc533a83291e77e,"[{'authorId': '9315255', 'name': 'H. Tizhoosh'}, {'authorId': '2229705', 'name': 'L. Pantanowitz'}]",2018.0,Journal of Pathology Informatics,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"In light of the recent success of artificial intelligence (AI) in computer vision applications, many researchers and physicians expect that AI would be able to assist in many tasks in digital pathology. Although opportunities are both manifest and tangible, there are clearly many challenges that need to be overcome in order to exploit the AI potentials in computational pathology. In this paper, we strive to provide a realistic account of all challenges and opportunities of adopting AI algorithms in digital pathology from both engineering and pathology perspectives.",261.0
Expert and Non-expert Opinion About Technological Unemployment,2a6b36705107d76491e82dcdd85f4d4e43c48e4f,"[{'authorId': '1733716', 'name': 'T. Walsh'}]",2017.0,International Journal of Automation and Computing,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,83.0
"AI: Augmentation, more so than automation",c22a3834c5ccec04063f75e69b835154c32c86f8,"[{'authorId': '47760885', 'name': 'S. Miller'}]",2018.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,27.0
"Outlook of Cloud, CPS and IoT in Manufacturing",8980cf2c028eede568a1ef7a7c1163f3e0b4f18a,"[{'authorId': '95266142', 'name': 'Lihui Wang'}, {'authorId': '2144796001', 'name': 'Xiangyu Wang'}]",2018.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,3.0
"Building Trust in Artificial Intelligence, Machine Learning, and Robotics",8662297398bfa04fbff3b473c961928bc01b3970,"[{'authorId': '1746885', 'name': 'K. Siau'}, {'authorId': '49337185', 'name': 'Weiyu Wang'}]",2018.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,238.0
"Forecasting the Impact of Artificial Intelligence, Part 3 of 4: The Potential Effects of AI on Businesses, Manufacturing, and Commerce",5afb6ed6dd39bd40fe5fa5d2050d34f115d166c8,"[{'authorId': '2276105', 'name': 'Spyros Makridakis'}]",2018.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"In part 3 of his 4 articles on Forecasting the Impact of Artificial Intelligence, Makdridakis focuses on firms and employment, the areas that may be of the greatest concern presently to mankind. Topics include changes already brought about by AI, contrasts with the industrial revolution, the impact on both developed and developing countries, and the dominant firms in AI now and those predicted to be in 20 years. Copyright International Institute of Forecasters, 2018",10.0
Regional Employment and Artificial Intelligence in Japan,3980a4abddf45cbae9df738afc6cba4993cbaf22,"[{'authorId': '120701158', 'name': 'Hamaguchi Nobuaki'}, {'authorId': '52409342', 'name': 'Kondo Keisuke'}]",2018.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"This study investigates employment risk caused by new technology, such as artificial intelligence (AI) and robotics, using the probability of computerization by Frey and Osborne (2017) and Japanese employment data. The new perspective of this study is the consideration of regional heterogeneity in labor markets due to the uneven geographical distribution of occupations, which is especially observed between male and female workers. This study finds that female workers are exposed to higher risks of computerization than male workers, since they tend to be engaged in occupations with a high probability of computerization. This tendency is more pronounced in larger cities. Our results suggest that supporting additional human capital investment alone is not enough as a risk alleviation strategy against new technology, and policymakers need to address structural labor market issues, such as gender biases for career progression and participation in decision-making positions, in the AI era to mitigate unequal risk of computerization between workers.",7.0
Artificial Intelligence in Health,106e7bcf5c010543d23c82c05b9966ed1ca60a51,[],2018.0,Lecture Notes in Computer Science,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,11.0
Collaborative Intelligence : Humans and AI Are Joining Forces,01ceac73fc71d545063f0ebafbc112c80d29b016,"[{'authorId': '12066635', 'name': 'Paul R. Daugherty'}]",2018.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,272.0
ARTIFICIAL INTELLIGENCE AND THE HUMAN BIOFIELD : NEW OPPORTUNITIES AND CHALLENGES,15973a13ea952460c734efa15c7281d593659f88,"[{'authorId': '2768080', 'name': 'B. Rubik'}, {'authorId': '32749460', 'name': 'H. Jabs'}]",2018.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"There is an organizing field of energy intimately connected with each person, the human biofield, which holds information central to a higher order of being. It has been proposed as having mind-like properties as super-regulator of the biochemistry and physiology of the organism, coordinating all life functions, promoting homeodynamics, and key to understanding life’s integral wholeness. Although brainwaves and heart waves are well characterized and clinically useful, the biofield has not yet been mapped. Artificial intelligence (AI) is essential to handle the data processing from biofield mapping of a large database of humans to elucidate the electromagnetic fields, acoustic fields, and subtle energy field components of human life. Moreover, AI could monitor health and well-being through the biofield via a variety of sensors and indicate on a daily basis which lifestyle choices would improve the biofield and enhance wellbeing. AI could also be programmed to manipulate the biofield to directly enhance well-being. Once the biofield is decoded, then direct communication between humans and AI through the biofield would be possible. Thus, a number of positive applications of AI to the biofield to enhance human well-being are possible. Nonetheless, the presence of a biofield around humans presents a dilemma for AI robots, which would not possess a biofield other than the electromagnetic properties of their electronic 1 Editor’s note: Foundations of Mind, the independent research group that has provided the papers for this special edition, has never taken either corporate or state money and is financed entirely by donations. Authors keep copyright without paying. The typical fee for this charged by open-access journals such as those published by PLOS, is around $2k. If you value this project, and wish to see further such proceedings from this group, we ask you to consider donating to Foundations of Mind – as little as $5 per download, through their website: http://www.foundationsofmind.org/donate. This will ensure there will be further published proceedings on the foundations of mind like this one for you and others to enjoy free. COSMOS AND HISTORY 154 components. So, even though robots may well exceed humans in certain cognitive tasks, robots would not possess a biofield, emotions, or an interior experience. Although they may be able to emulate emotions with certain facial expressions and vocal patterns, they may always be distinguished from humans as lacking the complex dynamic biofield of human beings that reflects the living state.",2.0
The power and limitations of machine learning and artificial intelligence in cardiac CT.,fcffab422478ed053b41f24fbb14a766ba74057e,"[{'authorId': '79659036', 'name': 'A. Varga-Szemes'}, {'authorId': '11858986', 'name': 'Brian E. Jacobs'}, {'authorId': '3924618', 'name': 'U. Schoepf'}]",2018.0,Journal of cardiovascular computed tomography,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,5.0
Digital discretion: A systematic literature review of ICT and street-level discretion,1dc06cb3fe09bcb138f3a33876609d5acc021c9e,"[{'authorId': '8340288', 'name': 'P. Busch'}, {'authorId': '1804146', 'name': 'H. Henriksen'}]",2018.0,Inf. Polity,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,69.0
Artificial intelligence for breast cancer screening: Opportunity or hype?,4361a8c3e0ccf9eee942388b40af6889d4ab4c9e,"[{'authorId': '6557501', 'name': 'N. Houssami'}, {'authorId': '1410213314', 'name': 'Christoph I. Lee'}, {'authorId': '144200490', 'name': 'D. Buist'}, {'authorId': '2064266862', 'name': 'Dacheng Tao'}]",2017.0,Breast,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,63.0
Manufacturing an Artificial Intelligence Revolution,46b7afcbfe8a3c4d7ffc3672c995d691af997f19,"[{'authorId': '1741241', 'name': 'Yarden Katz'}]",2017.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"While the term ""Artificial Intelligence"" (AI) was coined in the 1950s, in recent years AI has become a focus of attention in mainstream media. Yet the forces behind AI's revival have been unclear. I argue here that the ""AI"" label has been rebranded to promote a contested vision of world governance through big data. Major tech companies have played a key role in the rebranding, partly by hiring academics that work on big data (which has been effectively relabeled ""AI"") and helping to create the sense that super-human AI is imminent. However, I argue that the latest AI systems are premised on an old behaviorist view of intelligence that's far from encompassing human thought. In practice, the confusion around AI's capacities serves as a pretext for imposing more metrics upon human endeavors and advancing traditional neoliberal policies. The revived AI, like its predecessors, seeks intelligence with a ""view from nowhere"" (disregarding race, gender and class)---which can also be used to mask institutional power in visions of AI-based governance. Ultimately, AI's rebranding showcases how corporate interests can rapidly reconfigure academic fields. It also brings to light how a nebulous technical term (AI) may be exploited for political gain.",28.0
From Images to Actions: Opportunities for Artificial Intelligence in Radiology.,537dbf34f1022fe0d6285cca3a2b97657d5e1bcb,"[{'authorId': '145673195', 'name': 'C. E. Kahn'}]",2017.0,Radiology,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"Radiologists play an important role in guiding, vetting, and incorporating artificial intelligence systems into clinical practice.",59.0
"The architecture challenge: Future artificial-intelligence systems will require sophisticated architectures, and knowledge of the brain might guide their construction",fb7d9b2f84d331e3824d847691ef786246c365b0,"[{'authorId': '144716847', 'name': 'G. Baldassarre'}, {'authorId': '34631909', 'name': 'V. Santucci'}, {'authorId': '5191016', 'name': 'Emilio Cartoni'}, {'authorId': '1832617', 'name': 'D. Caligiore'}]",2017.0,Behavioral and Brain Sciences,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"Abstract In this commentary, we highlight a crucial challenge posed by the proposal of Lake et al. to introduce key elements of human cognition into deep neural networks and future artificial-intelligence systems: the need to design effective sophisticated architectures. We propose that looking at the brain is an important means of facing this great challenge.",6.0
Intelligent Manufacturing in the Context of Industry 4.0: A Review,d48f634238bab3a1e7e16f136c55e141383bf589,"[{'authorId': '2937841', 'name': 'R. Zhong'}, {'authorId': '1390531672', 'name': 'X. Xu'}, {'authorId': '2072067357', 'name': 'Eberhard Klotz'}, {'authorId': '1947024', 'name': 'S. Newman'}]",2017.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,1382.0
Thriving in the Age of Accelerations: A Brief Look at the Societal Effects of Artificial Intelligence and the Opportunities for Libraries,a0e2e72964d16fc9d598e9d939c938cfb1ba3f43,"[{'authorId': '2078810', 'name': 'Kenning Arlitsch'}, {'authorId': '49181062', 'name': 'Bruce Newell'}]",2017.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"ABSTRACT Fifty years of the compounding effects of Moore's Law have led to enormous advances in computer processing power. Increased network speeds, the availability of big data, and machine learning techniques have accelerated the development of artificial intelligence; this promises to dramatically change many industries, including libraries. This article offers some thoughts on the effects of automation on employment, the social and political fallout, and the threats and opportunities for academic and public libraries.",25.0
State of The Art-Intense Review on Artificial Intelligence Systems Application in Process Planning and Manufacturing,d0745071dbbcf79cfc6d728d775da4d44fa91f25,"[{'authorId': '35150692', 'name': 'S. P. L. Kumar'}]",2017.0,Engineering applications of artificial intelligence,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,110.0
Perceived Impacts of Industry 4.0 on Manufacturing Industry and Its Workforce: Case of Germany,cbdd8ada7d635cf8232c2422394d3beda5ef5bfd,"[{'authorId': '147483079', 'name': 'Mark Haeffner'}, {'authorId': '2178544', 'name': 'K. Panuwatwanich'}]",2017.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,13.0
Automation and Workforce in India: Terrible Consequences or Impossible?,21ce07aa929633d1a744c4aa83081dad052d3905,"[{'authorId': '1399626130', 'name': 'Vigneswara P Ilavarasan'}]",2017.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"The objectives of this note are twofold – to test the current discourse about the impact of technology on work in the light of employment in India, and how the projected gloomy future would unfold. The article attempts to meet the same through four sections. The first section briefly presents predictions about the impact of automation and the related assumptions. The second section is about workforce in India. The third section shares details about the secondary data based on which this article is written. The final section gauges the potential, impact, and possible scenarios of automation on Indian workforce and the society.",9.0
"Datification, Organizational Strategy, and IS Research: What's the Score?",4df94df4d2e0c3c75658ea53968a8d1f502199e3,"[{'authorId': '143981571', 'name': 'M. Markus'}]",2017.0,Journal of strategic information systems,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,53.0
Survey on artificial intelligence for additive manufacturing,90f0cc2a0f41a17c7b681d6ff87a3a2ecec53b01,"[{'authorId': '2109724909', 'name': 'Jimeng Yang'}, {'authorId': '2118048650', 'name': 'Yi Chen'}, {'authorId': '49015808', 'name': 'Weidong Huang'}, {'authorId': '30669451', 'name': 'Yun Li'}]",2017.0,International Conference on Automation and Computing,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"Additive manufacturing of three-dimensional objects are now more and more realised through 3D printing, known as an evolutional paradigm in the manufacturing industry. Artificial intelligence is currently finding wide applications to 3D printing for an intelligent, efficient, high quality, mass customised and service-oriented production process. This paper presents a comprehensive survey of artificial intelligence in 3D printing. Before a printing task begins, the printability of given 3D objects can be determined through a printability checker using machine learning. The prefabrication of slicing is accelerated through parallel slicing algorithms and the path planning is optimised intelligently. In the aspect of service and security, intelligent demand matching and resource allocation algorithms enable a Cloud service platform and evaluation model to provide clients with an on-demand service and access to a collection of shared resources. We also present three machine learning algorithms to detect product defects in the presence of cyber-attacks. Based on the reviews on various applications, printability with multi-indicators, reduction of complexity threshold, acceleration of prefabrication, real-time control, enhancement of security and defect detection for customised designs are seen of good opportunities for further research, especially in the era of Industry 4.0.",29.0
Artificial Intelligence Policy: A Primer and Roadmap,48aee85dfb2ec9f38891c04ccd9116019f2dff9b,"[{'authorId': '3014341', 'name': 'Ryan Calo'}]",2017.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"Talk of artificial intelligence is everywhere. People marvel at the capacity of machines to translate any language and master any game. Others condemn the use of secret algorithms to sentence criminal defendants or recoil at the prospect of machines gunning for blue, pink, and white-collar jobs. Some worry aloud that artificial intelligence will be humankind’s “final invention.”  This essay, prepared in connection with UC Davis Law Review's 50th anniversary symposium, explains why AI is suddenly on everyone's mind and provides a roadmap to the major policy questions AI raises. The essay is designed to help policymakers, investors, technologists, scholars, and students understand the contemporary policy environment around AI at least well enough to initiate their own exploration. Topics covered include: justice and equity, use of force, safety and certification, privacy (including data parity) and taxation and displacement of labor. In addition to these topics, the essay will touch briefly on a selection of broader systemic questions: institutional configuration and expertise, investment and procurement, removing hurdles to accountability and correcting mental models of AI.",148.0
Artificial Intelligence and Public Policy,005aced4eead356ed50a1b925d25508839db60fa,"[{'authorId': '3108936', 'name': 'Adam Thierer'}, {'authorId': '1741296150', 'name': ""Andrea Castillo O'Sullivan""}, {'authorId': '2070634502', 'name': 'Raymond Russell'}]",2017.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"There is growing interest in the market potential of artificial intelligence (AI) technologies and applications as well as in the potential risks that these technologies might pose. As a result, questions are being raised about the legal and regulatory governance of AI, machine learning, “autonomous” systems, and related robotic and data technologies. Fearing concerns about labor market effects, social inequality, and even physical harm, some have called for precautionary regulations that could have the effect of limiting AI development and deployment. In this paper, we recommend a different policy framework for AI technologies. At this nascent stage of AI technology development, we think a better case can be made for prudence, patience, and a continuing embrace of “permissionless innovation” as it pertains to modern digital technologies. Unless a compelling case can be made that a new invention will bring serious harm to society, innovation should be allowed to continue unabated, and problems, if they develop at all, can be addressed later.",45.0
Online Behavioral Advertising: A Literature Review and Research Agenda,5daf225dd716e2bdf8d9a64ffeebdc34ae8fdea7,"[{'authorId': '3423820', 'name': 'S. C. Boerman'}, {'authorId': '3235987', 'name': 'S. Kruikemeier'}, {'authorId': '150106539', 'name': 'Frederik J. Zuiderveen Borgesius'}]",2017.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"Advertisers are increasingly monitoring people's online behavior and using the information collected to show people individually targeted advertisements. This phenomenon is called online behavioral advertising (OBA). Although advertisers can benefit from OBA, the practice also raises concerns about privacy. Therefore, OBA has received much attention from advertisers, consumers, policymakers, and scholars. Despite this attention, there is neither a strong definition of OBA nor a clear accumulation of empirical findings. This article defines OBA and provides an overview of the empirical findings by developing a framework that identifies and integrates all factors that can explain consumer responses toward OBA. The framework suggests that the outcomes of OBA are dependent on advertiser-controlled factors (e.g., the level of personalization) and consumer-controlled factors (e.g., knowledge and perceptions about OBA and individual characteristics). The article also overviews the theoretical positioning of OBA by placing the theories that are used to explain consumers’ responses to OBA in our framework. Finally, we develop a research agenda and discuss implications for policymakers and advertisers.",231.0
Enterprise Cognitive Computing Applications: Opportunities and Challenges,788a5c9e40d6f749f7d5029a039be08e73ba5e0f,"[{'authorId': '144336221', 'name': 'Monideepa Tarafdar'}, {'authorId': '2492483', 'name': 'C. Beath'}, {'authorId': '2150933760', 'name': 'J. Ross'}]",2017.0,IT Professional,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"Enterprise cognitive computing (ECC) applications are generating a great deal of excitement for organizations. However, they have yet to demonstrate a business impact on a large scale. An important reason for this is a failure to understand how such applications can contribute to a company’s business objectives and the challenges associated with implementing them. In this article, the authors provide an overview of cognitive computing applications for the enterprise. In particular, they classify opportunities for developing ECC applications and describe challenges in implementing them. Their findings are based on a study of 51 ECC application initiatives across a broad range of industries in North America, Europe, and the Asia-Pacific region. Given the lack of systematic descriptions regarding what is possible from ECC, this article should be valuable to researchers and practitioners in unpacking the black box of cognitive computing.",23.0
Big Data Analytics: A Review on Theoretical Contributions and Tools Used in Literature,becb893edf5629482f8d8acc566a12054477aa3b,"[{'authorId': '37213839', 'name': 'Purva Grover'}, {'authorId': '2733956', 'name': 'A. Kar'}]",2017.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,147.0
Information and Inference,a213bd98d3b8437160e0205ff2fe9706851cf9c6,"[{'authorId': '2066256739', 'name': 'Paul Walton'}]",2017.0,Inf.,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"Inference is expressed using information and is therefore subject to the limitations of information. The conventions that determine the reliability of inference have developed in information ecosystems under the influence of a range of selection pressures. These conventions embed limitations in information measures like quality, pace and friction caused by selection trade-offs. Some selection pressures improve the reliability of inference; others diminish it by reinforcing the limitations of the conventions. This paper shows how to apply these ideas to inference in order to analyse the limitations; the analysis is applied to various theories of inference including examples from the philosophies of science and mathematics as well as machine learning. The analysis highlights the limitations of these theories and how different, seemingly competing, ideas about inference can relate to each other.",72.0
Public deliberation on government-managed social media: A study on Weibo users in China,f65f432b4fd12f58cb4003de62b05217fde2948f,"[{'authorId': '2575165', 'name': 'R. Medaglia'}, {'authorId': '3409776', 'name': 'Demi Zhu'}]",2017.0,Government Information Quarterly,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,65.0
"Human rights in the robot age: challenges arising from the use of robotics, artificial intelligence, and virtual and augmented reality",756e821e880de24e71e374f21f26e5b9f22840fb,"[{'authorId': '2094751941', 'name': 'Q. V. Est'}, {'authorId': '2074242209', 'name': 'J. Gerritsen'}, {'authorId': '40657567', 'name': 'L. Kool'}]",2017.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,34.0
Big Data Analytics for Physical Internet-based intelligent manufacturing shop floors,6d97dca6deb258c30381956e9af93b12bd5c6d03,"[{'authorId': '2937841', 'name': 'R. Zhong'}, {'authorId': '2153076066', 'name': 'Chen Xu'}, {'authorId': '46729206', 'name': 'Chao Chen'}, {'authorId': '144823068', 'name': 'G. Huang'}]",2017.0,International Journal of Production Research,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"Physical Internet (PI, π) has been widely used for transforming and upgrading the logistics and supply chain management worldwide. This study extends the PI concept into manufacturing shop floors where typical logistics resources are converted into smart manufacturing objects (SMOs) using Internet of Things (IoT) and wireless technologies to create a RFID-enabled intelligent shop floor environment. In such PI-based environment, enormous RFID data could be captured and collected. This study introduces a Big Data Analytics for RFID logistics data by defining different behaviours of SMOs. Several findings are significant. It is observed that task weight is primarily considered in the logistics decision-making in this case. Additionally, the highest residence time occurs in a buffer with the value of 12.17 (unit of time) which is 40.57% of the total delivery time. That implies the high work-in-progress inventory level in this buffer. Key findings and observations are generated into managerial implications, which are useful for various users to make logistics decisions under PI-enabled intelligent shop floors.",314.0
Machine Learning: An Applied Econometric Approach,9d75cc322a4e06d0a3a868cb91b04219a289c12c,"[{'authorId': '2062143', 'name': 'S. Mullainathan'}, {'authorId': '47281276', 'name': 'Jann Spiess'}]",2017.0,Journal of Economic Perspectives,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"Machines are increasingly doing “intelligent” things. Face recognition algorithms use a large dataset of photos labeled as having a face or not to estimate a function that predicts the presence y of a face from pixels x. This similarity to econometrics raises questions: How do these new empirical tools fit with what we know? As empirical economists, how can we use them? We present a way of thinking about machine learning that gives it its own place in the econometric toolbox. Machine learning not only provides new tools, it solves a different problem. Specifically, machine learning revolves around the problem of prediction, while many economic applications revolve around parameter estimation. So applying machine learning to economics requires finding relevant tasks. Machine learning algorithms are now technically easy to use: you can download convenient packages in R or Python. This also raises the risk that the algorithms are applied naively or their output is misinterpreted. We hope to make them conceptually easier to use by providing a crisper understanding of how these algorithms work, where they excel, and where they can stumble—and thus where they can be most usefully applied.",1051.0
"Big data: Dimensions, evolution, impacts, and challenges",f957ec0ae0a4bc19a3958a2bcff9223f97f58567,"[{'authorId': '2152634067', 'name': 'In Lee'}]",2017.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,344.0
Who Are Afraid of Losing Their Jobs to Artificial Intelligence and Robots? Evidence from a survey,028717bbc8c99ee6ca62e6c5a1fa18f191f37c64,"[{'authorId': '2671907', 'name': 'Daisuke Miyakawa'}, {'authorId': '145706755', 'name': 'Y. Miyauchi'}, {'authorId': '2069523217', 'name': 'Christian Perez'}]",2017.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"This study, using original survey data of 10,000 individuals, analyzes the possible impacts of artificial intelligence (AI) and robotics on employment. The first interest of this study is to ascertain, from the viewpoint of workers, what types of worker characteristics are associated with the perception of risk of jobs being replaced by the development of AI and robotics. The second interest is to identify, from the viewpoint of consumers, what types of services are likely to be replaced by AI and robotics. The results suggest that malleable/adaptable high skills acquired through higher education, particularly in science and engineering, are complementary with new technologies such as AI and robotics. At the same time, occupation-specific skills acquired by attending professional schools or holding occupational licenses, particularly those related to human-intensive services, are less likely to be replaced by AI and robotics.",5.0
Who Are Afraid of Losing Their Jobs to Artificial Intelligence and Robots? Evidence from a Survey,b97c0ce6eb53233a0a9e04bf5e1ea74b4c7d297f,"[{'authorId': '144963805', 'name': 'Masayuki Morikawa'}]",2017.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"This study, using original survey data of 10,000 individuals, analyzes the possible impacts of artificial intelligence (AI) and robotics on employment. The first interest of this study is to ascertain, from the viewpoint of workers, what types of worker characteristics are associated with the perception of risk of jobs being replaced by the development of AI and robotics. The second interest is to identify, from the viewpoint of consumers, what types of services are likely to be replaced by AI and robotics. The results suggest that malleable/adaptable high skills acquired through higher education, particularly in science and engineering, are complementary with new technologies such as AI and robotics. At the same time, occupation-specific skills acquired by attending professional schools or holding occupational licenses, particularly those related to human-intensive services, are less likely to be replaced by AI and robotics.",23.0
Mapping IS failure factors on PRINCE2® stages: an application of Interpretive Ranking Process (IRP),ccf073e339a84936c7fe859592856afefc2b2313,"[{'authorId': '2098810803', 'name': 'D. L. Hughes'}, {'authorId': '1724490', 'name': 'Yogesh Kumar Dwivedi'}, {'authorId': '1688149', 'name': 'N. Rana'}]",2017.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"Abstract The social, political and cultural issues faced by organisations and their senior management team in the delivery and adoption of strategic projects, is highly complex and problematic. Despite a mature body of literature, increasing levels of practitioner certification, application of standards and numerous government initiatives, improvements in success have been minimal. In this study, we analyse the key underlying factors surrounding the failure of Information Systems (IS) projects and explore the merits of articulating a narrative that focuses on senior management embracing practical pessimism. Specifically, we develop a hypothesis supported by empirical study that leverages expert’s views on the dominance and interrelationships between failure factors within PRINCE2® project stages using an Interpretive Ranking Process. Our findings establish how the concept of dominance between individual failure factors can necessitate senior management to make key informed and timely decisions that could potentially influence project outcomes based on an empirical derived, interpretive predictive framework.",53.0
The Enigma of Reason,3846925372bbbee0dc20da2e7fb17a6c8ae252c9,"[{'authorId': '46591041', 'name': 'H. Mercier'}, {'authorId': '1918233', 'name': 'D. Sperber'}]",2017.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,659.0
Loopthink: A Limitation of Medical Artificial Intelligence,ec18f59e969ada6deae9be3c7e74e3cf38f3d5c4,"[{'authorId': '30160796', 'name': 'W. P. Cheshire'}]",2017.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,4.0
"Can we control it? Autonomous robots threaten human identity, uniqueness, safety, and resources",1805d80f1ae9568c8e699e09742ed07322f4cd0a,"[{'authorId': '2159075', 'name': 'Jakub Złotowski'}, {'authorId': '46567605', 'name': 'K. Yogeeswaran'}, {'authorId': '1728894', 'name': 'C. Bartneck'}]",2017.0,Int. J. Hum. Comput. Stud.,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,146.0
A power-responsibility equilibrium framework for fairness: Understanding consumers' implicit privacy concerns for location-based services,8ab09e787f7f0a210dc9a5e6ffa5b76710032921,"[{'authorId': '2001889', 'name': 'Anjala S. Krishen'}, {'authorId': '3307344', 'name': 'Robyn L. Raschke'}, {'authorId': '152110735', 'name': 'A. Close'}, {'authorId': '51002248', 'name': 'P. Kachroo'}]",2017.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,43.0
"The responsive city: Engaging communities through data-smart governance, by Stephen Goldsmith and Susan Crawford",c7f750faa999ddf4d3d0e49418c301762dd130d8,"[{'authorId': '118289835', 'name': 'Tony Filipovitch'}]",2017.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"I have been waiting for a good, critical analysis of the opportunities and risks that the information revolution presents for local government practice. I am still waiting.The stated goal of this b...",110.0
#Republic: Divided Democracy in the Age of Social Media,53a230be179a5a2099e702a28d32af78486795f7,"[{'authorId': '3171769', 'name': 'C. Sunstein'}]",2017.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"From the New York Times bestselling author of Nudge and The World According to Star Wars, a revealing account of how today's Internet threatens democracy--and what can be done about it As the Internet grows more sophisticated, it is creating new threats to democracy. Social media companies such as Facebook can sort us ever more efficiently into groups of the like-minded, creating echo chambers that amplify our views. It's no accident that on some occasions, people of different political views cannot even understand one another. It's also no surprise that terrorist groups have been able to exploit social media to deadly effect. Welcome to the age of #Republic. In this revealing book, New York Times bestselling author Cass Sunstein shows how today's Internet is driving political fragmentation, polarization, and even extremism--and what can be done about it. He proposes practical and legal changes to make the Internet friendlier to democratic deliberation, showing that #Republic need not be an ironic term. Rather, it can be a rallying cry for the kind of democracy that citizens of diverse societies need most.",186.0
Implementing Lumberjacks and Black Swans Into Model-Based Tools to Support Human–Automation Interaction,be58d7b73cd79b3114c721dac2786d6975903b6b,"[{'authorId': '8318940', 'name': 'A. Sebok'}, {'authorId': '1772836', 'name': 'C. Wickens'}]",2017.0,Hum. Factors,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"Objective: The objectives were to (a) implement theoretical perspectives regarding human–automation interaction (HAI) into model-based tools to assist designers in developing systems that support effective performance and (b) conduct validations to assess the ability of the models to predict operator performance. Background: Two key concepts in HAI, the lumberjack analogy and black swan events, have been studied extensively. The lumberjack analogy describes the effects of imperfect automation on operator performance. In routine operations, an increased degree of automation supports performance, but in failure conditions, increased automation results in more significantly impaired performance. Black swans are the rare and unexpected failures of imperfect automation. Method: The lumberjack analogy and black swan concepts have been implemented into three model-based tools that predict operator performance in different systems. These tools include a flight management system, a remotely controlled robotic arm, and an environmental process control system. Results: Each modeling effort included a corresponding validation. In one validation, the software tool was used to compare three flight management system designs, which were ranked in the same order as predicted by subject matter experts. The second validation compared model-predicted operator complacency with empirical performance in the same conditions. The third validation compared model-predicted and empirically determined time to detect and repair faults in four automation conditions. Conclusion: The three model-based tools offer useful ways to predict operator performance in complex systems. Application: The three tools offer ways to predict the effects of different automation designs on operator performance.",52.0
Applications of artificial intelligence in intelligent manufacturing: a review,0dff4441dca73fbefffd9444553fc640d62a9c77,"[{'authorId': '8820182', 'name': 'Bo-Hu Li'}, {'authorId': '1987155', 'name': 'Baocun Hou'}, {'authorId': '2116673144', 'name': 'Wentao Yu'}, {'authorId': '2187182273', 'name': 'Xiaobing Lu'}, {'authorId': '2154926203', 'name': 'Chun-Wei Yang'}]",2017.0,Frontiers of Information Technology & Electronic Engineering,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"Based on research into the applications of artificial intelligence (AI) technology in the manufacturing industry in recent years, we analyze the rapid development of core technologies in the new era of ‘Internet plus AI’, which is triggering a great change in the models, means, and ecosystems of the manufacturing industry, as well as in the development of AI. We then propose new models, means, and forms of intelligent manufacturing, intelligent manufacturing system architecture, and intelligent manufacturing technology system, based on the integration of AI technology with information communications, manufacturing, and related product technology. Moreover, from the perspectives of intelligent manufacturing application technology, industry, and application demonstration, the current development in intelligent manufacturing is discussed. Finally, suggestions for the application of AI in intelligent manufacturing in China are presented.",360.0
Editorial from the new Editor-in-Chief: Artificial Intelligence in Medicine and the forthcoming challenges,1a6332c8e8fa807175a2b9d2b5736be28eb8e960,"[{'authorId': '116038179', 'name': 'Combi Carlo'}]",2017.0,Artif. Intell. Medicine,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,16.0
"A future that works: automation, employment, and productivity",e3ac9558f18234cdf92b730a7386ff16d446a1af,"[{'authorId': '144605904', 'name': 'J. Manyika'}, {'authorId': '143688672', 'name': 'Michael Chui'}, {'authorId': '100556814', 'name': 'Mehdi Miremadi'}, {'authorId': '2669735', 'name': 'J. Bughin'}, {'authorId': '2072512614', 'name': 'Katy George'}, {'authorId': '2059292942', 'name': 'Paul Willmott'}, {'authorId': '101655408', 'name': 'Martin Dewhurst'}]",2017.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"Advances in robotics, artificial intelligence, and machine learning are ushering in a new age of automation, as machines match or outperform human performance in a range of work activities, including ones requiring cognitive capabilities. In this report, part of our ongoing research into the future of work, we analyze the automation potential of the global economy, the factors that will determine the pace and extent of workplace adoption, and the economic impact associated with its potential.  Automation of activities can enable businesses to improve performance, by reducing errors and improving quality and speed, and in some cases achieving outcomes that go beyond human capabilities. Automation also contributes to productivity, as it has done historically. At a time of lackluster productivity growth, this would give a needed boost to economic growth and prosperity and help offset the impact of a declining share of the working-age population in many countries. Based on our scenario modeling, we estimate automation could raise productivity growth globally by 0.8 to 1.4 percent annually.  About half the activities people are paid almost $15 trillion in wages to do in the global economy have the potential to be automated by adapting currently demonstrated technology, according to our analysis of more than 2,000 work activities across 800 occupations. While less than 5 percent of all occupations can be automated entirely using demonstrated technologies, about 60 percent of all occupations have at least 30 percent of constituent activities that could be automated. More occupations will change than will be automated away.  Activities most susceptible to automation involve physical activities in highly structured and predictable environments, as well as the collection and processing of data. In the United States, these activities make up 51 percent of activities in the economy accounting for almost $2.7 trillion in wages. They are most prevalent in manufacturing, accommodation and food service, and retail trade, and include some middle-skill jobs.  Technical, economic, and social factors will determine the pace and extent of automation. Continued technical progress, for example in areas such as natural language processing, is a key factor. Beyond technical feasibility, the cost of technology, competition with labor including skills and supply and demand dynamics, performance benefits including and beyond labor cost savings, and social and regulatory acceptance will affect the pace and scope of automation. Our scenarios suggest that half of today's work activities could be automated by 2055, but …",588.0
The role of automated technology in the creation of copyright works: the challenges of artificial intelligence,57a1657ff02801c28a37832c610c8d7d388a5bb7,"[{'authorId': '32597936', 'name': 'Jesus Manuel Niebla Zatarain'}]",2017.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"ABSTRACT Technology today has an increasingly relevant role in areas traditionally considered restricted to humans. This position has been changing due to the increasing capacity of devices to carry out complex tasks without the need for any direct human intervention at all. An example of this can be found in devices that emulate, to a certain extent, human creative processes such as the conception of artistic works. These present a new (and from the commercial point of view interesting) reality: One where the human element is no longer considered irreplaceable in the creational stage of artistic works. This has a direct impact on the legal framework that surround these works; questioning whether the current legal framework of copyright is still capable of effectively protecting and incentivising human generated works. In the following article, this situation will be explored through a description of the potential impact of technology on human generated work and the position of current legal jurisdictions on this emerging landscape.",15.0
Self-Learning Monte Carlo Method in Fermion Systems,08a9561e1478240ae55d36bbb4277432a093945b,"[{'authorId': '120809902', 'name': 'Junwei Liu'}, {'authorId': '5060396', 'name': 'Huitao Shen'}, {'authorId': '50065362', 'name': 'Yang Qi'}, {'authorId': '47884751', 'name': 'Z. Meng'}, {'authorId': '2186885768', 'name': 'L. Fu'}]",2016.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"We develop the self-learning Monte Carlo (SLMC) method, a general-purpose numerical method recently introduced to simulate many-body systems, for studying interacting fermion systems. Our method uses a highly-efficient update algorithm, which we design and dub “cumulative update”, to generate new candidate configurations in the Markov chain based on a self-learned bosonic effective model. From general analysis and numerical study of the double exchange model as an example, we find the SLMC with cumulative update drastically reduces the computational cost of the simulation, while remaining statistically exact. Remarkably, its computational complexity is far less than the conventional algorithm with local updates.",11.0
The Challenge of Non-Technical Loss Detection using Artificial Intelligence: A Survey,7c1186883b630b155e3b5e0a474d31ef8f64e366,"[{'authorId': '3109852', 'name': 'Patrick O. Glauner'}, {'authorId': '49638140', 'name': 'Andre Boechat'}, {'authorId': '50627045', 'name': 'Lautaro Dolberg'}, {'authorId': '35164563', 'name': 'J. Meira'}, {'authorId': '1757227', 'name': 'R. State'}, {'authorId': '2023452', 'name': 'Franck Bettinger'}, {'authorId': '3322981', 'name': 'Yves Rangoni'}, {'authorId': '35264081', 'name': 'Diogo Duarte'}]",2016.0,International Journal of Computational Intelligence Systems,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"Detection of non-technical losses (NTL) which include electricity theft, faulty meters or billing errors has attracted increasing attention from researchers in electrical engineering and computer science. NTLs cause significant harm to the economy, as in some countries they may range up to 40% of the total electricity distributed. The predominant research direction is employing artificial intelligence to predict whether a customer causes NTL. This paper first provides an overview of how NTLs are defined and their impact on economies, which include loss of revenue and profit of electricity providers and decrease of the stability and reliability of electrical power grids. It then surveys the state-of-the-art research efforts in a up-to-date and comprehensive review of algorithms, features and data sets used. It finally identifies the key scientific and engineering challenges in NTL detection and suggests how they could be addressed in the future.",177.0
Machine learning phases of matter,dd308eb0d7be24e593fe355476057fc37ab5bf0e,"[{'authorId': '5048394', 'name': 'J. Carrasquilla'}, {'authorId': '3422513', 'name': 'R. Melko'}]",2016.0,Nature Physics,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,1029.0
"Predictive manufacturing systems in Industry 4.0: Trends, benefits and challenges",fb93d617c933f761d7915cd0c7ccf9869a28e8b7,"[{'authorId': '2065406961', 'name': 'Bojana Nikolić'}, {'authorId': '108146535', 'name': 'Jelena Ignjatić'}, {'authorId': '71447926', 'name': 'Nikola Suzić'}, {'authorId': '30881325', 'name': 'B. Stevanov'}, {'authorId': '29965675', 'name': 'Aleksandar Rikalovic'}]",2017.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,33.0
Swarm Intelligence: A Review of Algorithms,22756f6447c7e387e028b470d18b2505e960dcf1,"[{'authorId': '2055809588', 'name': 'Amrita Chakraborty'}, {'authorId': '2733956', 'name': 'A. Kar'}]",2017.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,210.0
Artifi cial Intelligence for the Real World,e9f39f0aced4299df5bad08ca96b6fc963aaf64b,"[{'authorId': '72729174', 'name': 'Rajeev Ronanki'}, {'authorId': '48721075', 'name': 'Andrew Nguyen'}]",2017.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,279.0
Factors influencing big data decision-making quality,d2bed4fe58f38dfdda55ac530521ed442bfc6ae7,"[{'authorId': '50817520', 'name': 'M. Janssen'}, {'authorId': '2678239', 'name': 'H. V. D. Voort'}, {'authorId': '143887884', 'name': 'A. Wahyudi'}]",2017.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,448.0
An empirical validation of a unified model of electronic government adoption (UMEGA),31ef08531ee5c7a6c699a9508768f0ba48cfb239,"[{'authorId': '1724490', 'name': 'Yogesh Kumar Dwivedi'}, {'authorId': '1688149', 'name': 'N. Rana'}, {'authorId': '50817520', 'name': 'M. Janssen'}, {'authorId': '1696747', 'name': 'Banita Lal'}, {'authorId': '2116399602', 'name': 'Michael D. Williams'}, {'authorId': '2054405211', 'name': 'Marc Clement'}]",2017.0,Government Information Quarterly,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,370.0
"Freud, Frankenstein and our fear of robots: projection in our cultural perception of technology",1176eb48c019731a80f3fe711a121f8277839995,"[{'authorId': '3419201', 'name': 'Michael Szollosy'}]",2017.0,Ai & Society,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,23.0
Paideia Education for learners' Competencies in the Age of Artificial Intelligence - The Google DeepMind Challenge Match,dece010e70b0af02b9bdfa19389159c15302af52,"[{'authorId': '1381229876', 'name': 'Mabyong Yoon'}, {'authorId': '73466259', 'name': 'Je-eun Baek'}]",2016.0,International Conference on Multimedia and Ubiquitous Engineering,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"Baduk was produced by Emperors Yao and Shun in ancient China to instruct their sons, and it is the most intricate game that has ever invented and demands a high level of both intuition and insight in order for the players of the game to check their overall strategic position and consider their moves accordingly. Baduk has a symbolic significance as it represents the highest limit of human intellectual ability, and has remained so until this day. The game is also the holy grail of artificial intelligence (AI). For this research study, we analyzed Baduk matches that took place between AlphaGo, a supercomputer, and Lee Sedol, one of the top Baduk players in the world, as well as examining archival research in order to discuss learning capabilities and the appropriate learning techniques for students in the age of AI. In March 2016, AlphaGo played against Lee and beat him, and he was arguably the world’s best Baduk player (professional nine-dan level) in Google’s DeepMind Challenge Match. During the match, AlphaGo introduced a new model in Baduk as its play deviated significantly from the common plays of professional Baduk players. The age in which AI surpasses human intelligence is considered to be a point of “singularity” in the history of human civilization. A singularity is a term denoting an unknown territory in which we cannot make easy predictions. In order to prepare for the new age of AI, we may have to go back to the beginning of human civilization to learn from the wisdom of the educational system of the ancients. To be specific, to prepare for this new age, we can learn from Baduk education of ancient China and the Paideia education of ancient Greece, from which we can deduce the strengths kept within human nature.",4.0
"Opinion: The dangers of faulty, biased, or malicious algorithms requires independent oversight",88d6bc751aea245cb591fe03e16e53c54f1dab7f,"[{'authorId': '1740403', 'name': 'B. Shneiderman'}]",2016.0,Proceedings of the National Academy of Sciences of the United States of America,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"The recent crash of a driverless car sends a clear warning about how algorithms can be deadly (1). Similarly, there are clear dangers in vital national services, such as communications, financial trading, healthcare, and transportation. These services depend on sophisticated algorithms, some relying on unpredictable artificial intelligence techniques, such as deep learning, that are increasingly embedded in complex software systems (2⇓–4). As search algorithms, high-speed trading, medical devices, and autonomous aircraft become more widely implemented, stronger checks become necessary to prevent failures (5, 6).



Proper independent oversight and investigation of flawed algorithms can help anticipate and improve quality, hence avoiding failures that lead to disaster. Image courtesy of Shutterstock/joloei.



What might help are traditional forms of independent oversight that use knowledgeable people who have powerful tools to anticipate, monitor, and retrospectively review operations of vital national services. The three forms of independent oversight that have been used in the past by industry and governments—planning oversight, continuous monitoring by knowledgeable review boards using advanced software, and a retrospective analysis of disasters—provide guidance for responsible technology leaders and concerned policy makers (7). Considering all three forms of oversight could lead to policies that prevent inadequate designs, biased outcomes, or criminal actions.

There is a long history of analyses of how poor design, unintentional bias, and malicious interventions can cause algorithms to trigger huge financial losses, promote unfair decisions, violate laws, and even cause deaths (8). Helen Nissenbaum, a scholar who focuses on the impact of technology on culture and society, identified the sources of bugs and biases in software, complaining about the “systematic erosion … 

[↵][1]1Email: ben{at}cs.umd.edu.

 [1]: #xref-corresp-1-1",50.0
Bio inspired computing - A review of algorithms and scope of applications,4b96b327c8c128e07266f4059b76d1bc98d08a3a,"[{'authorId': '2733956', 'name': 'A. Kar'}]",2016.0,Expert systems with applications,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,377.0
Six degrees of cultural diversity and R&D output efficiency,741eae502ed59ba862a6089855fc094973863903,"[{'authorId': '102927759', 'name': 'Annie Tubadji'}, {'authorId': '2965486', 'name': 'P. Nijkamp'}]",2016.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,9.0
The strategy of technology park for the development of IT industry in Pakistan,b7d5bb6c74706f5e63a1f57cdfb9ced5e81d7a89,"[{'authorId': '2546081', 'name': 'D. Yim'}, {'authorId': '31404452', 'name': 'H. Cho'}, {'authorId': '31380722', 'name': 'Chiung Song'}, {'authorId': '2108480606', 'name': 'Jaewon Lee'}, {'authorId': '2108644293', 'name': 'Seona Lee'}, {'authorId': '2107955374', 'name': 'Sinae Park'}]",2016.0,Portland International Conference on Management of Engineering and Technology,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"Pakistan has been experiencing Sow but steady economic growth in recent years and the software industry is booming thanks to abundant young population who works for programming, with good English skills. Recognizing tie development potential of software industry, Pakistan government has been promoting its development by providing various supports. However, it seems that the technological ecosystem is not fully developed, the university industry collaboration is weak and technological development is not advanced. In this context, tie Pakistani government is trying to introduce a new advanced Technology Park initiative to promote the industry as well as to provide the required work space. Nowadays. Technology Parks established by government are supporting the development of various industres. In the context, the research was done to evaluate whether the Technology Park initiative for IT industry would be also effective in Pakistan. In the research, the conceptual review was conducted for the Technology Park and SWOT analysis was done for the current Pakistan software developing. It is argued that the Technology Park can play an important role in developing Pakistan software industry when the right policy and management can be assured at the govemment level.",2.0
Visual Analytics in Urban Computing: An Overview,995328f2d7a1e45dbbc63c6f34841c6f5a847ced,"[{'authorId': '47833402', 'name': 'Yixian Zheng'}, {'authorId': '47203312', 'name': 'Wenchao Wu'}, {'authorId': '2144033820', 'name': 'Yuanzhe Chen'}, {'authorId': '145506027', 'name': 'Huamin Qu'}, {'authorId': '1726587', 'name': 'L. Ni'}]",2016.0,IEEE Transactions on Big Data,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"Nowadays, various data collected in urban context provide unprecedented opportunities for building a smarter city through urban computing. However, due to heterogeneity, high complexity and large volumes of these urban data, analyzing them is not an easy task, which often requires integrating human perception in analytical process, triggering a broad use of visualization. In this survey, we first summarize frequently used data types in urban visual analytics, and then elaborate on existing visualization techniques for time, locations and other properties of urban data. Furthermore, we discuss how visualization can be combined with automated analytical approaches. Existing work on urban visual analytics is categorized into two classes based on different outputs of such combinations: 1) For data exploration and pattern interpretation, we describe representative visual analytics tools designed for better insights of different types of urban data. 2) For visual learning, we discuss how visualization can help in three major steps of automated analytical approaches (i.e., cohort construction; feature selection & model construction; result evaluation & tuning) for a more effective machine learning or data mining process, leading to sort of artificial intelligence, such as a classifier, a predictor or a regression model. Finally, we outlook the future of urban visual analytics, and conclude the survey with potential research directions.",98.0
Information systems project failure – analysis of causal links using interpretive structural modelling,040ea134cb85ffd3bd10f833f467dcdf5fb0b42c,"[{'authorId': '2098810803', 'name': 'D. L. Hughes'}, {'authorId': '1724490', 'name': 'Yogesh Kumar Dwivedi'}, {'authorId': '1688149', 'name': 'N. Rana'}, {'authorId': '2973463', 'name': 'A. Simintiras'}]",2016.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"Abstract The analysis of the root causes of information systems project failure has been the subject of intense scrutiny for some time within industry and the academic community. Researchers have developed various models, notions of failure and categorisations to succinctly classify project failure into a set of key factors for organisations and project managers to focus on in their attempts to avoid failure. This study incorporates a technique titled: interpretive structural modelling as the methodology to formalise the relationships between the selected failure factors. This approach is positioned as a mechanism that can yield greater insights into the relationships between the factors surrounding project failure, thereby developing a better understanding of how these relationships can have a bearing on project outcomes. The findings identify key driving variables that are presented as having significant impact on the other factors within the model. A number of variables are also identified as being heavily dependent on other connected factors highlighting that a failure in one or more of these connected factors is likely to result in a failure in one or more of the dependent factors unless timely steps are taken to address these key issues. This research details a number of practical implications for senior management and project managers as well as the academic community. These considerations form an underlying thread within this study as specific practice-related implications are highlighted and discussed throughout the study.",102.0
Conceptualizing the Evolution and Future of Advertising,c2153e2475a7dc681da3815f422e942dde7481b4,"[{'authorId': '2157702950', 'name': 'V. Kumar'}, {'authorId': '119971212', 'name': 'Shaphali Gupta'}]",2016.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"Technology has irreversibly changed the way that firms identify who is more likely to buy, what they are more likely to buy, when they are more likely to buy, why they are likely to buy, and how they are likely to buy, repurchase, and recommend. It is now easier than ever for firms to have a body of evidence in the form of actual insights, as opposed to having mere guestimates, on consumers' apparent intentions to buy. Power dynamics have changed in the marketing climate, and firms have to factor in not only aggressive competitors but also empowered customers with limited attention spans. In such a climate, what lies in the future for advertising? And how can marketing managers make the most of the changing climate and maximize their returns on advertising? How can academics advance research related to maximizing the effectiveness and efficiency of advertising? These are the questions that this article addresses via an integrated framework that expounds all the factors related to customers, firms, technological environment, and data resources, as well as the contextual factors, including product life cycle, customer life cycle, and so on, and their collective impact on advertising strategy, which includes advertising content, media selection, message, and targeting.",111.0
The challenges and limits of big data algorithms in technocratic governance,5164e276115b4fc353050cfc88fb77c53764b880,"[{'authorId': '50817520', 'name': 'M. Janssen'}, {'authorId': '2184523', 'name': 'G. Kuk'}]",2016.0,Government Information Quarterly,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,175.0
Discovering phase transitions with unsupervised learning,24f8b9d8b9ec39322a6e141f5bd99f2e947fd22e,"[{'authorId': '2152511187', 'name': 'Lei Wang'}]",2016.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"Unsupervised learning is a discipline of machine learning which aims at discovering patterns in large data sets or classifying the data into several categories without being trained explicitly. We show that unsupervised learning techniques can be readily used to identify phases and phases transitions of many-body systems. Starting with raw spin configurations of a prototypical Ising model, we use principal component analysis to extract relevant low-dimensional representations of the original data and use clustering analysis to identify distinct phases in the feature space. This approach successfully finds physical concepts such as the order parameter and structure factor to be indicators of a phase transition. We discuss the future prospects of discovering more complex phases and phase transitions using unsupervised learning techniques.",374.0
Data Scientist,7ee986de710617168d8adde0aa35a8c079011a09,"[{'authorId': '2013713', 'name': 'George O. Strawn'}]",2016.0,IT Professional,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"The last installment of this column dealt with the specter of IT-caused unemployment. Here, the author considers a new IT-created employment opportunity--the data scientist. He looks at data, information, and knowledge and current IT job classifications to provide context, describes how big data has inspired the field of data science, and defines what data science is and what data scientists do.",223.0
"Big and Open Linked Data (BOLD) in research, policy, and practice",0ef049d87b1a05f98f6d69b9fede365a76f5a83a,"[{'authorId': '50817520', 'name': 'M. Janssen'}, {'authorId': '2184523', 'name': 'G. Kuk'}]",2016.0,Journal of Organizational Computing and Electronic Commerce,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"Abstract The value of data as a new economic asset class is seldom realized on its own. With less reliance on self-administered survey, it offers new insights into behaviors and patterns. Yet, it involves a huge undertaking of bringing together multiple actors from different disciplines and diverse practices to examine the underexplored relationships between types of data. There are different inquiry systems and research cycles to make sense out of big and open data (BOLD). We argue that deploying theories from diverse disciplines, and considering using different inquiry systems and research cycles, offers a more disciplined and robust methodological approach. This allows us to break through the limits of backward induction from the evidence by moving back and forward in exploring the unknown through BOLD. As such, we call for developing a variety of rigorous approaches to counterbalance the current theory-free practice in the analysis and use of BOLD.",67.0
A literature review on the levels of automation during the years. What are the different taxonomies that have been proposed?,c67bc0b3f88f5f78ad662f4f9fe5d644ae63e9f9,"[{'authorId': '2970176', 'name': 'M. Vagia'}, {'authorId': '2502274', 'name': 'A. Transeth'}, {'authorId': '1729361', 'name': 'S. Fjerdingen'}]",2016.0,Applied Ergonomics,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,174.0
"""The Computer Says No!"" -- A Case Study on Automated Decision-Making in Public Authorities",d3ec86acf69dc63e3fc342b122d3a3a28f3c89da,"[{'authorId': '2454318', 'name': 'E. Wihlborg'}, {'authorId': '37822514', 'name': 'Hannu Larsson'}, {'authorId': '2617486', 'name': 'Karin Hedström'}]",2016.0,Hawaii International Conference on System Sciences,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"In e-government automated decision-making increases, forming part of a trend towards ""smart"" and self-regulating systems. This necessitates the introduction of new relationships and practices, challenging the division of responsibilities in public administration. Using a case study approach, this paper elaborates on implications of automated decision-making for professional officers in a Swedish public organization. We conclude that automation should be framed in relation to the rules of law and ethics of justice. Furthermore, the roles and competences of professionals are changing, with automated systems beginning to resemble co-bureaucrats. Professionals can either make an alliance with the automated system or the client. This choice of strategy is related to the issues of legitimacy and professional competences. We also identify practices as being either a form of caring ethics or a formal legal ethic norm. Such practices should be further addressed to influence practices promoting legitimate systems citizens can trust.",37.0
Systematic lead time analysis,623e867583ee19293adb3a02dcfd0b422e67672f,"[{'authorId': '118829601', 'name': 'A. Jonsson'}, {'authorId': '96173526', 'name': 'Victor Svensson'}]",2016.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,5.0
Unleashing creativity with digital technology,5ab6f43e2c09c597e2bc114632f4c2d6990be2a8,"[{'authorId': '7871321', 'name': 'R. Austin'}]",2016.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"Technology can be deployed to augment the creative abilities of people and organizations and make new and valuable forms of innovation possible. Today's digital technologies have reached a level of maturation that enables, across many domains, a practical capability that may be called cheap and rapid iteration. To iterate is to try something different from what you tried last time. Iteration is the process that enables most forms of artistry. Painters often create numerous versions of a painting. Processes often become more creative when rapid iteration is affordable. Unfortunately, this is not the case in a lot of business domains. In the next five years, managers will awaken to a wide range of new possibilities. They'll act to improve creative capabilities, by figuring out how to deploy technologies to replace expensive physical trying with cheap virtual trying. In effect, they'll be constructing virtual rehearsal spaces, virtual laboratories, and inexpensive prototyping facilities. The aim won't be to design machines to take over peoples jobs, but rather to augment human capabilities.",9.0
A Review of Bio-Inspired Computing Methods and Potential Applications,d215ae04f1f48a44e1fb1674517e5db680380b84,"[{'authorId': '2055809588', 'name': 'Amrita Chakraborty'}, {'authorId': '2733956', 'name': 'A. Kar'}]",2016.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,18.0
"AI-based methodology of integrating affective design, engineering, and marketing for defining design specifications of new products",4ccd24116112d77478a6ace4539823ac1e052f55,"[{'authorId': '143781184', 'name': 'C. Kwong'}, {'authorId': '2213748614', 'name': 'Huimin Jiang'}, {'authorId': '2685738', 'name': 'Xinggang Luo'}]",2016.0,Engineering applications of artificial intelligence,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,68.0
Future Challenges of Robotics and Artificial Intelligence in Nursing: What Can We Learn from Monsters in Popular Culture?,37a599da1cc13306d860109f828888b954ee0fd9,"[{'authorId': '1414304294', 'name': 'Henrik Erikson'}, {'authorId': '1398479444', 'name': 'M. Salzmann-Erikson'}]",2016.0,The Permanente Journal,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"It is highly likely that artificial intelligence (AI) will be implemented in nursing robotics in various forms, both in medical and surgical robotic instruments, but also as different types of droids and humanoids, physical reinforcements, and also animal/pet robots. Exploring and discussing AI and robotics in nursing and health care before these tools become commonplace is of great importance. We propose that monsters in popular culture might be studied with the hope of learning about situations and relationships that generate empathic capacities in their monstrous existences. The aim of the article is to introduce the theoretical framework and assumptions behind this idea. Both robots and monsters are posthuman creations. The knowledge we present here gives ideas about how nursing science can address the postmodern, technologic, and global world to come. Monsters therefore serve as an entrance to explore technologic innovations such as AI. Analyzing when and why monsters step out of character can provide important insights into the conceptualization of caring and nursing as a science, which is important for discussing these empathic protocols, as well as more general insight into human knowledge. The relationship between caring, monsters, robotics, and AI is not as farfetched as it might seem at first glance.",28.0
Big Data's Disparate Impact,1d174f0e3c391368d0f3384a144a6c7487f2a143,"[{'authorId': '2881033', 'name': 'Solon Barocas'}, {'authorId': '46432110', 'name': 'Andrew D. Selbst'}]",2016.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"Advocates of algorithmic techniques like data mining argue that these techniques eliminate human biases from the decision-making process. But an algorithm is only as good as the data it works with. Data is frequently imperfect in ways that allow these algorithms to inherit the prejudices of prior decision makers. In other cases, data may simply reflect the widespread biases that persist in society at large. In still others, data mining can discover surprisingly useful regularities that are really just preexisting patterns of exclusion and inequality. Unthinking reliance on data mining can deny historically disadvantaged and vulnerable groups full participation in society. Worse still, because the resulting discrimination is almost always an unintentional emergent property of the algorithm’s use rather than a conscious choice by its programmers, it can be unusually hard to identify the source of the problem or to explain it to a court.This Essay examines these concerns through the lens of American antidiscrimination law — more particularly, through Title VII’s prohibition of discrimination in employment. In the absence of a demonstrable intent to discriminate, the best doctrinal hope for data mining’s victims would seem to lie in disparate impact doctrine. Case law and the Equal Employment Opportunity Commission’s Uniform Guidelines, though, hold that a practice can be justified as a business necessity when its outcomes are predictive of future employment outcomes, and data mining is specifically designed to find such statistical correlations. Unless there is a reasonably practical way to demonstrate that these discoveries are spurious, Title VII would appear to bless its use, even though the correlations it discovers will often reflect historic patterns of prejudice, others’ discrimination against members of protected groups, or flaws in the underlying dataAddressing the sources of this unintentional discrimination and remedying the corresponding deficiencies in the law will be difficult technically, difficult legally, and difficult politically. There are a number of practical limits to what can be accomplished computationally. For example, when discrimination occurs because the data being mined is itself a result of past intentional discrimination, there is frequently no obvious method to adjust historical data to rid it of this taint. Corrective measures that alter the results of the data mining after it is complete would tread on legally and politically disputed terrain. These challenges for reform throw into stark relief the tension between the two major theories underlying antidiscrimination law: anticlassification and antisubordination. Finding a solution to big data’s disparate impact will require more than best efforts to stamp out prejudice and bias; it will require a wholesale reexamination of the meanings of “discrimination” and “fairness.”",1853.0
Legal regulations and public policies for next-generation robots in Japan,e8b34e92c938bcfaff6ae8035604c64c2438bd11,"[{'authorId': '8678064', 'name': 'T. Nambu'}]",2016.0,Ai & Society,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,8.0
Digital Information and Value,e92e510879345fab6fcf7f36e87a836935940a4f,"[{'authorId': '2066256739', 'name': 'Paul Walton'}]",2015.0,Inf.,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"Digital information changes the ways in which people and organisations interact. This paper examines the nature of this change in the context of the author’s Model for Information (MfI). It investigates the relationship between outcomes and value, selection processes and some attributes of information and explores how this relationship changes in the move from analogue to digital information. Selection processes shape the evolution of information ecosystems in which conventions are established for the ways in which information is used. The conventions determine norms for information friction and information quality as well as the sources of information and channels used. Digital information reduces information friction, often dramatically, and changes information quality. The increasing use of analytics in business increasingly delivers predictive or prescriptive digital information. These changes are happening faster than information ecosystem conventions can change. The relationships established in the paper enable an analysis of, and guide changes to, these conventions enabling a more effective use of digital information.",10.0
"I, for One, Welcome Our New Computer Overlords",3e589cce766207a8ee7219fd41d39919edc33a80,"[{'authorId': '1746499', 'name': 'G. Booch'}]",2015.0,IEEE Software,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"Many fear the rise of superintelligent AIs. Such fears are at best unfounded and at worst misleading. The Web extra at https://youtu.be/geSb5Zp4qbM is an audio podcast of Grady Booch's On Computing column, in which he discusses why he rejects fears that machines with superintelligent computing powers will eventually spell the end of humanity.",6.0
Current status and advancement of cyber-physical systems in manufacturing,d2bdf13550c497681358733a3694dbd280dd49e4,"[{'authorId': '143714050', 'name': 'Lihui Wang'}, {'authorId': '1704998', 'name': 'Martin Törngren'}, {'authorId': '1768208', 'name': 'M. Onori'}]",2015.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,714.0
Visualizing Big Data with augmented and virtual reality: challenges and research agenda,3b795effaddd557445c2e70372e1cf56d346a19e,"[{'authorId': '2158388', 'name': 'Ekaterina Olshannikova'}, {'authorId': '2105429', 'name': 'A. Ometov'}, {'authorId': '1701549', 'name': 'Y. Koucheryavy'}, {'authorId': '143875035', 'name': 'Thomas Olsson'}]",2015.0,Journal of Big Data,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,236.0
A Study on the Internet of Things (IoT) Applications,ce55200790b541ec9ed4cc5846ff94c9b25ffa04,"[{'authorId': '47028092', 'name': 'Young-Mo Kang'}, {'authorId': '8301312', 'name': 'Mi-ran Han'}, {'authorId': '46218269', 'name': 'Kyeong-Seok Han'}, {'authorId': '49476678', 'name': 'Jong-Bae Kim'}]",2015.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"The government has adopted the field of “Internet of Things” as a national strategic project, announcing the Internet of Things master plan to achieve a leading country of hyper-connected digital revolution last May. The government has the promotional strategy of reinforcing the competitiveness in software (SW) sensor components devices, training specialists that will lead Internet of Things (IoT) services and products, and internalizing security for Internet of Things products and services. Internet of Things, thus expected to grow from ₩2 trillion and 300 billion last year to ₩30 trillion by 2020 in the market, refers to the Internet environment where people, things around, data, etc. are all connected to the wired and wireless network to mutually create, collect, share, and utilize information. Internet of Things is the technology and service that includes generating information (sensor) acquisition (parts, devices) sharing (Clyde) utilization (Big Data) application software. Internet of Things is getting a lot of attention from the public due to the effect of increasing export businesses and jobs and has become a buzzword among businesses. The Internet of Things market has a tendency to grow in the future as it is extended from social infrastructure (utilities, transportation, automation, etc.) and safety management to the consumer sector centered on life services. Sophisticated wireless communication technology is expected to form a huge network connected to all object units as a communication function.",69.0
The Importance of Trust for Personalized Online Advertising,19917c66ac5bfc52cadf5a8a0957e3e0a204a9f3,"[{'authorId': '50665823', 'name': 'Alexander Bleier'}, {'authorId': '2099485700', 'name': 'Maik Eisenbeiss'}]",2015.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,272.0
Revisiting the IT Productivity Paradox,dafa30588c143d5aa8d7119ee53e7adffa758206,"[{'authorId': '50288264', 'name': 'M. Brown'}]",2015.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"Twenty-five years after Nobel Laureate economist Robert Solow observed “seeing computers everywhere but in the productivity statistics,” the question of productivity gains from information technologies (IT) remains unanswered. This study examines the role of IT on one of the major indicators of police productivity: crime clearance rates. Relying on a two-wave cohort panel research design of roughly 700 police agencies, the study reveals that significant IT advances were made between the pre and post time periods in the provision of computerized crime data, crime analysis capabilities, and real-time communications. Nonetheless, using multiple hierarchical regression analysis, the study provides robust evidence for suggesting that computerization had little influence on productivity gains. The results of this study raise several very important issues pertaining to the goals of public organizations. While this study is limited to policing, a narrow time period, and internal IT systems, the results are nonetheless noteworthy. The research illustrates that conventional explanations for the IT productivity paradox do little to explain the shortfall. In closing, the article offers rival, but yet untested, explanations that may prove worthy of additional research.",26.0
Big and Open Linked Data (BOLD) to Create Smart Cities and Citizens: Insights from Smart Energy and Mobility Cases,f5dc897b0b7cf2dd7d97fd52c65d857cb4d714dd,"[{'authorId': '50817520', 'name': 'M. Janssen'}, {'authorId': '145107135', 'name': 'R. Matheus'}, {'authorId': '1696173', 'name': 'Anneke Zuiderwijk'}]",2015.0,International Conference on Electronic Government,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,51.0
"Artificial Intelligence-Based Techniques for Emerging Heterogeneous Network: State of the Arts, Opportunities, and Challenges",1d104ace71010ec69bc3f1b775a90c2a7ee5c98f,"[{'authorId': '2108503009', 'name': 'Xiaofei Wang'}, {'authorId': '47058025', 'name': 'Xiuhua Li'}, {'authorId': '143698682', 'name': 'Victor C. M. Leung'}]",2015.0,IEEE Access,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"Recently, mobile networking systems have been designed with more complexity of infrastructure and higher diversity of associated devices and resources, as well as more dynamical formations of networks, due to the fast development of current Internet and mobile communication industry. In such emerging mobile heterogeneous networks (HetNets), there are a large number of technical challenges focusing on the efficient organization, management, maintenance, and optimization, over the complicated system resources. In particular, HetNets have attracted great interest from academia and industry in deploying more effective solutions based on artificial intelligence (AI) techniques, e.g., machine learning, bio-inspired algorithms, fuzzy neural network, and so on, because AI techniques can naturally handle the problems of large-scale complex systems, such as HetNets towards more intelligent and automatic-evolving ones. In this paper, we discuss the state-of-the-art AI-based techniques for evolving the smarter HetNets infrastructure and systems, focusing on the research issues of self-configuration, self-healing, and self-optimization, respectively. A detailed taxonomy of the related AI-based techniques of HetNets is also shown by discussing the pros and cons for various AI-based techniques for different problems in HetNets. Opening research issues and pending challenges are concluded as well, which can provide guidelines for future research work.",132.0
Why Are There Still So Many Jobs? The History and Future of Workplace,3a8910a977b06c4c89a6cbdce7952e29d1feded2,"[{'authorId': '6703140', 'name': 'David Autor'}]",2015.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"In this essay, I begin by identifying the reasons that automation has not wiped out a majority of jobs over the decades and centuries. Automation does indeed substitute for laborâ��as it is typically intended to do. However, automation also complements labor, raises output in ways that leads to higher demand for labor, and interacts with adjustments in labor supply. Journalists and even expert commentators tend to overstate the extent of machine substitution for human labor and ignore the strong complementarities between automation and labor that increase productivity, raise earnings, and augment demand for labor. Changes in technology do alter the types of jobs available and what those jobs pay. In the last few decades, one noticeable change has been a ""polarization"" of the labor market, in which wage gains went disproportionately to those at the top and at the bottom of the income and skill distribution, not to those in the middle; however, I also argue, this polarization is unlikely to continue very far into future. The final section of this paper reflects on how recent and future advances in artificial intelligence and robotics should shape our thinking about the likely trajectory of occupational change and employment growth. I argue that the interplay between machine and human comparative advantage allows computers to substitute for workers in performing routine, codifiable tasks while amplifying the comparative advantage of workers in supplying problem-solving skills, adaptability, and creativity.",1203.0
Cultural Gravity Effects among Migrants: A Comparative Analysis of the EU15,ca0981ef228166527f4cec5d0666a87ec3bd1d6c,"[{'authorId': '102927759', 'name': 'Annie Tubadji'}, {'authorId': '2965486', 'name': 'P. Nijkamp'}]",2015.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"abstract This article introduces cultural gravity as a concept that serves to better disentangle the direction and magnitude of the effects from migration, which is controversial in recent literature. The aim is to test for cultural gravity effects on both the geographic concentration and human capital productivity of immigrants in the EU15 countries. Operationally, we proceed to construct an empirical cultural gravity measure and test it with the use of a composite cross-sectional database, comprising, inter alia, the World Value Survey and Eurostat Census data. After an initial exploration of relevant cultural data by means of multivariate statistical analysis, we present an extended formulation of a gravity model approached through logistic regression methods and a three-stage least-squares estimation. Our results clearly demonstrate the existence of a cultural gravity effect among immigrants. Finally, an interesting finding is that cultural gravity also plays a significant role in the context of the Culture-Based Development (CBD) growth model.",54.0
"The Internet of Things (IoT): Applications, investments, and challenges for enterprises",f48341821c5ce38272fb01ae4f43d12c2486901d,"[{'authorId': '145680277', 'name': 'In Lee'}, {'authorId': '118748459', 'name': 'Kyoochun Lee'}]",2015.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,1894.0
"Regulating Artificial Intelligence Systems: Risks, Challenges, Competencies, and Strategies",882e1ab2d7a508c6ebfb8884a549b85a2da4b385,"[{'authorId': '102200831', 'name': 'Matthew U. Scherer'}]",2015.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"Artificial intelligence technology (or AI) has developed rapidly during the past decade, and the effects of the AI revolution are already being keenly felt in many sectors of the economy. A growing chorus of commentators, scientists, and entrepreneurs has expressed alarm regarding the increasing role that autonomous machines are playing in society, with some suggesting that government regulation may be necessary to reduce the public risks that AI will pose. Unfortunately, the unique features of AI and the manner in which AI can be developed present both practical and conceptual challenges for the legal system. These challenges must be confronted if the legal system is to positively impact the development of AI and ensure that aggrieved parties receive compensation when AI systems cause harm. This article will explore the public risks associated with AI and the competencies of government institutions in managing those risks. It concludes with a proposal for an indirect form of AI regulation based on differential tort liability.",305.0
Extreme Working Hours in Western Europe and North America: A New Aspect of Polarization,920c09733cc8d7a6326d5c3a08263983ae95e754,"[{'authorId': '144099805', 'name': 'A. Burger'}]",2015.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"This paper analyzes the trends and root causes of extreme working hours in sixteen Western European countries, Canada, and the United States between 1970 and 2010. Earlier literature has revealed increasing trends in extreme working hours in the United States and recognized the negative repercussions of this new aspect of labor market polarization. As European average working hours have declined over the past decades, scholars have turned little attention to the analysis of extreme working hours in European countries. First, the article documents diverging patterns of extreme working hours in Western Europe. Whereas the Scandinavian and French ratios of workers with extreme hours remained very low, most other countries in Western Europe exhibit significantly higher ratios of extreme workers after the beginning of the 1990s than in the previous two decades. Second, the article detects the development of two diverging trajectories in the advanced capitalist world: one with a strong and stable labor regulation along with a balanced working hour profile and one with gradual deregulation along with an increasing ratio of long work weeks. Finally, using a series of pooled cross-section OLS estimations, the article tests five specific hypotheses, motivated by theories of the welfare state and political economy theories of globalization. The results provide strong empirical evidence for the notion that patterns of extreme working hours are not inherent in post-industrial development. The article uses data from the author’s extreme working hours standardized meta-database which had been compiled from two large micro data collections: the Luxembourg Income Study database (LIS) and the Multinational Time Use Study (MTUS).",28.0
A case analysis of embryonic data mining success,1fe9c024c4a01facf4cc498db67c72e292dbf203,"[{'authorId': '2440641', 'name': 'Uros Bole'}, {'authorId': '3193992', 'name': 'Aleš Popovič'}, {'authorId': '3057356', 'name': 'J. Zabkar'}, {'authorId': '34533476', 'name': 'G. Papa'}, {'authorId': '1782776', 'name': 'J. Jaklič'}]",2015.0,International Journal of Information Management,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,12.0
Unraveling the personalization paradox: The effect of information collection and trust-building strategies on online advertisement effectiveness,dbc4f3fa057b2c2f96873d926658edfcce3897d2,"[{'authorId': '122020879', 'name': 'Elizabeth Aguirre'}, {'authorId': '1852499', 'name': 'D. Mahr'}, {'authorId': '39957592', 'name': 'Dhruv Grewal'}, {'authorId': '3078618', 'name': 'K. Ruyter'}, {'authorId': '3209873', 'name': 'Martin Wetzels'}]",2015.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,412.0
Research on information systems failures and successes: Status update and future directions,459b8aee6c801d2869d22d19e2edc5cea134675c,"[{'authorId': '1724490', 'name': 'Yogesh Kumar Dwivedi'}, {'authorId': '1760915', 'name': 'D. Wastell'}, {'authorId': '1734224', 'name': 'Sven Laumer'}, {'authorId': '1804146', 'name': 'H. Henriksen'}, {'authorId': '144305675', 'name': 'M. D. Myers'}, {'authorId': '1698631', 'name': 'D. Bunker'}, {'authorId': '8518510', 'name': 'Amany R. Elbanna'}, {'authorId': '144092255', 'name': 'M. Ravishankar'}, {'authorId': '2948637', 'name': 'S. Srivastava'}]",2015.0,Inf. Syst. Frontiers,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,421.0
Measures of Information,736d26b889f59e7ef45e5215c15d5e8978476583,"[{'authorId': '2066256739', 'name': 'Paul Walton'}]",2015.0,Inf.,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"This paper builds an integrated framework of measures of information based on the Model for Information (MfI) developed by the author. Since truth is expressed using information, an analysis of truth depends on the nature of information and its limitations. These limitations include those implied by the geometry of information and those implied by the relativity of information. This paper proposes an approach to truth and truthlikeness that takes these limitations into account by incorporating measures of the quality of information. Another measure of information is the amount of information. This has played a role in two important theoretical difficulties—the Bar-Hillel Carnap paradox and the “scandal of deduction”. This paper further provides an analysis of the amount of information, based on MfI, and shows how the MfI approach can resolve these difficulties.",16.0
The Black Box Society: The Secret Algorithms That Control Money and Information,16d48c78afb6a9880486ce1b2111a611b4007557,"[{'authorId': '24272314', 'name': 'Frank A. Pasquale'}]",2015.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"Every day, corporations are connecting the dots about our personal behaviorsilently scrutinizing clues left behind by our work habits and Internet use. The data compiled and portraits created are incredibly detailed, to the point of being invasive. But who connects the dots about what firms are doing with this information? The Black Box Society argues that we all need to be able to do soand to set limits on how big data affects our lives. Hidden algorithms can make (or ruin) reputations, decide the destiny of entrepreneurs, or even devastate an entire economy. Shrouded in secrecy and complexity, decisions at major Silicon Valley and Wall Street firms were long assumed to be neutral and technical. But leaks, whistleblowers, and legal disputes have shed new light on automated judgment. Self-serving and reckless behavior is surprisingly common, and easy to hide in code protected by legal and real secrecy. Even after billions of dollars of fines have been levied, underfunded regulators may have only scratched the surface of this troubling behavior. Frank Pasquale exposes how powerful interests abuse secrecy for profit and explains ways to rein them in. Demanding transparency is only the first step. An intelligible society would assure that key decisions of its most important firms are fair, nondiscriminatory, and open to criticism. Silicon Valley and Wall Street need to accept as much accountability as they impose on others.",1415.0
Robots and humans as co-workers?,780f5541f273aa547386b28352ae1ce4cd374c77,"[{'authorId': '49196440', 'name': 'A. Moniz'}]",2013.0,arXiv.org,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"The design of work organisation systems with automated equipment is facing new challenges and the emergence of new concepts. The social aspects that are related with new concepts on the complex work environments (CWE) are becoming more relevant for that design. The work with autonomous systems implies options in the design of workplaces. Especially that happens in such complex environments. The concepts of “agents”, “co-working” or “human-centred technical systems” reveal new dimensions related to human-computer interaction (HCI). With an increase in the number and complexity of those human-technology interfaces, the capacities of human intervention can become limited, originating further problems. The case of robotics is used to exemplify the issues related with automation in working environments and the emergence of new HCI approaches that would include social implications. We conclude that studies on technology assessment of industrial robotics and autonomous agents on manufacturing environment should also focus on the human involvement strategies in organisations. A needed participatory strategy implies a new approach to workplaces design. This means that the research focus must be on the relation between technology and social dimensions not as separate entities, but integrated in the design of an interaction system.",1.0
Social media marketing and advertising,f19fec7df54b29fbf4c14417d39052c49850c91d,"[{'authorId': '1724490', 'name': 'Yogesh Kumar Dwivedi'}]",2015.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"Social media has recently gained tremendous fame as a highly impactful channel of communication in these modern times of digitized living. It has been put on a pedestal across varied streams for facilitating participatory interaction amongst businesses, groups, societies, organizations, consumers, communities, forums, and the like. This subject has received increased attention in the literature with many of its practical applications including social media marketing (SMM) being elaborated, analysed, and recorded by many studies. This study is aimed at collating the existing research on SMM to present a review of seventy one articles that will bring together the many facets of this rapidly blooming media marketing form. The surfacing limitations in the literature on social media have also been identified and potential research directions have been offered.",239.0
Measuring the Information Society Report,05c58816d7f72ed2eb2c3caeecb3536d94594e6f,[],2015.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"Since 2009, the International Telecommunication Union (ITU) has been publishing its annual ICT Development Index (IDI), which benchmarks countries’ performance with regard to ICT infrastructure, use and skills. The JRC analysis, conducted at ITU’s invitation, suggests that the conceptualized three-level structure of the 2015 IDI is statistically sound in terms of coherence and balance, with the overall index as well as the three sub-indices – on ICT access, use and skills – being driven by all the underlying components. The IDI has a very high statistical reliability of 0.96 and captures the single latent phenomenon underlying the three main dimensions of the IDI conceptual framework.",349.0
Understanding human management of automation errors,c8256a65fe153a564e2d266caa937c05633a8207,"[{'authorId': '39133197', 'name': 'S. McBride'}, {'authorId': '145912604', 'name': 'W. Rogers'}, {'authorId': '1689705', 'name': 'A. Fisk'}]",2014.0,Theoretical Issues in Ergonomics Science,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"Automation has the potential to aid humans with a diverse set of tasks and support overall system performance. Automated systems are not always reliable, and when automation errs, humans must engage in error management, which is the process of detecting, understanding and correcting errors. However, this process of error management in the context of human–automation interaction is not well understood. Therefore, we conducted a systematic review of the variables that contribute to error management. We examined relevant research in human–automation interaction and human error to identify critical automation, person, task and emergent variables. We propose a framework for management of automation errors to incorporate and build upon previous models. Further, our analysis highlights variables that may be addressed through design and training to positively influence error management. Additional efforts to understand the error-management process will contribute to automation designed and implemented to support safe and effective system performance.",46.0
Sustainable Policy Making: A Strategic Challenge for Artificial Intelligence,51587cbaa830869ac8f23d3b7314bd7c767b052a,"[{'authorId': '145063369', 'name': 'M. Milano'}, {'authorId': '1396410265', 'name': 'B. O’Sullivan'}, {'authorId': '1732054', 'name': 'M. Gavanelli'}]",2014.0,The AI Magazine,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"Policy making is an extremely complex process occurring in changing environments and affecting the three pillars of sustainable development: society, economy and the environment. Each political decision in fact implies some form of social reactions, it affects economic and financial aspects and has substantial environmental impacts. Improving decision making in this context could have a huge beneficial impact on all these aspects. There are a number of Artificial Intelligence techniques that could play an important role in improving the policy making process such as decision support and optimization techniques, game theory, data and opinion mining and agent-based simulation. We outline here some potential use of AI technology as it emerged by the European Union (EU) EU FP7 project ePolicy: Engineering the Policy Making Life-Cycle, and we identify some potential research challenges.",21.0
A Model for Information,611c48c7fc52e699d991cf257764221ed79e73dc,"[{'authorId': '2066256739', 'name': 'Paul Walton'}]",2014.0,Inf.,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"This paper uses an approach drawn from the ideas of computer systems modelling to produce a model for information itself. The model integrates evolutionary, static and dynamic views of information and highlights the relationship between symbolic content and the physical world. The model includes what information technology practitioners call “non-functional” attributes, which, for information, include information quality and information friction. The concepts developed in the model enable a richer understanding of Floridi’s questions “what is information?” and “the informational circle: how can information be assessed?” (which he numbers P1 and P12).",121.0
"Data quality for data science, predictive analytics, and big data in supply chain management: An introduction to the problem and suggestions for research and applications",37095b714dad5895d946b1f8435a3a38dee1be8b,"[{'authorId': '2717327', 'name': 'Benjamin T. Hazen'}, {'authorId': '2469374', 'name': 'C. Boone'}, {'authorId': '3431302', 'name': 'Jeremy D. Ezell'}, {'authorId': '1401781163', 'name': 'L. A. Jones-Farmer'}]",2014.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,662.0
The Cambridge Handbook of Artificial Intelligence,dc6de8369905d2feb57aab0c07d08c750670af5c,"[{'authorId': '3719793', 'name': 'Keith Frankish'}, {'authorId': '143913119', 'name': 'W. Ramsey'}]",2014.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"The technology and application of artificial intelligence (AI) throughout society continues to grow at unprecedented rates, which raises numerous legal questions that to date have been largely unexamined. Although AI now plays a role in almost all areas of society, the need for a better understanding of its impact, from legal and ethical perspectives, is pressing, and regulatory proposals are urgently needed. This book responds to these needs, identifying the issues raised by AI and providing practical recommendations for regulatory, technical, and theoretical frameworks aimed at making AI compatible with existing legal rules, principles, and democratic values. An international roster of authors including professors of specialized areas of law, technologists, and practitioners bring their expertise to the interdisciplinary nature of AI.",415.0
Human Performance Consequences of Stages and Levels of Automation,ce1d9a7be127ae29af2483fefc993ae1928b804b,"[{'authorId': '2904379', 'name': 'L. Onnasch'}, {'authorId': '1772836', 'name': 'C. Wickens'}, {'authorId': '119885983', 'name': 'Huiyang Li'}, {'authorId': '11631484', 'name': 'D. Manzey'}]",2014.0,Hum. Factors,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"Objective: We investigated how automation-induced human performance consequences depended on the degree of automation (DOA). Background: Function allocation between human and automation can be represented in terms of the stages and levels taxonomy proposed by Parasuraman, Sheridan, and Wickens. Higher DOAs are achieved both by later stages and higher levels within stages. Method: A meta-analysis based on data of 18 experiments examines the mediating effects of DOA on routine system performance, performance when the automation fails, workload, and situation awareness (SA). The effects of DOA on these measures are summarized by level of statistical significance. Results: We found (a) a clear automation benefit for routine system performance with increasing DOA, (b) a similar but weaker pattern for workload when automation functioned properly, and (c) a negative impact of higher DOA on failure system performance and SA. Most interesting was the finding that negative consequences of automation seem to be most likely when DOA moved across a critical boundary, which was identified between automation supporting information analysis and automation supporting action selection. Conclusion: Results support the proposed cost–benefit trade-off with regard to DOA. It seems that routine performance and workload on one hand, and the potential loss of SA and manual skills on the other hand, directly trade off and that appropriate function allocation can serve only one of the two aspects. Application: Findings contribute to the body of research on adequate function allocation by providing an overall picture through quantitatively combining data from a variety of studies across varying domains.",352.0
"Understanding online behavioural advertising: User knowledge, privacy concerns and online coping behaviour in Europe",91f4d423e807833b7e1be34c8a13d7f309b5041f,"[{'authorId': '2362031', 'name': 'E. Smit'}, {'authorId': '1952444', 'name': 'G. V. Noort'}, {'authorId': '2436561', 'name': 'H. Voorveld'}]",2014.0,Computers in Human Behavior,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,160.0
"Return of the Solow Paradox? It, Productivity, and Employment in U.S. Manufacturing",e54300b186bdd17a643504525795fe15a294daa9,"[{'authorId': '119530058', 'name': 'Daron Acemoglu'}, {'authorId': '6703140', 'name': 'David Autor'}, {'authorId': '2880987', 'name': 'David Dorn'}, {'authorId': '39558634', 'name': 'Gordon H. Hanson'}, {'authorId': '47839261', 'name': 'Brendan M. Price'}]",2014.0,Social Science Research Network,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"An increasingly influential ""technological-discontinuity"" paradigm suggests that IT-induced technological changes are rapidly raising productivity while making workers redundant. This paper explores the evidence for this view among the IT-using U.S. manufacturing industries. There is some limited support for more rapid productivity growth in IT-intensive industries depending on the exact measures, though not since the late 1990s. Most challenging to this paradigm, and our expectations, is that output contracts in IT-intensive industries relative to the rest of manufacturing. Productivity increases, when detectable, result from the even faster declines in employment.",191.0
An Economic Analysis of Online Advertising Using Behavioral Targeting,09418cc96446ba0a1b9c2bbd0014c7b6aa491c49,"[{'authorId': '1740146093', 'name': 'Jianqing Chen'}, {'authorId': '2797657', 'name': 'Jan Stallaert'}]",2010.0,MIS Q.,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"Online publishers and advertisers have recently shown increasing interest in using targeted advertising online. Such targeting allows them to present users with advertisements that are a better match, based on their past browsing and search behavior and other available information (e.g., hobbies registered on a web site). This technique, known as behavioral targeting, has been hailed as the new ""Holy Grail"" in online advertising because of its potential effectiveness. In this paper, we study the economic implications when an online publisher engages in behavioral targeting. The publisher auctions off an advertising slot and is paid on a cost-per-click basis. Using a horizontal differentiation model to capture the fit between a user and an advertisement being displayed, we identify the factors that affect the publisher's revenue, the advertisers' payoffs, and social welfare. We show that revenue for the online publisher in some circumstances can double when behavioral targeting is used. However, increased revenue for the publisher is not guaranteed: in some cases, the prices of advertising and hence the publisher's revenue can be lower, depending on the degree of competition and the advertisers' valuations. We identify two effects associated with behavioral targeting: a competitive effect and a propensity effect. The relative strength of the two effects determines whether the publisher's revenue is positively or negatively affected. We also demonstrate that, although social welfare is increased and small advertisers are better off under behavioral targeting, the dominant advertiser might be worse off and reluctant to switch from traditional advertising.",174.0
"Patient behavior and the benefits of artificial intelligence: the perils of ""dangerous"" literacy and illusory patient empowerment.",4a2c415a5e277f5bac67fe07dc1a8da55321b683,"[{'authorId': '2677434', 'name': 'P. Schulz'}, {'authorId': '102888359', 'name': 'Kent Nakamoto'}]",2013.0,Patient Education and Counseling,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,56.0
Where Not to Eat? Improving Public Policy by Predicting Hygiene Inspections Using Online Reviews,1c9996607c72d574f394122154014d2fc69d4165,"[{'authorId': '9985554', 'name': 'Jun Seok Kang'}, {'authorId': '145592791', 'name': 'Polina Kuznetsova'}, {'authorId': '2342599', 'name': 'Michael Luca'}, {'authorId': '1699545', 'name': 'Yejin Choi'}]",2013.0,Conference on Empirical Methods in Natural Language Processing,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"This paper offers an approach for governments to harness the information contained in social media in order to make public inspections and disclosure more efficient. As a case study, we turn to restaurant hygiene inspections – which are done for restaurants throughout the United States and in most of the world and are a frequently cited example of public inspections and disclosure. We present the first empirical study that shows the viability of statistical models that learn the mapping between textual signals in restaurant reviews and the hygiene inspection records from the Department of Public Health. The learned model achieves over 82% accuracy in discriminating severe offenders from places with no violation, and provides insights into salient cues in reviews that are indicative of the restaurant’s sanitary conditions. Our study suggests that public disclosure policy can be improved by mining public opinions from social media to target inspections and to provide alternative forms of disclosure to customers.",83.0
Artificial intelligence in health - the three big challenges.,f23909dfd5a1a3ea4997dd3042974860e80c4776,"[{'authorId': '35188864', 'name': 'Sankalp Khanna'}, {'authorId': '2064347636', 'name': 'A. Sattar'}, {'authorId': '2064883239', 'name': 'David Hansen'}]",2013.0,Australasian Medical Journal,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"The last twelve months have seen the already constrained Australian health space become an even more complex one. Faced by increasing demand for services, reduced funding and staffing, and pressures imposed by state and federal government health reform agendas, hospital based services are under increasing pressure to become more efficient in how they offer their services. There is a growing need for novel technologies that understand the complexities of hospital operations and offer much needed productivity gains in resource usage and patient service delivery.",14.0
"The potential benefits of using artificial intelligence for monthly rainfall forecasting for the Bowen Basin, Queensland, Australia",3ffd78aa8b07331d41fc9af060a7c0d23c771e1d,"[{'authorId': '46596841', 'name': 'J. Abbot'}, {'authorId': '4989054', 'name': 'J. Marohasy'}]",2013.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"The Bowen Basin contains the largest coal reserves in Australia. Prolonged heavy rainfall during the 2010-2011 wet-season severely affected industry operations with an estimated economic loss of A$5.7 billion (£3.8 billion). There was no explicit warning of the exceptionally wet conditions in the seasonal forecast from the Australian Bureau of Meteorology, which simply suggested a 50-55% probability of above median rainfall for the Bowen Basin. In this study, the value of using neural networks, a form of artificial intelligence, to forecast monthly rainfall for the town of Nebo in the Bowen Basin is explored. Neural networks facilitate the input of multiple climate indices and the exploration of their non-linear relationships. Through genetic optimisations of input variables related to temperatures, including atmospheric temperatures and sea surface temperatures expressed through the Inter-decadal Pacific Oscillation and Nino 3.4, it is possible to develop monthly rainfall forecasts for Nebo superior to the best seasonal forecasts from the Bureau of Meteorology. As neural networks employ far superior technology for exploring the patterns and relationships within historical data including climate indices they are to be preferred.",13.0
The effect of behavioral tracking practices on consumers' shopping evaluations and repurchase intention toward trusted online retailers,7a0dcfe86e3989c16bcbd041d84188e6bfe07bb2,"[{'authorId': '113523279', 'name': 'T. Jai'}, {'authorId': '96609094', 'name': 'L. Burns'}, {'authorId': '31420395', 'name': 'N. King'}]",2013.0,Computers in Human Behavior,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,63.0
Human-Automation Interaction Research,b57fe59722ec261711c400a05e44e6863c7b7b93,"[{'authorId': '143605034', 'name': 'P. Hancock'}, {'authorId': '3324157', 'name': 'R. Jagacinski'}, {'authorId': '3264674', 'name': 'R. Parasuraman'}, {'authorId': '1772836', 'name': 'C. Wickens'}, {'authorId': '2070344456', 'name': 'Glenn F. Wilson'}, {'authorId': '2773724', 'name': 'D. Kaber'}]",2013.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"Scientific research accesses the past to predict the future. The history of science is often best told by those who have lived it. Our purpose is to provide a brief history of human-automation interaction research, including a review of theories for describing human performance with automated systems, an accounting of automation effects on cognitive performance, a description of the origins of adaptive automation and key developments, and an identification of contemporary methods and issues in operator functional state classification. Based on this history and acknowledgements of the state of the art of human-automaton interaction, future predictions are offered.",96.0
Technical Change and the Relative Demand for Skilled Labor: The United States in Historical Perspective,f4fc8f8ced0108af7540c1aeab2e3e1a17c84934,"[{'authorId': '32277840', 'name': 'L. Katz'}, {'authorId': '13400637', 'name': 'R. Margo'}]",2013.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"This paper examines shifts over time in the relative demand for skilled labor in the United States. Although de-skilling in the conventional sense did occur overall in nineteenth century manufacturing, a more nuanced picture is that occupations ""hollowed out"": the share of ""middle-skill"" jobs - artisans - declined while those of ""high-skill"" - white collar, non-production workers - and ""low-skill"" - operatives and laborers increased. De-skilling did not occur in the aggregate economy; rather, the aggregate shares of low skill jobs decreased, middle skill jobs remained steady, and high skill jobs expanded from 1850 to the early twentieth century. The pattern of monotonic skill upgrading continued through much of the twentieth century until the recent ""polarization"" of labor demand since the late 1980s. New archival evidence on wages suggests that the demand for high skill (white collar) workers grew more rapidly than the supply starting well before the Civil War.",179.0
The Impact of IT Capabilities on Firm Performance: The Mediating Roles of Absorptive Capacity and Supply Chain Agility,ef8a2d08c9c30ec72f455a3309f8a324c8928bf7,"[{'authorId': '2748728', 'name': 'Hefu Liu'}, {'authorId': '1771684', 'name': 'Weiling Ke'}, {'authorId': '1792514', 'name': 'K. Wei'}, {'authorId': '145272367', 'name': 'Zhongsheng Hua'}]",2013.0,Decision Support Systems,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"Researchers and practitioners regard information technology (IT) as a competitive tool. However, current knowledge on IT capability mechanisms that affect firm performance remains unclear. Based on the dynamic capabilities perspective and the view of a hierarchy of capabilities, this article proposes a model to examine how IT capabilities (i.e., flexible IT infrastructure and IT assimilation) affect firm performance through absorptive capacity and supply chain agility in the supply chain context. Survey data show that absorptive capacity and supply chain agility fully mediate the influences of IT capabilities on firm performance. In addition to the direct effects, absorptive capacity also has indirect effects on firm performance by shaping supply chain agility. We conclude with implications and suggestions for future research.",558.0
The tangled web we have woven,1fd97f1888314b877d63ee5e8afaa5ff6a868eb4,"[{'authorId': '3132036', 'name': 'Eben Moglen'}]",2013.0,CACM,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,Seeking to protect the fundamental privacy of network interactions.,9.0
Customization of online advertising: The role of intrusiveness,8c141ac391167fc293871e27fd54be88b909988f,"[{'authorId': '46227318', 'name': 'J. V. Doorn'}, {'authorId': '120246572', 'name': 'J. Hoekstra'}]",2013.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,179.0
The “task approach” to labor markets: an overview,b1f4100dbec999e74d6b4bbc3e65d6d97ae2f85e,"[{'authorId': '6703140', 'name': 'David Autor'}]",2013.0,Social Science Research Network,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"An emerging literature argues that changes in the allocation of workplace “tasks” between capital and labor, and between domestic and foreign workers, has altered the structure of labor demand in industrialized countries and fostered employment polarization—that is, rising employment in the highest and lowest paid occupations. Analyzing this phenomenon within the canonical production function framework is challenging, however, because the assignment of tasks to labor and capital in the canonical model is essentially static. This essay sketches an alternative model of the assignment of skills to tasks based upon comparative advantage, reviews key conceptual and practical challenges that researchers face in bringing the “task approach” to the data, and cautions against two common pitfalls that pervade the growing task literature. I conclude with a cautiously optimistic forecast for the potential of the task approach to illuminate the interactions among skill supplies, technological capabilities, and trade and offshoring opportunities, in shaping the aggregate demand for skills, the assignment of skills to tasks, and the evolution of wages.ZusammenfassungIn einer wachsenden Literatur wird die Auffassung vertreten, dass Veränderungen in der Zuweisung von Arbeitsplatzaufgaben (tasks) zwischen Kapital und Arbeit und zwischen in- und ausländischen Arbeitskräften die Struktur der Arbeitskräftenachfrage in den Industrieländern verändert und eine Polarisierung von Beschäftigung gefördert hat – d.h. steigende Beschäftigungszahlen in den best- und schlechtestbezahlten Berufen. Eine Analyse dieses Phänomens im Rahmen der gängigen Produktionsfunktion ist jedoch schwierig, da die Zuweisung von Aufgaben zu Arbeit und Kapital in diesem Modell im Wesentlichen statisch ist. Dieses Essay skizziert ein Alternativmodell zur Zuweisung von Kompetenzen zu Aufgaben basierend auf dem komparativem Vorteil, bespricht wichtige konzeptionelle und praktische Schwierigkeiten, vor denen Forscher stehen, wenn sie den „TASKS-Ansatz“ in Daten übertragen wollen und warnt vor zwei gängigen Fallstricken, die die immer stärker wachsende Literatur zu diesem Thema durchziehen. Ich schließe mit einer vorsichtig optimistischen Vorhersage für die Möglichkeiten des TASKS-Ansatzes zur Erläuterung der Interaktionen zwischen Angebot an Kompetenzen, technologischem Potenzial und Handels- und Offshoringmöglichkeiten in der Gestaltung der aggregierten Nachfrage nach Kompetenzen, der Zuweisung von Kompetenzen zu Aufgaben und der Entwicklung von Löhnen.",419.0
The Fourth Industrial Revolution,31daf7c02547fe4e8faf6c96a2a54278c2d5ba8a,"[{'authorId': '37070123', 'name': 'K. Schwab'}]",2013.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,4080.0
Big Data Analytics,73fffcbf62acf0bc7968961a7241dc6b8a00eea3,"[{'authorId': '1620462567', 'name': 'Melnned M. Kantardzic'}]",2013.0,Lecture Notes in Computer Science,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,1354.0
Intelligent Learning Technologies: Applications of Artificial Intelligence to Contemporary and Emerging Educational Challenges,5b9cbb2079a58266e59a7362918e23e7126957ab,"[{'authorId': '2810613', 'name': 'V. Chaudhri'}, {'authorId': '144445231', 'name': 'H. Lane'}, {'authorId': '2121780', 'name': 'David Gunning'}, {'authorId': '1802395', 'name': 'J. Roschelle'}]",2013.0,The AI Magazine,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"10 AI MAGAZINE Copyright © 2013, Association for the Advancement of Artificial Intelligence. All rights reserved. ISSN 0738-4602 It is a time of remarkable transformation for education. Everyone recognizes the need to improve teacher effectiveness, to improve student engagement, and to create a twenty-first century education system that maximizes potential of every student. The challenges that must be addressed to make these improvements greatly exceed the scope of any single approach, whether it is educational technology, improved teacher training and better after school programs, and so on. In past research, AI — with its inextricable links to cognitive science, psychology, and mathematics — has proven a close fit for many of these challenging educational problems. Educators have long known that the most effective teaching method is one-on-one tutoring. Ever since Benjamin Bloom’s famous study,1 education researchers have aspired to mimic the holy grail of one-on-one tutoring — to achieve a two-sigma improvement in student learning. Early AI researchers saw this as an opportunity to build intelligent tutoring systems (ITSs) that could adapt and tailor instruction to the individual needs of the student. Although today’s systems fall short of the full two-sigma effect of a human tutor (roughly equivalent to two grade levels), intelligent tutors have demonstrated remarkable progress in that direction. In fact, researchers have struggled to replicate the twosigma effect suggesting that ITSs may already be as effective as human tutors. This suggests that the contributions of AI to education are perhaps more profound than previously believed and leads us to wonder why AI-based learning techIntroduction to the Special Articles in the Fall and Winter Issues",19.0
Society under threat… but not from AI,42682a75776f9cc3a51ad8e22d2389f2ee388548,"[{'authorId': '145341816', 'name': 'A. Narayanan'}]",2013.0,Ai & Society,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,4.0
A Review of Protocol Implementations and Energy Efficient Cross-Layer Design for Wireless Body Area Networks,87f6ddbfef6c3664483bde882066afabe10dc0ba,"[{'authorId': '2098810803', 'name': 'D. L. Hughes'}, {'authorId': '48631815', 'name': 'Xinheng Wang'}, {'authorId': '2118213606', 'name': 'Tao Chen'}]",2012.0,Italian National Conference on Sensors,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"The issues inherent in caring for an ever-increasing aged population has been the subject of endless debate and continues to be a hot topic for political discussion. The use of hospital-based facilities for the monitoring of chronic physiological conditions is expensive and ties up key healthcare professionals. The introduction of wireless sensor devices as part of a Wireless Body Area Network (WBAN) integrated within an overall eHealth solution could bring a step change in the remote management of patient healthcare. Sensor devices small enough to be placed either inside or on the human body can form a vital part of an overall health monitoring network. An effectively designed energy efficient WBAN should have a minimal impact on the mobility and lifestyle of the patient. WBAN technology can be deployed within a hospital, care home environment or in the patient's own home. This study is a review of the existing research in the area of WBAN technology and in particular protocol adaptation and energy efficient cross-layer design. The research reviews the work carried out across various layers of the protocol stack and highlights how the latest research proposes to resolve the various challenges inherent in remote continual healthcare monitoring.",71.0
Navigating foresight in a sea of expectations: lessons from the sociology of expectations,d38ed57d5016d28c6927526ed054a18b35097e48,"[{'authorId': '114886524', 'name': 'H. van Lente'}]",2012.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"Foresight can be described as the articulation of possible futures. It has a range of applications and is used with different methods, for different objectives and in different settings. Yet, anticipation in science and technology is not limited to foresight, but occurs in many more informal ways. This paper investigates the phenomenon that socio-technical developments are saturated with formal and informal anticipations and discusses the implications of this condition for foresight. The range of foresight studies is reviewed as well as the main results of the sociology of expectations, which studies the informal production and circulation of expectations in science and technology. Finally, three generic lessons from the sociology of expectations are derived, and it is discussed how these support or limit the ambitions of foresight.",49.0
Motivating Salespeople: What Really Works,75e889ba7ba6d8e6cfef1d602e1099ecab2870ae,"[{'authorId': '1740616', 'name': 'Thomas J. Steenburgh'}, {'authorId': '9957285', 'name': 'M. Ahearne'}]",2012.0,Harvard Business Review,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"No sales force consists entirely of stars; sales staffs are usually made up mainly of solid perfomers, with smaller groups of laggards and rainmakers. Though most compensation plans approach these three groups as if they were the same, research shows that each is motivated by something different. By accounting for those differences in their incentive programs, companies can coax better performance from all their salespeople. As the largest cadre, core performers typically represent the greatest opportunity, but they're often ignored by incentive plans. Contests with prizes that vary in nature and value (and don't all go to stars) will inspire them to ramp up their efforts, and tiered targets will guide them up the performance curve. Laggards need quarterly bonuses to stay on track; when they have only annual bonuses, their revenues will drop 10%, studies show. This group is also motivated by social pressure-especially from new talent on the sales bench. Stars tend to get the most attention in comp plans, but companies often go astray by capping their commissions to control costs. If firms instead remove commission ceilings and pay extra for overachievement, they'll see the sales needle really jump. The key is to treat sales compensation not as an expense to rein in but as a portfolio of investments to manage. Companies that do this will be rewarded with much higher returns.",502.0
"Absorptive Capacity and Information Systems Research: Review, Synthesis, and Directions for Future Research",ce82832787f6e80991f592794f85608914273b37,"[{'authorId': '2123431', 'name': 'Nicholas H. Roberts'}, {'authorId': '3291173', 'name': 'Pamela S. Galluch'}, {'authorId': '12597106', 'name': 'M. Dinger'}, {'authorId': '144894714', 'name': 'V. Grover'}]",2012.0,MIS Q.,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"Absorptive capacity is a firm's ability to identify, assimilate, transform, and apply valuable external knowledge. It is considered an imperative for business success. Modern information technologies perform a critical role in the development and maintenance of a firm's absorptive capacity. We provide an assessment of absorptive capacity in the information systems literature. IS scholars have used the absorptive capacity construct in diverse and often contradictory ways. Confusion surrounds how absorptive capacity should be conceptualized, its appropriate level of analysis, and how it can be measured. Our aim in reviewing this construct is to reduce such confusion by improving our understanding of absorptive capacity and guiding its effective use in IS research. We trace the evolution of the absorptive capacity construct in the broader organizational literature and pay special attention to its conceptualization, assumptions, and relationship to organizational learning. Following this, we investigate how absorptive capacity has been conceptualized, measured, and used in IS research. We also examine how absorptive capacity fits into distinct IS themes and facilitates understanding of various IS phenomena. Based on our analysis, we provide a framework through which IS researchers can more fully leverage the rich aspects of absorptive capacity when investigating the role of information technology in organizations.",503.0
Stay Away From Me,cdf577fb75bc2ba72b447ad0d92c510b10d4172b,"[{'authorId': '49510038', 'name': 'T. Baek'}, {'authorId': '2257551', 'name': 'Mariko Morimoto'}]",2012.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"This study attempts to identify the potential determinants of advertising avoidance in the context of personalized advertising media, including unsolicited commercial e-mail, postal direct mail, telemarketing, and text messaging. Using a self-administered survey (n = 442), the proposed model is tested with structural equation modeling analysis. The findings indicate that while ad skepticism partially mediates the relationship between ad avoidance and its three determinants (perceived personalization, privacy concerns, and ad irritation), both privacy concerns and ad irritation have a direct positive effect on ad avoidance. However, increased perceived personalization leads directly to decreased ad avoidance.",317.0
The growth of Diaspora - A decentralized online social network in the wild,f58fc6179286fd2aa5786a43d0f6879bfa0b62f2,"[{'authorId': '2595218', 'name': 'Ames Bielenberg'}, {'authorId': '1925360', 'name': 'Lara Helm'}, {'authorId': '35569143', 'name': 'Anthony Gentilucci'}, {'authorId': '145671780', 'name': 'D. Stefanescu'}, {'authorId': '2143622751', 'name': 'Honggang Zhang'}]",2012.0,2012 Proceedings IEEE INFOCOM Workshops,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"The Diaspora network [1] is a recently launched decentralized online social network with over 216; 000 users as of November 16; 2011. It is a network of independent, federated Diaspora servers that are administrated by individual users who allow Diaspora users' profiles to be hosted on their servers. In this paper we take a first look at the Diaspora network's overall growth in terms of number of users, the topology of its interconnected servers, and the reliability of those servers. We also present a simple analysis to explain the growth of the Diaspora network. Our timely measurement study of a real-world decentralized online social network sheds light on the evolution of such a network in practice, and provides valuable observations and insights that can help the future design and implementation of decentralized online social networking.",89.0
Adopters and non‐adopters of internet banking: a segmentation study,59a5945b5f358d90120939bcf74a4c3688e73cea,"[{'authorId': '119505679', 'name': 'Athanasios Patsiotis'}, {'authorId': '144616136', 'name': 'T. Hughes'}, {'authorId': '38895112', 'name': 'D. Webber'}]",2012.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"Purpose – This study examines internet banking adoption and resistance behaviour in Greece in order to develop profiles of adopters and non‐adopters of the service. The aim is to illustrate customers' resistance behaviour towards internet banking. The existing research does not explain resistance behaviour, since it does not clearly distinguish non‐adoption from resistance. Consequently, it has not recognised the different types of non‐adoption.Design/methodology/approach – A measuring instrument was developed and utilised in a survey of a convenience sample of 1,200 customers. The derived dimensionality of the relevant perceptual variables was used to explore the existence of different customer segments through cluster analysis.Findings – Three segments were identified, where the description of their profiles is based on customer perceptions of the service and general usage data. Across these segments adopters and non‐adopters were found to have different characteristics. With regard to demographics, onl...",84.0
Application of Artificial Intelligence and Data Mining Techniques to Financial Markets,79ee3317269c3916f9fcf72fba8f0a5f6b02e178,"[{'authorId': '118619039', 'name': 'Katarína Híľovská'}, {'authorId': '2890579', 'name': 'P. Koncz'}]",2012.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"The aim of artificial intelligence is to discover mechanisms of adaptation in a changing environment with utilisation of intelligence, for instance in the ability to exclude unlikely solutions. Artificial intelligence methods have extensive application in different fields such as medicine, games, transportation, or heavy industry. This paper deals with interdisciplinary issues – interconnection of artificial intelligence and finance. The paper briefly describes techniques of data mining, expert systems and agent based computation intelligence and specifies the types of tasks solved by these techniques in the context of financial tasks. It provides deeper insight into potential usage of intelligent systems on financial markets.",15.0
Singularity Hypotheses: An Overview,c41371fd189d40ee4af8061c7b7c11dd03caabec,"[{'authorId': '2445597', 'name': 'A. Eden'}, {'authorId': '143925399', 'name': 'E. Steinhart'}, {'authorId': '2067902151', 'name': 'David Pearce'}, {'authorId': '31925555', 'name': 'J. Moor'}]",2012.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,27.0
Accounting for Tastes,f2cf8942689425cc46422aca29033d3ef6491d5b,"[{'authorId': '143721102', 'name': 'F. Vogt'}, {'authorId': '3062983', 'name': 'D. Schwappach'}, {'authorId': '144792072', 'name': 'J. Bridges'}]",2012.0,PharmacoEconomics (Auckland),"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,294.0
Making advanced analytics work for you.,22f305b0b6d0fb4ca4eb68c092b9cf6f387aa172,"[{'authorId': '40495620', 'name': 'Dominic Barton'}, {'authorId': '98727007', 'name': 'David Court'}]",2012.0,Harvard Business Review,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"Senior leaders who write off the move toward big data as a lot of big talk are making, well, a big mistake. So argue McKinsey's Barton and Court, who worked with dozens of companies to figure out how to translate advanced analytics into nuts-and-bolts practices that affect daily operations on the front lines. The authors offer a useful guide for leaders and managers who want to take a deliberative approach to big data-but who also want to get started now. First, companies must identify the right data for their business, seek to acquire the information creatively from diverse sources, and secure the necessary IT support. Second, they need to build analytics models that are tightly focused on improving performance, making the models only as complex as business goals demand. Third, and most important, companies must transform their capabilities and culture so that the analytical results can be implemented from the C-suite to the front lines. That means developing simple tools that everyone in the organization can understand and teaching people why the data really matter. Embracing big data is as much about changing mind-sets as it is about crunching numbers. Executed with the right care and flexibility, this cultural shift could have payoffs that are, well, bigger than you expect.",429.0
Machine Learning in Action,be9811f7e6019d5cd59ff97829a44bb5577bab00,"[{'authorId': '145785408', 'name': 'P. Harrington'}]",2012.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"SummaryMachine Learning in Action is unique book that blends the foundational theories of machine learning with the practical realities of building tools for everyday data analysis. You'll use the flexible Python programming language to build programs that implement algorithms for data classification, forecasting, recommendations, and higher-level features like summarization and simplification. About the BookA machine is said to learn when its performance improves with experience. Learning requires algorithms and programs that capture data and ferret out the interesting or useful patterns. Once the specialized domain of analysts and mathematicians, machine learning is becoming a skill needed by many.Machine Learning in Action is a clearly written tutorial for developers. It avoids academic language and takes you straight to the techniques you'll use in your day-to-day work. Many (Python) examples present the core algorithms of statistical data processing, data analysis, and data visualization in code you can reuse. You'll understand the concepts and how they fit in with tactical tasks like classification, forecasting, recommendations, and higher-level features like summarization and simplification.Readers need no prior experience with machine learning or statistical processing. Familiarity with Python is helpful.Purchase includes free PDF, ePub, and Kindle eBooks downloadable at manning.com. What's InsideA no-nonsense introduction Examples showing common ML tasks Everyday data analysis Implementing classic algorithms like Apriori and Adaboos=================================== Table of ContentsPART 1 CLASSIFICATION Machine learning basics Classifying with k-Nearest Neighbors Splitting datasets one feature at a time: decision trees Classifying with probability theory: nave Bayes Logistic regression Support vector machines Improving classification with the AdaBoost meta algorithm PART 2 FORECASTING NUMERIC VALUES WITH REGRESSION Predicting numeric values: regression Tree-based regression PART 3 UNSUPERVISED LEARNING Grouping unlabeled items using k-means clustering Association analysis with the Apriori algorithm Efficiently finding frequent itemsets with FP-growth PART 4 ADDITIONAL TOOLS Using principal component analysis to simplify data Simplifying data with the singular value decomposition Big data and MapReduce",492.0
Predicting and Preventing Shootings among At-Risk Youth,f5847947bb4bf85b31b1b97bb30fd583de25c525,"[{'authorId': '120094192', 'name': 'D. Chandler'}, {'authorId': '37180493', 'name': 'Steven D. Levitt'}, {'authorId': '1798259', 'name': 'J. List'}]",2011.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"Each year, more than 250 students in the Chicago Public Schools (CPS) are shot. The authors of this paper worked with the leadership of CPS to build a predictive model of shootings that helped determine which students would be included in a highly targeted and resource intensive mentorship program. This paper describes our predictive model and offers a preliminary evaluation of the mentoring intervention performed by Youth Advocate Programs, Inc. (YAP). We find little evidence that the intervention reduces school misconducts or improves educational outcomes. The scale of intervention was too small to generate meaningful findings on shootings.",43.0
Sleights of mind: What the neuroscience of magic reveals about our everyday deceptions,081b034d3fbe17d1445dd30015c2f75622ac8c85,"[{'authorId': '34040188', 'name': 'Gustav Kuhn'}]",2011.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"Have you ever wondered how magicians are able to make things disappear in front of your eyes? It may come as little surprise to the readers of this journal that these feats are carried out without supernatural powers. Instead, magicians use a wide range of powerful psychological techniques to manipulate perception and awareness. Sleights of Mind: What the Neuroscience of Magic Reveals about Our Everyday Deceptions is written by Stephen Macknik and Susana Martinez-Conde, two well-established neuroscientists working at the Barrow Neurological Institute in Phoenix. Over the last three years, they have formed links with renowned magicians and have tried to explain magic tricks in terms of psychological and neurobiological processes. In this book, the authors describe their journey into the secretive world of magic. By describing some of the magicians’ techniques and revealing many (perhaps too many) of their secrets, the authors try to explain some of the psychological and neurobiological processes involved in everyday cognition. 
 
Sleights of Mind was preceded by journal publications in which the authors explored some of the links between magic and neuroscience (1). Their thesis is that scientists can learn much from magicians and that doing so may advance neuroscience by decades. Sleights of Mind is an extension of this, and in the book they declare themselves the founders of a new discipline they call neuromagic. This is a surprise, given that the authors, who head two large neuroscience research groups, have in fact not published any empirical work that has been inspired by magic. This presents the central flaw with this text: although the term neuromagic may be new, the authors’ claims of having founded this field of science do not reflect their own level of research in this area or pay due recognition to the established body of work that precedes this book (2). 
 
That said, the general topic of magic and neuroscience will be of interest to a fairly wide readership. The main substance of the book is based on interviews with some of the world’s most renowned magicians, who offer great insights into their rather secretive world. While some magicians will be perturbed by the way in which many well-kept secrets are revealed, the depths and details of these methods will provide an easy thrill for the nonmagician. For example, professional pickpocket Apollo Robbins offers intriguing insights into how he misdirects his marks’ attention in order to steal their possessions. Similarly, Teller describes one of the classics in magic, the “Miser’s Dream,” in which the magician plucks countless coins out of thin air and illustrates how magicians manipulate your assumptions about the world. Most of the examples are then followed by descriptions of everyday cognitive processes. The authors use the principle of misdirection to describe the way in which the visual system systematically selects the relevant information, resulting in a rather impoverished representation of the world, which leaves much scope for deception. While the book is valuable for its descriptions, many of the conclusions seem rather speculative and are rarely backed up by scientific evidence. 
 
The authors have immersed themselves in the world of magic and should be commended for their collaborations with eminent and insightful magicians. While the authors place themselves at the front of a “new” scientific field, they neglect both past and current research concerning the scientific study of magic. They fail to acknowledge that scientists as far back as the 1900s have explored the psychological mechanisms involved in conjuring (3). In addition, many of the concepts covered by this book are psychological rather than neurobiological, and the neuroscience used to describe magic often feels imposed. Given that the authors are active scientists, it is also perplexing that numerous recent scientific findings have been missed. For example, in the chapter on misdirection, the authors talk about how magicians use social cues (e.g., where they are looking) to misdirect people’s attention, but fail to mention research that has already investigated these issues (4). Similarly, they raise the possibility that individuals with autism who have difficulties in processing social information may be less readily misdirected and therefore more likely to detect the magician’s secret, without discussing work that has investigated these questions (5). 
 
Sleights of Mind offers interesting insights into the tricks and techniques used by magicians that will be of interest to many readers. Moreover, the use of magic as a vehicle to describe cognitive and neurological processes generally works well. However, I was disappointed by the presumptuous and unsubstantiated accounts and claims within the book. As the authors fail to propose a clear framework for their idea of a new “neuromagic” and do not present any of their own neuroscientific contributions to the field, this book should be taken as a personal account rather than the birth of a new scientific discipline.",76.0
Fuzzy Support Vector Machine for bankruptcy prediction,931715a963a76b5c2276c4d898b429414d26a9e1,"[{'authorId': '2024834167', 'name': 'A. Chaudhuri'}, {'authorId': '34299738', 'name': 'Kajal De'}]",2011.0,Applied Soft Computing,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,159.0
"Thinking, Fast and Slow",b8573a2585db1164fe4a528a300750e97768fd8d,"[{'authorId': '3683465', 'name': 'D. Kahneman'}]",2011.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"Daniel Kahneman, recipient of the Nobel Prize in Economic Sciences for his seminal work in psychology challenging the rational model of judgment and decision making, is one of the world's most important thinkers. His ideas have had a profound impact on many fields - including business, medicine, and politics - but until now, he has never brought together his many years of research in one book. In ""Thinking, Fast and Slow"", Kahneman takes us on a groundbreaking tour of the mind and explains the two systems that drive the way we think and make choices. One system is fast, intuitive, and emotional; the other is slower, more deliberative, and more logical. Kahneman exposes the extraordinary capabilities - and also the faults and biases - of fast thinking, and reveals the pervasive influence of intuitive impressions on our thoughts and behaviour. The importance of properly framing risks, the effects of cognitive biases on how we view others, the dangers of prediction, the right ways to develop skills, the pros and cons of fear and optimism, the difference between our experience and memory of events, the real components of happiness - each of these can be understood only by knowing how the two systems work together to shape our judgments and decisions. Drawing on a lifetime's experimental experience, Kahneman reveals where we can and cannot trust our intuitions and how we can tap into the benefits of slow thinking. He offers practical and enlightening insights into how choices are made in both our professional and our personal lives-and how we can use different techniques to guard against the mental glitches that often get us into trouble. ""Thinking, Fast and Slow"" will transform the way you take decisions and experience the world.",9027.0
Reassessing Data Quality for Information Products,c948aa121c43857f9478e5c0b7e25d31fdc8358d,"[{'authorId': '39191500', 'name': 'Debabrata Dey'}, {'authorId': '1739153328', 'name': 'Subodha Kumar'}]",2010.0,Management Sciences,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"The quality profile of information retrieved from a database using a query is quite important in the context of managerial decision making. Parssian et al. (Parssian, A., S. Sarkar, V. S. Jacob. 2004. Assessing data quality for information products: Impact of selection, projection, and Cartesian product. Management Sci.50(7) 967--982) propose a methodology to determine the quality profile of the result of a query from the quality profile of the source data. Although they consider an important problem, and the proposed methodology is quite useful in practice, some of their results for the selection operation are not correct in general. Here, we identify these errors and present appropriate corrections.",18.0
Sleights of Mind,7c0531fc7bcfcf875bf6f62a3f5d7c73fdbe5086,"[{'authorId': '1412340548', 'name': 'S. Martinez-Conde'}, {'authorId': '1969116', 'name': 'S. Macknik'}, {'authorId': '4029720', 'name': 'S. Blakeslee'}]",2010.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,24.0
Using ICTs to create a culture of transparency: E-government and social media as openness and anti-corruption tools for societies,f8c69613a6bb0490d624de8677e13e9ba5f7e254,"[{'authorId': '3005122', 'name': 'J. Bertot'}, {'authorId': '143762896', 'name': 'P. Jaeger'}, {'authorId': '25943188', 'name': 'J. Grimes'}]",2010.0,Government Information Quarterly,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,1905.0
Complacency and Bias in Human Use of Automation: An Attentional Integration,b0e5a85803bb959ed2cbd47c51009cc48059c02c,"[{'authorId': '3264674', 'name': 'R. Parasuraman'}, {'authorId': '11631484', 'name': 'D. Manzey'}]",2010.0,Hum. Factors,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"Objective: Our aim was to review empirical studies of complacency and bias in human interaction with automated and decision support systems and provide an integrated theoretical model for their explanation. Background: Automation-related complacency and automation bias have typically been considered separately and independently. Methods: Studies on complacency and automation bias were analyzed with respect to the cognitive processes involved. Results: Automation complacency occurs under conditions of multiple-task load, when manual tasks compete with the automated task for the operator’s attention. Automation complacency is found in both naive and expert participants and cannot be overcome with simple practice. Automation bias results in making both omission and commission errors when decision aids are imperfect.Automation bias occurs in both naive and expert participants, cannot be prevented by training or instructions, and can affect decision making in individuals as well as in teams.While automation bias has been conceived of as a special case of decision bias, our analysis suggests that it also depends on attentional processes similar to those involved in automation-related complacency. Conclusion: Complacency and automation bias represent different manifestations of overlapping automation-induced phenomena, with attention playing a central role. An integrated model of complacency and automation bias shows that they result from the dynamic interaction of personal, situational, and automation-related characteristics. Application: The integrated model and attentional synthesis provides a heuristic framework for further research on complacency and automation bias and design options for mitigating such effects in automated and decision support systems.",780.0
"Not “human” enough to be human but not “animal” enough to be animal – the case of the HFEA, cybrids and xenotransplantation in the UK",3700723c7adbc8dd9bb3720eb6991e24bc6cfd0e,"[{'authorId': '2339382', 'name': 'G. Haddow'}, {'authorId': '4799606', 'name': 'A. Bruce'}, {'authorId': '47020246', 'name': 'J. Calvert'}, {'authorId': '3262740', 'name': 'S. Harmon'}, {'authorId': '144989992', 'name': 'W. Marsden'}]",2010.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"Innovations in scientific and medical technologies, such as xenotransplantation and admixed embryos, invariably become the target of regulatory agencies and often demand new regulatory frameworks. In making decisions associated with these innovations, it is sometimes necessary for regulators to adopt certain positions about the status and significance of the human–animal embryo or body. In the UK, the regulatory and advisory bodies involved in the sphere of human/non-human transfer and exchange of material are: (1) the Human Fertilisation and Embryology Authority (HFEA); (2) the (defunct) UK Xenotransplantation Interm Regulatory Authority; and (3) the Home Office's Animal Procedures Committee (APC). In this article, we critically examine the reasons for the HFEA's involvement in regulating and advising in research which uses admixed embryos, given that the HFEA's remit is the government's fertility watchdog regulating in the area of human embryos. This expansion, we argue, was partly due to pressure from pro-cybrid supporters and the need to fill an institutional void left by the decommissioning of UKXIRA. Ironically, specific institutions such as UKXIRA may have been better placed to deal with animal–human fusions.",21.0
"Artificial intelligence for diagnostic purposes: principles, procedures and limitations",3ac8c15411b6bb061e3c262f3a3451cc775048f6,"[{'authorId': '52310864', 'name': 'T. Cleophas'}, {'authorId': '32500766', 'name': 'Toine Cleophas'}]",2010.0,Clinical Chemistry and Laboratory Medicine,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"Abstract Background: Back propagation (BP) artificial neural networks are a distribution-free method for data analysis based on layers of artificial neurons that transduce imputed information. It has been recognized as having a number of advantages compared to traditional methods including the possibility to process imperfect data, and complex non-linear data. The objective of this study was to review the principles, procedures, and limitations of BP artificial neural networks for a non-mathematical readership. Methods: A real data sample of weight, height and measured body surface area from 90 individuals was used as an example. SPSS 17.0 with neural network add-on was used for the analysis. The predicted body surface from a two hidden layer BP neural network was compared to the body surface calculated by the Haycock equation. Results: Both the predicted values from the neural network and from the Haycock equation were close to the measured values. A linear regression analysis with neural network as predictor produced an r2-value of 0.983, while the Haycock equation produced an r2-value of 0.995 (r2>0.95 is a criterion for accurate diagnostic testing). Conclusions: BP neural networks may, sometimes, predict clinical diagnoses with accuracies similar to those of other methods. However, traditional statistical procedures, such as regression analyses need to be added for testing their accuracies against alternative methods. Nonetheless, BP neural networks have great potential through their ability to learn by example instead of learning by theory. Clin Chem Lab Med 2010;48:159–65.",27.0
Cultural distance as a determinant of bilateral trade flows: do immigrants counter the effect of cultural differences?,bb029cd396e0f2c6177cba5a7846b52c4b2e416a,"[{'authorId': '6106567', 'name': 'B. Tadesse'}, {'authorId': '116804729', 'name': 'R. White'}]",2010.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"We introduce ‘cultural distance’ as a measure of the degree to which shared norms and values in one country differ from those in another country, and employ a modified gravity specification to examine whether such cultural differences affect the volume of trade flows. Employing data for US state-level exports to the 75 trading partners for which measures of cultural distance can be constructed, we find that greater cultural differences between the United States and a trading partner reduces state-level exports to that country. This result holds for aggregate exports, cultural and noncultural products exports as well, but with significantly different magnitudes. Immigrants are found to exert a pro-export effect that partially offsets the trade-inhibiting effects of cultural distance.",119.0
Profile of IS research published in the European Journal of Information Systems,df8a032683e60b7dd75d53afa0e108ba8bb705af,"[{'authorId': '1724490', 'name': 'Yogesh Kumar Dwivedi'}, {'authorId': '9352705', 'name': 'J. Kuljis'}]",2008.0,European Journal of Information Systems,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,114.0
"Data Wealth, Data Poverty, Science and Cyberinfrastructure 1",28b73be9811d9357f404446c10230c9b272d3d58,"[{'authorId': '144245984', 'name': 'S. Sawyer'}]",2008.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"Abstract Changes in access to data are leading to rapid ‘data wealth’ in some scientific fields, even as others remain ‘data‐poor’. Furthermore, the current attention towards developing computer‐based infrastructures and digital access to common data sets—the basics of scientific ‘cyberinfrastructures’—are too‐focused on fields of study characterized by data wealth. To better understand the implications of this twin pursuit of data wealth and cyberinfrastructure, I articulate how data‐poor scholarly fields differ from data‐rich fields. I then suggest four actions that scholars in data‐poor fields can take to improve their work’s value to science and society in lieu of being data‐rich and propose three design considerations for cyberinfrastructures that can better support data‐poor scholarly endeavors.",20.0
On the Role Played by Temporary Geographical Proximity in Knowledge Transmission,4b2fc32653e0c00859ef788c88579502453deaaf,"[{'authorId': '32188210', 'name': 'A. Torre'}]",2008.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"Torre A. On the role played by temporary geographical proximity in knowledge transmission, Regional Studies. This paper defends the thesis that geographical proximity remains essential for knowledge transfer, but not often implies the co-location of innovation and research activities. The need for geographical proximity now mostly affects certain stages of the process of production, research or development. Short- or medium-term visits are often sufficient for the partners to exchange the information needed for cooperation. The mobility of individuals makes it possible to implement this mechanism. Temporary geographical proximity implies a strong relation to space, but one that differs in nature from that described by the traditional approaches. Torre A. Rôle de la proximité géographique temporaire dans la transmission de la connaissance, Regional Studies. Dans cet article, nous défendons la thèse selon laquelle la proximité géographique demeure essentielle au transfert des connaissances mais qu'elle n'implique pas souvent la co-localisation d'activités d'innovation et de recherche. La nécessité de la proximité géographique affecte surtout, aujourd'hui, certaines étapes des processus de production, de recherche et de développement. Les visites à court ou moyen terme suffisent souvent aux partenaires pour échanger des informations nécessaires à leur coopération. La mobilité des individus permet de mettre en œuvre ce mécanisme. La proximité géographique temporaire induit une forte relation à l'espace mais une relation qui diffère en nature de celle qui est décrite par les approches classiques. Proximité géographique Proximité organisée Ubiquité Agrégats Torre A. Die Rolle der vorübergehenden geografischen Nähe zur Wissensübertragung, Regional Studies. In diesem Artikel verteidigen wir die These, dass eine geografische Nähe zur Wissensübertragung nach wie vor unverzichtbar ist, aber oft keinen gemeinsamen Standort der Innovations- und Forschungsarbeit voraussetzt. Die Notwendigkeit einer geografischen Nähe betrifft heute meistens bestimmte Phasen im Produktions-, Forschungs- oder Entwicklungsprozess. Oft sind kurze oder mittellange Besuche für die Partner ausreichend, um die für eine Zusammenarbeit benötigten Informationen auszutauschen. Die Mobilität der einzelnen Personen macht eine Umsetzung dieses Mechanismus möglich. Eine vorübergehende geografische Nähe setzt eine enge Verbindung zum Raum voraus, deren Beschaffenheit jedoch von den Beschreibungen der traditionellen Ansätze abweicht. Geografische Nähe Organisierte Nähe Ubiquität Cluster Torre A. El papel desempeñado por la proximidad geográfica temporal en la transmisión de conocimiento, Regional Studies. En este artículo defendemos la tesis de que la proximidad geográfica sigue siendo un factor fundamental para la transferencia de conocimientos aunque esto no suele implicar la ubicación conjunta de las actividades de innovación y las de investigación. La necesidad de proximidad geográfica ahora afecta sobre todo a ciertas fases del proceso de producción, investigación y desarrollo. Las visitas a corto o medio plazo son con frecuencia suficientes para que los socios intercambien la información que necesitan para cooperar. La movilidad de los individuos facilita la aplicación de este mecanismo. La proximidad geográfica temporal entraña una estrecha relación en el espacio pero que difiere en naturaleza de la que se describe en enfoques tradicionales. Proximidad geográfica Proximidad organizada Ubicuidad Agrupaciones",617.0
Humans: Still Vital After All These Years of Automation,8a5a4544e2faaa90dc2c2c561e5a5717c72f48c6,"[{'authorId': '3264674', 'name': 'R. Parasuraman'}, {'authorId': '1772836', 'name': 'C. Wickens'}]",2008.0,Hum. Factors,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"Objective: The authors discuss empirical studies of human-automation interaction and their implications for automation design. Background: Automation is prevalent in safety-critical systems and increasingly in everyday life. Many studies of human performance in automated systems have been conducted over the past 30 years. Methods: Developments in three areas are examined: levels and stages of automation, reliance on and compliance with automation, and adaptive automation. Results: Automation applied to information analysis or decision-making functions leads to differential system performance benefits and costs that must be considered in choosing appropriate levels and stages of automation. Human user dependence on automated alerts and advisories reflects two components of operator trust, reliance and compliance, which are in turn determined by the threshold designers use to balance automation misses and false alarms. Finally, adaptive automation can provide additional benefits in balancing workload and maintaining the user's situation awareness, although more research is required to identify when adaptation should be user controlled or system driven. Conclusions: The past three decades of empirical research on humans and automation has provided a strong science base that can be used to guide the design of automated systems. Application: This research can be applied to most current and future automated systems.",384.0
Scene completion using millions of photographs,edd5771531fe1f29a2ac60d8b5388e2a50944453,"[{'authorId': '48966748', 'name': 'James Hays'}, {'authorId': '1763086', 'name': 'Alexei A. Efros'}]",2007.0,Communications of the ACM,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"What can you do with a million images? In this paper, we present a new image completion algorithm powered by a huge database of photographs gathered from the Web. The algorithm patches up holes in images by finding similar image regions in the database that are not only seamless, but also semantically valid. Our chief insight is that while the space of images is effectively infinite, the space of semantically differentiable scenes is actually not that large. For many image completion tasks, we are able to find similar scenes which contain image fragments that will convincingly complete the image. Our algorithm is entirely data driven, requiring no annotations or labeling by the user. Unlike existing image completion methods, our algorithm can generate a diverse set of image completions and we allow users to select among them. We demonstrate the superiority of our algorithm over existing image completion approaches.",1075.0
Machine Ethics: Creating an Ethical Intelligent Agent,a2f8c2da5abf99e517c65efc55fd10a28cef7770,"[{'authorId': '145012477', 'name': 'Michael Anderson'}, {'authorId': '2120952', 'name': 'S. Anderson'}]",2007.0,The AI Magazine,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"The newly emerging field of machine ethics (Anderson and Anderson 2006) is concerned with adding an ethical dimension to machines. Unlike computer ethics -- which has traditionally focused on ethical issues surrounding humans' use of machines -- machine ethics is concerned with ensuring that the behavior of machines toward human users, and perhaps other machines as well, is ethically acceptable. In this article we discuss the importance of machine ethics, the need for machines that represent ethical principles explicitly, and the challenges facing those working on machine ethics. We also give an example of current research in the field that shows that it is possible, at least in a limited domain, for a machine to abstract an ethical principle from examples of correct ethical judgments and use that principle to guide its own behavior.",332.0
The Transforming Power of Complementary Assets,435b3d21e746940ef84ce3dd655d189d81d46a96,"[{'authorId': '47954701', 'name': 'A. Hughes'}, {'authorId': '153704801', 'name': 'M. Morton'}]",2006.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"The article discusses changes in information technology and the ability of businesses to exploit investments in complementary assets. A case study of Schneider National Inc. is presented in order to demonstrate how transformations in information technology interact with strategy, structure, and people. Schneider's implementation of satellite communications and tracking systems is reviewed. The impact which the technological advances and transformations had on drivers, customer service representatives, service team leaders, and dispatchers is examined. Schneider Dedicated and Schneider Logistics, two business units of Schneider National, are evaluated. INSETS: About the Research;The Nine Pieces of Schneider National",47.0
Human-Automation Interaction,ff6cb1d3fdad62898514f0ad49a2ae3f84855bd5,"[{'authorId': '1712194', 'name': 'T. Sheridan'}, {'authorId': '3264674', 'name': 'R. Parasuraman'}]",2005.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"Automation does not mean humans are replaced; quite the opposite. Increasingly, humans are asked to interact with automation in complex and typically large-scale systems, including aircraft and air traffic control, nuclear power, manufacturing plants, military systems, homes, and hospitals. This is not an easy or error-free task for either the system designer or the human operator/automation supervisor, especially as computer technology becomes ever more sophisticated. This review outlines recent research and challenges in the area, including taxonomies and qualitative models of human-automation interaction; descriptions of automation-related accidents and studies of adaptive automation; and social, political, and ethical issues. Keywords: Driver distraction; Language: en",255.0
Risky creatures: Institutional species boundary change in biotechnology regulation,30a90ed2caba27d391ce7293c22889bf73553a85,"[{'authorId': '145481942', 'name': 'Nick M. Brown'}, {'authorId': '2672971', 'name': 'M. Michael'}]",2004.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"Novel biotechnologies present acute difficulties to regulation for the very reason that they traverse the boundaries between existing regulatory authorities, their terms of reference, their disciplinary capabilities, and so on. To this extent, they are hybrid phenomena, difficult to categorise and a source of acute uncertainty. Moreover, most biotechnologies confuse and trouble the distinctions between different species. These species disruptions are particularly evident in transpecies transplantation, an illustrative case that we address in detail throughout this paper. From time to time, questions about the safety of such hybrid risks have inspired the establishment of equally hybrid regulatory agencies. Such 'institutional hybrids' are 'risky creatures' and are therefore important objects of social science critique if we are to understand whether or not they have successfully traversed otherwise dangerous gaps in risk regulation. To illustrate these processes of institutional species boundary change, we draw on two examples in the regulation of xenotransplantation. The first examines the establishment of a specific institutional hybrid body for regulating transpecies transplantation, and especially, the scope and limitations of its hybridity. In this way it becomes possible to see how biotechnology is carved up and distributed between various arms of regulation. Second, we document important changes in the definition of xenotransplantation, illustrating the acute difficulties faced by hybrid regulatory institutions in determining what developments they should and should not be regulating.",38.0
Inclusive Design: Design for the Whole Population,826cdab0f20a0415fd8e272a933d8fc4e54f438b,"[{'authorId': '48480791', 'name': 'J. Clarkson'}, {'authorId': '143983877', 'name': 'R. Coleman'}, {'authorId': '1783077', 'name': 'S. Keates'}, {'authorId': '152488966', 'name': 'C. Lebbon'}]",2003.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"Inclusive Design: What's in It for Me? presents a comprehensive review of current practice in inclusive design. With emphasis on new ideas for improvement and arguments for wider implementation in future, a unique combination of leading opinions on inclusive design from both industry and academia are offered. The theme throughout encourages a positive view of inclusive design as a good and profitable process and to produce a change to more effective approaches to ""design for all"". Inclusive Design is composed of two parts with a common chapter structure so that the business and design arguments in favour of inclusive design can be easily compared and assimilated: The Business Case presents the industrial and management benefits of inclusive design. It concentrates on demographic, legal and ethical reasons for all businesses being better off taking inclusivity into account in the design of their products or services. Case histories demonstrating the commercial success of inclusive design are drawn from the experiences of companies such as Tesco, Fiat and The Royal Mail. The Designers' Case focuses on the factors a designer needs to take into account when dealing with inclusivity. ""Who is going to use my design?"" ""What do they need from my design?"" ""How do I take any medical needs into account?"" ""Just how ""inclusive"" is my design?"" are all questions answered in this section which presents the necessary tools for effective inclusive design. This part of the book aims to convince a designer that inclusive design is a realistic goal. Inclusive Design will appeal to designers, researchers and students and to managers making decisions about the research and design strategies of their companies.",514.0
The Global Information Technology Report,4a261db6d65334638b31890a1f2285f9907bc465,"[{'authorId': '97794040', 'name': 'B. Lanvin'}, {'authorId': '144806650', 'name': 'S. Dutta'}, {'authorId': '98030932', 'name': 'Fiona Paua'}]",2003.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"From the Publisher: 
""That The Global Information Technology Report 2001-2002 received such a notable positive response from a broad range of stakeholders underscores the growing relevance of information technology (IT) in national economies and the continuing need for an assessment of the readiness of countries to participate in the Networked World. Recognizing the relevance of an the rapid changes in information technology, this report is an update to the 2001-2002 Report, which is the first and most comprehensive international assessment of the readiness of countries to capture the benefits of participating in the Networked World. With regional analyses and specific country case studies, essays on a variety of IT-related subjects, detailed country profiles, and country rankings comparing the global IT experience of different nations, this report remains the most authoritative documentation to date of how Its are being used around the world.""",908.0
Artificial intelligence-based sampling planning system for dynamic manufacturing process,f5633b554bb1cefc7f5d5485c7c4eb865d5f10ae,"[{'authorId': '2108620265', 'name': 'J. Lee'}]",2002.0,Expert systems with applications,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,29.0
Robot: The Future of Flesh and Machines,a048db83520c142b3d70e4eb1de3a6864ecc6dcf,"[{'authorId': '72419159', 'name': 'R. Brooks'}]",2002.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"The world is changing at an ever-increasing pace. Most of us have accepted the idea that the World Wide Web is now an important part of life and here to stay. Where will technology take us next? What is on the horizon? This text speaks to those who think about humanity's relationship to technology and our place in the world, discussing what will happen when intelligent robots become too smart.",76.0
A Case Analysis,48ece3c57deef174e224224fb9003f039410a542,"[{'authorId': '114829093', 'name': 'F. Miller'}, {'authorId': '3587209', 'name': 'A. Shorr'}]",2002.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"2002;121;1337-1342 Chest Franklin G. Miller and Andrew F. Shorr : A Case Analysis * Clinical Trials Ethical Assessment of Industry-Sponsored http://chestjournal.org/cgi/content/abstract/121/4/1337 services can be found online on the World Wide Web at: The online version of this article, along with updated information and ). ISSN: 0012-3692. http://www.chestjournal.org/misc/reprints.shtml ( copyright holder reproduced or distributed without the prior written permission of the 60062. All rights reserved. No part of this article or PDF may be American College of Chest Physicians, 3300 Dundee Road, Northbrook IL It has been published monthly since 1935. Copyright 2007 by the CHEST is the official journal of the American College of Chest Physicians.",23.0
Birds of a Feather: Homophily in Social Networks,228bafce55e6f1cbe2c1df75b1949a1fb9c93eb3,"[{'authorId': '70088928', 'name': 'M. McPherson'}, {'authorId': '1401879614', 'name': 'L. Smith-Lovin'}, {'authorId': '2115138711', 'name': 'J. Cook'}]",2001.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"Similarity breeds connection. This principle—the homophily principle—structures network ties of every type, including marriage, friendship, work, advice, support, information transfer, exchange, comembership, and other types of relationship. The result is that people's personal networks are homogeneous with regard to many sociodemographic, behavioral, and intrapersonal characteristics. Homophily limits people's social worlds in a way that has powerful implications for the information they receive, the attitudes they form, and the interactions they experience. Homophily in race and ethnicity creates the strongest divides in our personal environments, with age, religion, education, occupation, and gender following in roughly that order. Geographic propinquity, families, organizations, and isomorphic positions in social systems all create contexts in which homophilous relations form. Ties between nonsimilar individuals also dissolve at a higher rate, which sets the stage for the formation of niches (localize...",15659.0
ICTs and the possibilities for leapfrogging by developing countries,8a9faa1263afa5ecab9d0bb9de54df2925e29a51,"[{'authorId': '144919386', 'name': 'W. Steinmueller'}]",2001.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"T last half of the twentieth century was marked by steady advance in the ubiquity of information and communication technologies (ICTs) throughout the industrialized world. The emergence of a significant “digital divide” between industrialized and developing countries is reproducing existing patterns of inequality with regard to these new technologies (Castells, 1996). During this same period, however, substantial achievements in the few countries which succeeded in narrowing the economic divide separating them from the industrialized world often involved the export-oriented production of ICTs (Kim, 1997; Hobday, 1995b; Amsden, 1989) or, less commonly, their effective use in improving productivity or creating new markets. The continuing rapid decline in the prices of these technologies and the accompaniment of these price reductions with a growing range of applications suggest that they offer further opportunities for economic growth. In particular, ICTs are unique in a number of ways compared with the leading industries of the past that were responsible for industrial growth and development, such as steel, chemicals, and machinery. In many applications , and in some types of production, the conditions of entry for using and, in some cases, for producing ICTs do not require massive investment in fixed plant capacity or infrastructure or in the accumulation of experience. Moreover, ICT applications often appear to be complementary to efforts to improve the quality, speed and flexibility of production, offering a compensating advantage against existing shortcomings in production capacities (Lal, 2000). Because virtually all of the components and many of the systems embodying these technologies are internationally available from highly competitive",313.0
Famous First Bubbles: The Fundamentals of Early Manias,e0b3d5a68a4fdeda8dc6ca4a6931ba6bfe9e2007,"[{'authorId': '47224317', 'name': 'I. Duffy'}]",2000.0,Journal Economic History,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"There is no hoarier chestnut of financial history than the Tulip Mania, and hard on its heels are the stories of the infamous South Sea Bubble and the contemporaneous financial scheme of John Law. The first of these is part of the historical lexicon even of captains of industry and journalists, and has endured endless repetition, the tale usually based on Charles Mackay's highly embellished 1841 retelling (Memoirs of Extraordinary Popular Delusions. London: R. Bentley). The speculative bubbles of 1720 are more complex and better researched phenomena, which accounts, perhaps, for their less frequent invocations by laymen. Scholars, on the other hand, do not hesitate to misinterpret these events.",301.0
Artificial Intelligence: A Guide to Intelligent Systems,7c4a9643c701c0c91ea50fd587038f79187a0a5e,"[{'authorId': '144572492', 'name': 'M. Negnevitsky'}]",2001.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"From the Publisher: 
Virtually all the literature on artificial intelligence is expressed in the jargon of commuter science, crowded with complex matrix algebra and differential equations. Unlike many other books on computer intelligence, this one demonstrates that most ideas behind intelligent systems are simple and straightforward. The book has evolved from lectures given to students with little knowledge of calculus, and the reader needs no prerequisites associated with knowledge of any programming language. The methods used in the book have been extensively tested through several courses given by the author. 
 
The book provides an introduction to the field of computer intelligence, covering 
 
rule-based expert systems, 
fuzzy expert systems, 
frame-based expert systems, 
artificail neural networks, 
evolutionary computation, 
hybrid intelligent systems, 
knowledge engineering, 
data mining. 
 
 
In a university setting the book can be used as an introductory course within computer science, information systems or engineering departments. The book is also suitable as a self-study guide for non-computer science professionals, giving access to the state of the art in knowledge-based systems and computational intelligence. Everyone who faces challenging problems and cannot solve them using traditional approaches can benefit",2395.0
"Beyond Computation: Information Technology, Organizational Transformation and Business Performance",9ff69c38a1b618f5806d08cfba9a4a0f8c5d5fb4,"[{'authorId': '2841157', 'name': 'E. Brynjolfsson'}, {'authorId': '1746620', 'name': 'L. Hitt'}]",2000.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"To understand the economic value of computers, one must broaden the traditional definition of both the technology and its effects. Case studies and firm-level econometric evidence suggest that: 1) organizational ""investments"" have a large influence on the value of IT investments; and 2) the benefits of IT investment are often intangible and disproportionately difficult to measure. Our analysis suggests that the link between IT and increased productivity emerged well before the recent surge in the aggregate productivity statistics and that the current macroeconomic productivity revival may in part reflect the contributions of intangible capital accumulated in the past.",3070.0
Designing automation for human use: empirical studies and quantitative models,035e62d7c1954eb7e893d12a7caa3aa0785f9018,"[{'authorId': '3264674', 'name': 'R. Parasuraman'}]",2000.0,Ergonomics,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"An emerging knowledge base of human performance research can provide guidelines for designing automation that can be used effectively by human operators of complex systems. Which functions should be automated and to what extent in a given system? A model for types and levels of automation that provides a framework and an objective basis for making such choices is described. The human performance consequences of particular types and levels of automation constitute primary evaluative criteria for automation design when using the model. Four human performance areas are considered—mental workload, situation awareness, complacency and skill degradation. Secondary evaluative criteria include such factors as automation reliability, the risks of decision/action consequences and the ease of systems integration. In addition to this qualitative approach, quantitative models can inform design. Several computational and formal models of human interaction with automation that have been proposed by various researchers are reviewed. An important future research need is the integration of qualitative and quantitative approaches. Application of these models provides an objective basis for designing automation for effective human use.",271.0
A model for types and levels of human interaction with automation,14ae6f2231e09e226b99002aa04b5c70f3c59f2b,"[{'authorId': '3264674', 'name': 'R. Parasuraman'}, {'authorId': '1712194', 'name': 'T. Sheridan'}, {'authorId': '1772836', 'name': 'C. Wickens'}]",2000.0,IEEE Trans. Syst. Man Cybern. Part A,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"Technical developments in computer hardware and software now make it possible to introduce automation into virtually all aspects of human-machine systems. Given these technical capabilities, which system functions should be automated and to what extent? We outline a model for types and levels of automation that provides a framework and an objective basis for making such choices. Appropriate selection is important because automation does not merely supplant but changes human activity and can impose new coordination demands on the human operator. We propose that automation can be applied to four broad classes of functions: 1) information acquisition; 2) information analysis; 3) decision and action selection; and 4) action implementation. Within each of these types, automation can be applied across a continuum of levels from low to high, i.e., from fully manual to fully automatic. A particular system can involve automation of all four types at different levels. The human performance consequences of particular types and levels of automation constitute primary evaluative criteria for automation design using our model. Secondary evaluative criteria include automation reliability and the costs of decision/action consequences, among others. Examples of recommended types and levels of automation are provided to illustrate the application of the model to automation design.",3293.0
An analysis of expert systems for business decision making at different levels and in different roles,dfbc44a79581d295c0fcac46dfcd6b11d3c2c59d,"[{'authorId': '143698898', 'name': 'J. Edwards'}, {'authorId': '144521858', 'name': 'Y. Duan'}, {'authorId': '145455219', 'name': 'P. Robins'}]",2000.0,European Journal of Information Systems,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,33.0
Exploring Expertise: Issues and Perspectives,1754d4be55cac6238ffd40b69749fd5da809f99a,"[{'authorId': '100711040', 'name': 'W. Faulkner'}, {'authorId': '145217375', 'name': 'J. Fleck'}, {'authorId': '2116648759', 'name': 'Robin Williams'}]",1998.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,66.0
The Certainty Trough,7b48b20c2bba53f86ebf52eeb390c571e758c631,"[{'authorId': '123283352', 'name': 'D. MacKenzie'}]",1998.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,63.0
"Humans and Automation: Use, Misuse, Disuse, Abuse",1cf890ce8391ca958fbc173533d575efcb51f70a,"[{'authorId': '3264674', 'name': 'R. Parasuraman'}, {'authorId': '50615483', 'name': 'V. Riley'}]",1997.0,Hum. Factors,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"This paper addresses theoretical, empirical, and analytical studies pertaining to human use, misuse, disuse, and abuse of automation technology. Use refers to the voluntary activation or disengagement of automation by human operators. Trust, mental workload, and risk can influence automation use, but interactions between factors and large individual differences make prediction of automation use difficult. Misuse refers to over reliance on automation, which can result in failures of monitoring or decision biases. Factors affecting the monitoring of automation include workload, automation reliability and consistency, and the saliency of automation state indicators. Disuse, or the neglect or underutilization of automation, is commonly caused by alarms that activate falsely. This often occurs because the base rate of the condition to be detected is not considered in setting the trade-off between false alarms and omissions. Automation abuse, or the automation of functions by designers and implementation by managers without due regard for the consequences for human performance, tends to define the operator's roles as by-products of the automation. Automation abuse can also promote misuse and disuse of automation by human operators. Understanding the factors associated with each of these aspects of human use of automation can lead to improved system design, effective training methods, and judicious policies and procedures involving automation use.",3241.0
Accounting for Tastes,b403a723c3d4007d305233bb18bea941a6e2a98d,"[{'authorId': '143628419', 'name': 'G. Becker'}]",1997.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"Part I: Personal Capital *1. Preferences and Values *De Gustibus Non Est Disputandum * A Theory of Rational Addiction * Rational Addiction and the Effect of Price on Consumption * An Empirical Analysis of Cigarette Addiction * Habits, Addictions, and Traditions Part 2: Social Capital * The Economic Way of Looking at Life * A Theory of Social Interactions * A Note on Restaurant Pricing and Other Examples of Social Influences on Price * A Simple Theory of Advertising as a Good or Bad * Norms and the Formation of Preferences * Spouses and Beggars: Love and Sympathy * Acknowledgments * References * Index",799.0
Human Decision Makers and Automated Decision Aids: Made for Each Other?,ffb65e76ac46fd42d595ed9272296f0cbe8ca7aa,"[{'authorId': '2249256', 'name': 'K. Mosier'}, {'authorId': '2622061', 'name': 'L. Skitka'}]",1996.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,286.0
Information technology: Threats and opportunities for small and medium-sized enterprises,14d9c5f63afe7c75d625f4977d6c0e48938cc02a,"[{'authorId': '2793009', 'name': 'Sam Blili'}, {'authorId': '144687533', 'name': 'L. Raymond'}]",1993.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,448.0
Intelligent manufacturing,3cc2f98b873d141b6b977ad30db8493065db6ac2,"[{'authorId': '2072530251', 'name': 'H. Hayashi'}]",1993.0,IEEE spectrum,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"A description is given of a feasibility study launched by the international manufacturing community to establish a framework for an international, collaborative program focusing on such developments as an intelligent manufacturing system (IMS) or next-generation advanced manufacturing technologies. A second goal is to determine whether an international collaborative program in this area can be created and structured equitably and beneficially.<<ETX>>",50.0
A Keyword Classification Scheme for IS Research Literature: An Update,daf35d6baef2f4703038aa5ce84cc1956c955547,"[{'authorId': '2355048', 'name': 'H. Barki'}, {'authorId': '3344735', 'name': 'S. Rivard'}, {'authorId': '37482212', 'name': 'Jean Talbot'}]",1993.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"In June 1988 MIS Quarterly published a classification scheme of IS keywords. The development of this scheme was intended to provide a description of the discipline, introduce a common language, and enable research of the field's development. The scheme has been recently updated in order to incorporate the new research topics and methods, hence better reflecting the evolution of the IS discipline.",258.0
The American Review of Public Administration,ca973ada3ab1a1eed738b4c5f0c2017ae72b7dd4,"[{'authorId': '67109084', 'name': 'Margaret Ferley'}]",1993.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"To overcome agency problems, public sector reforms started to introduce businesslike incentive structures to motivate public officials. By neglecting internal behavioral incentives, however, these reforms often do not reach their stated goals. This research analyzes the governance structure of Benedictine monasteries to gain new insights into solving agency problems in public institutions. A comparison is useful because members of both organizational forms, public organizations and monasteries, see themselves as responsible participants in their community and claim to serve the public good. This research studies monastic governance from an economic perspective. Benedictine monasteries in Baden-Württemberg, Bavaria, and German-speaking Switzerland have an average lifetime of almost 500 years, and only a quarter of them broke up because of agency problems. The authors argue that they were able to survive for centuries because of an appropriate governance structure, relying strongly on the intrinsic motivation of the members and internal control mechanisms. This governance approach differs in several aspects from current public sector reforms.",995.0
Ai: The Tumultuous History Of The Search For Artificial Intelligence,58bc314bac09e6fcba9b244629b54513a08a8bf9,"[{'authorId': '2905778', 'name': 'D. Crevier'}]",1993.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"In the summer of 1956, a group of young scientists sat down to consider the astounding proposition that ""every aspect of learning or any other feature of intelligence can, in principle, be so precisely described that a machine can be made to simulate it."" Armed with their own enthusiasm, the excitement of the idea itself, and lots of government money, they predicted that the whole range of human intelligence would be programmable within their own lifetimes. Nearly half a century later, the field has grown tenfold - with mixed results. Based on extensive interviews with the major players in the history of artificial intelligence, including Marvin Minsky, Herbert Simon, Alan Newell, Raj Reddy and Patrick Winston, this book chronicles their successes, from robotics to world-class chess playing and, equally, their failures. With anecdotes about the founders and leaders and their celebrated feuds and intellectual gamesmanship, the book also discusses the next necessary breakthrough - teaching computers ""common sense"".",121.0
Artificial intelligence in flexible manufacturing systems,8e0369f1e72cd744c4e718a79089d8fe0faa1215,"[{'authorId': '152809911', 'name': 'P. Jain'}, {'authorId': '1966052', 'name': 'C. Mosier'}]",1992.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"Abstract Recently, extensive efforts have been focused on the application of artificial intelligence (AI) techniques to problems associated with the design and control of flexible manufacturing systems (FMSs). This paper surveys the key functional areas associated with FMS and discusses the application of AI techniques to their operation. The advantages of adopting knowledge-based approaches to the design and operational control of FMS are also discussed.",11.0
Computerization and Social Transformations,0ea156c118e41bfa51803ac74b757e9f58e59a6c,"[{'authorId': '1688343', 'name': 'R. Kling'}]",1991.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"This article examines the relationship between the use of computer-based systems and transformations in parts of the social order. Answers to this question rest heavily on the way computer-based systems are consumed -not just produced or dissemtnated. The discourse about computerezation advanced in many professional magazines and the mass media is saturated with talk about ""revolution, "" and yet substantial social changes are often difficult to cdentcfy in carefully designed empirical studies. The article examines qualitative case studies of computerization in welfare agencies, urban planning, accounting, marketing, and manufacturing to examine the ways that computerization alters social life in varced ways: sometemes restructuring relationships and in other cases reinforcing existing social relationships. The article also examines some of the theoret ical issues in studies of computerization, such as drawing boundaries. It concludes with some observations about the sociology of computer sctence as an academic discipline.",196.0
Management Information Systems: Managing the Digital Firm,cda97601b1365aaf84e75afa734b25ced423d830,"[{'authorId': '2351117', 'name': 'K. Laudon'}, {'authorId': '52477498', 'name': 'Jane P. Laudon'}]",2010.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"From the Publisher: 
It's not business as usual anymore . . . 
Why? Because digital firms are emerging. Businesses can no longer survive today without becoming digital. That's why you need the Laudons' Management Information Systems: Managing the Digital Firm, seventh edition, an indispensable text for anyone who wants to succeed in the e-business world. 
What is the Digital Firm? 
It's a firm where any piece of information required for business decisions is available at any time and anywhere in the organization. It's a firm where all the significant business relationships are digitally enabled. The Laudons will show you how to organize, manage, communicate, and lead as more firms go digital in the coming years. 
THE LAUDON ADVANTAGE 
The Laudons' Management Information Systems is the world's top-selling MIS text. Here you'll find opportunities to build the skills and acquire the knowledge you'll need to use information systems successfully in your business career. 
Leading-Edge 
If you want to know how to take maximum advantage of the latest technology and business trends, the Laudons are the place to start. Along with MIS foundation concepts, you'll find the most up-to-the-minute coverage of leading-edge topics, such as: digital firms, e-commerce, e-business, the wireless Web, enterprise systems, customer relationship management, supply chain management, application service providers, on-line storage services, optical networks, broadband access, peer-to-peer computing, business-to-business exchanges, scalability, and high-availability computing. 
The Laudon Management-Organisation-TechnologyFramework 
You'll need a framework to help you understand and analyze business problems and information systems as you move into the business world. The Laudons' Management-Organization-Technology framework is a well respected methodology in the field of Management Information Systems. You'll see it emphasized in cases, in-text explanations, and projects throughout the text. 
 
The Laudons' Management Information Systems, seventh edition, is a text that not only offers you the most current and well-respected insights into the MIS field but a companion you'll want to use over and over again in your current courses and future career.",2252.0
Practical engineering of knowledge-based systems,4481f6535557c08840103ef3d484b8a1218f5543,"[{'authorId': '2121933702', 'name': 'J. Bader'}, {'authorId': '143698898', 'name': 'J. Edwards'}, {'authorId': '1411465984', 'name': 'C. Harris-Jones'}, {'authorId': '96197432', 'name': 'D. Hannaford'}]",1988.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,35.0
Artificial Intelligence And Operations Research In Flexible Manufacturing Systems,66a03d9d86ec5f1a180c9b04037d4fb0105832c7,"[{'authorId': '143692578', 'name': 'A. Kusiak'}]",1987.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,In this paper an artificial intelligence and operations research approach to modelling flexible manufacturing systems are presented. They are demonstrated on two sample problems: the machine layout...,76.0
The industrial revolution and the regional geography of England,d31f4e3204f591e6ffd490d12ea44ecde6d24c84,"[{'authorId': '144729610', 'name': 'J. Langton'}]",1984.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"The purpose of this paper is to challenge the widespread belief, shared by geographers and historians, that industrial- ization destroyed regional distinctiveness in England as elsewhere. After an outline of the complex regional structure of pre-industrial England, it is demonstrated that all kinds of social movements and political pressure groups were regionally fragmented in the late eighteenth and early nineteenth centuries, that distinctive social and cultural traits began to be recognized as characteristic of particular regions, and that people began consciously to identify themselves with the regions in which they lived. This develop- ment of regionalism in England was dependent upon the essentially regional structure of the early industrial economy which, in turn, was related to the importance of waterway transport and the sparseness and fragmentation of the canal network.",149.0
Extensional Versus Intuitive Reasoning: The Conjunction Fallacy in Probability Judgment,b455189ca678237659f001316b2425667f04c286,"[{'authorId': '2064181', 'name': 'A. Tversky'}]",1983.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,3422.0
Development and Establishment in Artificial Intelligence,cf2ca6752da45b7beccc2af586f32afeb2d17113,"[{'authorId': '145217375', 'name': 'J. Fleck'}]",1982.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,47.0
The social control of technology,176a861bbfcfd1eecf1d4e4439c559eb5f697975,"[{'authorId': '5705745', 'name': 'D. Collingridge'}]",1980.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,529.0
Prospect theory: analysis of decision under risk,e01a13bd46de69254925a674b641145b77417e92,"[{'authorId': '3683465', 'name': 'D. Kahneman'}, {'authorId': '2064181', 'name': 'A. Tversky'}]",1979.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"Analysis of decision making under risk has been dominated by expected utility theory, which generally accounts for people's actions. Presents a critique of expected utility theory as a descriptive model of decision making under risk, and argues that common forms of utility theory are not adequate, and proposes an alternative theory of choice under risk called prospect theory. In expected utility theory, utilities of outcomes are weighted by their probabilities. Considers results of responses to various hypothetical decision situations under risk and shows results that violate the tenets of expected utility theory. People overweight outcomes considered certain, relative to outcomes that are merely probable, a situation called the ""certainty effect."" This effect contributes to risk aversion in choices involving sure gains, and to risk seeking in choices involving sure losses. In choices where gains are replaced by losses, the pattern is called the ""reflection effect."" People discard components shared by all prospects under consideration, a tendency called the ""isolation effect."" Also shows that in choice situations, preferences may be altered by different representations of probabilities. Develops an alternative theory of individual decision making under risk, called prospect theory, developed for simple prospects with monetary outcomes and stated probabilities, in which value is given to gains and losses (i.e., changes in wealth or welfare) rather than to final assets, and probabilities are replaced by decision weights. The theory has two phases. The editing phase organizes and reformulates the options to simplify later evaluation and choice. The edited prospects are evaluated and the highest value prospect chosen. Discusses and models this theory, and offers directions for extending prospect theory are offered. (TNM)",25631.0
The Joyless Economy: An Inquiry into Human Satisfaction and Consumer Dissatisfaction,c2e46ab3958332c250fa3d84b425590f456606ff,"[{'authorId': '117527001', 'name': 'T. Scitovsky'}]",1976.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"When this classic work was first published in 1976, its central tenet--more is not necessarily better--placed it in direct conflict with mainstream thought in economics. Within a few years, however, this apparently paradoxical claim was gaining wide acceptance. Scitovsky's ground-breaking book was the first to apply theories of behaviorist psychology to questions of consumer behavior and to do so in accessible, non-technical language. Setting out to analyze the failures of our consumerist lifestyle, Scitovsky concluded that people's need for stimulation is so vital that it can lead to violence if not satisfied by novelty--whether in challenging work, art, fashion, gadgets, late-model cars, or scandal. Though much of the book stands as a record of American post-war prosperity and its problems, the revised edition also takes into account recent social and economic changes. A new preface and a foreword by economist Robert Frank cover some of these issues. Two revised chapters discuss the assimilation of counter-cultural ideas throughout American society, especially ideas concerning the quality of life. Scitovsky draws fascinating connections between the new elite of college-educated consumers and the emergence of a growing underclass plagued by drugs and violence, perceptively tracing the reactions of these disparate groups to the problems of leisure and boredom. In the wake of the the so-called ""decade of greed"" and amidst calls for a ""kinder, gentler"" society, The Joyless Economy seems more timely than ever.",650.0
I: The Theory of Moral Sentiments,00b61d4fdca45a33c75bc375ffb4430190015a19,"[{'authorId': '49609369', 'name': 'D. Raphael'}, {'authorId': '31504936', 'name': 'A. Macfie'}]",1976.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"A scholarly edition of a work by Adam Smith. The edition presents an authoritative text, together with an introduction, commentary notes, and scholarly apparatus.",3969.0
The Economic Possibilities of Our Grandchildren,4038bf2bd145b2e65e77b36a4fa29390efa5e842,"[{'authorId': '16992205', 'name': 'J. Keynes'}]",1987.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,,947.0
Economic problems of socialism in the U.S.S.R,4e5c77bff6c25a2e213ed857d6b4a0e9d575e4cd,"[{'authorId': '69917687', 'name': 'I. Stalin'}]",1951.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"Economic Inequality Definition Investopedia Market abolitionist socialism replaces factor markets with direct calculation as the means to coordinate the activities of the various socially-owned economic enterprises that make up the economy. [4] [5] [6] More recent approaches to socialist planning and allocation have come from some economists and computer scientists proposing planning Economy of the Soviet Union Wikipedia May 17, 2021 · The two biggest experiments in communism in the 20th century were the Union of Soviet Socialist Republics (U.S.S.R.) and the People's Republic of ⋯ Soviet Union | History, Leaders, Flag, Map, & Anthem | Britannica This is a comprehensive assessment of the problems and merits of socialism that would be useful to the current generation that sees it as a utopian alternative to capitalism. Most current treatises focus on examples of socialism’s failures. Von Mises explains why all the various forms of socialism fail. Planned economy Wikipedia Nov 19, 2021 · Economic inequality refers to the disparities in income and wealth among individuals in a society. a weekly newsletter for the U.S. tea trade. In 2013, she was hired as senior editor to assist Socialism: An Economic and Sociological Analysis 6th Edition The government is fully controlled by the ruler. In contrast with a totalitarian state, some social and economic institutions exist outside of government control. Robert Altemeyer's early work on the ""authoritarian personality"" identified three particularly important aspects of the authoritarian personality: conventionalism Socialist Economies: How China, Cuba, and North Korea Work Feb 15, 2022 · Today, the term democratic socialism is generally used in the U.S. to distinguish this mediated version of socialism from authoritarian governments such as the U.S.S.R. and China. As the winner-takes-all voting system in the U.S. has produced a two-party system for most of American history, the rise of democratic socialism has led to a more Studio ATAO | Understanding Socialism in the U.S. The economy of the Soviet Union was based on state ownership of the means of production, collective farming, and industrial manufacturing.An administrative-command system managed a distinctive form of central planning.The Soviet economy was characterized by state control of investment, a dependence on natural resources, shortages (at the end of its existence), public ⋯ Political Science Glossary. Cruzio Internet Soviet Union, in full Union of Soviet Socialist Republics (U.S.S.R.), Russian Soyuz Sovetskikh Sotsialisticheskikh Respublik or Sovetsky Soyuz, former northern Eurasian empire (1917/22–1991) stretching from the Baltic and Black seas to the Pacific Ocean and, in its final years, consisting of 15 Soviet Socialist Republics (S.S.R.’s): Armenia, Azerbaijan, Belorussia ⋯",78.0
Expectation in Economics,37cd66c063a5e6f1f9dfd9e4a08dd453ab641969,"[{'authorId': '50530458', 'name': 'G. Shackle'}]",1951.0,,"['Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy']",1,"First published in 1952, as the second edition of a 1949 original, this book contains a discussion of the role of expectation in relation to the workings of the economy. For the purposes of the discussion, expectation is defined as the act of creating imaginary situations, of associating them with named future dates, and of assigning to each of the hypotheses thus formed a place on a scale measuring the degree of belief that a specified course of action will make this hypothesis come true. The text also contains appendices and a detailed index. This book will be of value to anyone with an interest in economic theory and methods of dealing with economic uncertainty.",289.0
